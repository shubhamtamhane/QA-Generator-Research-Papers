{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1d1b0e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing libraries\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0cdf75f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up model and api key for OpenAI \n",
    "\n",
    "import openai\n",
    "\n",
    "model_id = \"gpt-3.5-turbo\"\n",
    "openai.api_key = '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'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a5638bf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. What is the focus of the research paper \"Attention is all You Need\"?\n",
      "\n",
      "Answer 1: The focus of the research paper \"Attention is all You Need\" is on a new neural network architecture called the Transformer, which relies solely on attention mechanisms to process sequential data, such as natural language.\n",
      "\n",
      "Answer 2: The research paper \"Attention is all You Need\" concentrates on how the Transformer neural network architecture can deviate from traditional sequential processing techniques by employing only attention mechanisms to analyze sequential data.\n",
      "\n",
      "2. What is the significance of the research paper \"Attention is all You Need\" in the field of artificial intelligence?\n",
      "\n",
      "Answer 1: The research paper \"Attention is all You Need\" represents a pioneering step toward attaining higher levels of performance in automated machine translation, opening up new areas of research, and experimentation in the field of neural network design.\n",
      "\n",
      "Answer 2: The research paper \"Attention is all You Need\" presents a major advancement in the science of artificial intelligence by demonstrating that the Transformer neural network architecture is a viable replacement to the traditional RNN or LSTMs for sequence processing, allowing for more efficient and effective natural language models.\n",
      "Question : for the research paper \"attention is all you need\" generate 2 question and answers each for abstract only\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# We call this function and pass the new question and the last messages\n",
    "# NewQuestion is the brand new question we want to answer\n",
    "# lastmessage is the past conversatio that is passed along for context to have a conversation\n",
    "# API does not rememeber the past conversation so we have to do this so it can use that context\n",
    "def GetMessageMemory(NewQuestion,lastmessage):\n",
    "  # Append the new question to the last message\n",
    "    lastmessage.append({\"role\": \"user\", \"content\": NewQuestion})\n",
    "    # Make a call to ChatGPT API\n",
    "    msgcompletion = openai.ChatCompletion.create(\n",
    "    model=model_id,\n",
    "    messages=lastmessage\n",
    "  )\n",
    "    msgresponse = msgcompletion.choices[0].message.content\n",
    "    print(msgresponse)\n",
    "    print(\"Question : \" + NewQuestion)\n",
    "    return msgresponse\n",
    "\n",
    "\n",
    "# Sample example\n",
    "ss=GetMessageMemory('for the research paper \"attention is all you need\" generate 2 question and answers each for abstract only',[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fa21a1f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the data file\n",
    "\n",
    "df = pd.read_csv('section_merged_clean.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9e0fa2a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Storing section information in a Pandas series\n",
    "\n",
    "section_data = df['section_data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cf9ba336",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        We thank Nobuhiro Kaji for providing the ACP C...\n",
       "1        怒る (get angry), 悲しい (be sad), 寂しい (be lonely),...\n",
       "2        喜ぶ (rejoice), 嬉しい (be glad), 楽しい (be pleasant)...\n",
       "3        We used a Japanese BERT model pretrained with ...\n",
       "4        The dimension of the embedding layer was 256. ...\n",
       "                               ...                        \n",
       "12404    In the KB completion or link prediction task B...\n",
       "12405    Large-scale knowledge bases (KBs), such as YAG...\n",
       "12406    Table 3 compares the experimental results of o...\n",
       "12407    A knowledge base $\\mathcal {G}$ is a collectio...\n",
       "12408    We use the common Bernoulli trick BIBREF14 , B...\n",
       "Name: section_data, Length: 12409, dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['section_data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d6ab6410",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a empty dataframe where rows will be added\n",
    "\n",
    "qa_df = pd.DataFrame(columns=['context', 'question', 'answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ff31cfc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating list from 0 to 5000 with steps of 50\n",
    "\n",
    "a = list(range(0, 5000, 50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "371fa0a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Destination to store files\n",
    "\n",
    "path = r'C:\\Users\\stamhane\\Desktop\\spring2023\\nlp\\project\\dataset_files\\''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bfe23860",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is the Stacked DeBERT and how does it differ from other machine learning methods?\n",
      "Stacked DeBERT is a deep neural network that is specifically designed to be robust to noisy text in the form of sentences with missing and/or incorrect words. It differs from other machine learning methods, such as BERT and Semantic Hashing with neural classifier, by improving the accuracy performance through the implementation of novel denoising transformers which are able to reconstruct hidden embeddings from their respective incomplete hidden embeddings. Additionally, the model is able to extract richer data representations from the input data regardless of the completeness of the sentence, thus improving accuracy even in clean data.\n",
      "Question : for the text In this work, we proposed a novel deep neural network, robust to noisy text in the form of sentences with missing and/or incorrect words, called Stacked DeBERT. The idea was to improve the accuracy performance by improving the representation ability of the model with the implementation of novel denoising transformers. More specifically, our model was able to reconstruct hidden embeddings from their respective incomplete hidden embeddings. Stacked DeBERT was compared against three NLU service platforms and two other machine learning methods, namely BERT and Semantic Hashing with neural classifier. Our model showed better performance when evaluated on F1 scores in both Twitter sentiment and intent text with STT error classification tasks. The per-class F1 score was also evaluated in the form of normalized confusion matrices, showing that our model was able to improve the overall performance by better balancing the accuracy of each class, trading-off small decreases in high achieving class for significant improvements in lower performing ones. In the Chatbot dataset, accuracy improvement was achieved even without trade-off, with the highest achieving classes maintaining their accuracy while the lower achieving class saw improvement. Further evaluation on the F1-scores decay in the presence of noise demonstrated that our model is more robust than the baseline models when considering noisy data, be that in the form of incorrect sentences or sentences with STT error. Not only that, experiments on the Twitter dataset also showed improved accuracy in clean data, with complete sentences. We infer that this is due to our model being able to extract richer data representations from the input data regardless of the completeness of the sentence. For future works, we plan on evaluating the robustness of our model against other types of noise, such as word reordering, word insertion, and spelling mistakes in sentences. In order to improve the performance of our model, further experiments will be done in search for more appropriate hyperparameters and more complex neural classifiers to substitute the last feedforward network layer. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "1\n",
      "Question 1: What is the purpose of applying a Text-to-Speech and Speech-to-Text module in the intent classification task?\n",
      "\n",
      "Answer 1: The purpose of applying a Text-to-Speech and Speech-to-Text module in the intent classification task is to obtain incomplete sentences with STT error, as the dataset suffers from a lack of corresponding incomplete sentences. This helps in evaluating the model's robustness to different rates of data incompleteness.\n",
      "Question : for the text In the intent classification task, we are presented with a corpus that suffers from the opposite problem of the Twitter sentiment classification corpus. In the intent classification corpus, we have the complete sentences and intent labels but lack their corresponding incomplete sentences, and since our task revolves around text classification in incomplete or incorrect data, it is essential that we obtain this information. To remedy this issue, we apply a Text-to-Speech (TTS) module followed by a Speech-to-Text (STT) module to the complete sentences in order to obtain incomplete sentences with STT error. Due to TTS and STT modules available being imperfect, the resulting sentences have a reasonable level of noise in the form of missing or incorrectly transcribed words. Analysis on this dataset adds value to our work by enabling evaluation of our model's robustness to different rates of data incompleteness..The dataset used to evaluate the models' performance is the Chatbot Natural Language Unerstanding (NLU) Evaluation Corpus, introduced by Braun et al. BIBREF20 to test NLU services. It is a publicly available benchmark and is composed of sentences obtained from a German Telegram chatbot used to answer questions about public transport connections. The dataset has two intents, namely Departure Time and Find Connection with 100 train and 106 test samples, shown in Table TABREF18. Even though English is the main language of the benchmark, this dataset contains a few German station and street names..The incomplete dataset used for training is composed of lower-cased incomplete data obtained by manipulating the original corpora. The incomplete sentences with STT error are obtained in a 2-step process shown in Fig. FIGREF22. The first step is to apply a TTS module to the available complete sentence. Here, we apply gtts , a Google Text-to-Speech python library, and macsay , a terminal command available in Mac OS as say. The second step consists of applying an STT module to the obtained audio files in order to obtain text containing STT errors. The STT module used here was witai , freely available and maintained by Wit.ai. The mentioned TTS and STT modules were chosen according to code availability and whether it's freely available or has high daily usage limitations..Table TABREF24 exemplifies a complete and its respective incomplete sentences with different TTS-STT combinations, thus varying rates of missing and incorrect words. The level of noise in the STT imbued sentences is denoted by a inverted BLEU (iBLEU) score ranging from 0 to 1. The inverted BLEU score is denoted in Eq. (DISPLAY_FORM23):.where BLEU is a common metric usually used in machine translation tasks BIBREF21. We decide to showcase that instead of regular BLEU because it is more indicative to the amount of noise in the incomplete text, where the higher the iBLEU, the higher the noise. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "2\n",
      "Question 1: What is the reason for choosing Kaggle's Sentiment140 dataset for the model evaluation?\n",
      "\n",
      "Answer 1: The reason for choosing Kaggle's Sentiment140 dataset is that it contains naturally noisy and incorrect sentences obtained from Twitter, making it an ideal dataset for evaluating the performance of the model.\n",
      "Question : for the text In order to evaluate the performance of our model, we need access to a naturally noisy dataset with real human errors. Poor quality texts obtained from Twitter, called tweets, are then ideal for our task. For this reason, we choose Kaggle's two-class Sentiment140 dataset BIBREF18, which consists of spoken text being used in writing and without strong consideration for grammar or sentence correctness. Thus, it has many mistakes, as specified in Table TABREF11..Even though this corpus has incorrect sentences and their emotional labels, they lack their respective corrected sentences, necessary for the training of our model. In order to obtain this missing information, we outsource native English speakers from an unbiased and anonymous platform, called Amazon Mechanical Turk (MTurk) BIBREF19, which is a paid marketplace for Human Intelligence Tasks (HITs). We use this platform to create tasks for native English speakers to format the original incorrect tweets into correct sentences. Some examples are shown in Table TABREF12..After obtaining the correct sentences, our two-class dataset has class distribution as shown in Table TABREF14. There are 200 sentences used in the training stage, with 100 belonging to the positive sentiment class and 100 to the negative class, and 50 samples being used in the evaluation stage, with 25 negative and 25 positive. This totals in 300 samples, with incorrect and correct sentences combined. Since our goal is to evaluate the model's performance and robustness in the presence of noise, we only consider incorrect data in the testing phase. Note that BERT is a pre-trained model, meaning that small amounts of data are enough for appropriate fine-tuning. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "3\n",
      "Question 1: Which baseline models are used for comparison along with BERT?\n",
      "Answer 1: BERT is being compared with the following baseline models.\n",
      "Question : for the text Besides the already mentioned BERT, the following baseline models are also used for comparison. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "4\n",
      "Question 1: What are the three services being focused on and what type of services are they? \n",
      "\n",
      "Answer 1: The three services being focused on are Google Dialogflow, SAP Conversational AI, and Rasa. The first two are commercial services, while the third one is an open source service with two separate backends.\n",
      "Question : for the text We focus on the three following services, where the first two are commercial services and last one is open source with two separate backends: Google Dialogflow (formerly Api.ai) , SAP Conversational AI (formerly Recast.ai) and Rasa (spacy and tensorflow backend) . generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "5\n",
      "Question 1: What is the method proposed by Shridhar et al. to overcome out-of-vocabulary issues in word embeddings?\n",
      "\n",
      "Answer 1: Shridhar et al. proposed using hash tokens in the alphabet instead of single words, making it vocabulary independent. This method overcomes out-of-vocabulary issues commonly faced by traditional word embeddings.\n",
      "Question : for the text Shridhar et al. BIBREF12 proposed a word embedding method that doesn't suffer from out-of-vocabulary issues. The authors achieve this by using hash tokens in the alphabet instead of a single word, making it vocabulary independent. For classification, classifiers such as Multilayer Perceptron (MLP), Support Vector Machine (SVM) and Random Forest are used. A complete list of classifiers and training specifications are given in Section SECREF31. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "6\n",
      "Question 1: What is the main finding of the experimental results for the Intent Classification task on the Chatbot NLU Corpus with STT error?\n",
      "\n",
      "Answer 1: The main finding is that the model presented in the study outperforms all baseline models in both combinations of TTS-STT when presented with data containing STT error. The model's F1-scores are higher than those of the other models and its robustness in the presence of noise is stronger. Additionally, normalized confusion matrices show that the model is able to improve overall performance by improving the accuracy of one class while maintaining the high-achieving accuracy of the second one.\n",
      "Question : for the text Experimental results for the Intent Classification task on the Chatbot NLU Corpus with STT error can be seen in Table TABREF40. When presented with data containing STT error, our model outperforms all baseline models in both combinations of TTS-STT: gtts-witai outperforms the second placing baseline model by 0.94% with F1-score of 97.17%, and macsay-witai outperforms the next highest achieving model by 1.89% with F1-score of 96.23%..The table also indicates the level of noise in each dataset with the already mentioned iBLEU score, where 0 means no noise and higher values mean higher quantity of noise. As expected, the models' accuracy degrade with the increase in noise, thus F1-scores of gtts-witai are higher than macsay-witai. However, while the other models decay rapidly in the presence of noise, our model does not only outperform them but does so with a wider margin. This is shown with the increasing robustness curve in Fig. FIGREF41 and can be demonstrated by macsay-witai outperforming the baseline models by twice the gap achieved by gtts-witai..Further analysis of the results in Table TABREF40 show that, BERT decay is almost constant with the addition of noise, with the difference between the complete data and gtts-witai being 1.88 and gtts-witai and macsay-witai being 1.89. Whereas in Stacked DeBERT, that difference is 1.89 and 0.94 respectively. This is stronger indication of our model's robustness in the presence of noise..Additionally, we also present Fig. FIGREF42 with the normalized confusion matrices for BERT and Stacked DeBERT for sentences containing STT error. Analogously to the Twitter Sentiment Classification task, the per-class F1-scores show that our model is able to improve the overall performance by improving the accuracy of one class while maintaining the high-achieving accuracy of the second one. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "7\n",
      "Question 1: How does Stacked DeBERT perform compared to the baseline models in the Twitter Sentiment Classification task on Kaggle's Sentiment140 Corpus dataset?\n",
      "\n",
      "Answer 1: Stacked DeBERT outperforms the baseline models by 6% to 8% in the Twitter Sentiment Classification task on Kaggle's Sentiment140 Corpus dataset. It achieves an accuracy of 82% in the second version of the dataset, which considers the corrected tweets, against BERT's 76%.\n",
      "Question : for the text Experimental results for the Twitter Sentiment Classification task on Kaggle's Sentiment140 Corpus dataset, displayed in Table TABREF37, show that our model has better F1-micros scores, outperforming the baseline models by 6$\\%$ to 8$\\%$. We evaluate our model and baseline models on three versions of the dataset. The first one (Inc) only considers the original data, containing naturally incorrect tweets, and achieves accuracy of 80$\\%$ against BERT's 72$\\%$. The second version (Corr) considers the corrected tweets, and shows higher accuracy given that it is less noisy. In that version, Stacked DeBERT achieves 82$\\%$ accuracy against BERT's 76$\\%$, an improvement of 6$\\%$. In the last case (Inc+Corr), we consider both incorrect and correct tweets as input to the models in hopes of improving performance. However, the accuracy was similar to the first aforementioned version, 80$\\%$ for our model and 74$\\%$ for the second highest performing model. Since the first and last corpus gave similar performances with our model, we conclude that the Twitter dataset does not require complete sentences to be given as training input, in addition to the original naturally incorrect tweets, in order to better model the noisy sentences..In addition to the overall F1-score, we also present a confusion matrix, in Fig. FIGREF38, with the per-class F1-scores for BERT and Stacked DeBERT. The normalized confusion matrix plots the predicted labels versus the target/target labels. Similarly to Table TABREF37, we evaluate our model with the original Twitter dataset, the corrected version and both original and corrected tweets. It can be seen that our model is able to improve the overall performance by improving the accuracy of the lower performing classes. In the Inc dataset, the true class 1 in BERT performs with approximately 50%. However, Stacked DeBERT is able to improve that to 72%, although to a cost of a small decrease in performance of class 0. A similar situation happens in the remaining two datasets, with improved accuracy in class 0 from 64% to 84% and 60% to 76% respectively. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "8\n",
      "Question 1: How many times were the baseline and proposed models trained for the incomplete intent classification task?\n",
      "\n",
      "Answer 1: The baseline and proposed models were each trained 3 separate times for the incomplete intent classification task: complete data and one for each of the TTS-STT combinations (gtts-witai and macsay-witai).\n",
      "Question : for the text The baseline and proposed models are each trained 3 separate times for the incomplete intent classification task: complete data and one for each of the TTS-STT combinations (gtts-witai and macsay-witai). Regarding the sentiment classification from incorrect sentences task, the baseline and proposed models are each trained 3 times: original text, corrected text and incorrect with correct texts. The reported F1 scores are the best accuracies obtained from 10 runs. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "9\n",
      "Question 1: What is the maximum sequence length used in fine-tuning Conventional BERT on the given datasets?\n",
      "\n",
      "Answer 1: The maximum sequence length used in fine-tuning Conventional BERT on the given datasets is 128.\n",
      "Question : for the text Conventional BERT is a BERT-base-uncased model, meaning that it has 12 transformer blocks $L$, hidden size $H$ of 768, and 12 self-attention heads $A$. The model is fine-tuned with our dataset on 2 Titan X GPUs for 3 epochs with Adam Optimizer, learning rate of $2*10^{-5}$, maximum sequence length of 128, and warm up proportion of $0.1$. The train batch size is 4 for the Twitter Sentiment Corpus and 8 for the Chatbot Intent Classification Corpus. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "10\n",
      "Question 1: Are there any options for customizing the training configurations on the online platforms?\n",
      "\n",
      "Answer 1: No, there are no options for setting up or customizing the training configurations on the online platforms.\n",
      "Question : for the text No settable training configurations available in the online platforms. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "11\n",
      "What was the most successful classifier used in the study?\n",
      "\n",
      "The most successful classifier used in the study was MLP with 3 hidden layers of sizes $[300, 100, 50]$ respectively.\n",
      "Question : for the text Trained on 3-gram, feature vector size of 768 as to match the BERT embedding size, and 13 classifiers with parameters set as specified in the authors' paper so as to allow comparison: MLP with 3 hidden layers of sizes $[300, 100, 50]$ respectively; Random Forest with 50 estimators or trees; 5-fold Grid Search with Random Forest classifier and estimator $([50, 60, 70]$; Linear Support Vector Classifier with L1 and L2 penalty and tolerance of $10^{-3}$; Regularized linear classifier with Stochastic Gradient Descent (SGD) learning with regularization term $alpha=10^{-4}$ and L1, L2 and Elastic-Net penalty; Nearest Centroid with Euclidian metric, where classification is done by representing each class with a centroid; Bernoulli Naive Bayes with smoothing parameter $alpha=10^{-2}$; K-means clustering with 2 clusters and L2 penalty; and Logistic Regression classifier with L2 penalty, tolerance of $10^{-4}$ and regularization term of $1.0$. Most often, the best performing classifier was MLP. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "12\n",
      "Question 1: What optimizer is used to train the proposed model and what is the learning rate?\n",
      "\n",
      "Answer 1: The proposed model is trained using Adam Optimizer with a learning rate of $10^{-3}$.\n",
      "Question : for the text Our proposed model is trained in end-to-end manner on 2 Titan X GPUs, with training time depending on the size of the dataset and train batch size. The stack of multilayer perceptrons are trained for 100 and 1,000 epochs with Adam Optimizer, learning rate of $10^{-3}$, weight decay of $10^{-5}$, MSE loss criterion and batch size the same as BERT (4 for the Twitter Sentiment Corpus and 8 for the Chatbot Intent Classification Corpus). generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "13\n",
      "What is the focus of the paper?\n",
      "\n",
      "The focus of the paper is on developing a novel model architecture that is more robust to incomplete data, specifically incomplete intent and sentiment classification from incorrect sentences. The paper also proposes a new dataset related to these tasks.\n",
      "Question : for the text Understanding a user's intent and sentiment is of utmost importance for current intelligent chatbots to respond appropriately to human requests. However, current systems are not able to perform to their best capacity when presented with incomplete data, meaning sentences with missing or incorrect words. This scenario is likely to happen when one considers human error done in writing. In fact, it is rather naive to assume that users will always type fully grammatically correct sentences. Panko BIBREF0 goes as far as claiming that human accuracy regarding research paper writing is none when considering the entire document. This has been aggravated with the advent of internet and social networks, which allowed language and modern communication to be been rapidly transformed BIBREF1, BIBREF2. Take Twitter for instance, where information is expected to be readily communicated in short and concise sentences with little to no regard to correct sentence grammar or word spelling BIBREF3..Further motivation can be found in Automatic Speech Recognition (ASR) applications, where high error rates prevail and pose an enormous hurdle in the broad adoption of speech technology by users worldwide BIBREF4. This is an important issue to tackle because, in addition to more widespread user adoption, improving Speech-to-Text (STT) accuracy diminishes error propagation to modules using the recognized text. With that in mind, in order for current systems to improve the quality of their services, there is a need for development of robust intelligent systems that are able to understand a user even when faced with incomplete representation in language..The advancement of deep neural networks have immensely aided in the development of the Natural Language Processing (NLP) domain. Tasks such as text generation, sentence correction, image captioning and text classification, have been possible via models such as Convolutional Neural Networks and Recurrent Neural Networks BIBREF5, BIBREF6, BIBREF7. More recently, state-of-the-art results have been achieved with attention models, more specifically Transformers BIBREF8. Surprisingly, however, there is currently no research on incomplete text classification in the NLP community. Realizing the need of research in that area, we make it the focus of this paper. In this novel task, the model aims to identify the user's intent or sentiment by analyzing a sentence with missing and/or incorrect words. In the sentiment classification task, the model aims to identify the user's sentiment given a tweet, written in informal language and without regards for sentence correctness..Current approaches for Text Classification tasks focus on efficient embedding representations. Kim et al. BIBREF9 use semantically enriched word embeddings to make synonym and antonym word vectors respectively more and less similar in order to improve intent classification performance. Devlin et al. BIBREF10 propose Bidirectional Encoder Representations from Transformers (BERT), a powerful bidirectional language representation model based on Transformers, achieving state-of-the-art results on eleven NLP tasks BIBREF11, including sentiment text classification. Concurrently, Shridhar et al. BIBREF12 also reach state of the art in the intent recognition task using Semantic Hashing for feature representation followed by a neural classifier. All aforementioned approaches are, however, applied to datasets based solely on complete data..The incomplete data problem is usually approached as a reconstruction or imputation task and is most often related to missing numbers imputation BIBREF13. Vincent et al. BIBREF14, BIBREF15 propose to reconstruct clean data from its noisy version by mapping the input to meaningful representations. This approach has also been shown to outperform other models, such as predictive mean matching, random forest, Support Vector Machine (SVM) and Multiple imputation by Chained Equations (MICE), at missing data imputation tasks BIBREF16, BIBREF17. Researchers in those two areas have shown that meaningful feature representation of data is of utter importance for high performance achieving methods. We propose a model that combines the power of BERT in the NLP domain and the strength of denoising strategies in incomplete data reconstruction to tackle the tasks of incomplete intent and sentiment classification. This enables the implementation of a novel encoding scheme, more robust to incomplete data, called Stacked Denoising BERT or Stacked DeBERT. Our approach consists of obtaining richer input representations from input tokens by stacking denoising transformers on an embedding layer with vanilla transformers. The embedding layer and vanilla transformers extract intermediate input features from the input tokens, and the denoising transformers are responsible for obtaining richer input representations from them. By improving BERT with stronger denoising abilities, we are able to reconstruct missing and incorrect words' embeddings and improve classification accuracy. To summarize, our contribution is two-fold:.Novel model architecture that is more robust to incomplete data, including missing or incorrect words in text..Proposal of the novel tasks of incomplete intent and sentiment classification from incorrect sentences, and release of corpora related with these tasks..The remainder of this paper is organized in four sections, with Section SECREF2 explaining the proposed model. This is followed by Section SECREF3 which includes a detailed description of the dataset used for training and evaluation purposes and how it was obtained. Section SECREF4 covers the baseline models used for comparison, training specifications and experimental results. Finally, Section SECREF5 wraps up this paper with conclusion and future works. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "14\n",
      "What is the proposed model called and what is its purpose?\n",
      "The proposed model is called Stacked Denoising BERT (DeBERT) and its purpose is to improve the robustness and efficiency of BERT when applied to incomplete data such as tweets and text with STT error, by reconstructing hidden embeddings from sentences with missing words.\n",
      "Question : for the text We propose Stacked Denoising BERT (DeBERT) as a novel encoding scheming for the task of incomplete intent classification and sentiment classification from incorrect sentences, such as tweets and text with STT error. The proposed model, illustrated in Fig. FIGREF4, is structured as a stacking of embedding layers and vanilla transformer layers, similarly to the conventional BERT BIBREF10, followed by layers of novel denoising transformers. The main purpose of this model is to improve the robustness and efficiency of BERT when applied to incomplete data by reconstructing hidden embeddings from sentences with missing words. By reconstructing these hidden embeddings, we are able to improve the encoding scheme in BERT..The initial part of the model is the conventional BERT, a multi-layer bidirectional Transformer encoder and a powerful language model. During training, BERT is fine-tuned on the incomplete text classification corpus (see Section SECREF3). The first layer pre-processes the input sentence by making it lower-case and by tokenizing it. It also prefixes the sequence of tokens with a special character `[CLS]' and sufixes each sentence with a `[SEP]' character. It is followed by an embedding layer used for input representation, with the final input embedding being a sum of token embedddings, segmentation embeddings and position embeddings. The first one, token embedding layer, uses a vocabulary dictionary to convert each token into a more representative embedding. The segmentation embedding layer indicates which tokens constitute a sentence by signaling either 1 or 0. In our case, since our data are formed of single sentences, the segment is 1 until the first `[SEP]' character appears (indicating segment A) and then it becomes 0 (segment B). The position embedding layer, as the name indicates, adds information related to the token's position in the sentence. This prepares the data to be considered by the layers of vanilla bidirectional transformers, which outputs a hidden embedding that can be used by our novel layers of denoising transformers..Although BERT has shown to perform better than other baseline models when handling incomplete data, it is still not enough to completely and efficiently handle such data. Because of that, there is a need for further improvement of the hidden feature vectors obtained from sentences with missing words. With this purpose in mind, we implement a novel encoding scheme consisting of denoising transformers, which is composed of stacks of multilayer perceptrons for the reconstruction of missing words’ embeddings by extracting more abstract and meaningful hidden feature vectors, and bidirectional transformers for improved embedding representation. The embedding reconstruction step is trained on sentence embeddings extracted from incomplete data $h_{inc}$ as input and embeddings corresponding to its complete version $h_{comp}$ as target. Both input and target are obtained after applying the embedding layers and the vanilla transformers, as indicated in Fig. FIGREF4, and have shape $(N_{bs}, 768, 128)$, where $N_{bs}$ is the batch size, 768 is the original BERT embedding size for a single token, and 128 is the maximum sequence length in a sentence..The stacks of multilayer perceptrons are structured as two sets of three layers with two hidden layers each. The first set is responsible for compressing the $h_{inc}$ into a latent-space representation, extracting more abstract features into lower dimension vectors $z_1$, $z_2$ and $\\mathbf {z}$ with shape $(N_{bs}, 128, 128)$, $(N_{bs}, 32, 128)$, and $(N_{bs}, 12, 128)$, respectively. This process is shown in Eq. (DISPLAY_FORM5):.where $f(\\cdot )$ is the parameterized function mapping $h_{inc}$ to the hidden state $\\mathbf {z}$. The second set then respectively reconstructs $z_1$, $z_2$ and $\\mathbf {z}$ into $h_{rec_1}$, $h_{rec_2}$ and $h_{rec}$. This process is shown in Eq. (DISPLAY_FORM6):.where $g(\\cdot )$ is the parameterized function that reconstructs $\\mathbf {z}$ as $h_{rec}$..The reconstructed hidden sentence embedding $h_{rec}$ is compared with the complete hidden sentence embedding $h_{comp}$ through a mean square error loss function, as shown in Eq. (DISPLAY_FORM7):.After reconstructing the correct hidden embeddings from the incomplete sentences, the correct hidden embeddings are given to bidirectional transformers to generate input representations. The model is then fine-tuned in an end-to-end manner on the incomplete text classification corpus..Classification is done with a feedforward network and softmax activation function. Softmax $\\sigma $ is a discrete probability distribution function for $N_C$ classes, with the sum of the classes probability being 1 and the maximum value being the predicted class. The predicted class can be mathematically calculated as in Eq. (DISPLAY_FORM8):.where $o = W t + b$, the output of the feedforward layer used for classification. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "15\n",
      "Question 1: Who provided financial and technical support for this project?\n",
      "Answer 1: Amazon provided financial and technical support for this project, and we would like to acknowledge their help.\n",
      "Question : for the text We would like to acknowledge the help from Amazon in terms of financial and technical support. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "16\n",
      "Question 1: How long did the data collection for Gunrock's conversational data take place?\n",
      "Answer 1: The data collection for Gunrock's conversational data took place from January 5, 2019 to March 5, 2019.\n",
      "Question : for the text From January 5, 2019 to March 5, 2019, we collected conversational data for Gunrock. During this time, no other code updates occurred. We analyzed conversations for Gunrock with at least 3 user turns to avoid conversations triggered by accident. Overall, this resulted in a total of 34,432 user conversations. Together, these users gave Gunrock an average rating of 3.65 (median: 4.0), which was elicited at the end of the conversation (“On a scale from 1 to 5 stars, how do you feel about talking to this socialbot again?\"). Users engaged with Gunrock for an average of 20.92 overall turns (median 13.0), with an average of 6.98 words per utterance, and had an average conversation time of 7.33 minutes (median: 2.87 min.). We conducted three principal analyses: users' response depth (wordcount), backstory queries (backstorypersona), and interleaving of personal and factual responses (pets). generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "17\n",
      "What did the researchers model the overall (log) Rating with for users who asked at least one backstory question about Gunrock?\n",
      "\n",
      "The researchers modeled the overall (log) Rating with a linear regression by the (log) `Number of Backstory Questions Asked' (log transformed due to the variables' nonlinear relationship).\n",
      "Question : for the text We assessed the user's interest in Gunrock by tagging instances where the user triggered Gunrock's backstory (e.g., “What's your favorite color?\"). For users with at least one backstory question, we modeled overall (log) Rating with a linear regression by the (log) `Number of Backstory Questions Asked' (log transformed due to the variables' nonlinear relationship). We hypothesized that users who show greater curiosity about Gunrock will display higher overall ratings for the conversation based on her responses. Overall, the number of times users queried Gunrock's backstory was strongly related to the rating they gave at the end of the interaction (log:$\\beta $=0.10, SE=0.002, t=58.4, p$<$0.001)(see Figure 3). This suggests that maintaining a consistent personality — and having enough responses to questions the users are interested in — may improve user satisfaction. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "18\n",
      "Question 1: What is the purpose of Gunrock's animal module?\n",
      "\n",
      "Answer 1: The animal module on Gunrock includes a factual component where the system provides animal facts, as well as a more personalized component about pets. It is designed to engage users about animals in a more casual conversational style, eliciting follow-up questions if the user indicates they have a pet.\n",
      "Question : for the text Gunrock includes a specific topic module on animals, which includes a factual component where the system provides animal facts, as well as a more personalized component about pets. Our system is designed to engage users about animals in a more casual conversational style BIBREF14, eliciting follow-up questions if the user indicates they have a pet; if we are able to extract the pet's name, we refer to it in the conversation (e.g., “Oliver is a great name for a cat!\", “How long have you had Oliver?\"). In cases where the user does not indicate that they have a pet, the system solely provides animal facts. Therefore, the animal module can serve as a test of our interleaving strategy: we hypothesized that combining facts and personal questions — in this case about the user's pet — would lead to greater user satisfaction overall..We extracted conversations where Gunrock asked the user if they had ever had a pet and categorized responses as “Yes\", “No\", or “NA\" (if users did not respond with an affirmative or negative response). We modeled user rating with a linear regression model, with predictor of “Has Pet' (2 levels: Yes, No). We found that users who talked to Gunrock about their pet showed significantly higher overall ratings of the conversation ($\\beta $=0.15, SE=0.06, t=2.53, p$=$0.016) (see Figure 4). One interpretation is that interleaving factual information with more in-depth questions about their pet result in improved user experience. Yet, another interpretation is that pet owners may be more friendly and amenable to a socialbot; for example, prior research has linked differences in personality to pet ownership BIBREF15. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "19\n",
      "What did the study find about the relationship between users' word count and their engagement with Gunrock?\n",
      "\n",
      "The study found that users who, on average, produced utterances with more words gave significantly higher ratings and engaged with Gunrock for significantly greater number of turns. This suggests that Gunrock's ability to handle complex sentences and encourage active conversation leads to greater user engagement and satisfaction. However, it is also possible that users who are naturally more talkative enjoy talking to the bot in general, leading to higher ratings and longer conversations.\n",
      "Question : for the text Two unique features of Gunrock are its ability to dissect longer, complex sentences, and its methods to encourage users to be active conversationalists, elaborating on their responses. In prior work, even if users are able to drive the conversation, often bots use simple yes/no questions to control the conversational flow to improve understanding; as a result, users are more passive interlocutors in the conversation. We aimed to improve user engagement by designing the conversation to have more open-ended opinion/personal questions, and show that the system can understand the users' complex utterances (See nlu for details on NLU). Accordingly, we ask if users' speech behavior will reflect Gunrock's technical capability and conversational strategy, producing longer sentences..We assessed the degree of conversational depth by measuring users' mean word count. Prior work has found that an increase in word count has been linked to improved user engagement (e.g., in a social dialog system BIBREF13). For each user conversation, we extracted the overall rating, the number of turns of the interaction, and the user's per-utterance word count (averaged across all utterances). We modeled the relationship between word count and the two metrics of user engagement (overall rating, mean number of turns) in separate linear regressions..Results showed that users who, on average, produced utterances with more words gave significantly higher ratings ($\\beta $=0.01, SE=0.002, t=4.79, p$<$0.001)(see Figure 2) and engaged with Gunrock for significantly greater number of turns ($\\beta $=1.85, SE=0.05, t=35.58, p$<$0.001) (see Figure 2). These results can be interpreted as evidence for Gunrock's ability to handle complex sentences, where users are not constrained to simple responses to be understood and feel engaged in the conversation – and evidence that individuals are more satisfied with the conversation when they take a more active role, rather than the system dominating the dialog. On the other hand, another interpretation is that users who are more talkative may enjoy talking to the bot in general, and thus give higher ratings in tandem with higher average word counts. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "20\n",
      "Question 1: What are some practical applications of the design principles used in Gunrock's architecture?\n",
      "\n",
      "Answer 1: One practical application is applying these design principles to other social chatbots. Additionally, there are theoretical implications in terms of the nature of human-computer interaction. The results suggest that users interact with Gunrock in similar ways to other humans, engaging in chitchat about general topics, taking interest in Gunrock's backstory and persona, and even producing more information about themselves in return.\n",
      "Question : for the text Gunrock is a social chatbot that focuses on having long and engaging speech-based conversations with thousands of real users. Accordingly, our architecture employs specific modules to handle longer and complex utterances and encourages users to be more active in a conversation. Analysis shows that users' speech behavior reflects these capabilities. Longer sentences and more questions about Gunrocks's backstory positively correlate with user experience. Additionally, we find evidence for interleaved dialog flow, where combining factual information with personal opinions and stories improve user satisfaction. Overall, this work has practical applications, in applying these design principles to other social chatbots, as well as theoretical implications, in terms of the nature of human-computer interaction (cf. 'Computers are Social Actors' BIBREF16). Our results suggest that users are engaging with Gunrock in similar ways to other humans: in chitchat about general topics (e.g., animals, movies, etc.), taking interest in Gunrock's backstory and persona, and even producing more information about themselves in return. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "21\n",
      "Question 1: What are some of the limitations that Gunrock addresses in prior chatbots?\n",
      "\n",
      "Answer 1: Gunrock addresses several limitations in prior chatbots, including inconsistency and difficulty in complex sentence understanding, by employing multi-step language understanding modules, a natural language understanding module that can handle more complex sentences, and interleaving actions to elicit user opinions and provide responses. Additionally, Gunrock uses an extensive persona database to provide coherent profile information.\n",
      "Question : for the text Amazon Alexa Prize BIBREF0 provides a platform to collect real human-machine conversation data and evaluate performance on speech-based social conversational systems. Our system, Gunrock BIBREF1 addresses several limitations of prior chatbots BIBREF2, BIBREF3, BIBREF4 including inconsistency and difficulty in complex sentence understanding (e.g., long utterances) and provides several contributions: First, Gunrock's multi-step language understanding modules enable the system to provide more useful information to the dialog manager, including a novel dialog act scheme. Additionally, the natural language understanding (NLU) module can handle more complex sentences, including those with coreference. Second, Gunrock interleaves actions to elicit users' opinions and provide responses to create an in-depth, engaging conversation; while a related strategy to interleave task- and non-task functions in chatbots has been proposed BIBREF5, no chatbots to our knowledge have employed a fact/opinion interleaving strategy. Finally, we use an extensive persona database to provide coherent profile information, a critical challenge in building social chatbots BIBREF3. Compared to previous systems BIBREF4, Gunrock generates more balanced conversations between human and machine by encouraging and understanding more human inputs (see Table TABREF2 for an example). generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "22\n",
      "Question 1: What is Gunrock's architecture based on?\n",
      "\n",
      "Answer 1: Gunrock's architecture is based on the Amazon Conversational Bot Toolkit (CoBot), which is a flexible event-driven framework that provides ASR results and natural language processing pipelines through the Alexa Skills Kit (ASK).\n",
      "Question : for the text Figure FIGREF3 provides an overview of Gunrock's architecture. We extend the Amazon Conversational Bot Toolkit (CoBot) BIBREF6 which is a flexible event-driven framework. CoBot provides ASR results and natural language processing pipelines through the Alexa Skills Kit (ASK) BIBREF7. Gunrock corrects ASR according to the context (asr) and creates a natural language understanding (NLU) (nlu) module where multiple components analyze the user utterances. A dialog manager (DM) (dm) uses features from NLU to select topic dialog modules and defines an individual dialog flow. Each dialog module leverages several knowledge bases (knowledge). Then a natural language generation (NLG) (nlg) module generates a corresponding response. Finally, we markup the synthesized responses and return to the users through text to speech (TTS) (tts). While we provide an overview of the system in the following sections, for detailed system implementation details, please see the technical report BIBREF1. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "23\n",
      "Question 1: How does Gunrock correct ASR errors for named entities such as movie names?\n",
      "\n",
      "Answer 1: Gunrock uses domain knowledge to correct ASR errors for named entities such as movie names by comparing noun phrases to a knowledge base and extracting primary and secondary codes using The Double Metaphone Search Algorithm. It suggests a potential fix by code matching.\n",
      "Question : for the text Gunrock receives ASR results with the raw text and timestep information for each word in the sequence (without case information and punctuation). Keywords, especially named entities such as movie names, are prone to generate ASR errors without contextual information, but are essential for NLU and NLG. Therefore, Gunrock uses domain knowledge to correct these errors by comparing noun phrases to a knowledge base (e.g. a list of the most popular movies names) based on their phonetic information. We extract the primary and secondary code using The Double Metaphone Search Algorithm BIBREF8 for noun phrases (extracted by noun trunks) and the selected knowledge base, and suggest a potential fix by code matching. An example can be seen in User_3 and Gunrock_3 in Table TABREF2. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "24\n",
      "Question 1: How does the dialog manager maintain context during conversations?\n",
      "\n",
      "Answer 1: The dialog manager utilizes separate topic modules that use modular finite state transducers to execute various dialog segments processed by the NLU, thus allowing for deeper conversations that maintain context. Additionally, dialog flows within each module are determined by rule-based transitions between a fixed set of dialog states, and the dialog flow is customized to each user by tracking their attributes as dialog context. This provides a fresh experience each time and reduces the feeling of dialogs being scripted and repetitive.\n",
      "Question : for the text We implemented a hierarchical dialog manager, consisting of a high level and low level DMs. The former leverages NLU outputs for each segment and selects the most important segment for the system as the central element using heuristics. For example, “i just finished reading harry potter,\" triggers Sub-DM: Books. Utilizing the central element and features extracted from NLU, input utterances are mapped onto 11 possible topic dialog modules (e.g., movies, books, animals, etc.), including a backup module, retrieval..Low level dialog management is handled by the separate topic dialog modules, which use modular finite state transducers to execute various dialog segments processed by the NLU. Using topic-specific modules enables deeper conversations that maintain the context. We design dialog flows in each of the finite state machines, as well. Dialog flow is determined by rule-based transitions between a specified fixed set of dialog states. To ensure that our states and transitions are effective, we leverage large scale user data to find high probability responses and high priority responses to handle in different contexts. Meanwhile, dialog flow is customized to each user by tracking user attributes as dialog context. In addition, each dialog flow is adaptive to user responses to show acknowledgement and understanding (e.g., talking about pet ownership in the animal module). Based on the user responses, many dialog flow variations exist to provide a fresh experience each time. This reduces the feeling of dialogs being scripted and repetitive. Our dialog flows additionally interleave facts, opinions, experiences, and questions to make the conversation flexible and interesting..In the meantime, we consider feedback signals such as “continue\" and “stop\" from the current topic dialog module, indicating whether it is able to respond to the following request in the dialog flow, in order to select the best response module. Additionally, in all modules we allow mixed-initiative interactions; users can trigger a new dialog module when they want to switch topics while in any state. For example, users can start a new conversation about movies from any other topic module. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "25\n",
      "Question 1: What kind of personality does Gunrock have?\n",
      "\n",
      "Answer 1: Gunrock is modeled as a female individual who is positive, outgoing, and interested in science and technology. Her responses are designed to elicit a consistent personality within and across modules.\n",
      "Question : for the text All topic dialog modules query knowledge bases to provide information to the user. To respond to general factual questions, Gunrock queries the EVI factual database , as well as other up-to-date scraped information appropriate for the submodule, such as news and current showing movies in a specific location from databases including IMDB. One contribution of Gunrock is the extensive Gunrock Persona Backstory database, consisting of over 1,000 responses to possible questions for Gunrock as well as reasoning for her responses for roughly 250 questions (see Table 2). We designed the system responses to elicit a consistent personality within and across modules, modeled as a female individual who is positive, outgoing, and is interested in science and technology. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "26\n",
      "Question 1: How does Gunrock avoid repetitive and non-specific responses in dialog systems?\n",
      "Answer 1: Gunrock uses a template manager to select from handcrafted response templates based on the dialog state. One dialog state can map to multiple response templates with similar semantic or functional content but differing surface forms. When a response template is selected, any slots are substituted with actual contents, including queried information for news and specific data for weather. Response templates corresponding to different dialog acts are dynamically composed to give the final response.\n",
      "Question : for the text In order to avoid repetitive and non-specific responses commonly seen in dialog systems BIBREF10, Gunrock uses a template manager to select from a handcrafted response templates based on the dialog state. One dialog state can map to multiple response templates with similar semantic or functional content but differing surface forms. Among these response templates for the same dialog state, one is randomly selected without repetition to provide variety unless all have been exhausted. When a response template is selected, any slots are substituted with actual contents, including queried information for news and specific data for weather. For example, to ground a movie name due to ASR errors or multiple versions, one template is “Are you talking about {movie_title} released in {release_year} starring {actor_name} as {actor_role}?\". Module-specific templates were generated for each topic (e.g., animals), but some of the templates are generalizable across different modules (e.g., “What’s your favorite [movie $|$ book $|$ place to visit]?\").In many cases, response templates corresponding to different dialog acts are dynamically composed to give the final response. For example, an appropriate acknowledgement for the user’s response can be combined with a predetermined follow-up question. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "27\n",
      "Question 1: How does Gunrock maintain the completeness and meaning of user utterances?\n",
      "\n",
      "Answer 1: Gunrock splits the corrected raw ASR text into sentences and masks named entities before segmentation to maintain the completeness and meaning of user utterances. It also uses coreference resolution to replace nouns with their actual reference and a constituency parser to generate noun phrases. Gunrock detects topic, named entities, and sentiment in parallel and uses a human-machine dialog act scheme called MIDAS to extract the intent for each segment. The NLU components analyzed on each segment in a user utterance are sent to the DM and NLG module for state tracking and generation, respectively.\n",
      "Question : for the text Gunrock is designed to engage users in deeper conversation; accordingly, a user utterance can consist of multiple units with complete semantic meanings. We first split the corrected raw ASR text into sentences by inserting break tokens. An example is shown in User_3 in Table TABREF2. Meanwhile, we mask named entities before segmentation so that a named entity will not be segmented into multiple parts and an utterance with a complete meaning is maintained (e.g.,“i like the movie a star is born\"). We also leverage timestep information to filter out false positive corrections. After segmentation, our coreference implementation leverages entity knowledge (such as person versus event) and replaces nouns with their actual reference by entity ranking. We implement coreference resolution on entities both within segments in a single turn as well as across multiple turns. For instance, “him\" in the last segment in User_5 is replaced with “bradley cooper\" in Table TABREF2. Next, we use a constituency parser to generate noun phrases from each modified segment. Within the sequence pipeline to generate complete segments, Gunrock detects (1) topic, (2) named entities, and (3) sentiment using ASK in parallel. The NLU module uses knowledge graphs including Google Knowledge Graph to call for a detailed description of each noun phrase for understanding..In order to extract the intent for each segment, we designed MIDAS, a human-machine dialog act scheme with 23 tags and implemented a multi-label dialog act classification model using contextual information BIBREF9. Next, the NLU components analyzed on each segment in a user utterance are sent to the DM and NLG module for state tracking and generation, respectively. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "28\n",
      "Question 1: What modifications are made to the TTS system after NLG?\n",
      "\n",
      "Answer 1: The modifications made to the TTS system after NLG include adjusting the system to improve the expressiveness of the voice to convey that the system is an engaged and active participant in the conversation. A rule-based system is used to systematically add interjections, specifically Alexa Speechcons, and fillers to approximate human-like cognitive-emotional expression. This is done to make the voice more natural and relatable to users. For more information, refer to the relevant citations.\n",
      "Question : for the text After NLG, we adjust the TTS of the system to improve the expressiveness of the voice to convey that the system is an engaged and active participant in the conversation. We use a rule-based system to systematically add interjections, specifically Alexa Speechcons, and fillers to approximate human-like cognitive-emotional expression BIBREF11. For more on the framework and analysis of the TTS modifications, see BIBREF12. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "29\n",
      "Question 1: What are the three proposed approaches for the task of subjectivity detection? \n",
      "\n",
      "Answer 1: The three proposed approaches are optimized BERT-based models, distilled pretrained models, and the use of ensemble methods.\n",
      "Question : for the text In this section, we outline baseline models like $BERT_{large}$. We further propose three approaches: optimized BERT-based models, distilled pretrained models, and the use of ensemble methods for the task of subjectivity detection. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "30\n",
      "Question 1: What is the main difference between FastText and BiLSTM models for text classification?\n",
      "\n",
      "Answer 1: FastText uses bag of words and bag of n-grams as features to capture partial information about the local word order efficiently. On the other hand, BiLSTM uses memory based on history information to learn long-distance features and predict the output.\n",
      "Question : for the text FastTextBIBREF4: It uses bag of words and bag of n-grams as features for text classification, capturing partial information about the local word order efficiently..BiLSTM: Unlike feedforward neural networks, recurrent neural networks like BiLSTMs use memory based on history information to learn long-distance features and then predict the output. We use a two-layer BiLSTM architecture with GloVe word embeddings as a strong RNN baseline..BERT BIBREF5: It is a contextualized word representation model that uses bidirectional transformers, pretrained on a large $3.3B$ word corpus. We use the $BERT_{large}$ model finetuned on the training dataset. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "31\n",
      "What is the advantage of using distilled BERT-based models?\n",
      "\n",
      "Using distilled BERT-based models results in significantly smaller and faster models with comparable performance to their undistilled versions. The models are pre-trained by leveraging distillation knowledge, which allows for more efficient detection of subjectivity. This is introduced in the paper cited as BIBREF8.\n",
      "Question : for the text Optimized BERT-based models: We use BERT-based models optimized as in BIBREF6 and BIBREF7, pretrained on a dataset as large as twelve times as compared to $BERT_{large}$, with bigger batches, and longer sequences. ALBERT, introduced in BIBREF7, uses factorized embedding parameterization and cross-layer parameter sharing for parameter reduction. These optimizations have led both the models to outperform $BERT_{large}$ in various benchmarking tests, like GLUE for text classification and SQuAD for Question Answering..Distilled BERT-based models: Secondly, we propose to use distilled BERT-based models, as introduced in BIBREF8. They are smaller general-purpose language representation model, pre-trained by leveraging distillation knowledge. This results in significantly smaller and faster models with performance comparable to their undistilled versions. We finetune these pretrained distilled models on the training corpus to efficiently detect subjectivity..BERT-based ensemble models: Lastly, we use the weighted-average ensembling technique to exploit the predictions made by different variations of the above models. Ensembling methodology entails engendering a predictive model by utilizing predictions from multiple models in order to improve Accuracy and F1, decrease variance, and bias. We experiment with variations of $RoBERTa_{large}$, $ALBERT_{xxlarge.v2}$, $DistilRoBERTa$ and $BERT$ and outline selected combinations in tab:experimental-results. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "32\n",
      "Question 1: What corpus was used for the experiments in this paper and how many sentences did it consist of?\n",
      "\n",
      "Answer 1: The experiments in this paper were performed on a general Wikipedia corpus that consisted of more than 360k pre and post subjective bias neutralized sentences.\n",
      "Question : for the text In this paper, we investigated BERT-based architectures for sentence level subjective bias detection. We perform our experiments on a general Wikipedia corpus consisting of more than $360k$ pre and post subjective bias neutralized sentences. We found our proposed architectures to outperform the existing baselines significantly. BERT-based ensemble consisting of RoBERTa, ALBERT, DistillRoBERTa, and BERT led to the highest F1 and Accuracy. In the future, we would like to explore document-level detection of subjective bias, multi-word mitigation of the bias, applications of detecting the bias in recommendation systems. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "33\n",
      "Question 1: What is the WNC dataset and how was it split for the experiment?\n",
      "\n",
      "Answer 1: The WNC dataset is a collection of $180k$ biased sentences and their neutral counterparts crawled from $423,823$ Wikipedia revisions between 2004 and 2019. The dataset was split into two parts in a $90:10$ Train-Test split, and the evaluation was performed on the held-out test set.\n",
      "Question : for the text We perform our experiments on the WNC dataset open-sourced by the authors of BIBREF2. It consists of aligned pre and post neutralized sentences made by Wikipedia editors under the neutral point of view. It contains $180k$ biased sentences, and their neutral counterparts crawled from $423,823$ Wikipedia revisions between 2004 and 2019. We randomly shuffled these sentences and split this dataset into two parts in a $90:10$ Train-Test split and perform the evaluation on the held-out test dataset..For all BERT-based models, we use a learning rate of $2*10^{-5}$, a maximum sequence length of 50, and a weight decay of $0.01$ while finetuning the model. We use FastText's recently open-sourced automatic hyperparameter optimization functionality while training the model. For the BiLSTM baseline, we use a dropout of $0.05$ along with a recurrent dropout of $0.2$ in two 64 unit sized stacked BiLSTMs, using softmax activation layer as the final dense layer. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "34\n",
      "Question 1: Which model outperforms all other non-ensemble models and the baselines for all metrics in the experimental results?\n",
      "\n",
      "Answer 1: $RoBERTa_{large}$ outperforms all other non-ensemble models and the baselines for all metrics in the experimental results.\n",
      "Question : for the text tab:experimental-results shows the performance of different models on the WNC corpus evaluated on the following four metrics: Precision, Recall, F1, and Accuracy. Our proposed methodology, the use of finetuned optimized BERT based models, and BERT-based ensemble models outperform the baselines for all the metrics..Among the optimized BERT based models, $RoBERTa_{large}$ outperforms all other non-ensemble models and the baselines for all metrics. It further achieves a maximum recall of $0.681$ for all the proposed models. We note that DistillRoBERTa, a distilled model, performs competitively, achieving $69.69\\%$ accuracy, and $0.672$ F1 score. This observation shows that distilled pretrained models can replace their undistilled counterparts in a low-computing environment..We further observe that ensemble models perform better than optimized BERT-based models and distilled pretrained models. Our proposed ensemble comprising of $RoBERTa_{large}$, $ALBERT_{xxlarge.v2}$, $DistilRoBERTa$ and $BERT$ outperforms all the proposed models obtaining $0.704$ F1 score, $0.733$ precision, and $71.61\\%$ Accuracy. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "35\n",
      "Question 1: What is the purpose of detecting subjective language in factual information?\n",
      "\n",
      "Answer 1: The purpose of detecting subjective language in factual information is to differentiate between biased and objective language, as subjective bias is often pervasive in texts like news and textbooks. This helps ensure that factual information is presented objectively and without influence from emotional state or viewpoints.\n",
      "Question : for the text In natural language, subjectivity refers to the aspects of communication used to express opinions, evaluations, and speculationsBIBREF0, often influenced by one's emotional state and viewpoints. Writers and editors of texts like news and textbooks try to avoid the use of biased language, yet subjective bias is pervasive in these texts. More than $56\\%$ of Americans believe that news sources do not report the news objectively , thus implying the prevalence of the bias. Therefore, when presenting factual information, it becomes necessary to differentiate subjective language from objective language..There has been considerable work on capturing subjectivity using text-classification models ranging from linguistic-feature-based modelsBIBREF1 to finetuned pre-trained word embeddings like BERTBIBREF2. The detection of bias-inducing words in a Wikipedia statement was explored in BIBREF1. The authors propose the \"Neutral Point of View\" (NPOV) corpus made using Wikipedia revision history, containing Wikipedia edits that are specifically designed to remove subjective bias. They use logistic regression with linguistic features, including factive verbs, hedges, and subjective intensifiers to detect bias-inducing words. In BIBREF2, the authors extend this work by mitigating subjective bias after detecting bias-inducing words using a BERT-based model. However, they primarily focused on detecting and mitigating subjective bias for single-word edits. We extend their work by incorporating multi-word edits by detecting bias at the sentence level. We further use their version of the NPOV corpus called Wiki Neutrality Corpus(WNC) for this work..The task of detecting sentences containing subjective bias rather than individual words inducing the bias has been explored in BIBREF3. However, they conduct majority of their experiments in controlled settings, limiting the type of articles from which the revisions were extracted. Their attempt to test their models in a general setting is dwarfed by the fact that they used revisions from a single Wikipedia article resulting in just 100 instances to evaluate their proposed models robustly. Consequently, we perform our experiments in the complete WNC corpus, which consists of $423,823$ revisions in Wikipedia marked by its editors over a period of 15 years, to simulate a more general setting for the bias..In this work, we investigate the application of BERT-based models for the task of subjective language detection. We explore various BERT-based models, including BERT, RoBERTa, ALBERT, with their base and large specifications along with their native classifiers. We propose an ensemble model exploiting predictions from these models using multiple ensembling techniques. We show that our model outperforms the baselines by a margin of $5.6$ of F1 score and $5.95\\%$ of Accuracy. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "36\n",
      "Question 1: Who provided detailed and helpful feedback for the text?\n",
      "Answer 1: Sebastian Ebert and Samuel Bowman provided detailed and helpful feedback for the text.\n",
      "Question : for the text We would like to thank Sebastian Ebert and Samuel Bowman for their detailed and helpful feedback. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "37\n",
      "What is the reason for lower Pearson and MSE correlations in Table 4 compared to Table 3?\n",
      "\n",
      "The reason for lower Pearson and MSE correlations in Table 4 compared to Table 3 is that the outputs of each given system in Table 4 are of comparable quality, resulting in similar datapoints that are easier to fit for the linear function used for MSE. Pearson, on the other hand, is lower due to its invariance to linear transformations of both variables.\n",
      "Question : for the text The results per compression system (cf. Table 4 ) look different from the correlations in Table 3 : Pearson and MSE are both lower. This is due to the outputs of each given system being of comparable quality. Therefore, the datapoints are similar and, thus, easier to fit for the linear function used for MSE. Pearson, in contrast, is lower due to its invariance to linear transformations of both variables. Note that this effect is smallest for ILP, which has uniformly distributed targets ( $\\text{Var}(Y) = 0.35$ vs. $\\text{Var}(Y) = 0.17$ for SEQ2SEQ)..Comparing the metrics, the two SLOR approaches perform best for SEQ2SEQ and T3. In particular, they outperform the best word-overlap metric baseline by $0.244$ and $0.097$ Pearson correlation as well as $0.012$ and $0.012$ MSE, respectively. Since T3 is an abstractive system, we can conclude that WordSLOR and WPSLOR are applicable even for systems that are not limited to make use of a fixed repertoire of words..For ILP and NAMAS, word-overlap metrics obtain best results. The differences in performance, however, are with a maximum difference of $0.072$ for Pearson and ILP much smaller than for SEQ2SEQ. Thus, while the differences are significant, word-overlap metrics do not outperform our SLOR approaches by a wide margin. Recall, additionally, that word-overlap metrics rely on references being available, while our proposed approaches do not require this. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "38\n",
      "What are the two SLOR-based metrics that perform best across all domains? \n",
      "\n",
      "The two SLOR-based metrics that perform best across all domains are WordSLOR and WPSLOR.\n",
      "Question : for the text Looking next at the correlations for all models but different domains (cf. Table 5 ), we first observe that the results across domains are similar, i.e., we do not observe the same effect as in Subsection \"Analysis I: Fluency Evaluation per Compression System\" . This is due to the distributions of scores being uniform ( $\\text{Var}(Y) \\in [0.28, 0.36]$ )..Next, we focus on an important question: How much does the performance of our SLOR-based metrics depend on the domain, given that the respective LMs are trained on Gigaword, which consists of news data?.Comparing the evaluation performance for individual metrics, we observe that, except for letters, WordSLOR and WPSLOR perform best across all domains: they outperform the best word-overlap metric by at least $0.019$ and at most $0.051$ Pearson correlation, and at least $0.004$ and at most $0.014$ MSE. The biggest difference in correlation is achieved for the journal domain. Thus, clearly even LMs which have been trained on out-of-domain data obtain competitive performance for fluency evaluation. However, a domain-specific LM might additionally improve the metrics' correlation with human judgments. We leave a more detailed analysis of the importance of the training data's domain for future work. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "39\n",
      "Question 1: What is ROUGE-L BIBREF1 and how is it used in compression tasks?\n",
      "\n",
      "Answer 1: ROUGE-L BIBREF1 is a baseline metric commonly used in compression tasks to measure the similarity of two sentences based on their longest common subsequence. Generated and reference compressions are tokenized and lowercased, and for multiple references, only the reference with the highest score is used for each example.\n",
      "Question : for the text Our first baseline is ROUGE-L BIBREF1 , since it is the most commonly used metric for compression tasks. ROUGE-L measures the similarity of two sentences based on their longest common subsequence. Generated and reference compressions are tokenized and lowercased. For multiple references, we only make use of the one with the highest score for each example..We compare to the best n-gram-overlap metrics from toutanova2016dataset; combinations of linguistic units (bi-grams (LR2) and tri-grams (LR3)) and scoring measures (recall (R) and F-score (F)). With multiple references, we consider the union of the sets of n-grams. Again, generated and reference compressions are tokenized and lowercased..We further compare to the negative LM cross-entropy, i.e., the log-probability which is only normalized by sentence length. The score of a sentence $S$ is calculated as .$$\\text{NCE}(S) = \\tfrac{1}{|S|} \\ln (p_M(S))$$   (Eq. 22) .with $p_M(S)$ being the probability assigned to the sentence by a LM. We employ the same LMs as for SLOR, i.e., LMs trained on words (WordNCE) and WordPieces (WPNCE)..Our next baseline is perplexity, which corresponds to the exponentiated cross-entropy: .$$\\text{PPL}(S) = \\exp (-\\text{NCE}(S))$$   (Eq. 24) .Due to its popularity, we also performed initial experiments with BLEU BIBREF17 . Its correlation with human scores was so low that we do not consider it in our final experiments. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "40\n",
      "Question 1: What are the shortcomings of using word-overlap metrics for automatic compression evaluation?\n",
      "\n",
      "Answer 1: Word-overlap metrics, like ROUGE, have shortcomings when used for compression evaluation as they correlate best for extractive compression and may not generalize well for abstractive systems.\n",
      "Question : for the text Automatic compression evaluation has mostly had a strong focus on content. Hence, word-overlap metrics like ROUGE BIBREF1 have been widely used for compression evaluation. However, they have certain shortcomings, e.g., they correlate best for extractive compression, while we, in contrast, are interested in an approach which generalizes to abstractive systems. Alternatives include success rate BIBREF28 , simple accuracy BIBREF29 , which is based on the edit distance between the generation and the reference, or word accuracy BIBREF30 , the equivalent for multiple references. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "41\n",
      "What is SLOR and why is it effective for evaluating fluency in automatic compression at the sentence level?\n",
      "\n",
      "SLOR is a LM score that takes into account sentence length and unigram probabilities, and it is effective for evaluating fluency in automatic compression at the sentence level because it correlates significantly better with fluency ratings than traditional word-overlap metrics on a benchmark dataset.\n",
      "Question : for the text We empirically confirmed the effectiveness of SLOR, a LM score which accounts for the effects of sentence length and individual unigram probabilities, as a metric for fluency evaluation of the NLG task of automatic compression at the sentence level. We further introduced WPSLOR, an adaptation of SLOR to WordPieces, which reduced both model size and training time at a similar evaluation performance. Our experiments showed that our proposed referenceless metrics correlate significantly better with fluency ratings for the outputs of compression systems than traditional word-overlap metrics on a benchmark dataset. Additionally, they can be applied even in settings where no references are available, or would be costly to obtain. Finally, for given references, we proposed the reference-based metric ROUGE-LM, which consists of a combination of WPSLOR and ROUGE. Thus, we were able to obtain an even more accurate fluency evaluation. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "42\n",
      "Question 1: What is the advantage of using mean squared error (MSE) instead of Pearson correlation for evaluating metrics?\n",
      "Answer 1: The advantage of using MSE is that it has an interpretable meaning: the expected error made by a given metric as compared to the human rating. Additionally, MSE is able to accurately judge a metric's performance for sentences of very similar quality, unlike Pearson correlation which is either not defined or 0 in such cases.\n",
      "Question : for the text Following earlier work BIBREF2 , we evaluate our metrics using Pearson correlation with human judgments. It is defined as the covariance divided by the product of the standard deviations: .$$\\rho _{X,Y} = \\frac{\\text{cov}(X,Y)}{\\sigma _X \\sigma _Y}$$   (Eq. 28) .Pearson cannot accurately judge a metric's performance for sentences of very similar quality, i.e., in the extreme case of rating outputs of identical quality, the correlation is either not defined or 0, caused by noise of the evaluation model. Thus, we additionally evaluate using mean squared error (MSE), which is defined as the squares of residuals after a linear transformation, divided by the sample size: .$$\\text{MSE}_{X,Y} = \\underset{f}{\\min }\\frac{1}{|X|}\\sum \\limits _{i = 1}^{|X|}{(f(x_i) - y_i)^2}$$   (Eq. 30) .with $f$ being a linear function. Note that, since MSE is invariant to linear transformations of $X$ but not of $Y$ , it is a non-symmetric quasi-metric. We apply it with $Y$ being the human ratings. An additional advantage as compared to Pearson is that it has an interpretable meaning: the expected error made by a given metric as compared to the human rating. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "43\n",
      "Question 1: Why has there been criticism of evaluating NLG tasks with word-overlap metrics?\n",
      "\n",
      "Answer 1: The criticism stems from the need for better evaluation of machine translation and unsupervised dialogue generation tasks as it has been shown that automatic metrics such as BLEU, ROUGE, and METEOR do not correlate strongly with human judgments and may not be reliable. The need for better metrics that correlate more strongly with human judgments has been emphasized, and alternative approaches such as the challenge set approach and human-targeted metrics have been proposed.\n",
      "Question : for the text In the sense that we promote an explicit evaluation of fluency, our work is in line with previous criticism of evaluating NLG tasks with a single score produced by word-overlap metrics..The need for better evaluation for machine translation (MT) was expressed, e.g., by callison2006re, who doubted the meaningfulness of BLEU, and claimed that a higher BLEU score was neither a necessary precondition nor a proof of improved translation quality. Similarly, song2013bleu discussed BLEU being unreliable at the sentence or sub-sentence level (in contrast to the system-level), or for only one single reference. This was supported by isabelle-cherry-foster:2017:EMNLP2017, who proposed a so-called challenge set approach as an alternative. graham-EtAl:2016:COLING performed a large-scale evaluation of human-targeted metrics for machine translation, which can be seen as a compromise between human evaluation and fully automatic metrics. They also found fully automatic metrics to correlate only weakly or moderately with human judgments. bojar2016ten further confirmed that automatic MT evaluation methods do not perform well with a single reference. The need of better metrics for MT has been addressed since 2008 in the WMT metrics shared task BIBREF31 , BIBREF32 ..For unsupervised dialogue generation, liu-EtAl:2016:EMNLP20163 obtained close to no correlation with human judgements for BLEU, ROUGE and METEOR. They contributed this in a large part to the unrestrictedness of dialogue answers, which makes it hard to match given references. They emphasized that the community should move away from these metrics for dialogue generation tasks, and develop metrics that correlate more strongly with human judgments. elliott-keller:2014:P14-2 reported the same for BLEU and image caption generation. duvsek2017referenceless suggested an RNN to evaluate NLG at the utterance level, given only the input meaning representation. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "44\n",
      "What is the best compression system for fluency according to the experiment on the compression dataset? \n",
      "\n",
      "ILP produces the best output according to the experiment on the compression dataset.\n",
      "Question : for the text We experiment on the compression dataset by toutanova2016dataset. It contains single sentences and two-sentence paragraphs from the Open American National Corpus (OANC), which belong to 4 genres: newswire, letters, journal, and non-fiction. Gold references are manually created and the outputs of 4 compression systems (ILP (extractive), NAMAS (abstractive), SEQ2SEQ (extractive), and T3 (abstractive); cf. toutanova2016dataset for details) for the test data are provided. Each example has 3 to 5 independent human ratings for content and fluency. We are interested in the latter, which is rated on an ordinal scale from 1 (disfluent) through 3 (fluent). We experiment on the 2955 system outputs for the test split..Average fluency scores per system are shown in Table 2 . As can be seen, ILP produces the best output. In contrast, NAMAS is the worst system for fluency. In order to be able to judge the reliability of the human annotations, we follow the procedure suggested by TACL732 and used by toutanova2016dataset, and compute the quadratic weighted $\\kappa $ BIBREF14 for the human fluency scores of the system-generated compressions as $0.337$ . generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "45\n",
      "Question 1: What is the purpose of the main experiment presented in the text?\n",
      "\n",
      "Answer 1: The purpose of the main experiment presented in the text is to evaluate the performances of WordSLOR and WPSLOR as fluency evaluation metrics.\n",
      "Question : for the text Now, we present our main experiment, in which we assess the performances of WordSLOR and WPSLOR as fluency evaluation metrics. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "46\n",
      "What is the purpose of using available fluency annotations in the first setting?\n",
      "The purpose of using available fluency annotations in the first setting is to allow for a proof of concept.\n",
      "Question : for the text First, we assume a setting in which we have the following available: (i) system outputs whose fluency is to be evaluated, (ii) reference generations for evaluating system outputs, (iii) a small set of system outputs with references, which has been annotated for fluency by human raters, and (iv) a large unlabeled corpus for training a LM. Note that available fluency annotations are often uncommon in real-world scenarios; the reason we use them is that they allow for a proof of concept. In this setting, we train scikit's BIBREF18 support vector regression model (SVR) with the default parameters on predicting fluency, given WPSLOR and ROUGE-L-mult. We use 500 of our total 2955 examples for each of training and development, and the remaining 1955 for testing..Second, we simulate a setting in which we have only access to (i) system outputs which should be evaluated on fluency, (ii) reference compressions, and (iii) large amounts of unlabeled text. In particular, we assume to not have fluency ratings for system outputs, which makes training a regression model impossible. Note that this is the standard setting in which word-overlap metrics are applied. Under these conditions, we propose to normalize both given scores by mean and variance, and to simply add them up. We call this new reference-based metric ROUGE-LM. In order to make this second experiment comparable to the SVR-based one, we use the same 1955 test examples. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "47\n",
      "Question 1: What is the main difference between fluency evaluation and grammatical error detection/correction in NLP?\n",
      "\n",
      "Answer 1: While fluency evaluation is also related to grammatical error detection and correction, it differs in that it focuses on how errors affect the readability and comprehension of the text, rather than simply identifying or correcting them. In other words, fluency evaluation is concerned with how errors matter to humans.\n",
      "Question : for the text Fluency evaluation is related to grammatical error detection BIBREF19 , BIBREF20 , BIBREF21 , BIBREF22 and grammatical error correction BIBREF23 , BIBREF24 , BIBREF25 , BIBREF26 , BIBREF27 . However, it differs from those in several aspects; most importantly, it is concerned with the degree to which errors matter to humans..Work on automatic fluency evaluation in NLP has been rare. heilman2014predicting predicted the fluency (which they called grammaticality) of sentences written by English language learners. In contrast to ours, their approach is supervised. stent2005evaluating and cahill2009correlating found only low correlation between automatic metrics and fluency ratings for system-generated English paraphrases and the output of a German surface realiser, respectively. Explicit fluency evaluation of NLG, including compression and the related task of summarization, has mostly been performed manually. vadlapudi-katragadda:2010:SRW used LMs for the evaluation of summarization fluency, but their models were based on part-of-speech tags, which we do not require, and they were non-neural. Further, they evaluated longer texts, not single sentences like we do. toutanova2016dataset compared 80 word-overlap metrics for evaluating the content and fluency of compressions, finding only low correlation with the latter. However, they did not propose an alternative evaluation. We aim at closing this gap. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "48\n",
      "What is the hypothesis mentioned in the passage regarding ROUGE and SLOR?\n",
      "\n",
      "The hypothesis mentioned in the passage is that ROUGE should contain information about fluency that is complementary to SLOR, and that references can be used for fluency evaluation if available.\n",
      "Question : for the text ROUGE was shown to correlate well with ratings of a generated text's content or meaning at the sentence level BIBREF2 . We further expect content and fluency ratings to be correlated. In fact, sometimes it is difficult to distinguish which one is problematic: to illustrate this, we show some extreme examples—compressions which got the highest fluency rating and the lowest possible content rating by at least one rater, but the lowest fluency score and the highest content score by another—in Table 6 . We, thus, hypothesize that ROUGE should contain information about fluency which is complementary to SLOR, and want to make use of references for fluency evaluation, if available. In this section, we experiment with two reference-based metrics – one trainable one, and one that can be used without fluency annotations, i.e., in the same settings as pure word-overlap metrics. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "49\n",
      "What is the importance of fluency evaluation in natural language generation systems?\n",
      " \n",
      "Fluency evaluation is important in natural language generation systems because it helps in producing sentences that are perceived as natural by a human addressee. This leads to higher user satisfaction and user trust and makes interactions more natural, avoiding misunderstandings. However, fluency evaluation is a challenging task as NLG systems can generate in an abstractive way, making it difficult to match correct output with a finite number of given references.\n",
      "Question : for the text Producing sentences which are perceived as natural by a human addressee—a property which we will denote as fluency throughout this paper —is a crucial goal of all natural language generation (NLG) systems: it makes interactions more natural, avoids misunderstandings and, overall, leads to higher user satisfaction and user trust BIBREF0 . Thus, fluency evaluation is important, e.g., during system development, or for filtering unacceptable generations at application time. However, fluency evaluation of NLG systems constitutes a hard challenge: systems are often not limited to reusing words from the input, but can generate in an abstractive way. Hence, it is not guaranteed that a correct output will match any of a finite number of given references. This results in difficulties for current reference-based evaluation, especially of fluency, causing word-overlap metrics like ROUGE BIBREF1 to correlate only weakly with human judgments BIBREF2 . As a result, fluency evaluation of NLG is often done manually, which is costly and time-consuming..Evaluating sentences on their fluency, on the other hand, is a linguistic ability of humans which has been the subject of a decade-long debate in cognitive science. In particular, the question has been raised whether the grammatical knowledge that underlies this ability is probabilistic or categorical in nature BIBREF3 , BIBREF4 , BIBREF5 . Within this context, lau2017grammaticality have recently shown that neural language models (LMs) can be used for modeling human ratings of acceptability. Namely, they found SLOR BIBREF6 —sentence log-probability which is normalized by unigram log-probability and sentence length—to correlate well with acceptability judgments at the sentence level..However, to the best of our knowledge, these insights have so far gone disregarded by the natural language processing (NLP) community. In this paper, we investigate the practical implications of lau2017grammaticality's findings for fluency evaluation of NLG, using the task of automatic compression BIBREF7 , BIBREF8 as an example (cf. Table 1 ). Specifically, we test our hypothesis that SLOR should be a suitable metric for evaluation of compression fluency which (i) does not rely on references; (ii) can naturally be applied at the sentence level (in contrast to the system level); and (iii) does not need human fluency annotations of any kind. In particular the first aspect, i.e., SLOR not needing references, makes it a promising candidate for automatic evaluation. Getting rid of human references has practical importance in a variety of settings, e.g., if references are unavailable due to a lack of resources for annotation, or if obtaining references is impracticable. The latter would be the case, for instance, when filtering system outputs at application time..We further introduce WPSLOR, a novel, WordPiece BIBREF9 -based version of SLOR, which drastically reduces model size and training time. Our experiments show that both approaches correlate better with human judgments than traditional word-overlap metrics, even though the latter do rely on reference compressions. Finally, investigating the case of available references and how to incorporate them, we combine WPSLOR and ROUGE to ROUGE-LM, a novel reference-based metric, and increase the correlation with human fluency ratings even further. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "50\n",
      "Question 1: What is the dataset used to train the LSTM LMs in this text?\n",
      "\n",
      "Answer 1: The LSTM LMs in this text are trained on the English Gigaword corpus, which consists of news data.\n",
      "Question : for the text We train our LSTM LMs on the English Gigaword corpus BIBREF15 , which consists of news data..The hyperparameters of all LMs are tuned using perplexity on a held-out part of Gigaword, since we expect LM perplexity and final evaluation performance of WordSLOR and, respectively, WPSLOR to correlate. Our best networks consist of two layers with 512 hidden units each, and are trained for $2,000,000$ steps with a minibatch size of 128. For optimization, we employ ADAM BIBREF16 . generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "1\n",
      "Question 1: What is SLOR and what is its intuition?\n",
      "\n",
      "Answer 1: SLOR is a score used to measure the fluency and proficiency of non-native speakers of a language. Its intuition is based on the notion of language complexity, in which an ideal speaker should use shorter and simpler sentences, and have fewer grammatical errors. Therefore, the higher the SLOR score, the better the language proficiency of the speaker.\n",
      "Question : for the text In this section, we first describe SLOR and the intuition behind this score. Then, we introduce WordPieces, before explaining how we combine the two. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "2\n",
      "What is the goal of the lau2017grammaticality study?\n",
      "The goal of the lau2017grammaticality study was to model human acceptability judgments automatically to gain insight into the nature of human perception of acceptability and to answer the question if humans judge acceptability on a gradient scale.\n",
      "Question : for the text Acceptability judgments, i.e., speakers' judgments of the well-formedness of sentences, have been the basis of much linguistics research BIBREF10 , BIBREF11 : a speakers intuition about a sentence is used to draw conclusions about a language's rules. Commonly, “acceptability” is used synonymously with “grammaticality”, and speakers are in practice asked for grammaticality judgments or acceptability judgments interchangeably. Strictly speaking, however, a sentence can be unacceptable, even though it is grammatical – a popular example is Chomsky's phrase “Colorless green ideas sleep furiously.” BIBREF3 In turn, acceptable sentences can be ungrammatical, e.g., in an informal context or in poems BIBREF12 ..Scientists—linguists, cognitive scientists, psychologists, and NLP researcher alike—disagree about how to represent human linguistic abilities. One subject of debates are acceptability judgments: while, for many, acceptability is a binary condition on membership in a set of well-formed sentences BIBREF3 , others assume that it is gradient in nature BIBREF13 , BIBREF2 . Tackling this research question, lau2017grammaticality aimed at modeling human acceptability judgments automatically, with the goal to gain insight into the nature of human perception of acceptability. In particular, they tried to answer the question: Do humans judge acceptability on a gradient scale? Their experiments showed a strong correlation between human judgments and normalized sentence log-probabilities under a variety of LMs for artificial data they had created by translating and back-translating sentences with neural models. While they tried different types of LMs, best results were obtained for neural models, namely recurrent neural networks (RNNs)..In this work, we investigate if approaches which have proven successful for modeling acceptability can be applied to the NLP problem of automatic fluency evaluation. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "3\n",
      "What two metrics correlate best with human judgments according to the study?\n",
      "WordSLOR and WPSLOR correlate best with human judgments according to the study.\n",
      "Question : for the text As shown in Table 3 , WordSLOR and WPSLOR correlate best with human judgments: WordSLOR (respectively WPSLOR) has a $0.025$ (respectively $0.008$ ) higher Pearson correlation than the best word-overlap metric ROUGE-L-mult, even though the latter requires multiple reference compressions. Furthermore, if we consider with ROUGE-L-single a setting with a single given reference, the distance to WordSLOR increases to $0.048$ for Pearson correlation. Note that, since having a single reference is very common, this result is highly relevant for practical applications. Considering MSE, the top two metrics are still WordSLOR and WPSLOR, with a $0.008$ and, respectively, $0.002$ lower error than the third best metric, ROUGE-L-mult. .Comparing WordSLOR and WPSLOR, we find no significant differences: $0.017$ for Pearson and $0.006$ for MSE. However, WPSLOR uses a more compact LM and, hence, has a shorter training time, since the vocabulary is smaller ( $16,000$ vs. $128,000$ tokens)..Next, we find that WordNCE and WPNCE perform roughly on par with word-overlap metrics. This is interesting, since they, in contrast to traditional metrics, do not require reference compressions. However, their correlation with human fluency judgments is strictly lower than that of their respective SLOR counterparts. The difference between WordSLOR and WordNCE is bigger than that between WPSLOR and WPNCE. This might be due to accounting for differences in frequencies being more important for words than for WordPieces. Both WordPPL and WPPPL clearly underperform as compared to all other metrics in our experiments..The traditional word-overlap metrics all perform similarly. ROUGE-L-mult and LR2-F-mult are best and worst, respectively..Results are shown in Table 7 . First, we can see that using SVR (line 1) to combine ROUGE-L-mult and WPSLOR outperforms both individual scores (lines 3-4) by a large margin. This serves as a proof of concept: the information contained in the two approaches is indeed complementary..Next, we consider the setting where only references and no annotated examples are available. In contrast to SVR (line 1), ROUGE-LM (line 2) has only the same requirements as conventional word-overlap metrics (besides a large corpus for training the LM, which is easy to obtain for most languages). Thus, it can be used in the same settings as other word-overlap metrics. Since ROUGE-LM—an uninformed combination—performs significantly better than both ROUGE-L-mult and WPSLOR on their own, it should be the metric of choice for evaluating fluency with given references. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "4\n",
      "Question 1: What is the intuition behind subtracting unigram log-probabilities in the SLOR scoring method?\n",
      "\n",
      "Answer 1: The intuition behind subtracting unigram log-probabilities is that a token which is rare on its own (in contrast to being rare at a given position in the sentence) should not bring down the sentence's rating.\n",
      "Question : for the text SLOR assigns to a sentence $S$ a score which consists of its log-probability under a given LM, normalized by unigram log-probability and length: .$$\\text{SLOR}(S) = &\\frac{1}{|S|} (\\ln (p_M(S)) \\\\\\nonumber &- \\ln (p_u(S)))$$   (Eq. 8) . where $p_M(S)$ is the probability assigned to the sentence under the LM. The unigram probability $p_u(S)$ of the sentence is calculated as .$$p_u(S) = \\prod _{t \\in S}p(t)$$   (Eq. 9) .with $p(t)$ being the unconditional probability of a token $t$ , i.e., given no context..The intuition behind subtracting unigram log-probabilities is that a token which is rare on its own (in contrast to being rare at a given position in the sentence) should not bring down the sentence's rating. The normalization by sentence length is necessary in order to not prefer shorter sentences over equally fluent longer ones. Consider, for instance, the following pair of sentences: .$$\\textrm {(i)} ~ ~ &\\textrm {He is a citizen of France.}\\nonumber \\\\\n",
      "\\textrm {(ii)} ~ ~ &\\textrm {He is a citizen of Tuvalu.}\\nonumber $$   (Eq. 11) . Given that both sentences are of equal length and assuming that France appears more often in a given LM training set than Tuvalu, the length-normalized log-probability of sentence (i) under the LM would most likely be higher than that of sentence (ii). However, since both sentences are equally fluent, we expect taking each token's unigram probability into account to lead to a more suitable score for our purposes..We calculate the probability of a sentence with a long-short term memory (LSTM, hochreiter1997long) LM, i.e., a special type of RNN LM, which has been trained on a large corpus. More details on LSTM LMs can be found, e.g., in sundermeyer2012lstm. The unigram probabilities for SLOR are estimated using the same corpus. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "5\n",
      "Question 1: What is the advantage of using WPSLOR over WordSLOR?\n",
      "\n",
      "Answer 1: WPSLOR incorporates a language model (LM) trained on a corpus that has been split by a WordPiece model, resulting in a smaller vocabulary and a faster training time of around 12 hours compared to roughly 5 days for the word-based WordSLOR version.\n",
      "Question : for the text We propose a novel version of SLOR, by incorporating a LM which is trained on a corpus which has been split by a WordPiece model. This leads to a smaller vocabulary, resulting in a LM with less parameters, which is faster to train (around 12h compared to roughly 5 days for the word-based version in our experiments). We will refer to the word-based SLOR as WordSLOR and to our newly proposed WordPiece-based version as WPSLOR. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "6\n",
      "Question 1: What is the advantage of using Sub-word units like WordPieces in NLP?\n",
      "\n",
      "Answer 1: Sub-word units like WordPieces provide a compromise between characters and words, resulting in a smaller vocabulary, improved handling of rare words, and more information than characters. This reduces model size and training time and enhances NLP performance.\n",
      "Question : for the text Sub-word units like WordPieces BIBREF9 are getting increasingly important in NLP. They constitute a compromise between characters and words: On the one hand, they yield a smaller vocabulary, which reduces model size and training time, and improve handling of rare words, since those are partitioned into more frequent segments. On the other hand, they contain more information than characters..WordPiece models are estimated using a data-driven approach which maximizes the LM likelihood of the training corpus as described in wu2016google and 6289079. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "7\n",
      "Question 1: What were the sources of funding for this work?\n",
      "Answer 1: The work was partly supported by the Chist-Era project IGLU with contribution from the Belgian Fonds de la Recherche Scientique (FNRS), contract no. R.50.11.15.F, and by the FSO project VCYCLE with contribution from the Belgian Waloon Region, contract no. 1510501.\n",
      "Question : for the text This work was partly supported by the Chist-Era project IGLU with contribution from the Belgian Fonds de la Recherche Scientique (FNRS), contract no. R.50.11.15.F, and by the FSO project VCYCLE with contribution from the Belgian Waloon Region, contract no. 1510501. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "8\n",
      "Question 1: What is the common input used by all three models in the decoding phase?\n",
      "\n",
      "Answer 1: All three models use the annotation sequence as input in the decoding phase to derive a time-dependent context vector that contains relevant information in the image to help predict the current target word.\n",
      "Question : for the text We evaluate three models of the image attention mechanism INLINEFORM0 of equation EQREF11 . They have in common the fact that at each time step INLINEFORM1 of the decoding phase, all approaches first take as input the annotation sequence INLINEFORM2 to derive a time-dependent context vector that contain relevant information in the image to help predict the current target word INLINEFORM3 . Even though these models differ in how the time-dependent context vector is derived, they share the same subsequent steps. For each mechanism, we propose two hand-picked illustrations showing where the attention is placed in an image. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "9\n",
      "Question 1: What improvements were shown on the Flickr30K Entities dataset with the different attention mechanisms and tweaks for the image modality?\n",
      "\n",
      "Answer 1: The improvements and encouraging results were shown overall on the Flickr30K Entities dataset with the different attention mechanisms and tweaks for the image modality. Even though some flaws were identified, it can be concluded that images are a helpful resource for the machine in a translation task.\n",
      "Question : for the text We have tried different attention mechanism and tweaks for the image modality. We showed improvements and encouraging results overall on the Flickr30K Entities dataset. Even though we identified some flaws of the current attention mechanisms, we can conclude pretty safely that images are an helpful resource for the machine in a translation task. We are looking forward to try out richer and more suitable features for multimodal translation (ie. dense captioning features). Another interesting approach would be to use visually grounded word embeddings to capture visual notions of semantic relatedness. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "10\n",
      "Question 1: What is the structure of the conditional GRU model?\n",
      "Answer 1: The conditional GRU model consists of two stacked GRU activations called INLINEFORM0 and INLINEFORM1 and an attention mechanism INLINEFORM2 in between. The first recurrent cell computes a hidden state proposal, while the attention mechanism computes attention over the source sentence. Finally, the second recurrent cell computes the hidden state by looking at the intermediate representation and context vector.\n",
      "Question : for the text The conditional GRU consists of two stacked GRU activations called INLINEFORM0 and INLINEFORM1 and an attention mechanism INLINEFORM2 in between (called ATT in the footnote paper). At each time-step INLINEFORM3 , REC1 firstly computes a hidden state proposal INLINEFORM4 based on the previous hidden state INLINEFORM5 and the previously emitted word INLINEFORM6 : DISPLAYFORM0 . Then, the attention mechanism computes INLINEFORM0 over the source sentence using the annotations sequence INLINEFORM1 and the intermediate hidden state proposal INLINEFORM2 : DISPLAYFORM0 .Finally, the second recurrent cell INLINEFORM0 , computes the hidden state INLINEFORM1 of the INLINEFORM2 by looking at the intermediate representation INLINEFORM3 and context vector INLINEFORM4 : DISPLAYFORM0  generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "11\n",
      "Question 1: What dataset was used for the Multimodal Machine Translation experiments?\n",
      "\n",
      "Answer 1: The Multi30K dataset BIBREF17, which is an extended version of the Flickr30K Entities, was used for Multimodal Machine Translation experiments.\n",
      "Question : for the text For this experiments on Multimodal Machine Translation, we used the Multi30K dataset BIBREF17 which is an extended version of the Flickr30K Entities. For each image, one of the English descriptions was selected and manually translated into German by a professional translator. As training and development data, 29,000 and 1,014 triples are used respectively. A test set of size 1000 is used for metrics evaluation. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "12\n",
      "What is the mechanism used for making a hard choice to attend only one annotation in the model?\n",
      "The mechanism used is a function that returns a sampled intermediate latent variable based upon a multinomial distribution parameterized by an indicator one-hot variable.\n",
      "Question : for the text This model is a stochastic and sampling-based process where, at every timestep INLINEFORM0 , we are making a hard choice to attend only one annotation. This corresponds to one spatial location in the image. Hard attention has previously been used in the context of object recognition BIBREF11 , BIBREF12 and later extended to image description generation BIBREF2 . In the context of multimodal NMT, we can follow BIBREF2 icml2015xuc15 because both our models involve the same process on images..The mechanism INLINEFORM0 is now a function that returns a sampled intermediate latent variables INLINEFORM1 based upon a multinouilli distribution parameterized by INLINEFORM2 : DISPLAYFORM0 .where INLINEFORM0 an indicator one-hot variable which is set to 1 if the INLINEFORM1 -th annotation (out of INLINEFORM2 ) is the one used to compute the context vector INLINEFORM3 : DISPLAYFORM0 . Context vector INLINEFORM0 is now seen as the random variable of this distribution. We define the variational lower bound INLINEFORM1 on the marginal log evidence INLINEFORM2 of observing the target sentence INLINEFORM3 given modality annotations INLINEFORM4 . DISPLAYFORM0 .The learning rules can be derived by taking derivatives of the above variational free energy INLINEFORM0 with respect to the model parameter INLINEFORM1 : DISPLAYFORM0 .In order to propagate a gradient through this process, the summation in equation EQREF26 can then be approximated using Monte Carlo based sampling defined by equation EQREF24 : DISPLAYFORM0 .To reduce variance of the estimator in equation EQREF27 , we use a moving average baseline estimated as an accumulated sum of the previous log likelihoods with exponential decay upon seeing the INLINEFORM0 -th mini-batch: DISPLAYFORM0  generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "13\n",
      "What are the three optimizations added to the attention mechanism regarding the image modality?\n",
      "\n",
      "The three optimizations added to the attention mechanism regarding the image modality are computing a gating scalar, doubling the output size of trainable parameters, and using a grounding attention. These optimizations lead to a better use of the image by the model and improved translation scores overall.\n",
      "Question : for the text Three optimizations can be added to the attention mechanism regarding the image modality. All lead to a better use of the image by the model and improved the translation scores overall..At every decoding step INLINEFORM0 , we compute a gating scalar INLINEFORM1 according to the previous decoder state INLINEFORM2 : DISPLAYFORM0 .It is then used to compute the time-dependent image context vector : DISPLAYFORM0 . BIBREF2 icml2015xuc15 empirically found it to put more emphasis on the objects in the image descriptions generated with their model..We also double the output size of trainable parameters INLINEFORM0 , INLINEFORM1 and INLINEFORM2 in equation EQREF18 when it comes to compute the expected annotations over the image annotation sequence. More formally, given the image annotation sequence INLINEFORM3 , the tree matrices are of size INLINEFORM4 , INLINEFORM5 and INLINEFORM6 respectively. We noticed a better coverage of the objects in the image by the alpha weights..Lastly, we use a grounding attention inspired by BIBREF15 delbrouck2017multimodal. The mechanism merge each spatial location INLINEFORM0 in the annotation sequence INLINEFORM1 with the initial decoder state INLINEFORM2 obtained in equation EQREF7 with non-linearity : DISPLAYFORM0 . where INLINEFORM0 is INLINEFORM1 function. The new annotations go through a L2 normalization layer followed by two INLINEFORM2 convolutional layers (of size INLINEFORM3 respectively) to obtain INLINEFORM4 weights, one for each spatial location. We normalize the weights with a softmax to obtain a soft attention map INLINEFORM5 . Each annotation INLINEFORM6 is then weighted according to its corresponding INLINEFORM7 : DISPLAYFORM0 . This method can be seen as the removal of unnecessary information in the image annotations according to the source sentence. This attention is used on top of the others - before decoding - and is referred as \"grounded image\" in Table TABREF41 . generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "14\n",
      "What is the attention mechanism in the neural network model used for machine translation?\n",
      "\n",
      "The attention mechanism in the neural network model used for machine translation learns to focus on different parts of the input sentence while decoding, allowing it to capture the most relevant information and output the best translation. It has also shown to work with other modalities such as images, where it can learn to attend to the salient parts of an image.\n",
      "Question : for the text In machine translation, neural networks have attracted a lot of research attention. Recently, the attention-based encoder-decoder framework BIBREF0 , BIBREF1 has been largely adopted. In this approach, Recurrent Neural Networks (RNNs) map source sequences of words to target sequences. The attention mechanism is learned to focus on different parts of the input sentence while decoding. Attention mechanisms have shown to work with other modalities too, like images, where their are able to learn to attend the salient parts of an image, for instance when generating text captions BIBREF2 . For such applications, Convolutional Neural Networks (CNNs) such as Deep Residual BIBREF3 have shown to work best to represent images..Multimodal models of texts and images empower new applications such as visual question answering or multimodal caption translation. Also, the grounding of multiple modalities against each other may enable the model to have a better understanding of each modality individually, such as in natural language understanding applications..In the field of Machine Translation (MT), the efficient integration of multimodal information still remains a challenging task. It requires combining diverse modality vector representations with each other. These vector representations, also called context vectors, are computed in order the capture the most relevant information in a modality to output the best translation of a sentence..To investigate the effectiveness of information obtained from images, a multimodal machine translation shared task BIBREF4 has been addressed to the MT community. The best results of NMT model were those of BIBREF5 huang2016attention who used LSTM fed with global visual features or multiple regional visual features followed by rescoring. Recently, BIBREF6 CalixtoLC17b proposed a doubly-attentive decoder that outperformed this baseline with less data and without rescoring..Our paper is structured as follows. In section SECREF2 , we briefly describe our NMT model as well as the conditional GRU activation used in the decoder. We also explain how multi-modalities can be implemented within this framework. In the following sections ( SECREF3 and SECREF4 ), we detail three attention mechanisms and explain how we tweak them to work as well as possible with images. Finally, we report and analyze our results in section SECREF5 then conclude in section SECREF6 . generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "15\n",
      "What is local attention and how is it different from soft and stochastic attention?\n",
      "Local attention is a mechanism that focuses on a small subset of the image annotations, selecting a patch in the annotation sequence and selectively focusing on a small window of context around it. This approach is a trade-off between the soft and hard attentional models and is differentiable, whereas the stochastic attention requires more complicated techniques such as variance reduction and reinforcement learning to train. The soft attention attends the whole image, which can be difficult to learn, especially because the number of annotations is usually large.\n",
      "Question : for the text In this section, we propose a local attentional mechanism that chooses to focus only on a small subset of the image annotations. Local Attention has been used for text-based translation BIBREF13 and is inspired by the selective attention model of BIBREF14 gregor15 for image generation. Their approach allows the model to select an image patch of varying location and zoom. Local attention uses instead the same \"zoom\" for all target positions and still achieved good performance. This model can be seen as a trade-off between the soft and hard attentional models. The model picks one patch in the annotation sequence (one spatial location) and selectively focuses on a small window of context around it. Even though an image can't be seen as a temporal sequence, we still hope that the model finds points of interest and selects the useful information around it. This approach has an advantage of being differentiable whereas the stochastic attention requires more complicated techniques such as variance reduction and reinforcement learning to train as shown in section SECREF22 . The soft attention has the drawback to attend the whole image which can be difficult to learn, especially because the number of annotations INLINEFORM0 is usually large (presumably to keep a significant spatial granularity)..More formally, at every decoding step INLINEFORM0 , the model first generates an aligned position INLINEFORM1 . Context vector INLINEFORM2 is derived as a weighted sum over the annotations within the window INLINEFORM3 where INLINEFORM4 is a fixed model parameter chosen empirically. These selected annotations correspond to a squared region in the attention maps around INLINEFORM7 . The attention mask INLINEFORM8 is of size INLINEFORM9 . The model predicts INLINEFORM10 as an aligned position in the annotation sequence (referred as Predictive alignment (local-m) in the author's paper) according to the following equation: DISPLAYFORM0 .where INLINEFORM0 and INLINEFORM1 are both trainable model parameters and INLINEFORM2 is the annotation sequence length INLINEFORM3 . Because of the sigmoid, INLINEFORM4 . We use equation EQREF18 and EQREF19 respectively to compute the expected alignment vector INLINEFORM5 and the attention mask INLINEFORM6 . In addition, a Gaussian distribution centered around INLINEFORM7 is placed on the alphas in order to favor annotations near INLINEFORM8 : DISPLAYFORM0 .where standard deviation INLINEFORM0 . We obtain context vector INLINEFORM1 by following equation . generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "16\n",
      "What is the MNMT model proposed by CalixtoLC17b and how is it different from the attention-based NMT model?\n",
      "\n",
      "The MNMT model proposed by CalixtoLC17b is a doubly attentive decoder, which extends the attention-based NMT model. It computes a new time-dependent context vector based on the same intermediate hidden state proposal and uses it as an additional input to a modified version of the REC2 model. The probabilities for the next target word also take into account the new context vector. In multimodal NMT, the second modality is usually an image computed into feature maps with the help of a CNN. The annotations are spatial features.\n",
      "Question : for the text Recently, BIBREF6 CalixtoLC17b proposed a doubly attentive decoder (referred as the \"MNMT\" model in the author's paper) which can be seen as an expansion of the attention-based NMT model proposed in the previous section. Given a sequence of second a modality annotations INLINEFORM0 , we also compute a new context vector based on the same intermediate hidden state proposal INLINEFORM1 : DISPLAYFORM0 .This new time-dependent context vector is an additional input to a modified version of REC2 which now computes the final hidden state INLINEFORM0 using the intermediate hidden state proposal INLINEFORM1 and both time-dependent context vectors INLINEFORM2 and INLINEFORM3 : DISPLAYFORM0 . The probabilities for the next target word (from equation EQREF5 ) also takes into account the new context vector INLINEFORM0 : DISPLAYFORM0 .where INLINEFORM0 is a new trainable parameter..In the field of multimodal NMT, the second modality is usually an image computed into feature maps with the help of a CNN. The annotations INLINEFORM0 are spatial features (i.e. each annotation represents features for a specific region in the image) . We follow the same protocol for our experiments and describe it in section SECREF5 . generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "17\n",
      "Question 1: What is the architecture used for neural machine translation in this section?\n",
      "\n",
      "Answer 1: The architecture used for neural machine translation in this section is the attention-based encoder-decoder framework with recurrent neural networks, as implemented by BahdanauCB14.\n",
      "Question : for the text In this section, we detail the neural machine translation architecture by BIBREF1 BahdanauCB14, implemented as an attention-based encoder-decoder framework with recurrent neural networks (§ SECREF2 ). We follow by explaining the conditional GRU layer (§ SECREF8 ) - the gating mechanism we chose for our RNN - and how the model can be ported to a multimodal version (§ SECREF13 ). generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "18\n",
      "Question 1: How do the attention mechanisms in the multimodal translation model detect objects in an image?\n",
      "\n",
      "Answer 1: The attention mechanisms in the multimodal translation model have learned to detect objects within a scene, and the gating scalar decides whether or not to look at the picture depending on whether or not the word being decoded is an object. This is illustrated by the fact that the scores of the translation model undergo a massive drop without the gating scalar, indicating that the attention mechanisms don't understand the more complex relationships between objects in the scene. However, it is also noted that the machine could miss important information for a multimodal translation task since the features used by the attention mechanism are strongly object-oriented.\n",
      "Question : for the text For space-saving and ergonomic reasons, we only discuss about the hard stochastic and soft attention, the latter being a generalization of the local attention..As we can see in Figure FIGREF44 , the soft attention model is looking roughly at the same region of the image for every decoding step INLINEFORM0 . Because the words \"hund\"(dog), \"wald\"(forest) or \"weg\"(way) in left image are objects, they benefit from a high gating scalar. As a matter of fact, the attention mechanism has learned to detect the objects within a scene (at every time-step, whichever word we are decoding as shown in the right image) and the gating scalar has learned to decide whether or not we have to look at the picture (or more accurately whether or not we are translating an object). Without this scalar, the translation scores undergo a massive drop (as seen in BIBREF16 caglayan2016does) which means that the attention mechanisms don't really understand the more complex relationships between objects, what is really happening in the scene. Surprisingly, the gating scalar happens to be really low in the stochastic attention mechanism: a significant amount of sentences don't have a summed gating scalar INLINEFORM1 0.10. The model totally discards the image in the translation process..It is also worth to mention that we use a ResNet trained on 1.28 million images for a classification tasks. The features used by the attention mechanism are strongly object-oriented and the machine could miss important information for a multimodal translation task. We believe that the robust architecture of both encoders INLINEFORM0 combined with a GRU layer and word-embeddings took care of the right translation for relationships between objects and time-dependencies. Yet, we noticed a common misbehavior for all our multimodal models: if the attention loose track of the objects in the picture and \"gets lost\", the model still takes it into account and somehow overrides the information brought by the text-based annotations. The translation is then totally mislead. We illustrate with an example:.The monomodal translation has a sentence-level BLEU of 82.16 whilst the soft attention and hard stochastic attention scores are of 16.82 and 34.45 respectively. Figure FIGREF47 shows the attention maps for both mechanism. Nevertheless, one has to concede that the use of images indubitably helps the translation as shown in the score tabular. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "19\n",
      "What is the largest improvement in metrics seen with the use of hard stochastic attention mechanism? \n",
      "\n",
      "The largest improvement in metrics seen with the use of hard stochastic attention mechanism is +0.4 METEOR.\n",
      "Question : for the text We notice a nice overall progress over BIBREF6 CalixtoLC17b multimodal baseline, especially when using the stochastic attention. With improvements of +1.51 BLEU and -2.2 TER on both precision-oriented metrics, the model shows a strong similarity of the n-grams of our candidate translations with respect to the references. The more recall-oriented metrics METEOR scores are roughly the same across our models which is expected because all attention mechanisms share the same subsequent step at every time-step INLINEFORM0 , i.e. taking into account the attention weights of previous time-step INLINEFORM1 in order to compute the new intermediate hidden state proposal and therefore the new context vector INLINEFORM2 . Again, the largest improvement is given by the hard stochastic attention mechanism (+0.4 METEOR): because it is modeled as a decision process according to the previous choices, this may reinforce the idea of recall. We also remark interesting improvements when using the grounded mechanism, especially for the soft attention. The soft attention may benefit more of the grounded image because of the wide range of spatial locations it looks at, especially compared to the stochastic attention. This motivates us to dig into more complex grounding techniques in order to give the machine a deeper understanding of the modalities..Note that even though our baseline NMT model is basically the same as BIBREF6 CalixtoLC17b, our experiments results are slightly better. This is probably due to the different use of dropout and subwords. We also compared our results to BIBREF16 caglayan2016does because our multimodal models are nearly identical with the major exception of the gating scalar (cfr. section SECREF4 ). This motivated some of our qualitative analysis and hesitation towards the current architecture in the next section. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "20\n",
      "What is the soft attentional model?\n",
      "\n",
      "The soft attentional model is a mechanism used in natural language processing tasks, such as translation and image description generation, which considers all annotations when deriving the context vector. It consists of a single feed-forward network that computes an expected alignment between modality annotations and the target word to be emitted at the current time step. The modality time-dependent context vector is then computed as a weighted sum over the annotation vectors.\n",
      "Question : for the text Soft attention has firstly been used for syntactic constituency parsing by BIBREF10 NIPS2015Vinyals but has been widely used for translation tasks ever since. One should note that it slightly differs from BIBREF1 BahdanauCB14 where their attention takes as input the previous decoder hidden state instead of the current (intermediate) one as shown in equation EQREF11 . This mechanism has also been successfully investigated for the task of image description generation BIBREF2 where a model generates an image's description in natural language. It has been used in multimodal translation as well BIBREF6 , for which it constitutes a state-of-the-art..The idea of the soft attentional model is to consider all the annotations when deriving the context vector INLINEFORM0 . It consists of a single feed-forward network used to compute an expected alignment INLINEFORM1 between modality annotation INLINEFORM2 and the target word to be emitted at the current time step INLINEFORM3 . The inputs are the modality annotations and the intermediate representation of REC1 INLINEFORM4 : DISPLAYFORM0 .The vector INLINEFORM0 has length INLINEFORM1 and its INLINEFORM2 -th item contains a score of how much attention should be put on the INLINEFORM3 -th annotation in order to output the best word at time INLINEFORM4 . We compute normalized scores to create an attention mask INLINEFORM5 over annotations: DISPLAYFORM0 . Finally, the modality time-dependent context vector INLINEFORM0 is computed as a weighted sum over the annotation vectors (equation ). In the above expressions, INLINEFORM1 , INLINEFORM2 and INLINEFORM3 are trained parameters. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "21\n",
      "Question 1: What is the neural network used for in this text?\n",
      "\n",
      "Answer 1: The neural network is used to directly model the conditional probability of the translation of a source sentence.\n",
      "Question : for the text Given a source sentence INLINEFORM0 , the neural network directly models the conditional probability INLINEFORM1 of its translation INLINEFORM2 . The network consists of one encoder and one decoder with one attention mechanism. The encoder computes a representation INLINEFORM3 for each source sentence and a decoder generates one target word at a time and by decomposing the following conditional probability : DISPLAYFORM0 .Each source word INLINEFORM0 and target word INLINEFORM1 are a column index of the embedding matrix INLINEFORM2 and INLINEFORM3 . The encoder is a bi-directional RNN with Gated Recurrent Unit (GRU) layers BIBREF7 , BIBREF8 , where a forward RNN INLINEFORM4 reads the input sequence as it is ordered (from INLINEFORM5 to INLINEFORM6 ) and calculates a sequence of forward hidden states INLINEFORM7 . A backward RNN INLINEFORM8 reads the sequence in the reverse order (from INLINEFORM9 to INLINEFORM10 ), resulting in a sequence of backward hidden states INLINEFORM11 . We obtain an annotation for each word INLINEFORM12 by concatenating the forward and backward hidden state INLINEFORM13 . Each annotation INLINEFORM14 contains the summaries of both the preceding words and the following words. The representation INLINEFORM15 for each source sentence is the sequence of annotations INLINEFORM16 ..The decoder is an RNN that uses a conditional GRU (cGRU, more details in § SECREF8 ) with an attention mechanism to generate a word INLINEFORM0 at each time-step INLINEFORM1 . The cGRU uses it's previous hidden state INLINEFORM2 , the whole sequence of source annotations INLINEFORM3 and the previously decoded symbol INLINEFORM4 in order to update it's hidden state INLINEFORM5 : DISPLAYFORM0 .In the process, the cGRU also computes a time-dependent context vector INLINEFORM0 . Both INLINEFORM1 and INLINEFORM2 are further used to decode the next symbol. We use a deep output layer BIBREF9 to compute a vocabulary-sized vector : DISPLAYFORM0 .where INLINEFORM0 , INLINEFORM1 , INLINEFORM2 , INLINEFORM3 are model parameters. We can parameterize the probability of decoding each word INLINEFORM4 as: DISPLAYFORM0 .The initial state of the decoder INLINEFORM0 at time-step INLINEFORM1 is initialized by the following equation : DISPLAYFORM0 .where INLINEFORM0 is a feedforward network with one hidden layer. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "22\n",
      "What framework are all the models built on top of?\n",
      "\n",
      "All our models are built on top of the nematus framework (BIBREF18).\n",
      "Question : for the text All our models are build on top of the nematus framework BIBREF18 . The encoder is a bidirectional RNN with GRU, one 1024D single-layer forward and one 1024D single-layer backward RNN. Word embeddings for source and target language are of 620D and trained jointly with the model. Word embeddings and other non-recurrent matrices are initialized by sampling from a Gaussian INLINEFORM0 , recurrent matrices are random orthogonal and bias vectors are all initialized to zero..To create the image annotations used by our decoder, we used a ResNet-50 pre-trained on ImageNet and extracted the features of size INLINEFORM0 at its res4f layer BIBREF3 . In our experiments, our decoder operates on the flattened 196 INLINEFORM1 1024 (i.e INLINEFORM2 ). We also apply dropout with a probability of 0.5 on the embeddings, on the hidden states in the bidirectional RNN in the encoder as well as in the decoder. In the decoder, we also apply dropout on the text annotations INLINEFORM3 , the image features INLINEFORM4 , on both modality context vector and on all components of the deep output layer before the readout operation. We apply dropout using one same mask in all time steps BIBREF19 ..We also normalize and tokenize English and German descriptions using the Moses tokenizer scripts BIBREF20 . We use the byte pair encoding algorithm on the train set to convert space-separated tokens into subwords BIBREF21 , reducing our vocabulary size to 9226 and 14957 words for English and German respectively..All variants of our attention model were trained with ADADELTA BIBREF22 , with mini-batches of size 80 for our monomodal (text-only) NMT model and 40 for our multimodal NMT. We apply early stopping for model selection based on BLEU4 : training is halted if no improvement on the development set is observed for more than 20 epochs. We use the metrics BLEU4 BIBREF23 , METEOR BIBREF24 and TER BIBREF25 to evaluate the quality of our models' translations. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "23\n",
      "What is the main advantage of the proposed model over the baseline models?\n",
      "The main advantage of the proposed model over the baseline models is that it incorporates topic information, which helps to improve the accuracy in discriminating between correct comments and plausible comments.\n",
      "Question : for the text We analyze the performance of the proposed method under the semi-supervised setting. We train the supervised IR model with different numbers of paired data. Figure FIGREF39 shows the curve (blue) of the recall1 score. As expected, the performance grows as the paired dataset becomes larger. We further combine the supervised IR with our unsupervised model, which is trained with full unpaired data (4.8M) and different number of paired data (from 50K to 4.8M). It shows that IR+Proposed can outperform the supervised IR model given the same paired dataset. It concludes that the proposed model can exploit the unpaired data to further improve the performance of the supervised model..Although our proposed model can achieve better performance than previous models, there are still remaining two questions: why our model can outperform them, and how to further improve the performance. To address these queries, we perform error analysis to analyze the error types of our model and the baseline models. We select TF-IDF, S2S, and IR as the representative baseline models. We provide 200 unique comments as the candidate sets, which consists of four types of comments as described in the above retrieval evaluation setting: Correct, Plausible, Popular, and Random. We rank the candidate comment set with four models (TF-IDF, S2S, IR, and Proposed+IR), and record the types of top-1 comments..Figure FIGREF40 shows the percentage of different types of top-1 comments generated by each model. It shows that TF-IDF prefers to rank the plausible comments as the top-1 comments, mainly because it matches articles with the comments based on the similarity of the lexicon. Therefore, the plausible comments, which are more similar in the lexicon, are more likely to achieve higher scores than the correct comments. It also shows that the S2S model is more likely to rank popular comments as the top-1 comments. The reason is the S2S model suffers from the mode collapse problem and data sparsity, so it prefers short and general comments like “Great” or “That's right”, which appear frequently in the training set. The correct comments often contain new information and different language models from the training set, so they do not obtain a high score from S2S..IR achieves better performance than TF-IDF and S2S. However, it still suffers from the discrimination between the plausible comments and correct comments. This is mainly because IR does not explicitly model the underlying topics. Therefore, the correct comments which are more relevant in topic with the articles get lower scores than the plausible comments which are more literally relevant with the articles. With the help of our proposed model, proposed+IR achieves the best performance, and achieves a better accuracy to discriminate the plausible comments and the correct comments. Our proposed model incorporates the topic information, so the correct comments which are more similar to the articles in topic obtain higher scores than the other types of comments. According to the analysis of the error types of our model, we still need to focus on avoiding predicting the plausible comments. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "24\n",
      "Question 1: What is CommentIQ?\n",
      "\n",
      "Answer 1: CommentIQ is a system proposed by Park et al. that assists comment moderators in identifying high quality comments.\n",
      "Question : for the text There are few studies regarding machine commenting. Qin et al. QinEA2018 is the first to propose the article commenting task and a dataset, which is used to evaluate our model in this work. More studies about the comments aim to automatically evaluate the quality of the comments. Park et al. ParkSDE16 propose a system called CommentIQ, which assist the comment moderators in identifying high quality comments. Napoles et al. NapolesTPRP17 propose to discriminating engaging, respectful, and informative conversations. They present a Yahoo news comment threads dataset and annotation scheme for the new task of identifying “good” online conversations. More recently, Kolhaatkar and Taboada KolhatkarT17 propose a model to classify the comments into constructive comments and non-constructive comments. In this work, we are also inspired by the recent related work of natural language generation models BIBREF18 , BIBREF19 . generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "25\n",
      "Question 1: What are the unsupervised baseline models used for comparison with the proposed model?\n",
      "\n",
      "Answer 1: The unsupervised baseline models used for comparison with the proposed model are TF-IDF (Lexical, Non-Neural), LDA (Topic, Non-Neural), and NVDM (Lexical, Neural).\n",
      "Question : for the text We compare our model with several unsupervised models and supervised models..Unsupervised baseline models are as follows:.TF-IDF (Lexical, Non-Neural) is an important unsupervised baseline. We use the concatenation of the title and the body as the query to retrieve the candidate comment set by means of the similarity of the tf-idf value. The model is trained on unpaired articles and comments, which is the same as our proposed model..LDA (Topic, Non-Neural) is a popular unsupervised topic model, which discovers the abstract \"topics\" that occur in a collection of documents. We train the LDA with the articles and comments in the training set. The model retrieves the comments by the similarity of the topic representations..NVDM (Lexical, Neural) is a VAE-based approach for document modeling BIBREF10 . We compare our model with this baseline to demonstrate the effect of modeling topic..The supervised baseline models are:.S2S (Generative) BIBREF11 is a supervised generative model based on the sequence-to-sequence network with the attention mechanism BIBREF12 . The model uses the titles and the bodies of the articles as the encoder input, and generates the comments with the decoder..IR (Retrieval) BIBREF0 is a supervised retrieval-based model, which trains a convolutional neural network (CNN) to take the articles and a comment as inputs, and output the relevance score. The positive instances for training are the pairs in the training set, and the negative instances are randomly sampled using the negative sampling technique BIBREF13 . generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "26\n",
      "Question 1: What is the mode collapse problem in the sequence-to-sequence model for machine commenting and why does it occur?\n",
      "\n",
      "Answer 1: The mode collapse problem occurs in the sequence-to-sequence model for machine commenting because of the contradiction between the complex pattern of generating comments and the limited parallel data. Despite the input articles being various, the outputs of the model are very similar. The comments are often weakly related to the input articles, and part of the information in the comments is external. Therefore, it requires much more paired data for the supervised model to alleviate the mode collapse problem.\n",
      "Question : for the text Here, we first introduce the challenges of building a well-performed machine commenting system..The generative model, such as the popular sequence-to-sequence model, is a direct choice for supervised machine commenting. One can use the title or the content of the article as the encoder input, and the comments as the decoder output. However, we find that the mode collapse problem is severed with the sequence-to-sequence model. Despite the input articles being various, the outputs of the model are very similar. The reason mainly comes from the contradiction between the complex pattern of generating comments and the limited parallel data. In other natural language generation tasks, such as machine translation and text summarization, the target output of these tasks is strongly related to the input, and most of the required information is involved in the input text. However, the comments are often weakly related to the input articles, and part of the information in the comments is external. Therefore, it requires much more paired data for the supervised model to alleviate the mode collapse problem..One article can have multiple correct comments, and these comments can be very semantically different from each other. However, in the training set, there is only a part of the correct comments, so the other correct comments will be falsely regarded as the negative samples by the supervised model. Therefore, many interesting and informative comments will be discouraged or neglected, because they are not paired with the articles in the training set..There is a semantic gap between articles and comments. In machine translation and text summarization, the target output mainly shares the same points with the source input. However, in article commenting, the comments often have some external information, or even tell an opposite opinion from the articles. Therefore, it is difficult to automatically mine the relationship between articles and comments. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "27\n",
      "Question 1: What was the proposed method for training a machine commenting model?\n",
      "\n",
      "Answer 1: The proposed method for training a machine commenting model was using topics to bridge the semantic gap between articles and comments by introducing a variation topic model to represent the topics and matching the articles and comments by the similarity of their topics.\n",
      "Question : for the text We explore a novel way to train a machine commenting model in an unsupervised manner. According to the properties of the task, we propose using the topics to bridge the semantic gap between articles and comments. We introduce a variation topic model to represent the topics, and match the articles and comments by the similarity of their topics. Experiments show that our topic-based approach significantly outperforms previous lexicon-based models. The model can also profit from paired corpora and achieves state-of-the-art performance under semi-supervised scenarios. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "28\n",
      "Question 1: What is the source of the Chinese dataset used in this study?\n",
      "Answer 1: The dataset used in this study is collected from Tencent News, which is one of the most popular Chinese websites for news and opinion articles.\n",
      "Question : for the text We select a large-scale Chinese dataset BIBREF0 with millions of real comments and a human-annotated test set to evaluate our model. The dataset is collected from Tencent News, which is one of the most popular Chinese websites for news and opinion articles. The dataset consists of 198,112 news articles. Each piece of news contains a title, the content of the article, and a list of the users' comments. Following the previous work BIBREF0 , we tokenize all text with the popular python package Jieba, and filter out short articles with less than 30 words in content and those with less than 20 comments. The dataset is split into training/validation/test sets, and they contain 191,502/5,000/1,610 pieces of news, respectively. The whole dataset has a vocabulary size of 1,858,452. The average lengths of the article titles and content are 15 and 554 Chinese words. The average comment length is 17 words. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "29\n",
      "Question 1: What metrics were used to compare the model outputs with the references in generative evaluation?\n",
      "\n",
      "Answer 1: Four popular metrics (BLEU, METEOR, ROUGE, and CIDEr) were used to compare the model outputs with the references in generative evaluation.\n",
      "Question : for the text Following previous work BIBREF0 , we evaluate the models under the generative evaluation setting. The retrieval-based models generate the comments by selecting a comment from the candidate set. The candidate set contains the comments in the training set. Unlike the retrieval evaluation, the reference comments may not appear in the candidate set, which is closer to real-world settings. Generative-based models directly generate comments without a candidate set. We compare the generated comments of either the retrieval-based models or the generative models with the five reference comments. We select four popular metrics in text generation to compare the model outputs with the references: BLEU BIBREF14 , METEOR BIBREF15 , ROUGE BIBREF16 , CIDEr BIBREF17 ..Table TABREF32 shows the performance for our models and the baselines in generative evaluation. Similar to the retrieval evaluation, our proposed model outperforms the other unsupervised methods, which are TF-IDF, NVDM, and LDA, in generative evaluation. Still, the supervised IR achieves better scores than the seq2seq model. With the help of our proposed model, both IR and S2S achieve an improvement under the semi-supervised scenarios. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "30\n",
      "Question 1: What is the purpose of pruning the vocabulary in the model?\n",
      "\n",
      "Answer 1: The purpose of pruning the vocabulary in the model is to only leave the 30,000 most frequent words in order to reduce the size of the vocabulary and make the model more efficient.\n",
      "Question : for the text The hidden size of the model is 512, and the batch size is 64. The number of topics INLINEFORM0 is 100. The weight INLINEFORM1 in Eq. EQREF26 is 1.0 under the semi-supervised setting. We prune the vocabulary, and only leave 30,000 most frequent words in the vocabulary. We train the model for 20 epochs with the Adam optimizing algorithms BIBREF8 . In order to alleviate the KL vanishing problem, we set the initial learning to INLINEFORM2 , and use batch normalization BIBREF9 in each layer. We also gradually increase the KL term from 0 to 1 after each epoch. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "31\n",
      "What is the main challenge faced in building an intelligent and interactive agent to comment on articles?\n",
      "The main challenge faced in building an intelligent and interactive agent to comment on articles is the ability to comprehend the article, summarize the main ideas, mine the opinions, and generate natural language.\n",
      "Question : for the text Making article comments is a fundamental ability for an intelligent machine to understand the article and interact with humans. It provides more challenges because commenting requires the abilities of comprehending the article, summarizing the main ideas, mining the opinions, and generating the natural language. Therefore, machine commenting is an important problem faced in building an intelligent and interactive agent. Machine commenting is also useful in improving the activeness of communities, including online forums and news websites. Article comments can provide extended information and external opinions for the readers to have a more comprehensive understanding of the article. Therefore, an article with more informative and interesting comments will attract more attention from readers. Moreover, machine commenting can kick off the discussion about an article or a topic, which helps increase user engagement and interaction between the readers and authors..Because of the advantage and importance described above, more recent studies have focused on building a machine commenting system with neural models BIBREF0 . One bottleneck of neural machine commenting models is the requirement of a large parallel dataset. However, the naturally paired commenting dataset is loosely paired. Qin et al. QinEA2018 were the first to propose the article commenting task and an article-comment dataset. The dataset is crawled from a news website, and they sample 1,610 article-comment pairs to annotate the relevance score between articles and comments. The relevance score ranges from 1 to 5, and we find that only 6.8% of the pairs have an average score greater than 4. It indicates that the naturally paired article-comment dataset contains a lot of loose pairs, which is a potential harm to the supervised models. Besides, most articles and comments are unpaired on the Internet. For example, a lot of articles do not have the corresponding comments on the news websites, and the comments regarding the news are more likely to appear on social media like Twitter. Since comments on social media are more various and recent, it is important to exploit these unpaired data..Another issue is that there is a semantic gap between articles and comments. In machine translation and text summarization, the target output mainly shares the same points with the source input. However, in article commenting, the comment does not always tell the same thing as the corresponding article. Table TABREF1 shows an example of an article and several corresponding comments. The comments do not directly tell what happened in the news, but talk about the underlying topics (e.g. NBA Christmas Day games, LeBron James). However, existing methods for machine commenting do not model the topics of articles, which is a potential harm to the generated comments..To this end, we propose an unsupervised neural topic model to address both problems. For the first problem, we completely remove the need of parallel data and propose a novel unsupervised approach to train a machine commenting system, relying on nothing but unpaired articles and comments. For the second issue, we bridge the articles and comments with their topics. Our model is based on a retrieval-based commenting framework, which uses the news as the query to retrieve the comments by the similarity of their topics. The topic is represented with a variational topic, which is trained in an unsupervised manner..The contributions of this work are as follows: generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "32\n",
      "Question 1: What are some research challenges of machine commenting?\n",
      "\n",
      "Answer 1: Some research challenges of machine commenting include accurately understanding the context and tone of the conversation, generating relevant and engaging comments, avoiding offensive or inappropriate language, and maintaining consistency in the commenting style.\n",
      "Question : for the text In this section, we highlight the research challenges of machine commenting, and provide some solutions to deal with these challenges. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "33\n",
      "What is the model used for obtaining the representations of articles and comments?\n",
      "\n",
      "The model used for obtaining the representations of articles and comments is a neural variational topic model, based on the variational autoencoder framework.\n",
      "Question : for the text We obtain the representations of articles INLINEFORM0 and comments INLINEFORM1 with a neural variational topic model. The neural variational topic model is based on the variational autoencoder framework, so it can be trained in an unsupervised manner. The model encodes the source text into a representation, from which it reconstructs the text..We concatenate the title and the body to represent the article. In our model, the representations of the article and the comment are obtained in the same way. For simplicity, we denote both the article and the comment as “document”. Since the articles are often very long (more than 200 words), we represent the documents into bag-of-words, for saving both the time and memory cost. We denote the bag-of-words representation as INLINEFORM0 , where INLINEFORM1 is the one-hot representation of the word at INLINEFORM2 position, and INLINEFORM3 is the number of words in the vocabulary. The encoder INLINEFORM4 compresses the bag-of-words representations INLINEFORM5 into topic representations INLINEFORM6 : DISPLAYFORM0 DISPLAYFORM1 .where INLINEFORM0 , INLINEFORM1 , INLINEFORM2 , and INLINEFORM3 are the trainable parameters. Then the decoder INLINEFORM4 reconstructs the documents by independently generating each words in the bag-of-words: DISPLAYFORM0 DISPLAYFORM1 .where INLINEFORM0 is the number of words in the bag-of-words, and INLINEFORM1 is a trainable matrix to map the topic representation into the word distribution..In order to model the topic information, we use a Dirichlet prior rather than the standard Gaussian prior. However, it is difficult to develop an effective reparameterization function for the Dirichlet prior to train VAE. Therefore, following BIBREF6 , we use the Laplace approximation BIBREF7 to Dirichlet prior INLINEFORM0 : DISPLAYFORM0 DISPLAYFORM1 .where INLINEFORM0 denotes the logistic normal distribution, INLINEFORM1 is the number of topics, and INLINEFORM2 is a parameter vector. Then, the variational lower bound is written as: DISPLAYFORM0 .where the first term is the KL-divergence loss and the second term is the reconstruction loss. The mean INLINEFORM0 and the variance INLINEFORM1 are computed as follows: DISPLAYFORM0 DISPLAYFORM1 .We use the INLINEFORM0 and INLINEFORM1 to generate the samples INLINEFORM2 by sampling INLINEFORM3 , from which we reconstruct the input INLINEFORM4 ..At the training stage, we train the neural variational topic model with the Eq. EQREF22 . At the testing stage, we use INLINEFORM0 to compute the topic representations of the article INLINEFORM1 and the comment INLINEFORM2 . generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "34\n",
      "Question 1: What is the purpose of using semi-supervised training in this proposed approach?\n",
      "\n",
      "Answer 1: The purpose of using semi-supervised training in this proposed approach is to combine the advantages of both supervised and unsupervised learning methods, thus improving the accuracy and efficiency of the commenting framework.\n",
      "Question : for the text We now introduce our proposed approach as an implementation of the solutions above. We first give the definition and the denotation of the problem. Then, we introduce the retrieval-based commenting framework. After that, a neural variational topic model is introduced to model the topics of the comments and the articles. Finally, semi-supervised training is used to combine the advantage of both supervised and unsupervised learning. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "35\n",
      "What are the different parts of the candidate comment set used for evaluation?\n",
      "The candidate comment set consists of the following parts: Correct comments provided by humans, plausible comments retrieved based on cosine similarity, popular comments based on frequency, and randomly selected comments from the training set.\n",
      "Question : for the text For text generation, automatically evaluate the quality of the generated text is an open problem. In particular, the comment of a piece of news can be various, so it is intractable to find out all the possible references to be compared with the model outputs. Inspired by the evaluation methods of dialogue models, we formulate the evaluation as a ranking problem. Given a piece of news and a set of candidate comments, the comment model should return the rank of the candidate comments. The candidate comment set consists of the following parts:.Correct: The ground-truth comments of the corresponding news provided by the human..Plausible: The 50 most similar comments to the news. We use the news as the query to retrieve the comments that appear in the training set based on the cosine similarity of their tf-idf values. We select the top 50 comments that are not the correct comments as the plausible comments..Popular: The 50 most popular comments from the dataset. We count the frequency of each comments in the training set, and select the 50 most frequent comments to form the popular comment set. The popular comments are the general and meaningless comments, such as “Yes”, “Great”, “That's right', and “Make Sense”. These comments are dull and do not carry any information, so they are regarded as incorrect comments..Random: After selecting the correct, plausible, and popular comments, we fill the candidate set with randomly selected comments from the training set so that there are 200 unique comments in the candidate set..Following previous work, we measure the rank in terms of the following metrics:.Recall@k: The proportion of human comments found in the top-k recommendations..Mean Rank (MR): The mean rank of the human comments..Mean Reciprocal Rank (MRR): The mean reciprocal rank of the human comments..The evaluation protocol is compatible with both retrieval models and generative models. The retrieval model can directly rank the comments by assigning a score for each comment, while the generative model can rank the candidates by the model's log-likelihood score..Table TABREF31 shows the performance of our models and the baselines in retrieval evaluation. We first compare our proposed model with other popular unsupervised methods, including TF-IDF, LDA, and NVDM. TF-IDF retrieves the comments by similarity of words rather than the semantic meaning, so it achieves low scores on all the retrieval metrics. The neural variational document model is based on the neural VAE framework. It can capture the semantic information, so it has better performance than the TF-IDF model. LDA models the topic information, and captures the deeper relationship between the article and comments, so it achieves improvement in all relevance metrics. Finally, our proposed model outperforms all these unsupervised methods, mainly because the proposed model learns both the semantics and the topic information..We also evaluate two popular supervised models, i.e. seq2seq and IR. Since the articles are very long, we find either RNN-based or CNN-based encoders cannot hold all the words in the articles, so it requires limiting the length of the input articles. Therefore, we use an MLP-based encoder, which is the same as our model, to encode the full length of articles. In our preliminary experiments, the MLP-based encoder with full length articles achieves better scores than the RNN/CNN-based encoder with limited length articles. It shows that the seq2seq model gets low scores on all relevant metrics, mainly because of the mode collapse problem as described in Section Challenges. Unlike seq2seq, IR is based on a retrieval framework, so it achieves much better performance. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "36\n",
      "What is the main challenge in retrieval-based commenting? \n",
      "\n",
      "The main challenges in retrieval-based commenting are how to evaluate the matching of the articles and comments, and how to efficiently compute the matching scores due to the large number of comments in the pool.\n",
      "Question : for the text Given an article, the retrieval-based method aims to retrieve a comment from a large pool of candidate comments. The article consists of a title INLINEFORM0 and a body INLINEFORM1 . The comment pool is formed from a large scale of candidate comments INLINEFORM2 , where INLINEFORM3 is the number of the unique comments in the pool. In this work, we have 4.5 million human comments in the candidate set, and the comments are various, covering different topics from pets to sports..The retrieval-based model should score the matching between the upcoming article and each comments, and return the comments which is matched with the articles the most. Therefore, there are two main challenges in retrieval-based commenting. One is how to evaluate the matching of the articles and comments. The other is how to efficiently compute the matching scores because the number of comments in the pool is large..To address both problems, we select the “dot-product” operation to compute matching scores. More specifically, the model first computes the representations of the article INLINEFORM0 and the comments INLINEFORM1 . Then the score between article INLINEFORM2 and comment INLINEFORM3 is computed with the “dot-product” operation: DISPLAYFORM0 .The dot-product scoring method has proven a successful in a matching model BIBREF1 . The problem of finding datapoints with the largest dot-product values is called Maximum Inner Product Search (MIPS), and there are lots of solutions to improve the efficiency of solving this problem. Therefore, even when the number of candidate comments is very large, the model can still find comments with the highest efficiency. However, the study of the MIPS is out of the discussion in this work. We refer the readers to relevant articles for more details about the MIPS BIBREF2 , BIBREF3 , BIBREF4 , BIBREF5 . Another advantage of the dot-product scoring method is that it does not require any extra parameters, so it is more suitable as a part of the unsupervised model. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "37\n",
      "Question 1: What is the advantage of using the retrieval model over the generative model in selecting comments?\n",
      "\n",
      "Answer 1: The retrieval model is less prone to suffer from the mode collapse problem, can produce more predictable and controllable comments, and can be combined with the generative model to produce new comments.\n",
      "Question : for the text Facing the above challenges, we provide three solutions to the problems..Given a large set of candidate comments, the retrieval model can select some comments by matching articles with comments. Compared with the generative model, the retrieval model can achieve more promising performance. First, the retrieval model is less likely to suffer from the mode collapse problem. Second, the generated comments are more predictable and controllable (by changing the candidate set). Third, the retrieval model can be combined with the generative model to produce new comments (by adding the outputs of generative models to the candidate set)..The unsupervised learning method is also important for machine commenting to alleviate the problems descried above. Unsupervised learning allows the model to exploit more data, which helps the model to learn more complex patterns of commenting and improves the generalization of the model. Many comments provide some unique opinions, but they do not have paired articles. For example, many interesting comments on social media (e.g. Twitter) are about recent news, but require redundant work to match these comments with the corresponding news articles. With the help of the unsupervised learning method, the model can also learn to generate these interesting comments. Additionally, the unsupervised learning method does not require negative samples in the training stage, so that it can alleviate the negative sampling bias..Although there is semantic gap between the articles and the comments, we find that most articles and comments share the same topics. Therefore, it is possible to bridge the semantic gap by modeling the topics of both articles and comments. It is also similar to how humans generate comments. Humans do not need to go through the whole article but are capable of making a comment after capturing the general topics. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "38\n",
      "in except\n",
      "38\n",
      "Question 1: What kind of training data was used to train the BERT model exclusive to the German language? \n",
      "\n",
      "Answer 1: The German BERT model was trained from scratch on German Wikipedia, news articles, and court decisions.\n",
      "Question : for the text Although the pre-trained BERT language models are multilingual and, therefore, support German, we rely on a BERT model that was exclusively pre-trained on German text, as published by the German company Deepset AI. This model was trained from scratch on the German Wikipedia, news articles and court decisions. Deepset AI reports better performance for the German BERT models compared to the multilingual models on previous German shared tasks (GermEval2018-Fine and GermEval 2014). generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "1\n",
      "Question 1: What is the main contribution of this paper in regards to text processing?\n",
      "\n",
      "Answer 1: The main contribution of this paper is the inclusion of additional metadata using a state-of-the-art approach for text processing, which includes enriching BERT with knowledge graph embeddings based on Wikidata, and collecting information on authors for the particular document classification task. This approach allows for a transfer learning setup in which relatively little training data is available, and utilizes both text-based features and document metadata for document classification.\n",
      "Question : for the text With ever-increasing amounts of data available, there is an increase in the need to offer tooling to speed up processing, and eventually making sense of this data. Because fully-automated tools to extract meaning from any given input to any desired level of detail have yet to be developed, this task is still at least supervised, and often (partially) resolved by humans; we refer to these humans as knowledge workers. Knowledge workers are professionals that have to go through large amounts of data and consolidate, prepare and process it on a daily basis. This data can originate from highly diverse portals and resources and depending on type or category, the data needs to be channelled through specific down-stream processing pipelines. We aim to create a platform for curation technologies that can deal with such data from diverse sources and that provides natural language processing (NLP) pipelines tailored to particular content types and genres, rendering this initial classification an important sub-task..In this paper, we work with the dataset of the 2019 GermEval shared task on hierarchical text classification BIBREF0 and use the predefined set of labels to evaluate our approach to this classification task..Deep neural language models have recently evolved to a successful method for representing text. In particular, Bidirectional Encoder Representations from Transformers (BERT; BIBREF1) outperformed previous state-of-the-art methods by a large margin on various NLP tasks. We adopt BERT for text-based classification and extend the model with additional metadata provided in the context of the shared task, such as author, publisher, publishing date, etc..A key contribution of this paper is the inclusion of additional (meta) data using a state-of-the-art approach for text processing. Being a transfer learning approach, it facilitates the task solution with external knowledge for a setup in which relatively little training data is available. More precisely, we enrich BERT, as our pre-trained text representation model, with knowledge graph embeddings that are based on Wikidata BIBREF2, add metadata provided by the shared task organisers (title, author(s), publishing date, etc.) and collect additional information on authors for this particular document classification task. As we do not rely on text-based features alone but also utilize document metadata, we consider this as a document classification problem. The proposed approach is an attempt to solve this problem exemplary for single dataset provided by the organisers of the shared task. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "2\n",
      "Question 1: What is the BERT model and how has it been applied to document classification?\n",
      "\n",
      "Answer 1: The BERT (Bidirectional Encoder Representations from Transformers) model is a pre-trained embedding model that is used in a wide range of NLP tasks. Researchers have adapted BERT for document classification tasks by introducing a fully-connected layer over the final hidden state that contains one neuron each for representing an input token. They also optimize the model using soft-max classifier parameters to weight the hidden state layer, resulting in state of the art results in experiments based on four popular datasets.\n",
      "Question : for the text A central challenge in work on genre classification is the definition of a both rigid (for theoretical purposes) and flexible (for practical purposes) mode of representation that is able to model various dimensions and characteristics of arbitrary text genres. The size of the challenge can be illustrated by the observation that there is no clear agreement among researchers regarding actual genre labels or their scope and consistency. There is a substantial amount of previous work on the definition of genre taxonomies, genre ontologies, or sets of labels BIBREF3, BIBREF4, BIBREF5, BIBREF6, BIBREF7. Since we work with the dataset provided by the organisers of the 2019 GermEval shared task, we adopt their hierarchy of labels as our genre palette. In the following, we focus on related work more relevant to our contribution..With regard to text and document classification, BERT (Bidirectional Encoder Representations from Transformers) BIBREF1 is a pre-trained embedding model that yields state of the art results in a wide span of NLP tasks, such as question answering, textual entailment and natural language inference learning BIBREF8. BIBREF9 are among the first to apply BERT to document classification. Acknowledging challenges like incorporating syntactic information, or predicting multiple labels, they describe how they adapt BERT for the document classification task. In general, they introduce a fully-connected layer over the final hidden state that contains one neuron each representing an input token, and further optimize the model choosing soft-max classifier parameters to weight the hidden state layer. They report state of the art results in experiments based on four popular datasets. An approach exploiting Hierarchical Attention Networks is presented by BIBREF10. Their model introduces a hierarchical structure to represent the hierarchical nature of a document. BIBREF10 derive attention on the word and sentence level, which makes the attention mechanisms react flexibly to long and short distant context information during the building of the document representations. They test their approach on six large scale text classification problems and outperform previous methods substantially by increasing accuracy by about 3 to 4 percentage points. BIBREF11 (the organisers of the GermEval 2019 shared task on hierarchical text classification) use shallow capsule networks, reporting that these work well on structured data for example in the field of visual inference, and outperform CNNs, LSTMs and SVMs in this area. They use the Web of Science (WOS) dataset and introduce a new real-world scenario dataset called Blurb Genre Collection (BGC)..With regard to external resources to enrich the classification task, BIBREF12 experiment with external knowledge graphs to enrich embedding information in order to ultimately improve language understanding. They use structural knowledge represented by Wikidata entities and their relation to each other. A mix of large-scale textual corpora and knowledge graphs is used to further train language representation exploiting ERNIE BIBREF13, considering lexical, syntactic, and structural information. BIBREF14 propose and evaluate an approach to improve text classification with knowledge from Wikipedia. Based on a bag of words approach, they derive a thesaurus of concepts from Wikipedia and use it for document expansion. The resulting document representation improves the performance of an SVM classifier for predicting text categories. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "3\n",
      "Question 1: What setup outperforms all others for sub-tasks A and B in terms of F1-score?\n",
      "\n",
      "Answer 1: The setup using BERT-German with metadata features and author embeddings (1) outperforms all other setups with an F1-score of 87.20 for task A and 64.70 for task B.\n",
      "Question : for the text Table TABREF34 shows the results of our experiments. As prescribed by the shared task, the essential evaluation metric is the micro-averaged F1-score. All scores reported in this paper are obtained using models that are trained on the training set and evaluated on the validation set. For the final submission to the shared task competition, the best-scoring setup is used and trained on the training and validation sets combined..We are able to demonstrate that incorporating metadata features and author embeddings leads to better results for both sub-tasks. With an F1-score of 87.20 for task A and 64.70 for task B, the setup using BERT-German with metadata features and author embeddings (1) outperforms all other setups. Looking at the precision score only, BERT-German with metadata features (2) but without author embeddings performs best..In comparison to the baseline (7), our evaluation shows that deep transformer models like BERT considerably outperform the classical TF-IDF approach, also when the input is the same (using the title and blurb only). BERT-German (4) and BERT-Multilingual (5) are only using text-based features (title and blurb), whereby the text representations of the BERT-layers are directly fed into the classification layer..To establish the information gain of author embeddings, we train a linear classifier on author embeddings, using this as the only feature. The author-only model (6) is exclusively evaluated on books for which author embeddings are available, so the numbers are based on a slightly smaller validation set. With an F1-score of 61.99 and 32.13 for sub-tasks A and B, respectively, the author model yields the worst result. However, the information contained in the author embeddings help improve performance, as the results of the best-performing setup show. When evaluating the best model (1) only on books for that author embeddings are available, we find a further improvement with respect to F1 score (task A: from 87.20 to 87.81; task B: 64.70 to 65.74). generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "4\n",
      "Question 1: What is the drawback of the TGRID-V dataset?\n",
      "Answer 1: The TGRID-V dataset contains a significant amount of French, Dutch, and Latin texts, which are filtered out to constrain the dataset to only German texts.\n",
      "Question : for the text The Digital Library in the TextGrid Repository represents an extensive collection of German texts in digital form BIBREF3. It was mined from http://zeno.org and covers a time period from the mid 16th century up to the first decades of the 20th century. It contains many important texts that can be considered as part of the literary canon, even though it is far from complete (e.g. it contains only half of Rilke’s work). We find that around 51k texts are annotated with the label ’verse’ (TGRID-V), not distinguishing between ’lyric verse’ and ’epic verse’. However, the average length of these texts is around 150 token, dismissing most epic verse tales. Also, the poems are distributed over 229 authors, where the average author contributed 240 poems (median 131 poems). A drawback of TGRID-V is the circumstance that it contains a noticeable amount of French, Dutch and Latin (over 400 texts). To constrain our dataset to German, we filter foreign language material with a stopword list, as training a dedicated language identification classifier is far beyond the scope of this work. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "5\n",
      "Question 1: What is the reason for choosing 100 topics for the topic model in this study?\n",
      "\n",
      "Answer 1: The authors chose 100 topics instead of a lower number that might be more straightforward to interpret because they want to use these topics as features for downstream tasks later on. Furthermore, using 100 topics provides a reasonable distinctness of topics and allows for capturing more style features in poetry.\n",
      "Question : for the text We approach diachronic variation of poetry from two perspectives. First, as distant reading task to visualize the development of clearly interpretable topics over time. Second, as a downstream task, i.e. supervised machine learning task to determine the year (the time-slot) of publication for a given poem. We infer topic distributions over documents as features and pit them against a simple style baseline..We use the implementation of LDA as it is provided in genism BIBREF4. LDA assumes that a particular document contains a mixture of few salient topics, where words are semantically related. We transform our documents (of wordforms) to a bag of words representation, filter stopwords (function words), and set the desired number of topics=100 and train for 50 epochs to attain a reasonable distinctness of topics. We choose 100 topics (rather than a lower number that might be more straightforward to interpret) as we want to later use these topics as features for downstream tasks. We find that wordforms (instead of lemma) are more useful for poetry topic models, as these capture style features (rhyme), orthographic variations ('hertz' instead of 'herz'), and generally offer more interpretable results. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "6\n",
      "Question 1: What features were used in the style baseline for the supervised classification experiments?\n",
      "\n",
      "Answer 1: Features used in the style baseline for the supervised classification experiments included line length, poem length (in tokens, syllables, lines), cadence (number of syllables of last word in line), soundscape (ratio of closed to open syllables), and a proxy for meter, the number of syllables of the first word in the line.\n",
      "Question : for the text To test whether topic models can be used for dating poetry or attributing authorship, we perform supervised classification experiments with Random Forest Ensemble classifiers. We find that we obtain better results by training and testing on stanzas instead of full poems, as we have more data available. Also, we use 50 year slots (instead of 25) to ease the task..For each document we determine a class label for a time slot. The slot 1575–1624 receives the label 0, the slot 1625–1674 the label 1, etc.. In total, we have 7 classes (time slots)..As a baseline, we implement rather straightforward style features, such as line length, poem length (in token, syllables, lines), cadence (number of syllables of last word in line), soundscape (ratio of closed to open syllables, see BIBREF5), and a proxy for metre, the number of syllables of the first word in the line..We split the data randomly 70:30 training:testing, where a 50:50 shows (5 points) worse performance. We then train Random Forest Ensemble classifiers and perform a grid search over their parameters to determine the best classifier. Please note that our class sizes are quite imbalanced..The Style baseline achieves an Accuracy of 83%, LDA features 89% and a combination of the two gets 90%. However, training on full poems reduces this to 42—52%..The most informative features (by information gain) are: Topic11 (.067), Topic 37 (.055), Syllables Per Line (.046), Length of poem in syllables (.031), Topic19 (.029), Topic98 (.025), Topic27 ('virtue') (.023), and Soundscape (.023)..For authorship attribution, we also use a 70:30 random train:test split and use the author name as class label. We only choose the most frequent 180 authors. We find that training on stanzas gives us 71% Accuracy, but when trained on full poems, we only get 13% Accuracy. It should be further investigated is this is only because of a surplus of data. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "7\n",
      "What are some potential improvements that could be made to the current method of using Latent Dirichlet Allocation for visualizing trends in poetry topics? \n",
      "\n",
      "Some potential improvements include using stanzas instead of whole poems for more available data, exploring the use of better topic models, determining if better style features will outperform semantics, and only selecting clear trending and peaking topics through co-variance for improved results.\n",
      "Question : for the text We have shown the viability of Latent Dirichlet Allocation for a visualization of topic trends (the evolution of what people talk about in poetry). While most topics are easily interpretable and show a clear trend, others are quite noisy. For an exploratory experiment, the classification into time slots and for authors attribution is very promising, however far from perfect. It should be investigated whether using stanzas instead of whole poems only improves results because of more available data. Also, it needs to be determined if better topic models can deliver a better baseline for diachronic change in poetry, and if better style features will outperform semantics. Finally, only selecting clear trending and peaking topics (through co-variance) might further improve the results. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "8\n",
      "What is the methodology used to analyze the trends in topics over time in the dataset?\n",
      "\n",
      "The methodology used to analyze the trends in topics over time in the dataset involved retrieving the most important words for all 100 topics, interpreting them as aggregated topics, binning the documents into time slots of 25 years width, aggregating all documents in each slot, and plotting the trajectories for each single topic.\n",
      "Question : for the text We retrieve the most important (likely) words for all 100 topics and interpret these (sorted) word lists as aggregated topics, e.g. topic 27 (figure 2) contains: Tugend (virtue), Kunst (art), Ruhm (fame), Geist (spirit), Verstand (mind) and Lob (praise). This topic as a whole describes the concept of ’artistic virtue’..In certain clusters (topics) we find poetic residuals, such that rhyme words often cluster together (as they stand in proximity), e.g. topic 52 with: Mund (mouth), Grund (cause, ground), rund (round)..To discover trends of topics over time, we bin our documents into time slots of 25 years width each. See figure 1 for a plot of the number of documents per bin. The chosen binning slots offer enough documents per slot for our experiments. To visualize trends of singular topics over time, we aggregate all documents d in slot s and add the probabilities of topic t given d and divide by the number of all d in s. This gives us the average probability of a topic per timeslot. We then plot the trajectories for each single topic. See figures 2–6 for a selection of interpretable topic trends. Please note that the scaling on the y-axis differ for each topic, as some topics are more pronounced in the whole dataset overall..Some topic plots are already very revealing. The topic ‘artistic virtue’ (figure 2, left) shows a sharp peak around 1700—1750, outlining the period of Enlightenment. Several topics indicate Romanticism, such as ‘flowers’ (figure 2, right), ‘song’ (figure 3, left) or ‘dust, ghosts, depths’ (not shown). The period of 'Vormärz' or 'Young Germany' is quite clear with the topic ‘German Nation’ (figure 3, right). It is however hardly distinguishable from romantic topics..We find that the topics 'Beautiful Girls' (figure 4, left) and 'Life & Death' (figure 4, right) are always quite present over time, while 'Girls' is more prounounced in Romanticism, and 'Death' in Barock..We find that the topic 'Fire' (figure 5, left) is a fairly modern concept, that steadily rises into modernity, possibly because of the trope 'love is fire'. Next to it, the topic 'Family' (figure 5, right) shows wild fluctuation over time..Finally, figure 6 shows topics that are most informative for the downstream classification task: Topic 11 'World, Power, Time' (left) is very clearly a Barock topic, ending at 1750, while topic 19 'Heaven, Depth, Silence' is a topic that rises from Romanticism into Modernity. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "9\n",
      "What is Alibaba.com? \n",
      "\n",
      "Alibaba.com is the world's largest cross-border business to business (B2B) E-commerce platform that supports 17 languages for customers all around the world. It currently has a large knowledge graph of products, where the entity is either the product or product category, and contains lots of information such as the entity name, images, and attributes. The website has accumulated millions of product enquires and communication cycles usually begin with a product enquiry.\n",
      "Question : for the text Alibaba.com is currently the world's largest cross-border business to business(B2B) E-commerce platform and it supports 17 languages for customers from all over the world. On the website, English is the dorminant language and accounts for around 50% of the traffic. The website has already accumulated a very large knowledge graph of products, and the entity here is the product or the product category; and every entity has lots of information such as the entity name, images and many attributes without ordering information. The entities are also connected by taxonomy structure and similar products usually belong to the same category/sub-category..Since the B2B procurement usually involves a large amount of money, the business will be a long process beginning with a product enquiry. Generally speaking, when customers are interested in some product, they will start a communication cycle with a seller by sending a product enquiry to the seller. In the product enquiry, customers will specify their requirements and ask questions about the product. Their requirements and questions usually refer to the most important attributes of the product. Fig. FIGREF5 shows an enquery example. Alibaba.com has accumulated tens of millions of product enquires, and we would like to leverage these information, in combination of the product knowledge graph we have, to figure out the most important attributes for each category of products..In our application scenario, the product knowledge graph is the existing knowledge graph and the enquiry data is the external textual data source. From now on, we will use our application scenario to explain the details of our proposed algorithm..We propose an unsupervised learning framework for extracting important product attributes from product enquiries. By calculating the semantic similarity between each enquiry sentence and each attribute of the product to which the enquiry corresponds to, we identify the product attributes that the customer cares about most..The attributes described in the enquiry may contain attribute names or attribute values or other expressions, for example, either the word “color” or a color instance word “purple” is mentioned. Therefore, when calculating the semantic similarity between enquiry sentences and product attributes, we need both attribute names and attribute values. The same as any other knowledge graph, the product attributes in our knowledge graph we use contain noises and mistakes. We need to clean and normalize the attribute data before consuming it. We will introduce the detail of our data cleaning process in Section SECREF14 . generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "10\n",
      "Question 1: What method did the paper propose for identifying important attributes for entities in knowledge graphs?\n",
      "\n",
      "Answer 1: The paper proposed a new general method of using external textual data and semantic matching via word/sub-word embeddings to identify important attributes for entities in knowledge graphs, which outperformed previous methods of using naive string matching and counting.\n",
      "Question : for the text In this paper, we proposed a new general method of identifying important attributes for entities from a knowledge graph. This is a relatively new task and our proposed method of using external textual data and performing semantic matching via word/sub-word embeddings obtained better result compared to other work of using naive string matching and counting. In addition, we also successfully applied the detected important attributes in our real world application of smart composing. In summary, the method is extensible to any knowledge graph without attribute importance information and outperforms previous method..In future work, there are two major areas with potential of improving the detection accuracy. The first one is about sentence splitting. What we are trying to get is semantic cohesive unit, which can be used to match an attribute, and there might be more comprehensive method than the simple splitting by sentence ending punctuations. The second one is about improving the word embedding quality. We have implemented an in-house improved version of Fasttext, which is adapted to our data source. It is highly possible to use the improved word embedding on purpose of obtaining higher semantic matching precision. As for the application, we will try to use more statistical models in the natural language generation part of the smart composing framework of consuming the detected important attributes. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "11\n",
      "Question 1: What specific type of entity attributes were chosen for further study in the knowledge graph data analysis?\n",
      "\n",
      "Answer 1: The product-specific entity attributes were chosen for further study in the knowledge graph data analysis.\n",
      "Question : for the text For our knowledge graph data, entity(product) attributes can be roughly divided into clusters of transaction order specific ones and product specific ones, in this paper, we choose the product specific ones for further study. We also need to point out that we only focus on the recommended communication language on the Alibaba.com platform, which is English..To construct the evaluation dataset, top 14 categories are first chosen based on their business promotion features, and 3 millions typical products under each category were then chosen to form the attribute candidates. After preprocessing and basic filtering, top product specific attributes from the 14 different categories are chosen to be manually labeled by our annotators..For each category, annotators each are asked to choose at most 10 important attributes from buyers perspective. After all annotators complete their annotations, attributes are then sorted according to the summed votes. In the end, 111 important attributes from the 14 categories are kept for final evaluation..Outside of the evaluation explained in this paper, we actually have performed the matching on more than 4,000 catetories covering more than 100 million products and more than 20 million enquires. Due to limited annotation resources, we can only sample a small numbered categories(14 here) to evaluate the proposed algorithm here. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "12\n",
      "Question 1: What are the three extra processes carried out for each sentence in the product enquiries and attributes data preprocessing?\n",
      "\n",
      "Answer 1: The three extra processes carried out for each sentence in the product enquiries and attributes data preprocessing are Spelling Correction, Regular Measures and Numbers, and Stop Words Dropping.\n",
      "Question : for the text The product enquiries and attributes data preprocessing is shown in Algorithm 1. algorithmAlgorithm Data Preprocess Algorithm [1] INLINEFORM0 INLINEFORM1 : INLINEFORM2 INLINEFORM3 INLINEFORM4 : INLINEFORM5 Invalid INLINEFORM6 filter INLINEFORM7 Split INLINEFORM8 to sentences sentence INLINEFORM9 in INLINEFORM10 INLINEFORM11 INLINEFORM12 return INLINEFORM13 .Firstly, for every product enquiry, we convert the original html textual data into the plain text. Secondly we filter out the useless enquires, such as non-English enquires and spams. The regular expressions and spam detection are used to detect non-English enquiries and spams respectively. Thirdly we get sentence list INLINEFORM0 with spliting every enquiry into sentences as described in section 2.2. Then for every sentence INLINEFORM1 in INLINEFORM2 , we need to do extra three processes: a)Spelling Correction. b)Regular Measures and Numbers. c)Stop Words Dropping..Spelling Correction. Since quite a lot of the product enquires and self-filled attributes were misspelled, we have replaced the exact words by fuzzyfied search using Levenshtein distance. The method uses fuzzyfied search, only if the exact match is not found. Some attributes are actually the same, such as \"type\" and \"product type\", we merge these same attributes by judging whether the attributes are contained..Regular Measures and Numbers. Attributes of number type have their values composed of numbers and units, such as INLINEFORM0 , INLINEFORM1 , INLINEFORM2 , INLINEFORM3 , etc. We replace all numbers (in any notation, e.g., floating point, scientific, arithmetical expression, etc.) with a unique token ( INLINEFORM4 ). For the same reason, each unit of measure is replaced with a corresponding token, eg., INLINEFORM5 is replaced with centimeter area..Stop Words Dropping. Stop words appear to be of little value in the proposed matching algorithm. By removing the stop words we can focus on the important words instead. In our business scenario, we built a stop words list for foreign trade e-commerce..Finally, we get the valid sentences INLINEFORM0 . generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "13\n",
      "What is the advantage of FastText over word2vec or glove?\n",
      "\n",
      "FastText is able to cover rare words and out-of-vocabulary (OOV) words, while word2vec and glove fail to provide accurate vector representations for these words. Additionally, character n-grams embeddings tend to perform superior to word2vec and glove on smaller datasets, and FastText is more efficient and its training is relatively fast.\n",
      "Question : for the text FastText is a library created by the Facebook Research for efficient learning of word representations and sentence classification. Here, we just use the word representation functionality of it..FastText models morphology by considering subword units, and representing words by a sum of its character n-grams BIBREF17 . In the original model the authors choose to use the binary logistic loss and the loss for a single instance is written as below: INLINEFORM0 .By denoting the logistic loss function INLINEFORM0 , the loss over a sentence is: INLINEFORM1 .The scoring function between a word INLINEFORM0 and a context word INLINEFORM1 is: INLINEFORM2 .In the above functions, INLINEFORM0 is a set of negative examples sampled from the vocabulary, INLINEFORM1 is the set of indices of words surrounding word INLINEFORM2 , INLINEFORM3 is the set of n-grams appearing in word INLINEFORM4 , INLINEFORM5 is the size of the dictionary we have for n-grams, INLINEFORM6 is a vector representation to each n-gram INLINEFORM7 ..Compared with word2vec or glove, FastText has following advantages:.It is able to cover rare words and out-of-vocabulary(OOV) words. Since the basic modeling units in FastText are ngrams, and both rare words and OOV ones can obtain efficient word representations from their composing ngrams. Word2vec and glove both fail to provide accurate vector representations for these words. In our application, the training data is written by end customers, and there are many misspellings which easily become OOV words..Character n-grams embeddings tend to perform superior to word2vec and glove on smaller datasets..FastText is more efficient and its training is relatively fast. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "14\n",
      "Question 1: How does the system compute the matching between an enquiry sentence and a product attribute?\n",
      "\n",
      "Answer 1: The system compares each sentence with each attribute of a product category and calculates a score between the sentence and attribute based on formula INLINEFORM0. The top two attributes with scores above the threshold INLINEFORM4 are selected for each sentence. This is done because there may be multiple attributes for each sentence and some sentences may not contain attribute information.\n",
      "Question : for the text In this section, how to compute the matching between an enquiry sentence and a product attribute is explained in detail. Our explanation here is for a certain product category, and other categories are the same..As you can see in Fig. FIGREF12 , each sentence is compared with each attribute of a product category that the product belongs to. We now get a score between a sentence INLINEFORM0 and an attribute INLINEFORM1 , INLINEFORM2 INLINEFORM3 .where INLINEFORM0 is all the possible values for this INLINEFORM1 , INLINEFORM2 is the word vector for INLINEFORM3 . According to this formula, we can get top two attributes whose scores are above the threshold INLINEFORM4 for each sentence. We choose two attributes instead of one because there may be more than one attribute for each sentence. In addition, some sentences are greetings or self-introduction and do not contain the attribute information of the product, so we require that the score to be higher than a certain threshold. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "15\n",
      "Question 1: Is the proposed method limited to a specific application scenario?\n",
      "\n",
      "Answer 1: No, the scope of the proposed method is not limited to our use case. It is possible to extend it to any existing knowledge graph without attribute importance information.\n",
      "Question : for the text In this section, we will introduce our proposed method in detail. We use our application scenario to explain the logic behind the method, but the scope is not limited to our use case, and it is possible to extend to any existing knowledge graph without attribute importance information. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "16\n",
      "What is the proposed algorithm for attribute selection in product enquiries?\n",
      "\n",
      "The proposed algorithm for attribute selection in product enquiries is based on sentence and attribute similarity thresholds, and selects the 5 most important attributes for each category. It achieved the best performance compared to other methods such as GloVe, word2vec, and TextRank, with an average F1-measure of 0.47.\n",
      "Question : for the text The existing co-occurrence methods do not suit our application scenario at all, since exact string matching is too strong a requirement and initial trial has shown its incompetency. In stead we implemented an improved version of their method based on TextRank as our baseline. In addition, we also tested multiple semantic matching algorithms for comparison with our chosen method..TextRank: TextRank is a graph-based ranking model for text processing. BIBREF18 It is an unsupervised algorithm for keyword extraction. Since product attributes are usually the keywords in enquiries, we can compare these keywords with the category attributes and find the most important attributes. This method consists of three steps. The first step is to merge all enquiries under one category as an article. The second step is to extract the top 50 keywords for each category. The third step is to find the most important attributes by comparing top keywords with category attributes..Word2vec BIBREF19 : We use the word vector trained by BIBREF19 as the distributed representation of words. Then we get the enquiry sentence representation and category attribute representation. Finally we collect the statistics about the matched attributes of each category, and select the most frequent attributes under the same category..GloVe BIBREF20 : GloVe is a global log-bilinear regression model for the unsupervised learning of word representations, which utilizes the ratios of word-word co-occurrence probabilities. We use the GloVe method to train the distributed representation of words. And attribute selection procedure is the same as word2vec..Proposed method: the detail of our proposed algorithm has been carefully explained in Section SECREF2 . There are several thresholds we need to pick in the experimentation setup. Based on trial and error analysis, we choose 0.75 as the sentence and attribute similarity threshold, which balances the precision and recall relatively well. In our application, due to product enquiry length limitation, customers usually don't refer to more than five attributes in their initial approach to the seller, we choose to keep 5 most important attributes for each category..Evaluation is conducted by comparing the output of the systems with the manual annotated answers, and we calculate the precision and recall rate. INLINEFORM0 INLINEFORM1 .where INLINEFORM0 is the manually labeled attributes , INLINEFORM1 is the detected important attributes..Table 1 depicts the algorithm performance of each category and the overall average metrics among all categories for our approach and other methods. It can be observed that our proposed method achieves the best performance. The average F1-measure of our approach is 0.47, while the average F1-measure values of “GloVe”, “word2vect” and \"TextRank\" are 0.46, 0.42 and 0.20 respectively. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "17\n",
      "Question 1: What is the main issue with the approaches that formulate the entity attribute ranking problem as a post-processing step of automated attribute-value extraction?\n",
      "\n",
      "Answer 1: The main issue with these approaches is the poor quality of the pattern rules used, which leads to the ranking process being used to identify relatively more precise attributes from all attribute candidates.\n",
      "Question : for the text Many proposed approaches formulate the entity attribute ranking problem as a post processing step of automated attribute-value extraction. In BIBREF0 , BIBREF1 , BIBREF2 , Pasca et al. firstly extract potential class-attribute pairs using linguistically motivated patterns from unstructured text including query logs and query sessions, and then score the attributes using the Bayes model. In BIBREF3 , Rahul Rai proposed to identify product attributes from customer online reviews using part-of-speech(POS) tagging patterns, and to evaluate their importance with several different frequency metrics. In BIBREF4 , Lee et al. developed a system to extract concept-attribute pairs from multiple data sources, such as Probase, general web documents, query logs and external knowledge base, and aggregate the weights from different sources into one consistent typicality score using a Ranking SVM model. Those approaches typically suffer from the poor quality of the pattern rules, and the ranking process is used to identify relatively more precise attributes from all attribute candidates..As for an already existing knowledge graph, there is plenty of work in literature dealing with ranking entities by relevance without or with a query. In BIBREF5 , Li et al. introduced the OntoRank algorithm for ranking the importance of semantic web objects at three levels of granularity: document, terms and RDF graphs. The algorithm is based on the rational surfer model, successfully used in the Swoogle semantic web search engine. In BIBREF6 , Hogan et al. presented an approach that adapted the well-known PageRank/HITS algorithms to semantic web data, which took advantage of property values to rank entities. In BIBREF7 , BIBREF8 , authors also focused on ranking entities, sorting the semantic web resources based on importance, relevance and query length, and aggregating the features together with an overall ranking model..Just a few works were designated to specifically address the problem of computing attribute rankings in a given Knowledge Graph. Ibminer BIBREF9 introduced a tool for infobox(alias of an entity card) template suggestion, which collected attributes from different sources and then sorted them by popularity based on their co-occurrences in the dataset. In BIBREF10 , using the structured knowledge base, intermediate features were computed, including the importance or popularity of each entity type, IDF computation for each attribute on a global basis, IDF computation for entity types etc., and then the features were aggregated to train a classifier. Also, a similar approach in BIBREF11 was designed with more features extracted from GoogleSuggestChars data. In BIBREF12 , Ali et al. introduced a new set of features that utilizes semantic information about entities as well as information from top-ranked documents from a general search engine. In order to experiment their approach, they collected a dataset by exploiting Wikipedia infoboxes, whose ordering of attributes reflect the collaborative effort of a large community of users, which might not be accurate. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "18\n",
      "Question 1: What method performs best in dealing with natural language corpus with misspellings?\n",
      "\n",
      "Answer 1: According to our experiments and analysis, the FastText algorithm performs best in dealing with natural language corpus usually with spelling mistakes. FastText represents words by a sum of its character n-grams and is much more robust against misspellings compared to other word representation learning algorithms such as GloVe and word2vec.\n",
      "Question : for the text In all our experiments, we find that FastText method outperforms other methods. By analyzing all results, we observe that semantic similarity based methods are more effective than the previous method which we implemented based on TextRank. This conclusion is understandable because lots of enquiries do not simply mention attribute words exactly, but some semantically related words are also used..Evaluating FastText, GloVe and word2vec, we show that compared to other word representation learning algorithms, the FastText performs best. We sample and analyze the category attributes and find that many self-filled attributes contain misspellings. The FastText algorithm represents words by a sum of its character n-grams and it is much robust against problems like misspellings. In summary, FastText has greater advantages in dealing with natural language corpus usually with spelling mistakes..We also applied the detected attributes in the automatic enquiry generation task and we obtained significantly better generated enquiries compared to previous rigid templates. Due to space limitation, we skip the explanation and leave it for future publications. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "19\n",
      "What is the core potential of a knowledge graph?\n",
      "The core potential of a knowledge graph is its capability of reasoning and inferring.\n",
      "Question : for the text Knowledge graph(KG) has been proposed for several years and its most prominent application is in web search, for example, Google search triggers a certain entity card when a user's query matches or mentions an entity based on some statistical model. The core potential of a knowledge graph is about its capability of reasoning and inferring, and we have not seen revolutionary breakthrough in such areas yet. One main obstacle is obviously the lack of sufficient knowledge graph data, including entities, entities' descriptions, entities' attributes, and relationship between entities. A full functional knowledge graph supporting general purposed reasoning and inference might still require long years of the community's innovation and hardworking. On the other hand, many less demanding applications have great potential benefiting from the availability of information from the knowledge graph, such as query understanding and document understanding in information retrieval/search engines, simple inference in question answering systems, and easy reasoning in domain-limited decision support tools. Not only academy, but also industry companies have been heavily investing in knowledge graphs, such as Google's knowledge graph, Amazon's product graph, Facebook's Graph API, IBM's Watson, and Microsoft's Satori etc..In the existing knowledge graph, such as Wikidata and DBpedia, usually attributes do not have order or priorities, and we don't know which attributes are more important and of more interest to users. Such importance score of attributes is a vital piece of information in many applications of knowledge graph. The most important application is the triggered entity card in search engine when a customer's query gets hit for an entity. An entity usually has a large amount of attributes, but an entity card has limited space and can only show the most significant information; attribute importance's presence can make the displaying of an entity card easy to implement. Attribute importance also has great potential of playing a significant role in search engine, how to decide the matching score between the query and attribute values. If the query matches a very important attribute, and the relevance contribution from such a match should be higher than matching an ignorable attribute. Another application is in e-commerce communications, and one buyer initiates a communication cycle with a seller by sending a product enquiry. Writing the enquiry on a mobile phone is inconvenient and automatic composing assistance has great potential of improving customer experience by alleviating the writing burden. In the product enquiry, customers need to specify their requirements and ask questions about products, and their requirements and questions are usually about the most important attributes of the products. If we can identify out important attributes of products, we can help customers to draft the enquiry automatically to reduce their input time. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "20\n",
      "What is the focus of the research proposal?\n",
      "\n",
      "The focus of the research proposal is to identify important attributes in existing knowledge graphs by using extra user-generated data source for evaluating attribute importance and state-of-the-art word/sub-word embedding techniques to match external data with attribute definition and values from entities in knowledge graphs. The proposed method has general extensibility to any knowledge graph without attribute importance.\n",
      "Question : for the text There have been broad researches on entity detection, relationship extraction, and also missing relationship prediction. For example: BIBREF13 , BIBREF14 and BIBREF15 explained how to construct a knowledge graph and how to perform representation learning on knowledge graphs. Some research has been performed on attribute extraction, such as BIBREF16 and BIBREF4 ; the latter one is quite special that it also simultaneously computes the attribute importance. As for modeling attribute importance for an existing knowledge graph which has completed attribute extractions, we found only a few existing research, all of which used simple co-occurrences to rank entity attributes. In reality, many knowledge graphs do not contain attribute importance information, for example, in the most famous Wikidata, a large amount of entities have many attributes, and it is difficult to know which attributes are significant and deserve more attention. In this research we focus on identifying important attributes in existing knowledge graphs. Specifically, we propose a new method of using extra user generated data source for evaluating the attribute importance, and we use the recently proposed state-of-the-art word/sub-word embedding techniques to match the external data with the attribute definition and values from entities in knowledge graphs. And then we use the statistics obtained from the matching to compare the attribute importance. Our method has general extensibility to any knowledge graph without attribute importance. When there is a possibility of finding external textual data source, our proposed method will work, even if the external data does not exactly match the attribute textual data, since the vector embedding performs semantic matching and does not require exact string matching..The remaining of the paper is organized as follows: Section SECREF2 explains our proposed method in detail, including what kind of external data is required, and how to process the external data, and also how to perform the semantic matching and how to rank the attributes by statistics. Section SECREF3 introduces our experimentations, including our experimentation setup, data introduction and experimental result compared to other methods we do not employ. Section SECREF3 also briefly introduces our real world application scenario in e-commerce communication. Section SECREF4 draws the conclusion from our experimentations and analysis, and also we point out promising future research directions. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "21\n",
      "What metrics were analyzed for their correlation with model performance scores in the down-sampling experiments?\n",
      "\n",
      "The three proposed characteristic metrics, diversity, density, and homogeneity, were analyzed for their correlation with model performance scores in the down-sampling experiments.\n",
      "Question : for the text We calculate and show in Table TABREF35 the Pearson's correlations between the three proposed characteristic metrics, i.e., diversity, density, and homogeneity, and model performance scores from down-sampling experiments in Table TABREF29 and Table TABREF30. Correlations higher than $0.5$ are highlighted in bold. As mentioned before, model performance is highly correlated with density and homogeneity, both are computed on the train set. Diversity is only correlated with Snips SL F1 score at a moderate level...These are consistent with our simulation results, which shows that random sampling of a dataset does not necessarily affect the diversity but can reduce the density and marginally homogeneity due to the decreasing of data points in the embedding space. However, the simultaneous huge drops of model performance, density, and homogeneity imply that there is only limited redundancy and more informative data points are being thrown away when down-sampling. Moreover, results also suggest that model performance on text classification tasks corresponds not only with data diversity but also with training data density and homogeneity as well. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "22\n",
      "Question 1: What were pre-trained language embeddings used for in this work?\n",
      "\n",
      "Answer 1: Pre-trained language embeddings were used in this work to efficiently characterize text datasets and propose several characteristic metrics to describe the diversity, density, and homogeneity of text collections without using any labels.\n",
      "Question : for the text In this work, we proposed several characteristic metrics to describe the diversity, density, and homogeneity of text collections without using any labels. Pre-trained language embeddings are used to efficiently characterize text datasets. Simulation and experiments showed that our intrinsic metrics are robust and highly correlated with model performance on different text classification tasks. We would like to apply the diversity, density, and homogeneity metrics for text data augmentation and selection in a semi-supervised manner as our future work. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "23\n",
      "Question 1: What are the two real-world text classification tasks used in experiments?\n",
      "Answer 1: The two real-world text classification tasks used in experiments are sentiment analysis and Spoken Language Understanding (SLU).\n",
      "Question : for the text The two real-world text classification tasks we used for experiments are sentiment analysis and Spoken Language Understanding (SLU). generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "24\n",
      "Question 1: What is the architecture of BERT used for pretraining?\n",
      "\n",
      "Answer 1: BERT uses the Transformer, a multi-headed self-attention architecture, for pretraining. This architecture can produce different representation vectors for the same token in various sequences, allowing for contextual embeddings.\n",
      "Question : for the text BERT is a self-supervised language model pretraining approach based on the Transformer BIBREF24, a multi-headed self-attention architecture that can produce different representation vectors for the same token in various sequences, i.e., contextual embeddings..When pretraining, BERT concatenates two sequences as input, with special tokens $[CLS], [SEP], [EOS]$ denoting the start, separation, and end, respectively. BERT is then pretrained on a large unlabeled corpus with objective-masked language model (MLM), which randomly masks out tokens, and the model predicts the masked tokens. The other classification task is next sentence prediction (NSP). NSP is to predict whether two sequences follow each other in the original text or not..In this work, we use the pretrained $\\text{BERT}_{\\text{BASE}}$ which has 12 layers (L), 12 self-attention heads (A), and 768 hidden dimension (H) as the language embedding to compute the proposed data metrics. The off-the-shelf pretrained BERT is obtained from GluonNLP. For each sequence $x_i = (x_{i1}, ..., x_{il})$ with length $l$, BERT takes $[CLS], x_{i1}, ..., x_{il}, [EOS]$ as input and generates embeddings $\\lbrace e_{CLS}, e_{i1}, ..., e_{il}, e_{EOS}\\rbrace $ at the token level. To obtain the sequence representation, we use a mean pooling over token embeddings:.where $e_i \\in \\mathbb {R}^{H}$. A text collection $\\lbrace x_1, ..., x_m\\rbrace $, i.e., a set of token sequences, is then transformed into a group of H-dimensional vectors $\\lbrace e_1, ..., e_m\\rbrace $..We compute each metric as described previously, using three BERT layers L1, L6, and L12 as the embedding space, respectively. The calculated metric values are averaged over layers for each class and averaged over classes weighted by class size as the final value for a dataset. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "25\n",
      "Question 1: What are the three proposed characteristic metrics discussed in the text?\n",
      "Answer 1: The three proposed characteristic metrics discussed in the text are diversity, density, and homogeneity.\n",
      "Question : for the text We will discuss the three proposed characteristic metrics, i.e., diversity, density, and homogeneity, and model performance scores from down-sampling experiments on the two public benchmark datasets, in the following subsections: generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "26\n",
      "What is the sentiment classification accuracy for Table TABREF29 without down-sampling?\n",
      "\n",
      "The sentiment classification accuracy for Table TABREF29 without down-sampling is 92.66%.\n",
      "Question : for the text In Table TABREF29, the sentiment classification accuracy is $92.66\\%$ without down-sampling, which is consistent with the reported GluonNLP BERT model performance on SST-2. It also indicates SST-2 training data are differentiable between label classes, i.e., from the positive class to the negative class, which satisfies our assumption for the characteristic metrics..Decreasing the training set size does not reduce performance until it is randomly down-sampled to only $20\\%$ of the original size. Meanwhile, density and homogeneity metrics also decrease significantly (highlighted in bold in Table TABREF29), implying a clear relationship between these metrics and model performance. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "27\n",
      "What is the biggest drop in performance observed in the Snips dataset?\n",
      "\n",
      "The biggest drop in performance observed in the Snips dataset is in the SL F1 score, which drops to 87.20% when the training set is down-sampled to only 10% of the original size.\n",
      "Question : for the text In Table TABREF30, the Snips dataset seems to be distinct between IC/SL classes since the IC accurcy and SL F1 are as high as $98.71\\%$ and $96.06\\%$ without down-sampling, respectively. Similar to SST-2, this implies that Snips training data should also support the inter-class differentiability assumption for our proposed characteristic metrics..IC accuracy on Snips remains higher than $98\\%$ until we down-sample the training set to $20\\%$ of the original size. In contrast, SL F1 score is more sensitive to the down-sampling of the training set, as it starts decreasing when down-sampling. When the training set is only $10\\%$ left, SL F1 score drops to $87.20\\%$..The diversity metric does not decrease immediately until the training set equals to or is less than $40\\%$ of the original set. This implies that random sampling does not impact the diversity, if the sampling rate is greater than $40\\%$. The training set is very likely to contain redundant information in terms of text diversity. This is supported by what we observed as model has consistently high IC/SL performances between $40\\%$-$100\\%$ down-sampling ratios..Moreover, the biggest drop of density and homogeneity (highlighted in bold in Table TABREF30) highly correlates with the biggest IC/SL drop, at the point the training set size is reduced from $20\\%$ to $10\\%$. This suggests that our proposed metrics can be used as a good indicator of model performance and for characterizing text datasets. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "28\n",
      "What is the dataset used for sentiment analysis experiments?\n",
      "\n",
      "The dataset used for sentiment analysis experiments is SST-2 (Stanford Sentiment Treebank, version 2) with train/dev/test splits provided and two types of sentence labels, i.e., positive and negative.\n",
      "Question : for the text In the first task, we use the SST-2 (Stanford Sentiment Treebank, version 2) dataset BIBREF25 to conduct sentiment analysis experiments. SST-2 is a sentence binary classification dataset with train/dev/test splits provided and two types of sentence labels, i.e., positive and negative..The second task involves two essential problems in SLU, which are intent classification (IC) and slot labeling (SL). In IC, the model needs to detect the intention of a text input (i.e., utterance, conveys). For example, for an input of I want to book a flight to Seattle, the intention is to book a flight ticket, hence the intent class is bookFlight. In SL, the model needs to extract the semantic entities that are related to the intent. From the same example, Seattle is a slot value related to booking the flight, i.e., the destination. Here we experiment with the Snips dataset BIBREF26, which is widely used in SLU research. This dataset contains test spoken utterances (text) classified into one of 7 intents..In both tasks, we used the open-sourced GluonNLP BERT model to perform text classification. For evaluation, sentiment analysis is measured in accuracy, whereas IC and SL are measured in accuracy and F1 score, respectively. BERT is fine-tuned on train/dev sets and evaluated on test sets..We down-sampled SST-2 and Snips training sets from $100\\%$ to $10\\%$ with intervals being $10\\%$. BERT's performance is reported for each down-sampled setting in Table TABREF29 and Table TABREF30. We used entire test sets for all model evaluations..To compare, we compute the proposed data metrics, i.e., diversity, density, and homogeneity, on the original and the down-sampled training sets. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "29\n",
      "What are the proposed characteristic metrics for summarizing a collection of texts?\n",
      "\n",
      "The proposed characteristic metrics for summarizing a collection of texts are diversity, density, and homogeneity. They measure the dispersion, sparsity, and uniformity of the distribution of the text collection in a high-dimensional embedding space. These metrics can provide quantitative insight for understanding and comparing different text collections from various linguistic perspectives.\n",
      "Question : for the text Characteristic metrics are a set of unsupervised measures that quantitatively describe or summarize the properties of a data collection. These metrics generally do not use ground-truth labels and only measure the intrinsic characteristics of data. The most prominent example is descriptive statistics that summarizes a data collection by a group of unsupervised measures such as mean or median for central tendency, variance or minimum-maximum for dispersion, skewness for symmetry, and kurtosis for heavy-tailed analysis..In recent years, text classification, a category of Natural Language Processing (NLP) tasks, has drawn much attention BIBREF0, BIBREF1, BIBREF2 for its wide-ranging real-world applications such as fake news detection BIBREF3, document classification BIBREF4, and spoken language understanding (SLU) BIBREF5, BIBREF6, BIBREF7, a core task of conversational assistants like Amazon Alexa or Google Assistant..However, there are still insufficient characteristic metrics to describe a collection of texts. Unlike numeric or categorical data, simple descriptive statistics alone such as word counts and vocabulary size are difficult to capture the syntactic and semantic properties of a text collection..In this work, we propose a set of characteristic metrics: diversity, density, and homogeneity to quantitatively summarize a collection of texts where the unit of texts could be a phrase, sentence, or paragraph. A text collection is first mapped into a high-dimensional embedding space. Our characteristic metrics are then computed to measure the dispersion, sparsity, and uniformity of the distribution. Based on the choice of embedding methods, these characteristic metrics can help understand the properties of a text collection from different linguistic perspectives, for example, lexical diversity, syntactic variation, and semantic homogeneity. Our proposed diversity, density, and homogeneity metrics extract hard-to-visualize quantitative insight for a better understanding and comparison between text collections..To verify the effectiveness of proposed characteristic metrics, we first conduct a series of simulation experiments that cover various scenarios in two-dimensional as well as high-dimensional vector spaces. The results show that our proposed quantitative characteristic metrics exhibit several desirable and intuitive properties such as robustness and linear sensitivity of the diversity metric with respect to random down-sampling. Besides, we investigate the relationship between the characteristic metrics and the performance of a renowned model, BERT BIBREF8, on the text classification task using two public benchmark datasets. Our results demonstrate that there are high correlations between text classification model performance and the characteristic metrics, which shows the efficacy of our proposed metrics. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "30\n",
      "Question 1: What is the motivation behind designing the proposed characteristic metrics to be sensitive to text collections of different properties while being robust to the curse of dimensionality?\n",
      "\n",
      "Answer 1: The embedding vectors generated by an embedding method often lie in a high-dimensional space, which motivates the design of characteristic metrics to be sensitive to text collections of different properties while being robust to the curse of dimensionality.\n",
      "Question : for the text We introduce our proposed diversity, density, and homogeneity metrics with their detailed formulations and key intuitions..Our first assumption is, for classification, high-quality training data entail that examples of one class are as differentiable and distinct as possible from another class. From a fine-grained and intra-class perspective, a robust text cluster should be diverse in syntax, which is captured by diversity. And each example should reflect a sufficient signature of the class to which it belongs, that is, each example is representative and contains certain salient features of the class. We define a density metric to account for this aspect. On top of that, examples should also be semantically similar and coherent among each other within a cluster, where homogeneity comes in play..The more subtle intuition emerges from the inter-class viewpoint. When there are two or more class labels in a text collection, in an ideal scenario, we would expect the homogeneity to be monotonically decreasing. Potentially, the diversity is increasing with respect to the number of classes since text clusters should be as distinct and separate as possible from one another. If there is a significant ambiguity between classes, the behavior of the proposed metrics and a possible new metric as a inter-class confusability measurement remain for future work..In practice, the input is a collection of texts $\\lbrace x_1, x_2, ..., x_m\\rbrace $, where $x_i$ is a sequence of tokens $x_{i1}, x_{i2}, ..., x_{il}$ denoting a phrase, a sentence, or a paragraph. An embedding method $\\mathcal {E}$ then transforms $x_i$ into a vector $\\mathcal {E}(x_i)=e_i$ and the characteristic metrics are computed with the embedding vectors. For example,.Note that these embedding vectors often lie in a high-dimensional space, e.g. commonly over 300 dimensions. This motivates our design of characteristic metrics to be sensitive to text collections of different properties while being robust to the curse of dimensionality..We then assume a set of clusters created over the generated embedding vectors. In classification tasks, the embeddings pertaining to members of a class form a cluster, i.e., in a supervised setting. In an unsupervised setting, we may apply a clustering algorithm to the embeddings. It is worth noting that, in general, the metrics are independent of the assumed underlying grouping method. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "31\n",
      "Question 1: What is the reason behind the introduction of dimension normalization in the density metric computation?\n",
      "Answer 1: The introduction of dimension normalization is necessary to accommodate the impact of high-dimensionality and avoid exploding or vanishing density values. It assumes that most variance can be explained or captured in a sub-space of a dimension $\\sqrt{H}$, and computes the geometric mean of their radii as the effective radius.\n",
      "Question : for the text Another interesting characteristic is the sparsity of the text embedding cluster. The density metric is proposed to estimate the number of samples that falls within a unit of volume in an embedding space..Following the assumption mentioned above, a straight-forward definition of the volume can be written as:.up to a constant factor. However, when the dimension goes higher, this formulation easily produces exploding or vanishing density values, i.e., goes to infinity or zero..To accommodate the impact of high-dimensionality, we impose a dimension normalization. Specifically, we introduce a notion of effective axes, which assumes most variance can be explained or captured in a sub-space of a dimension $\\sqrt{H}$. We group all the axes in this sub-space together and compute the geometric mean of their radii as the effective radius. The dimension-normalized volume is then formulated as:.Given a set of embedding vectors $\\lbrace e_1, ..., e_m\\rbrace $, we define the density metric as:.In practice, the computed density metric values often follow a heavy-tailed distribution, thus sometimes its $\\log $ value is reported and denoted as $density (log\\-scale)$. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "32\n",
      "What is the diversity metric used to estimate the dispersion of a cluster of embedding vectors?\n",
      "\n",
      "The diversity metric used to estimate the dispersion of a cluster of embedding vectors is the geometric mean of the radii across all axes, where the radius is defined as the standard deviation (i.e., square root of variance) of the ellipsoid along that axis.\n",
      "Question : for the text Embedding vectors of a given group of texts $\\lbrace e_1, ..., e_m\\rbrace $ can be treated as a cluster in the high-dimensional embedding space. We propose a diversity metric to estimate the cluster's dispersion or spreadness via a generalized sense of the radius..Specifically, if a cluster is distributed as a multi-variate Gaussian with a diagonal covariance matrix $\\Sigma $, the shape of an isocontour will be an axis-aligned ellipsoid in $\\mathbb {R}^{H}$. Such isocontours can be described as:.where $x$ are all possible points in $\\mathbb {R}^{H}$ on an isocontour, $c$ is a constant, $\\mu $ is a given mean vector with $\\mu _j$ being the value along $j$-th axis, and $\\sigma ^2_j$ is the variance of the $j$-th axis..We leverage the geometric interpretation of this formulation and treat the square root of variance, i.e., standard deviation, $\\sqrt{\\sigma ^2_j}$ as the radius $r_j$ of the ellipsoid along the $j$-th axis. The diversity metric is then defined as the geometric mean of radii across all axes:.where $\\sigma _i$ is the standard deviation or square root of the variance along the $i$-th axis..In practice, to compute a diversity metric, we first calculate the standard deviation of embedding vectors along each dimension and take the geometric mean of all calculated values. Note that as the geometric mean acts as a dimensionality normalization, it makes the diversity metric work well in high-dimensional embedding spaces such as BERT. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "33\n",
      "What is the homogeneity metric used for in this text? \n",
      "\n",
      "The homogeneity metric is proposed to summarize the uniformity of a cluster distribution, specifically how uniformly the embedding vectors of the samples in a group of texts are distributed in the embedding space.\n",
      "Question : for the text The homogeneity metric is proposed to summarize the uniformity of a cluster distribution. That is, how uniformly the embedding vectors of the samples in a group of texts are distributed in the embedding space. We propose to quantitatively describe homogeneity by building a fully-connected, edge-weighted network, which can be modeled by a Markov chain model. A Markov chain's entropy rate is calculated and normalized to be in $[0, 1]$ range by dividing by the entropy's theoretical upper bound. This output value is defined as the homogeneity metric detailed as follows:.To construct a fully-connected network from the embedding vectors $\\lbrace e_1, ..., e_m\\rbrace $, we compute their pairwise distances as edge weights, an idea similar to AttriRank BIBREF22. As the Euclidean distance is not a good metric in high-dimensions, we normalize the distance by adding a power $\\log (n\\_dim)$. We then define a Markov chain model with the weight of $edge(i, j)$ being.and the conditional probability of transition from $i$ to $j$ can be written as.All the transition probabilities $p(i \\rightarrow j)$ are from the transition matrix of a Markov chain. An entropy of this Markov chain can be calculated as.where $\\nu _i$ is the stationary distribution of the Markov chain. As self-transition probability $p(i \\rightarrow i)$ is always zero because of zero distance, there are $(m - 1)$ possible destinations and the entropy's theoretical upper bound becomes.Our proposed homogeneity metric is then normalized into $[0, 1]$ as a uniformity measure:.The intuition is that if some samples are close to each other but far from all the others, the calculated entropy decreases to reflect the unbalanced distribution. In contrast, if each sample can reach other samples within more-or-less the same distances, the calculated entropy as well as the homogeneity measure would be high as it implies the samples could be more uniformly distributed. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "34\n",
      "Question 1: What are some examples of language representation methods used for NLP tasks?\n",
      "\n",
      "Answer 1: Some examples of language representation methods used for NLP tasks include n-gram, word2vec, GloVe, fastText, skip-thought vectors, self-attentive sentence encoders, ULMFiT, ELMo, OpenAI GPT, and BERT.\n",
      "Question : for the text A building block of characteristic metrics for text collections is the language representation method. A classic way to represent a sentence or a paragraph is n-gram, with dimension equals to the size of vocabulary. More advanced methods learn a relatively low dimensional latent space that represents each word or token as a continuous semantic vector such as word2vec BIBREF9, GloVe BIBREF10, and fastText BIBREF11. These methods have been widely adopted with consistent performance improvements on many NLP tasks. Also, there has been extensive research on representing a whole sentence as a vector such as a plain or weighted average of word vectors BIBREF12, skip-thought vectors BIBREF13, and self-attentive sentence encoders BIBREF14..More recently, there is a paradigm shift from non-contextualized word embeddings to self-supervised language model (LM) pretraining. Language encoders are pretrained on a large text corpus using a LM-based objective and then re-used for other NLP tasks in a transfer learning manner. These methods can produce contextualized word representations, which have proven to be effective for significantly improving many NLP tasks. Among the most popular approaches are ULMFiT BIBREF2, ELMo BIBREF15, OpenAI GPT BIBREF16, and BERT BIBREF8. In this work, we adopt BERT, a transformer-based technique for NLP pretraining, as the backbone to embed a sentence or a paragraph into a representation vector..Another stream of related works is the evaluation metrics for cluster analysis. As measuring property or quality of outputs from a clustering algorithm is difficult, human judgment with cluster visualization tools BIBREF17, BIBREF18 are often used. There are unsupervised metrics to measure the quality of a clustering result such as the Calinski-Harabasz score BIBREF19, the Davies-Bouldin index BIBREF20, and the Silhouette coefficients BIBREF21. Complementary to these works that model cross-cluster similarities or relationships, our proposed diversity, density and homogeneity metrics focus on the characteristics of each single cluster, i.e., intra cluster rather than inter cluster relationships. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "35\n",
      "Question 1: What is the dimensionality of the spaces used in the simulation experiments?\n",
      "\n",
      "Answer 1: The simulation experiments were conducted in both 2-dimensional space and 768-dimensional space, which is the same dimensionality as the output of the chosen embedding method-BERT.\n",
      "Question : for the text To verify that each proposed characteristic metric holds its desirable and intuitive properties, we conduct a series of simulation experiments in 2-dimensional as well as 768-dimensional spaces. The latter has the same dimensionality as the output of our chosen embedding method-BERT, in the following Experiments section. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "36\n",
      "What is the desired property of the diversity metric?\n",
      "\n",
      "The desired property of the diversity metric is that it remains almost the same to the down-sampling of an input cluster, indicating that it is insensitive to the size of inputs.\n",
      "Question : for the text Figure FIGREF24 summarizes calculated diversity metrics in the first row, density metrics in the second row, and homogeneity metrics in the third row, for all simulation scenarios..The diversity metric is robust as its values remain almost the same to the down-sampling of an input cluster. This implies the diversity metric has a desirable property that it is insensitive to the size of inputs. On the other hand, it shows a linear relationship to varying spreads. It is another intuitive property for a diversity metric that it grows linearly with increasing dispersion or variance of input data. With more outliers or more sub-clusters, the diversity metric can also reflect the increasing dispersion of cluster distributions but is less sensitive in high-dimensional spaces..For the density metrics, it exhibits a linear relationship to the size of inputs when down-sampling, which is desired. When increasing spreads, the trend of density metrics corresponds well with human intuition. Note that the density metrics decrease at a much faster rate in higher-dimensional space as log-scale is used in the figure. The density metrics also drop when adding outliers or having multiple distant sub-clusters. This makes sense since both scenarios should increase the dispersion of data and thus increase our notion of volume as well. In multiple sub-cluster scenario, the density metric becomes less sensitive in the higher-dimensional space. The reason could be that the sub-clusters are distributed only along one axis and thus have a smaller impact on volume in higher-dimensional spaces..As random down-sampling or increasing variance of each axis should not affect the uniformity of a cluster distribution, we expect the homogeneity metric remains approximately the same values. And the proposed homogeneity metric indeed demonstrates these ideal properties. Interestingly, for outliers, we first saw huge drops of the homogeneity metric but the values go up again slowly when more outliers are added. This corresponds well with our intuitions that a small number of outliers break the uniformity but more outliers should mean an increase of uniformity because the distribution of added outliers themselves has a high uniformity..For multiple sub-clusters, as more sub-clusters are presented, the homogeneity should and does decrease as the data are less and less uniformly distributed in the space..To sum up, from all simulations, our proposed diversity, density, and homogeneity metrics indeed capture the essence or intuition of dispersion, sparsity, and uniformity in a cluster distribution. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "37\n",
      "Question 1: What are the four simulation scenarios used to investigate the behavior of the proposed quantitative characteristic metrics?\n",
      "\n",
      "Answer 1: The four simulation scenarios are: down-sampling, varying spread, outliers, and multiple sub-clusters. Down-sampling involves creating Gaussian blobs with decreasing numbers of data points. Varying spread involves generating Gaussian blobs with different standard deviations for each axis. Outliers involves adding randomly placed outlier data points. Multiple sub-clusters involves creating multiple clusters at increasing distances along one axis.\n",
      "Question : for the text The base simulation setup is a randomly generated isotropic Gaussian blob that contains $10,000$ data points with the standard deviation along each axis to be $1.0$ and is centered around the origin. All Gaussian blobs are created using make_blobs function in the scikit-learn package..Four simulation scenarios are used to investigate the behavior of our proposed quantitative characteristic metrics:.Down-sampling: Down-sample the base cluster to be $\\lbrace 90\\%, 80\\%, ..., 10\\%\\rbrace $ of its original size. That is, create Gaussian blobs with $\\lbrace 9000, ..., 1000\\rbrace $ data points;.Varying Spread: Generate Gaussian blobs with standard deviations of each axis to be $\\lbrace 2.0, 3.0, ..., 10.0\\rbrace $;.Outliers: Add $\\lbrace 50, 100, ..., 500\\rbrace $ outlier data points, i.e., $\\lbrace 0.5\\%, ..., 5\\%\\rbrace $ of the original cluster size, randomly on the surface with a fixed norm or radius;.Multiple Sub-clusters: Along the 1th-axis, with $10,000$ data points in total, create $\\lbrace 1, 2, ..., 10\\rbrace $ clusters with equal sample sizes but at increasing distance..For each scenario, we simulate a cluster and compute the characteristic metrics in both 2-dimensional and 768-dimensional spaces. Figure FIGREF17 visualizes each scenario by t-distributed Stochastic Neighbor Embedding (t-SNE) BIBREF23. The 768-dimensional simulations are visualized by down-projecting to 50 dimensions via Principal Component Analysis (PCA) followed by t-SNE. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "38\n",
      "What is the main finding of the paper on the global governance of development?\n",
      "\n",
      "The paper uncovered the main development topics discussed by governments in the UN General Debate, and the structural factors that influence the degree to which governments discuss international development, shedding light on state preferences regarding the international development agenda in the UN.\n",
      "Question : for the text Despite decisions taken in international organisations having a huge impact on development initiatives and outcomes, we know relatively little about the agenda-setting process around the global governance of development. Using a novel approach that applies NLP methods to a new dataset of speeches in the UN General Debate, this paper has uncovered the main development topics discussed by governments in the UN, and the structural factors that influence the degree to which governments discuss international development. In doing so, the paper has shed some light on state preferences regarding the international development agenda in the UN. The paper more broadly demonstrates how text analytic approaches can help us to better understand different aspects of global governance. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "39\n",
      "Question 1: How was the optimal number of topics for STM analysis assessed?\n",
      "\n",
      "Answer 1: The optimal number of topics for STM analysis was assessed by focusing on exclusivity and semantic coherence measures, as recommended in the original STM paper. The semantic coherence measure proposed by BIBREF5, which is closely related to the point-wise mutual information measure posited by BIBREF6, was used to evaluate topic quality. Exclusivity scores for each topic were determined based on highly frequent words in a given topic that do not appear very often in other topics, as described in BIBREF7. A set of candidate models ranging between 3 and 50 topics was generated, and exclusivity and semantic coherence were plotted with a linear regression overlaid. The 16-topic model was selected as it had the largest positive residual in the regression fit and provided higher exclusivity at the same level of semantic coherence. The topic quality was evaluated by highest probability words, which were presented in Figure FIGREF4.\n",
      "Question : for the text We assess the optimal number of topics that need to be specified for the STM analysis. We follow the recommendations of the original STM paper and focus on exclusivity and semantic coherence measures. BIBREF5 propose semantic coherence measure, which is closely related to point-wise mutual information measure posited by BIBREF6 to evaluate topic quality. BIBREF5 show that semantic coherence corresponds to expert judgments and more general human judgments in Amazon's Mechanical Turk experiments..Exclusivity scores for each topic follows BIBREF7 . Highly frequent words in a given topic that do not appear very often in other topics are viewed as making that topic exclusive. Cohesive and exclusive topics are more semantically useful. Following BIBREF8 we generate a set of candidate models ranging between 3 and 50 topics. We then plot the exclusivity and semantic coherence (numbers closer to 0 indicate higher coherence), with a linear regression overlaid (Figure FIGREF3 ). Models above the regression line have a “better” exclusivity-semantic coherence trade off. We select the 16-topic model, which has the largest positive residual in the regression fit, and provides higher exclusivity at the same level of semantic coherence. The topic quality is usually evaluated by highest probability words, which is presented in Figure FIGREF4 . generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "40\n",
      "What is the impact of wealth on the extent to which countries discuss the two international development topics in their GD statements?\n",
      "\n",
      "The figure in the text (FIGREF9) shows that there is a decline in the proportion of both topics as countries become wealthier until around $30,000 when there is an increase in discussion of Topic 7 (sustainable development). There is a further pronounced increase in the extent countries discuss Topic 7 at around $60,000 per capita. However, there is a decline in expected topic proportions for both Topic 2 (economic development) and Topic 7 for the very wealthiest countries.\n",
      "Question : for the text We next look at the relationship between topic proportions and structural factors. The data for these structural covariates is taken from the World Bank's World Development Indicators (WDI) unless otherwise stated. Confidence intervals produced by the method of composition in STM allow us to pick up statistical uncertainty in the linear regression model..Figure FIGREF9 demonstrates the effect of wealth (GDP per capita) on the the extent to which states discuss the two international development topics in their GD statements. The figure shows that the relationship between wealth and the topic proportions linked to international development differs across Topic 2 and Topic 7. Discussion of Topic 2 (economic development) remains far more constant across different levels of wealth than Topic 7. The poorest states tend to discuss both topics more than other developing nations. However, this effect is larger for Topic 7. There is a decline in the proportion of both topics as countries become wealthier until around $30,000 when there is an increase in discussion of Topic 7. There is a further pronounced increase in the extent countries discuss Topic 7 at around $60,000 per capita. However, there is a decline in expected topic proportions for both Topic 2 and Topic 7 for the very wealthiest countries..Figure FIGREF10 shows the expected topic proportions for Topic 2 and Topic 7 associated with different population sizes. The figure shows a slight surge in the discussion of both development topics for countries with the very smallest populations. This reflects the significant amount of discussion of development issues, particularly sustainable development (Topic 7) by the small island developing states (SIDs). The discussion of Topic 2 remains relatively constant across different population sizes, with a slight increase in the expected topic proportion for the countries with the very largest populations. However, with Topic 7 there is an increase in expected topic proportion until countries have a population of around 300 million, after which there is a decline in discussion of Topic 7. For countries with populations larger than 500 million there is no effect of population on discussion of Topic 7. It is only with the very largest populations that we see a positive effect on discussion of Topic 7..We would also expect the extent to which states discuss international development in their GD statements to be impacted by the amount of aid or official development assistance (ODA) they receive. Figure FIGREF11 plots the expected topic proportion according to the amount of ODA countries receive. Broadly-speaking the discussion of development topics remains largely constant across different levels of ODA received. There is, however, a slight increase in the expected topic proportions of Topic 7 according to the amount of ODA received. It is also worth noting the spikes in discussion of Topic 2 and Topic 7 for countries that receive negative levels of ODA. These are countries that are effectively repaying more in loans to lenders than they are receiving in ODA. These countries appear to raise development issues far more in their GD statements, which is perhaps not altogether surprising..We also consider the effects of democracy on the expected topic proportions of both development topics using the Polity IV measure of democracy BIBREF10 . Figure FIGREF12 shows the extent to which states discuss the international development topics according to their level of democracy. Discussion of Topic 2 is fairly constant across different levels of democracy (although there are some slight fluctuations). However, the extent to which states discuss Topic 7 (sustainable development) varies considerably across different levels of democracy. Somewhat surprisingly the most autocratic states tend to discuss Topic 7 more than the slightly less autocratic states. This may be because highly autocratic governments choose to discuss development and environmental issues to avoid a focus on democracy and human rights. There is then an increase in the expected topic proportion for Topic 7 as levels of democracy increase reaching a peak at around 5 on the Polity scale, after this there is a gradual decline in discussion of Topic 7. This would suggest that democratizing or semi-democratic countries (which are more likely to be developing countries with democratic institutions) discuss sustainable development more than established democracies (that are more likely to be developed countries)..We also plot the results of the analysis as the difference in topic proportions for two different values of the effect of conflict. Our measure of whether a country is experiencing a civil conflict comes from the UCDP/PRIO Armed Conflict Dataset BIBREF11 . Point estimates and 95% confidence intervals are plotted in Figure FIGREF13 . The figure shows that conflict affects only Topic 7 and not Topic 2. Countries experiencing conflict are less likely to discuss Topic 7 (sustainable development) than countries not experiencing conflict. The most likely explanation is that these countries are more likely to devote a greater proportion of their annual statements to discussing issues around conflict and security than development. The fact that there is no effect of conflict on Topic 2 is interesting in this regard..Finally, we consider regional effects in Figure FIGREF14 . We use the World Bank's classifications of regions: Latin America and the Caribbean (LCN), South Asia (SAS), Sub-Saharan Africa (SSA), Europe and Central Asia (ECS), Middle East and North Africa (MEA), East Asia and the Pacific (EAS), North America (NAC). The figure shows that states in South Asia, and Latin America and the Caribbean are likely to discuss Topic 2 the most. States in South Asia and East Asia and the Pacific discuss Topic 7 the most. The figure shows that countries in North America are likely to speak about Topic 7 least..The analysis of discussion of international development in annual UN General Debate statements therefore uncovers two principle development topics: economic development and sustainable development. We find that discussion of Topic 2 is not significantly impacted by country-specific factors, such as wealth, population, democracy, levels of ODA, and conflict (although there are regional effects). However, we find that the extent to which countries discuss sustainable development (Topic 7) in their annual GD statements varies considerably according to these different structural factors. The results suggest that broadly-speaking we do not observe linear trends in the relationship between these country-specific factors and discussion of Topic 7. Instead, we find that there are significant fluctuations in the relationship between factors such as wealth, democracy, etc., and the extent to which these states discuss sustainable development in their GD statements. These relationships require further analysis and exploration. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "41\n",
      "Question 1: What is the General Debate and why is it important for understanding international development priorities of governments?\n",
      "\n",
      "Answer 1: The General Debate is a yearly event where the heads of state and other high-level country representatives gather at the United Nations General Assembly to present their views on key issues in international politics, including international development. The statements made during the General Debate are an important source of information on governments' policy preferences on international development over time, as it provides an opportunity for governments to raise the issues they consider the most important and put different policy issues on the international agenda. The General Debate is not institutionally connected to decision-making in the UN, meaning that governments face few external constraints when delivering these speeches, and can be viewed as a key forum for governments to identify like-minded members and establish international development priorities.\n",
      "Question : for the text Decisions made in international organisations are fundamental to international development efforts and initiatives. It is in these global governance arenas that the rules of the global economic system, which have a huge impact on development outcomes are agreed on; decisions are made about large-scale funding for development issues, such as health and infrastructure; and key development goals and targets are agreed on, as can be seen with the Millennium Development Goals (MDGs). More generally, international organisations have a profound influence on the ideas that shape international development efforts BIBREF0 ..Yet surprisingly little is known about the agenda-setting process for international development in global governance institutions. This is perhaps best demonstrated by the lack of information on how the different goals and targets of the MDGs were decided, which led to much criticism and concern about the global governance of development BIBREF1 . More generally, we know little about the types of development issues that different countries prioritise, or whether country-specific factors such as wealth or democracy make countries more likely to push for specific development issues to be put on the global political agenda..The lack of knowledge about the agenda setting process in the global governance of development is in large part due to the absence of obvious data sources on states' preferences about international development issues. To address this gap we employ a novel approach based on the application of natural language processing (NLP) to countries' speeches in the UN. Every September, the heads of state and other high-level country representatives gather in New York at the start of a new session of the United Nations General Assembly (UNGA) and address the Assembly in the General Debate. The General Debate (GD) provides the governments of the almost two hundred UN member states with an opportunity to present their views on key issues in international politics – including international development. As such, the statements made during GD are an invaluable and, largely untapped, source of information on governments' policy preferences on international development over time..An important feature of these annual country statements is that they are not institutionally connected to decision-making in the UN. This means that governments face few external constraints when delivering these speeches, enabling them to raise the issues that they consider the most important. Therefore, the General Debate acts “as a barometer of international opinion on important issues, even those not on the agenda for that particular session” BIBREF2 . In fact, the GD is usually the first item for each new session of the UNGA, and as such it provides a forum for governments to identify like-minded members, and to put on the record the issues they feel the UNGA should address. Therefore, the GD can be viewed as a key forum for governments to put different policy issues on international agenda..We use a new dataset of GD statements from 1970 to 2016, the UN General Debate Corpus (UNGDC), to examine the international development agenda in the UN BIBREF3 . Our application of NLP to these statements focuses in particular on structural topic models (STMs) BIBREF4 . The paper makes two contributions using this approach: (1) It sheds light on the main international development issues that governments prioritise in the UN; and (2) It identifies the key country-specific factors associated with governments discussing development issues in their GD statements. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "42\n",
      "Question 1: What method did the analysis use to identify the key international development topics in the UN General Debates? \n",
      "\n",
      "Answer 1: The analysis used a structural topic model to identify the key international development topics discussed in the UN General Debates. This method enabled the researchers to model the prevalence of topics in the context of structural covariates and control for region fixed effects and time trends.\n",
      "Question : for the text In the analysis we consider the nature of international development issues raised in the UN General Debates, and the effect of structural covariates on the level of developmental rhetoric in the GD statements. To do this, we first implement a structural topic model BIBREF4 . This enables us to identify the key international development topics discussed in the GD. We model topic prevalence in the context of the structural covariates. In addition, we control for region fixed effects and time trend. The aim is to allow the observed metadata to affect the frequency with which a topic is discussed in General Debate speeches. This allows us to test the degree of association between covariates (and region/time effects) and the average proportion of a document discussing a topic. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "43\n",
      "What are the two principal \"international development\" topics identified through the STM analysis of UN General Debate statements?\n",
      "The two principal \"international development\" topics identified through the STM analysis of UN General Debate statements are Topic 2 - Economic development and the global system, and Topic 7 - Sustainable development.\n",
      "Question : for the text Figure FIGREF4 provides a list of the main topics (and the highest probability words associated these topics) that emerge from the STM of UN General Debate statements. In addition to the highest probability words, we use several other measures of key words (not presented here) to interpret the dimensions. This includes the FREX metric (which combines exclusivity and word frequency), the lift (which gives weight to words that appear less frequently in other topics), and the score (which divides the log frequency of the word in the topic by the log frequency of the word in other topics). We provide a brief description of each of the 16 topics here..Topic 1 - Security and cooperation in Europe..The first topic is related to issues of security and cooperation, with a focus on Central and Eastern Europe..Topic 2 - Economic development and the global system..This topic is related to economic development, particularly around the global economic system. The focus on `trade', `growth', `econom-', `product', `growth', `financ-', and etc. suggests that Topic 2 represent a more traditional view of international development in that the emphasis is specifically on economic processes and relations..Topic 3 - Nuclear disarmament..This topic picks up the issue of nuclear weapons, which has been a major issue in the UN since its founding..Topic 4 - Post-conflict development..This topic relates to post-conflict development. The countries that feature in the key words (e.g. Rwanda, Liberia, Bosnia) have experienced devastating civil wars, and the emphasis on words such as `develop', `peace', `hope', and `democrac-' suggest that this topic relates to how these countries recover and move forward..Topic 5 - African independence / decolonisation..This topic picks up the issue of African decolonisation and independence. It includes the issue of apartheid in South Africa, as well as racism and imperialism more broadly..Topic 6 - Africa..While the previous topic focused explicitly on issues of African independence and decolonisation, this topic more generally picks up issues linked to Africa, including peace, governance, security, and development..Topic 7 - Sustainable development..This topic centres on sustainable development, picking up various issues linked to development and climate change. In contrast to Topic 2, this topic includes some of the newer issues that have emerged in the international development agenda, such as sustainability, gender, education, work and the MDGs..Topic 8 - Functional topic..This topic appears to be comprised of functional or process-oriented words e.g. `problem', `solution', `effort', `general', etc..Topic 9 - War..This topic directly relates to issues of war. The key words appear to be linked to discussions around ongoing wars..Topic 10 - Conflict in the Middle East..This topic clearly picks up issues related to the Middle East – particularly around peace and conflict in the Middle East..Topic 11 - Latin America..This is another topic with a regional focus, picking up on issues related to Latin America..Topic 12 - Commonwealth..This is another of the less obvious topics to emerge from the STM in that the key words cover a wide range of issues. However, the places listed (e.g. Australia, Sri Lanka, Papua New Guinea) suggest the topic is related to the Commonwealth (or former British colonies)..Topic 13 - International security..This topic broadly captures international security issues (e.g. terrorism, conflict, peace) and in particularly the international response to security threats, such as the deployment of peacekeepers..Topic 14 - International law..This topic picks up issues related to international law, particularly connected to territorial disputes..Topic 15 - Decolonisation..This topic relates more broadly to decolonisation. As well as specific mention of decolonisation, the key words include a range of issues and places linked to the decolonisation process..Topic 16 - Cold War..This is another of the less tightly defined topics. The topics appears to pick up issues that are broadly related to the Cold War. There is specific mention of the Soviet Union, and detente, as well as issues such as nuclear weapons, and the Helsinki Accords..Based on these topics, we examine Topic 2 and Topic 7 as the principal “international development” topics. While a number of other topics – for example post-conflict development, Africa, Latin America, etc. – are related to development issues, Topic 2 and Topic 7 most directly capture aspects of international development. We consider these two topics more closely by contrasting the main words linked to these two topics. In Figure FIGREF6 , the word clouds show the 50 words most likely to mentioned in relation to each of the topics..The word clouds provide further support for Topic 2 representing a more traditional view of international development focusing on economic processes. In addition to a strong emphasis on 'econom-', other key words, such as `trade', `debt', `market', `growth', `industri-', `financi-', `technolog-', `product', and `argicultur-', demonstrate the narrower economic focus on international development captured by Topic 2. In contrast, Topic 7 provides a much broader focus on development, with key words including `climat-', `sustain', `environ-', `educ-', `health', `women', `work', `mdgs', `peac-', `govern-', and `right'. Therefore, Topic 7 captures many of the issues that feature in the recent Sustainable Development Goals (SDGs) agenda BIBREF9 ..Figure FIGREF7 calculates the difference in probability of a word for the two topics, normalized by the maximum difference in probability of any word between the two topics. The figure demonstrates that while there is a much high probability of words, such as `econom-', `trade', and even `develop-' being used to discuss Topic 2; words such as `climat-', `govern-', `sustain', `goal', and `support' being used in association with Topic 7. This provides further support for the Topic 2 representing a more economistic view of international development, while Topic 7 relating to a broader sustainable development agenda..We also assess the relationship between topics in the STM framework, which allows correlations between topics to be examined. This is shown in the network of topics in Figure FIGREF8 . The figure shows that Topic 2 and Topic 7 are closely related, which we would expect as they both deal with international development (and share key words on development, such as `develop-', `povert-', etc.). It is also worth noting that while Topic 2 is more closely correlated with the Latin America topic (Topic 11), Topic 7 is more directly correlated with the Africa topic (Topic 6). generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "44\n",
      "Question 1: What are some novel features supported by QnAMaker?\n",
      "Answer 1: QnAMaker supports personality-grounded chit-chat, active learning based on user-interaction feedback, and hierarchical extraction for multi-turn conversations.\n",
      "Question : for the text We demonstrate QnAMaker: a service to add a conversational layer over semi-structured user data. In addition to query-answering, we support novel features like personality-grounded chit-chat, active learning based on user-interaction feedback (Figure FIGREF40), and hierarchical extraction for multi-turn conversations (Figure FIGREF41). The goal of the demonstration will be to show how easy it is to create an intelligent bot using QnAMaker. All the demonstrations will be done on the production website Demo Video can be seen here. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "45\n",
      "Question 1: Can QnAMaker be used for any type of data? \n",
      "\n",
      "Answer 1: Yes, QnAMaker is not domain-specific and can be used for any type of data. Its performance has been evaluated across various domains and has significant F1/AUC scores. Hybrid of deep learning (CDSSM) and machine learning features give QnAMaker's ranking model low computation cost and high explainability.\n",
      "Question : for the text QnAMaker is not domain-specific and can be used for any type of data. To support this claim, we measure our system's performance for datasets across various domains. The evaluations are done by managed judges who understands the knowledge base and then judge user queries relevance to the QA pairs (binary labels). Each query-QA pair is judged by two judges. We filter out data for which judges do not agree on the label. Chit-chat in itself can be considered as a domain. Thus, we evaluate performance on given KB both with and without chit-chat data (last two rows in Table TABREF19), as well as performance on just chit-chat data (2nd row in Table TABREF19). Hybrid of deep learning(CDSSM) and machine learning features give our ranking model low computation cost, high explainability and significant F1/AUC score. Based on QnAMaker usage, we observed these trends:.Around 27% of the knowledge bases created use pre-built persona-based chitchat, out of which, $\\sim $4% of the knowledge bases are created for chit-chat alone. The highest used personality is Professional which is used in 9% knowledge bases..Around $\\sim $25% developers have enabled active learning suggestions. The acceptance to reject ratio for active learning suggestions is 0.31..25.5% of the knowledge bases use one URL as a source while creation. $\\sim $41% of the knowledge bases created use different sources like multiple URLs. 15.19% of the knowledge bases use both URL and editorial content as sources. Rest use just editorial content. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "46\n",
      "Question 1: What new features will be supported by QnAMaker in the future?\n",
      "\n",
      "Answer 1: QnAMaker will soon support Answer Span and KB-grounded response generation in addition to user-defined personas for chit-chat learned from user-documents. The system is also experimenting with semantic vector-based search and transformer-based models for re-ranking in order to improve its ranking system. It is also working on enhancing its extraction capabilities to work with unstructured documents and images.\n",
      "Question : for the text The system currently doesn't highlight the answer span and does not generate answers taking the KB as grounding. We will be soon supporting Answer Span BIBREF9 and KB-grounded response generation BIBREF10 in QnAMaker. We are also working on user-defined personas for chit-chat (automatically learned from user-documents). We aim to enhance our extraction to be able to work for any unstructured document as well as images. We are also experimenting on improving our ranking system by using semantic vector-based search as our retrieval and transformer-based models for re-ranking. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "47\n",
      "Question 1: What is QnAMaker and how does it simplify bot creation?\n",
      "\n",
      "Answer 1: QnAMaker is a tool that extracts Question-Answer pairs from user data and stores them in a Knowledge Base (KB) for use in bot creation. It also provides a conversational layer over the KB, allowing developers to answer user queries with natural language processing (NLP). QnAMaker offers unique features such as a persona-based chit-chat layer, active-learning, and support for over 35 languages. It follows a Server-Client architecture and gives users complete control over their data.\n",
      "Question : for the text QnAMaker aims to simplify the process of bot creation by extracting Question-Answer (QA) pairs from data given by users into a Knowledge Base (KB) and providing a conversational layer over it. KB here refers to one instance of azure search index, where the extracted QA are stored. Whenever a developer creates a KB using QnAMaker, they automatically get all NLP capabilities required to answer user's queries. There are other systems such as Google's Dialogflow, IBM's Watson Discovery which tries to solve this problem. QnAMaker provides unique features for the ease of development such as the ability to add a persona-based chit-chat layer on top of the bot. Additionally, bot developers get automatic feedback from the system based on end-user traffic and interaction which helps them in enriching the KB; we call this feature active-learning. Our system also allows user to add Multi-Turn structure to KB using hierarchical extraction and contextual ranking. QnAMaker today supports over 35 languages, and is the only system among its competitors to follow a Server-Client architecture; all the KB data rests only in the client's subscription, giving users total control over their data. QnAMaker is part of Microsoft Cognitive Service and currently runs using the Microsoft Azure Stack. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "48\n",
      "Question 1: How are suggestions for improving the quality of KBs generated using active learning?\n",
      "\n",
      "Answer 1: Suggestions for improving KB quality using active learning are generated based on end-user feedback and the ranker's implicit signals. The active learning system disambiguates conflicting signals by showing suggestions to bot developers and optimizes the number of suggestions shown using DB-Scan clustering.\n",
      "Question : for the text The majority of the KBs are created using existing FAQ pages or manuals but to improve the quality it requires effort from the developers. Active learning generates suggestions based on end-user feedback as well as ranker's implicit signals. For instance, if for a query, CDSSM feature was confident that one QnA should be ranked higher whereas wordnet feature thought other QnA should be ranked higher, active learning system will try to disambiguate it by showing this as a suggestion to the bot developer. To avoid showing similar suggestions to developers, DB-Scan clustering is done which optimizes the number of suggestions shown. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "49\n",
      "Question 1: What is the QnAMaker Portal?\n",
      "\n",
      "Answer 1: The QnAMaker Portal is a Graphical User Interface (GUI) designed to ease the use of management APIs for creating and managing knowledge bases for bots.\n",
      "Question : for the text As shown in Figure FIGREF4, humans can have two different kinds of roles in the system: Bot-Developers who want to create a bot using the data they have, and End-Users who will chat with the bot(s) created by bot-developers. The components involved in the process are:.QnAMaker Portal: This is the Graphical User Interface (GUI) for using QnAMaker. This website is designed to ease the use of management APIs. It also provides a test pane..QnaMaker Management APIs: This is used for the extraction of Question-Answer (QA) pairs from semi-structured content. It then passes these QA pairs to the web app to create the Knowledge Base Index..Azure Search Index: Stores the KB with questions and answers as indexable columns, thus acting as a retrieval layer..QnaMaker WebApp: Acts as a layer between the Bot, Management APIs, and Azure Search Index. WebApp does ranking on top of retrieved results. WebApp also handles feedback management for active learning..Bot: Calls the WebApp with the User's query to get results. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "50\n",
      "Question 1: What is the process for creating a bot? \n",
      "\n",
      "Answer 1: The process for creating a bot involves three steps: creating a QnaMaker Resource in Azure, using Management APIs to create/update/delete your knowledge base, and creating a bot using any framework and calling the WebApp hosted in Azure to get your queries answered.\n",
      "Question : for the text Creating a bot is a 3-step process for a bot developer:.Create a QnaMaker Resource in Azure: This creates a WebApp with binaries required to run QnAMaker. It also creates an Azure Search Service for populating the index with any given knowledge base, extracted from user data.Use Management APIs to Create/Update/Delete your KB: The Create API automatically extracts the QA pairs and sends the Content to WebApp, which indexes it in Azure Search Index. Developers can also add persona-based chat content and synonyms while creating and updating their KBs..Bot Creation: Create a bot using any framework and call the WebApp hosted in Azure to get your queries answered. There are Bot-Framework templates provided for the same. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "1\n",
      "Question 1: What is the role of the Extraction component in the QnAMaker WebApp?\n",
      "\n",
      "Answer 1: The Extraction component is responsible for understanding a given document and extracting potential QA pairs. These QA pairs are used to create a KB to be consumed later on by the QnAMaker WebApp to answer user queries. The Extraction component extracts basic blocks such as text and lines from the document, followed by understanding the layout of the document. Each element is then tagged according to its type, and agglomerative clustering is used to identify hierarchy and form an intent tree. The intent tree is further augmented with entities using CRF-based sequence labeling to resolve potential ambiguity.\n",
      "Question : for the text The Extraction component is responsible for understanding a given document and extracting potential QA pairs. These QA pairs are in turn used to create a KB to be consumed later on by the QnAMaker WebApp to answer user queries. First, the basic blocks from given documents such as text, lines are extracted. Then the layout of the document such as columns, tables, lists, paragraphs, etc is extracted. This is done using Recursive X-Y cut BIBREF0. Following Layout Understanding, each element is tagged as headers, footers, table of content, index, watermark, table, image, table caption, image caption, heading, heading level, and answers. Agglomerative clustering BIBREF1 is used to identify heading and hierarchy to form an intent tree. Leaf nodes from the hierarchy are considered as QA pairs. In the end, the intent tree is further augmented with entities using CRF-based sequence labeling. Intents that are repeated in and across documents are further augmented with their parent intent, adding more context to resolve potential ambiguity. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "2\n",
      "Question 1: How does QnAMaker handle chit-chat queries in bots?\n",
      "\n",
      "Answer 1: QnAMaker now allows bot developers to directly enable handling chit-chat queries like “hi,\" “thank you,\" and “what's up\" in their bots. They also provide the flexibility of grounding responses in a specific personality, such as professional, witty, friendly, caring, or enthusiastic. Additionally, QnAMaker has a list of 100+ predefined intents and a curated list of queries for each intent, with a separate query understanding layer for ranking these intents. The arbitration between chit-chat answers and user's knowledge base answers is handled by using a chat-domain classifier.\n",
      "Question : for the text We add support for bot-developers to directly enable handling chit-chat queries like “hi\", “thank you\", “what's up\" in their QnAMaker bots. In addition to chit-chat, we also give bot developers the flexibility to ground responses for such queries in a specific personality: professional, witty, friendly, caring, or enthusiastic. For example, the “Humorous\" personality can be used for a casual bot, whereas a “Professional\" personality is more suited in case of banking FAQs or task-completion bots. There is a list of 100+ predefined intents BIBREF7. There is a curated list of queries for each of these intents, along with a separate query understanding layer for ranking these intents. The arbitration between chit-chat answers and user's knowledge base answers is handled by using a chat-domain classifier BIBREF8. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "3\n",
      "Question 1: What is the retrieval layer used in QnAMaker and how does it work? \n",
      "\n",
      "Answer 1: QnAMaker uses Azure Search Index as its retrieval layer, which is based on inverted indexing and TF-IDF scores. Azure Search also provides fuzzy matching based on edit-distance, lemmatization and normalization, making it robust to spelling mistakes. The retrieved results are then re-ranked on top of the Azure Search Index, lowering the burden on QnAMaker WebApp which gets less than 100 results to re-rank.\n",
      "Question : for the text QnAMaker uses Azure Search Index as it's retrieval layer, followed by re-ranking on top of retrieved results (Figure FIGREF21). Azure Search is based on inverted indexing and TF-IDF scores. Azure Search provides fuzzy matching based on edit-distance, thus making retrieval robust to spelling mistakes. It also incorporates lemmatization and normalization. These indexes can scale up to millions of documents, lowering the burden on QnAMaker WebApp which gets less than 100 results to re-rank..Different customers may use QnAMaker for different scenarios such as banking task completion, answering FAQs on company policies, or fun and engagement. The number of QAs, length of questions and answers, number of alternate questions per QA can vary significantly across different types of content. Thus, the ranker model needs to use features that are generic enough to be relevant across all use cases. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "4\n",
      "Question 1: What are the benefits of knowing about XYZ? \n",
      "Answer 1: To know about the benefits of XYZ, you can look at its usefulness in different areas and industries, such as healthcare, education, and technology. Additionally, knowing about XYZ can help you make better-informed decisions and stay up-to-date on the latest advancements in the field.\n",
      "Question : for the text We extend the features for contextual ranking by modifying the candidate QAs and user query in these ways:.$Query_{modified}$ = Query + Previous Answer; For instance, if user query is “yes\" and the previous answer is “do you want to know about XYZ\", the current query becomes “do you want to know about XYZ yes\"..Candidate QnA pairs are appended with its parent Questions and Answers; no contextual information is used from the user's query. For instance, if a candidate QnA has a question “benefits\" and its parent question was “know about XYZ\", the candidate QA's question is changed to “know about XYZ benefits\"..The features mentioned in Section SECREF20 are calculated for the above combinations also. These features carry contextual information. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "5\n",
      "Question 1: What is the price of a table?\n",
      "\n",
      "Answer 1: The price of furniture varies depending on the type and quality of the table. Can you please specify the type of table you are looking for?\n",
      "Question : for the text Going into granular features and the exact empirical formulas used is out of the scope of this paper. The broad level features used while ranking are:.WordNet: There are various features generated using WordNet BIBREF2 matching with questions and answers. This takes care of word-level semantics. For instance, if there is information about “price of furniture\" in a KB and the end-user asks about “price of table\", the user will likely get a relevant answer. The scores of these WordNet features are calculated as a function of:.Distance of 2 words in the WordNet graph.Distance of Lowest Common Hypernym from the root.Knowledge-Base word importance (Local IDFs).Global word importance (Global IDFs).This is the most important feature in our model as it has the highest relative feature gain..CDSSM: Convolutional Deep Structured Semantic Models BIBREF3 are used for sentence-level semantic matching. This is a dual encoder model that converts text strings (sentences, queries, predicates, entity mentions, etc) into their vector representations. These models are trained using millions of Bing Query Title Click-Through data. Using the source-model for vectorizing user query and target-model for vectorizing answer, we compute the cosine similarity between these two vectors, giving the relevance of answer corresponding to the query..TF-IDF: Though sentence-to-vector models are trained on huge datasets, they fail to effectively disambiguate KB specific data. This is where a standard TF-IDF BIBREF4 featurizer with local and global IDFs helps. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "6\n",
      "Question 1: What method is used to prevent overfitting in the gradient-boosted decision tree ranking model?\n",
      "\n",
      "Answer 1: Tolerant Pruning is used to prevent overfitting in the gradient-boosted decision tree ranking model.\n",
      "Question : for the text We use gradient-boosted decision trees as our ranking model to combine all the features. Early stopping BIBREF5 based on Generality-to-Progress ratio is used to decide the number of step trees and Tolerant Pruning BIBREF6 helps prevent overfitting. We follow incremental training if there is small changes in features or training data so that the score distribution is not changed drastically. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "7\n",
      "Question 1: What components does the pre-processing layer use to normalize user queries?\n",
      "\n",
      "Answer 1: The pre-processing layer uses components such as Language Detection, Lemmatization, Speller, and Word Breaker to normalize user queries. It also removes junk characters and stop-words from the user's query.\n",
      "Question : for the text The pre-processing layer uses components such as Language Detection, Lemmatization, Speller, and Word Breaker to normalize user queries. It also removes junk characters and stop-words from the user's query. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "8\n",
      "Question 1: What is the purpose of the feature templates designed in the first task?\n",
      "\n",
      "Answer 1: The feature templates are designed to group grammars by the length of the source side and target side, to calculate relative frequency, reversed relative frequency, lexical probability and reversed lexical probability for all possible subranges of [1, 10] on both sides of a hierarchical grammar.\n",
      "Question : for the text We compare our method with MERT and MIRA in two tasks, iterative training, and N-best list rerank. We do not list PRO BIBREF12 as our baseline, as Cherry et al.BIBREF10 have compared PRO with MIRA and MERT massively..In the first task, we align the FBIS data (about 230K sentence pairs) with GIZA++, and train a 4-gram language model on the Xinhua portion of Gigaword corpus. A hierarchical phrase-based (HPB) model (Chiang, 2007) is tuned on NIST MT 2002, and tested on MT 2004 and 2005. All features are eight basic ones BIBREF20 and extra 220 group features. We design such feature templates to group grammars by the length of source side and target side, (feat-type,a$\\le $src-side$\\le $b,c$\\le $tgt-side$\\le $d), where the feat-type denotes any of the relative frequency, reversed relative frequency, lexical probability and reversed lexical probability, and [a, b], [c, d] enumerate all possible subranges of [1, 10], as the maximum length on both sides of a hierarchical grammar is limited to 10. There are 4 $\\times $ 55 extra group features..In the second task, we rerank an N-best list from a HPB system with 7491 features from a third party. The system uses six million parallel sentence pairs available to the DARPA BOLT Chinese-English task. This system includes 51 dense features (translation probabilities, provenance features, etc.) and up to 7440 sparse features (mostly lexical and fertility-based). The language model is a 6-gram model trained on a 10 billion words, including the English side of our parallel corpora plus other corpora such as Gigaword (LDC2011T07) and Google News. For the tuning and test sets, we use 1275 and 1239 sentences respectively from the LDC2010E30 corpus. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "9\n",
      "What is the average size of the N-best list after being de-duplicated and how many features does it have?\n",
      "\n",
      "Answer 1: The average size of the N-best list after being de-duplicated is around 300 and it has 7491 features.\n",
      "Question : for the text After being de-duplicated, the N-best list has an average size of around 300, and with 7491 features. Refer to Formula DISPLAY_FORM9, this is ideal to use the Plackett-Luce model. Results are shown in Figure FIGREF12. We observe some interesting phenomena..First, the Plackett-Luce models boost the training BLEU very greatly, even up to 2.5 points higher than MIRA. This verifies our assumption, richer features benefit BLEU, though they are optimized towards a different objective..Second, the over-fitting problem of the Plackett-Luce models PL($k$) is alleviated with moderately large $k$. In PL(1), the over-fitting is quite obvious, the portion in which the curve overpasses MIRA is the smallest compared to other $k$, and its convergent performance is below the baseline. When $k$ is not smaller than 5, the curves are almost above the MIRA line. After 500 L-BFGS iterations, their performances are no less than the baseline, though only by a small margin..This experiment displays, in large-scale features, the Plackett-Luce model correlates with BLEU score very well, and alleviates overfitting in some degree. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "10\n",
      "Question 1: What is the Plackett-Luce method and how does it compare to MERT or MIRA in the development and testing data?\n",
      "\n",
      "Answer 1: The Plackett-Luce method is a training algorithm for machine translation models that considers the quality of hypotheses from the 2nd to the kth. While it does not optimize BLEU and has relatively fewer features compared to the size of N-best lists, PL(k) systems perform better than MERT in testing. However, PL(k) systems do not perform as well as MERT or MIRA in the development data. Interestingly, MIRA wins first in training and still performs quite well in testing. PL(1) is equivalent to a max-entropy based algorithm, and as k is increased, performance generally improves but reaches a maximum around k=5 before decreasing slowly.\n",
      "Question : for the text We conduct a full training of machine translation models. By default, a decoder is invoked for at most 40 times, and each time it outputs 200 hypotheses to be combined with those from previous iterations and sent into tuning algorithms..In getting the ground-truth permutations, there are many ties with the same sentence-level BLEU, and we just take one randomly. In this section, all systems have only around two hundred features, hence in Plackett-Luce based training, we sample 30 hypotheses in an accumulative $n$best list in each round of training..All results are shown in Table TABREF10, we can see that all PL($k$) systems does not perform well as MERT or MIRA in the development data, this maybe due to that PL($k$) systems do not optimize BLEU and the features here are relatively not enough compared to the size of N-best lists (empirical Formula DISPLAY_FORM9). However, PL($k$) systems are better than MERT in testing. PL($k$) systems consider the quality of hypotheses from the 2th to the $k$th, which is guessed to act the role of the margin like SVM in classification . Interestingly, MIRA wins first in training, and still performs quite well in testing..The PL(1) system is equivalent to a max-entropy based algorithm BIBREF14 whose dual problem is actually maximizing the conditional probability of one oracle hypothesis. When we increase the $k$, the performances improve at first. After reaching a maximum around $k=5$, they decrease slowly. We explain this phenomenon as this, when features are rich enough, higher BLEU scores could be easily fitted, then longer ground-truth permutations include more useful information. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "11\n",
      "What is the motivation behind the development of a simpler tuning method for large-scale features in statistical machine translation (SMT)?\n",
      "\n",
      "The motivation behind the development of a simpler tuning method for large-scale features in SMT is to provide an alternative to margin infused relaxed algorithms (MIRAs) that consider margin losses related to sentence-level BLEUs. The observation on minimum error rate training (MERT) suggests that an ideal model should benefit the total N-best list, and better hypotheses should be assigned with higher ranks. This can decrease the error risk of top1 result on unseen data. The new method is based on Plackett's model of a permutation and is applicable in large-scale features.\n",
      "Question : for the text Since Och BIBREF0 proposed minimum error rate training (MERT) to exactly optimize objective evaluation measures, MERT has become a standard model tuning technique in statistical machine translation (SMT). Though MERT performs better by improving its searching algorithm BIBREF1, BIBREF2, BIBREF3, BIBREF4, it does not work reasonably when there are lots of features. As a result, margin infused relaxed algorithms (MIRA) dominate in this case BIBREF6, BIBREF7, BIBREF8, BIBREF9, BIBREF10..In SMT, MIRAs consider margin losses related to sentence-level BLEUs. However, since the BLEU is not decomposable into each sentence, these MIRA algorithms use some heuristics to compute the exact losses, e.g., pseudo-document BIBREF8, and document-level loss BIBREF9..Recently, another successful work in large-scale feature tuning include force decoding basedBIBREF11, classification based BIBREF12..We aim to provide a simpler tuning method for large-scale features than MIRAs. Out motivation derives from an observation on MERT. As MERT considers the quality of only top1 hypothesis set, there might have more-than-one set of parameters, which have similar top1 performances in tuning, but have very different topN hypotheses. Empirically, we expect an ideal model to benefit the total N-best list. That is, better hypotheses should be assigned with higher ranks, and this might decrease the error risk of top1 result on unseen data..PlackettBIBREF13 offered an easy-to-understand theory of modeling a permutation. An N-best list is assumedly generated by sampling without replacement. The $i$th hypothesis to sample relies on those ranked after it, instead of on the whole list. This model also supports a partial permutation which accounts for top $k$ positions in a list, regardless of the remaining. When taking $k$ as 1, this model reduces to a standard conditional probabilistic training, whose dual problem is actual the maximum entropy based BIBREF14. Although Och BIBREF0 substituted direct error optimization for a maximum entropy based training, probabilistic models correlate with BLEU well when features are rich enough. The similar claim also appears in BIBREF15. This also make the new method be applicable in large-scale features. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "12\n",
      "Question 1: What is the key idea behind the Plackett-Luce model for predicting ranks of horses in gambling?\n",
      "\n",
      "Answer 1: The key idea behind the Plackett-Luce model is that the choice in each position of a rank only depends on the candidates not chosen at previous stages. The probabilities of generating a rank are calculated iteratively, with the probabilities of each horse being chosen at each stage taking into account the probabilities of the horses not chosen in previous stages. This procedure generates a probability distribution over a set of permutations, which can be used to predict the likelihood of different ranks for a given set of horses.\n",
      "Question : for the text Plackett-Luce was firstly proposed to predict ranks of horses in gambling BIBREF13. Let $\\mathbf {r}=(r_{1},r_{2}\\ldots r_{N})$ be $N$ horses with a probability distribution $\\mathcal {P}$ on their abilities to win a game, and a rank $\\mathbf {\\pi }=(\\pi (1),\\pi (2)\\ldots \\pi (|\\mathbf {\\pi }|))$ of horses can be understood as a generative procedure, where $\\pi (j)$ denotes the index of the horse in the $j$th position..In the 1st position, there are $N$ horses as candidates, each of which $r_{j}$ has a probability $p(r_{j})$ to be selected. Regarding the rank $\\pi $, the probability of generating the champion is $p(r_{\\pi (1)})$. Then the horse $r_{\\pi (1)}$ is removed from the candidate pool..In the 2nd position, there are only $N-1$ horses, and their probabilities to be selected become $p(r_{j})/Z_{2}$, where $Z_{2}=1-p(r_{\\pi (1)})$ is the normalization. Then the runner-up in the rank $\\pi $, the $\\pi (2)$th horse, is chosen at the probability $p(r_{\\pi (2)})/Z_{2}$. We use a consistent terminology $Z_{1}$ in selecting the champion, though $Z_{1}$ equals 1 trivially..This procedure iterates to the last rank in $\\pi $. The key idea for the Plackett-Luce model is the choice in the $i$th position in a rank $\\mathbf {\\pi }$ only depends on the candidates not chosen at previous stages. The probability of generating a rank $\\pi $ is given as follows.where $Z_{j}=1-\\sum _{t=1}^{j-1}p(r_{\\pi (t)})$..We offer a toy example (Table TABREF3) to demonstrate this procedure..Theorem 1 The permutation probabilities $p(\\mathbf {\\pi })$ form a probability distribution over a set of permutations $\\Omega _{\\pi }$. For example, for each $\\mathbf {\\pi }\\in \\Omega _{\\pi }$, we have $p(\\mathbf {\\pi })>0$, and $\\sum _{\\pi \\in \\Omega _{\\pi }}p(\\mathbf {\\pi })=1$..We have to note that, $\\Omega _{\\pi }$ is not necessarily required to be completely ranked permutations in theory and in practice, since gamblers might be interested in only the champion and runner-up, and thus $|\\mathbf {\\pi }|\\le N$. In experiments, we would examine the effects on different length of permutations, systems being termed $PL(|\\pi |)$..Theorem 2 Given any two permutations $\\mathbf {\\pi }$ and $\\mathbf {\\pi }\\prime $, and they are different only in two positions $p$ and $q$, $p<q$, with $\\pi (p)=\\mathbf {\\pi }\\prime (q)$ and $\\pi (q)=\\mathbf {\\pi }\\prime (p)$. If $p(\\pi (p))>p(\\pi (q))$, then $p(\\pi )>p(\\pi \\prime )$..In other words, exchanging two positions in a permutation where the horse more likely to win is not ranked before the other would lead to an increase of the permutation probability..This suggests the ground-truth permutation, ranked decreasingly by their probabilities, owns the maximum permutation probability on a given distribution. In SMT, we are motivated to optimize parameters to maximize the likelihood of ground-truth permutation of an N-best hypotheses..Due to the limitation of space, see BIBREF13, BIBREF16 for the proofs of the theorems. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "13\n",
      "What is the optimization objective in SMT and how is it penalized?\n",
      "\n",
      "The optimization objective in SMT is to maximize the log-likelihood of the ground-truth permutations of hypotheses while penalizing it using a zero-mean and unit-variance Gaussian prior. The score of a hypothesis is defined as the inner product of the weight vector and the feature vector. The log-likelihood function is smooth, differentiable, and concave with the weight vector, and its local maximal solution is also a global maximum. The optimization objective is penalized using a zero-mean and unit-variance Gaussian prior.\n",
      "Question : for the text In SMT, let $\\mathbf {f}=(f_{1},f_{2}\\ldots )$ denote source sentences, and $\\mathbf {e}=(\\lbrace e_{1,1},\\ldots \\rbrace ,\\lbrace e_{2,1},\\ldots \\rbrace \\ldots )$ denote target hypotheses. A set of features are defined on both source and target side. We refer to $h(e_{i,*})$ as a feature vector of a hypothesis from the $i$th source sentence, and its score from a ranking function is defined as the inner product $h(e_{i,*})^{T}w$ of the weight vector $w$ and the feature vector..We first follow the popular exponential style to define a parameterized probability distribution over a list of hypotheses..The ground-truth permutation of an $n$best list is simply obtained after ranking by their sentence-level BLEUs. Here we only concentrate on their relative ranks which are straightforward to compute in practice, e.g. add 1 smoothing. Let $\\pi _{i}^{*}$ be the ground-truth permutation of hypotheses from the $i$th source sentences, and our optimization objective is maximizing the log-likelihood of the ground-truth permutations and penalized using a zero-mean and unit-variance Gaussian prior. This results in the following objective and gradient:.where $Z_{i,j}$ is defined as the $Z_{j}$ in Formula (1) of the $i$th source sentence..The log-likelihood function is smooth, differentiable, and concave with the weight vector $w$, and its local maximal solution is also a global maximum. Iteratively selecting one parameter in $\\alpha $ for tuning in a line search style (or MERT style) could also converge into the global global maximum BIBREF17. In practice, we use more fast limited-memory BFGS (L-BFGS) algorithm BIBREF18. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "14\n",
      "Question 1: What is the empirical formula provided to measure richness in machine translation?\n",
      "\n",
      "Answer 1: The empirical formula provided to measure richness in machine translation is based on the number of features, with a threshold of r is 5. The greater the number of features, the richer the translation.\n",
      "Question : for the text The log-likelihood of a Plackett-Luce model is not a strict upper bound of the BLEU score, however, it correlates with BLEU well in the case of rich features. The concept of “rich” is actually qualitative, and obscure to define in different applications. We empirically provide a formula to measure the richness in the scenario of machine translation..The greater, the richer. In practice, we find a rough threshold of r is 5..In engineering, the size of an N-best list with unique hypotheses is usually less than several thousands. This suggests that, if features are up to thousands or more, the Plackett-Luce model is quite suitable here. Otherwise, we could reduce the size of N-best lists by sampling to make $r$ beyond the threshold..Their may be other efficient sampling methods, and here we adopt a simple one. If we want to $m$ samples from a list of hypotheses $\\mathbf {e}$, first, the $\\frac{m}{3}$ best hypotheses and the $\\frac{m}{3}$ worst hypotheses are taken by their sentence-level BLEUs. Second, we sample the remaining hypotheses on distribution $p(e_{i})\\propto \\exp (h(e_{i})^{T}w)$, where $\\mathbf {w}$ is an initial weight from last iteration. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "15\n",
      "Question 1: Who is being thanked for proofreading?\n",
      "Answer 1: Junghoo (John) Cho is being thanked for proofreading.\n",
      "Question : for the text We would like to thank Junghoo (John) Cho for proofreading. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "16\n",
      "Question 1: What is the purpose of using Byte Pair Encoding (BPE) in the architecture shown in Fig. FIGREF4?\n",
      "\n",
      "Answer 1: The purpose of using Byte Pair Encoding (BPE) in the architecture shown in Fig. FIGREF4 is to compress the transcribed utterance by splitting words into fundamental subword units (pairs of bytes or BPs) and reducing the embedded vocabulary size. This helps in reducing the amount of data the architecture needs to process, and improves its efficiency.\n",
      "Question : for the text The preliminary architecture is shown in Fig. FIGREF4. For a given transcribed utterance, it is firstly encoded with Byte Pair Encoding (BPE) BIBREF14, a compression algorithm splitting words to fundamental subword units (pairs of bytes or BPs) and reducing the embedded vocabulary size. Then we use a BiLSTM BIBREF15 encoder and the output state of the BiLSTM is regarded as a vector representation for this utterance. Finally, a fully connected Feed-forward Neural Network (FNN) followed by a softmax layer, labeled as a multilayer perceptron (MLP) module, is used to perform the domain/intent classification task based on the vector..For convenience, we simplify the whole process in Fig.FIGREF4 as a mapping $BM$ (Baseline Mapping) from the input utterance $S$ to an estimated tag's probability $p(\\tilde{t})$, where $p(\\tilde{t}) \\leftarrow BM(S)$. The $Baseline$ is trained on transcription and evaluated on ASR 1st best hypothesis ($S=\\text{ASR}\\ 1^{st}\\ \\text{best})$. The $Oracle$ is trained on transcription and evaluated on transcription ($S = \\text{Transcription}$). We name it Oracle simply because we assume that hypotheses are noisy versions of transcription. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "17\n",
      "Question 1: What are the different ways ASR $n$-best hypotheses are utilized during evaluation?\n",
      "\n",
      "Answer 1: ASR $n$-best hypotheses are utilized during evaluation in three different ways: through Majority Vote, Sort by Score, and Rerank (Oracle) models. Majority Vote involves applying the BM model on each hypothesis independently and combining predictions by picking the majority predicted label. Sort by Score involves parallel evaluation on all hypotheses, followed by sorting predictions by the corresponding confidence score and choosing the one with the highest score. Finally, Rerank (Oracle) selects the hypothesis with the smallest edit distance to transcription during evaluation and uses its corresponding prediction.\n",
      "Question : for the text Besides the Baseline and Oracle, where only ASR 1-best hypothesis is considered, we also perform experiments to utilize ASR $n$-best hypotheses during evaluation. The models evaluating with $n$-bests and a BM (pre-trained on transcription) are called Direct Models (in Fig. FIGREF7):.Majority Vote. We apply the BM model on each hypothesis independently and combine the predictions by picking the majority predicted label, i.e. Music...Sort by Score. After parallel evaluation on all hypotheses, sort the prediction by the corresponding confidence score and choose the one with the highest score, i.e. Video...Rerank (Oracle). Since the current rerank models (e.g., BIBREF1, BIBREF3, BIBREF4) attempt to select the hypothesis most similar to transcription, we propose the Rerank (Oracle), which picks the hypothesis with the smallest edit distance to transcription (assume it is the $a$-th best) during evaluation and uses its corresponding prediction. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "18\n",
      "Question 1: How does integrating multiple hypotheses improve the performance of the SLU system?\n",
      "\n",
      "Answer 1: Integrating multiple hypotheses into the SLU system can improve its robustness to ASR errors and lead to significant accuracy improvements in domain and intent classifications, with relative gains ranging from 14% to 25%. The improvement is more significant for a subset of testing data where ASR first best is different from transcription. Additionally, utilizing more hypotheses can further improve the performance.\n",
      "Question : for the text This paper improves the SLU system robustness to ASR errors by integrating $n$-best hypotheses in different ways, e.g. the aggregation of predictions from hypotheses or the concatenation of hypothesis text or embedding. We can achieve significant classification accuracy improvements over production-quality baselines on domain and intent classifications, 14% to 25% relative gains. The improvement is more significant for a subset of testing data where ASR first best is different from transcription. We also observe that with more hypotheses utilized, the performance can be further improved. In the future, we aim to employ additional features (e.g. confidence scores for hypotheses or tokens) to integrate $n$-bests more efficiently, where we can train a function $f$ to obtain a weight for each hypothesis embedding before pooling. Another direction is using deep learning framework to embed the word lattice BIBREF16 or confusion network BIBREF17, BIBREF18, which can provide a compact representation of multiple hypotheses and more information like times, in the SLU system. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "19\n",
      "Question 1: How many domains were requests derived from for the annotated user utterances used in the experiments?\n",
      "\n",
      "Answer 1: Requests were derived from 23 domains for the $\\sim$ 8.7M annotated anonymised user utterances used in the experiments.\n",
      "Question : for the text We conduct our experiments on $\\sim $ 8.7M annotated anonymised user utterances. They are annotated and derived from requests across 23 domains. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "20\n",
      "What is the impact of using PoolingAvg on accuracy for 8 popular domains compared to the baseline?\n",
      "\n",
      "The PoolingAvg consistently improves the accuracy for all 8 domains when compared to the baseline, according to the results exhibited in Fig. FIGREF29.\n",
      "Question : for the text Among all the 23 domains, we choose 8 popular domains for further comparisons between the Baseline and the best model of Table TABREF24, PoolingAvg. Fig. FIGREF29 exhibits the results. We could find the PoolingAvg consistently improves the accuracy for all 8 domains..In the previous experiments, the number of utilized hypotheses for each utterance during evaluation is five, which means we use the top 5 interpretations when the size of ASR recognition list is not smaller than 5 and use all the interpretations otherwise. Changing the number of hypotheses while evaluation, Fig. FIGREF30 shows a monotonic increase with the access to more hypotheses for the PoolingAvg and PoolingMax (Sort by Score is shown because it is the best achievable direct model while the Rerank (Oracle) is not realistic). The growth becomes gentle after four hypotheses are leveraged. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "21\n",
      "Question 1: What is the best model used in domain-specific intent classification?\n",
      "\n",
      "Answer 1: The best model used in domain-specific intent classification is PoolingAvg.\n",
      "Question : for the text Since another downstream task, intent classification, is similar to domain classification, we just show the best model in domain classification, PoolingAvg, on domain-specific intent classification for three popular domains due to space limit. As Table TABREF32 shows, the margins of using multiple hypotheses with PoolingAvg are significant as well. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "22\n",
      "What is the main benefit of using multiple hypotheses in the proposed integration models?\n",
      "\n",
      "The main benefit of using multiple hypotheses in the proposed integration models is gained when ASR 1st best disagrees with the transcription, allowing for more information and outperforming ASR errors.\n",
      "Question : for the text To further detect the reason for improvements, we split the test set into two parts based on whether ASR first best agrees with transcription and evaluate separately. Comparing Table TABREF26 and Table TABREF27, obviously the benefits of using multiple hypotheses are mainly gained when ASR 1st best disagrees with the transcription. When ASR 1st best agrees with transcription, the proposed integration models can also keep the performance. Under that condition, we can still improve a little (3.56%) because, by introducing multiple ASR hypotheses, we could have more information and when the transcription/ASR 1st best does not appear in the training set's transcriptions, its $n$-bests list may have similar hypotheses included in the training set's $n$-bests. Then, our integration model trained on $n$-best hypotheses as well has clue to predict. The series of comparisons reveal that our approaches integrating the hypotheses are robust to the ASR errors and whenever the ASR model makes mistakes, we can outperform more significantly. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "23\n",
      "Question 1: What model gets the highest relative improvement in multi-class domain classification?\n",
      "\n",
      "Answer 1: Among the models attempting to integrate $n$-best hypotheses during training, PoolingAvg gets the highest relative improvement, which is 14.29%, according to the text Table TABREF24.\n",
      "Question : for the text Table TABREF24 shows the relative error reduction (RErr) of Baseline, Oracle and our proposed models on the entire test set ($\\sim $ 300K utterances) for multi-class domain classification. We can see among all the direct methods, predicting based on the hypothesis most similar to the transcription (Rerank (Oracle)) is the best..As for the other models attempting to integrate the $n$-bests during training, PoolingAvg gets the highest relative improvement, 14.29%. It as well turns out that all the integration methods outperform direct models drastically. This shows that having access to $n$-best hypotheses during training is crucial for the quality of the predicted semantics. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "24\n",
      "Question 1: What methods are proposed to integrate multiple interpretations during training in the discussed models?\n",
      "Answer 1: The proposed methods to integrate multiple interpretations during training in the discussed models can be divided into two groups, as shown in Fig. FIGREF11. Both methods involve embedding for byte pairs, BiLSTM and MLP parameters, and are trained on $n$-best hypotheses from ASR.\n",
      "Question : for the text All the above mentioned models apply the BM trained on one interpretation (transcription). Their abilities to take advantage of multiple interpretations are actually not trained. As a further step, we propose multiple ways to integrate the $n$-best hypotheses during training. The explored methods can be divided into two groups as shown in Fig. FIGREF11. Let $H_1, H_2,..., H_n $ denote all the hypotheses from ASR and $bp_{H_k, i} \\in BPs$ denotes the $i$-th pair of bytes (BP) in the $k^{th}$ best hypothesis. The model parameters associated with the two possible ways both contain: embedding $e_{bp}$ for pairs of bytes, BiLSTM parameters $\\theta $ and MLP parameters $W, b$. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "25\n",
      "Question 1: What are the PoolingAvg/Max models and how do they work?\n",
      "\n",
      "Answer 1: The PoolingAvg/Max models are integration approaches developed to isolate the embedding process among hypotheses and summarize the features by a pooling layer. For each hypothesis, a sequence of hidden states is obtained from BiLSTM and its final output state is obtained by concatenating the first and last hidden state. The models stack all the output states vertically and employ the top 1 hypothesis for padding to enhance reliability. The Pooling layer (Max/Avg pooling with n by 1 sliding window and stride 1) is then applied to the concatenation to achieve one unified representation and produce one score per possible tag for the given task.\n",
      "Question : for the text The concatenation of hypothesized text leverages the $n$-best list by transferring information among hypotheses in an embedding framework, BiLSTM. However, since all the layers have access to both the preceding and subsequent information, the embedding among $n$-bests will influence each other, which confuses the embedding and makes the whole framework sensitive to the noise in hypotheses..As the second group of integration approaches, we develop models, PoolingAvg/Max, on the concatenation of hypothesis embedding, which isolate the embedding process among hypotheses and summarize the features by a pooling layer. For each hypothesis (e.g., $i^{th}$ best in Eqn. DISPLAY_FORM16 with $j$ pairs of bytes), we could get a sequence of hidden states from BiLSTM and obtain its final output state by concatenating the first and last hidden state ($h_{output_i}$ in Eqn. DISPLAY_FORM17). Then, we stack all the output states vertically as shown in Eqn. SECREF15. Note that in the real data, we will not always have a fixed size of hypotheses list. For a list with $r$ ($<n$) interpretations, we get the embedding for each of them and pad with the embedding of the first best hypothesis until a fixed size $n$. When $r\\ge n$, we only stack the top $n$ embeddings. We employ $h_{output_1}$ for padding to enhance the influence of the top 1 hypothesis, which is more reliable. Finally, one unified representation could be achieved via Pooling (Max/Avg pooling with $n$ by 1 sliding window and stride 1) on the concatenation and one score could be produced per possible tag for the given task. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "26\n",
      "Question 1: How does the basic integration method encode hypotheses and separators?\n",
      "\n",
      "Answer 1: The basic integration method encodes connected hypotheses and separators via BiLSTM to a sequence of hidden state vectors. Each hidden state vector is the concatenation of forward and backward states. The last state of the forward and backward LSTM is concatenated to form the output vector, which is then fed into an MLP module to determine the probability of a specific tag.\n",
      "Question : for the text The basic integration method (Combined Sentence) concatenates the $n$-best hypothesized text. We separate hypotheses with a special delimiter ($<$SEP$>$). We assume BPE totally produces $m$ BPs (delimiters are not split during encoding). Suppose the $n^{th}$ hypothesis has $j$ pairs. The entire model can be formulated as:.In Eqn. DISPLAY_FORM13, the connected hypotheses and separators are encoded via BiLSTM to a sequence of hidden state vectors. Each hidden state vector, e.g. $h_1$, is the concatenation of forward $h_{1f}$ and backward $h_{1b}$ states. The concatenation of the last state of the forward and backward LSTM forms the output vector of BiLSTM (concatenation denoted as $[,]$). Then, in Eqn. DISPLAY_FORM14, the MLP module defines the probability of a specific tag (domain or intent) $\\tilde{t}$ as the normalized activation ($\\sigma $) output after linear transformation of the output vector. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "27\n",
      "Question 1: What is the purpose of reranking the n-best hypotheses in a SLU system? \n",
      "\n",
      "Answer 1: The purpose of reranking the n-best hypotheses in a SLU system is to select the best interpretation from the list and reduce the word error rate of speech recognition. This is achieved through involving morphological, lexical or syntactic features, confidence scores, and other features like the number of tokens and rank position.\n",
      "Question : for the text Currently, voice-controlled smart devices are widely used in multiple areas to fulfill various tasks, e.g. playing music, acquiring weather information and booking tickets. The SLU system employs several modules to enable the understanding of the semantics of the input speeches. When there is an incoming speech, the ASR module picks it up and attempts to transcribe the speech. An ASR model could generate multiple interpretations for most speeches, which can be ranked by their associated confidence scores. Among the $n$-best hypotheses, the top-1 hypothesis is usually transformed to the NLU module for downstream tasks such as domain classification, intent classification and named entity recognition (slot tagging). Multi-domain NLU modules are usually designed hierarchically BIBREF0. For one incoming utterance, NLU modules will firstly classify the utterance as one of many possible domains and the further analysis on intent classification and slot tagging will be domain-specific..In spite of impressive development on the current SLU pipeline, the interpretation of speech could still contain errors. Sometimes the top-1 recognition hypothesis of ASR module is ungrammatical or implausible and far from the ground-truth transcription BIBREF1, BIBREF2. Among those cases, we find one interpretation exact matching with or more similar to transcription can be included in the remaining hypotheses ($2^{nd}- n^{th}$)..To illustrate the value of the $2^{nd}- n^{th}$ hypotheses, we count the frequency of exact matching and more similar (smaller edit distance compared to the 1st hypothesis) to transcription for different positions of the $n$-best hypotheses list. Table TABREF1 exhibits the results. For the explored dataset, we only collect the top 5 interpretations for each utterance ($n = 5$). Notably, when the correct recognition exists among the 5 best hypotheses, 50% of the time (sum of the first row's percentages) it occurs among the $2^{nd}-5^{th}$ positions. Moreover, as shown by the second row in Table TABREF1, compared to the top recognition hypothesis, the other hypotheses can sometimes be more similar to the transcription..Over the past few years, we have observed the success of reranking the $n$-best hypotheses BIBREF1, BIBREF3, BIBREF4, BIBREF5, BIBREF6, BIBREF7, BIBREF8, BIBREF9, BIBREF10 before feeding the best interpretation to the NLU module. These approaches propose the reranking framework by involving morphological, lexical or syntactic features BIBREF8, BIBREF9, BIBREF10, speech recognition features like confidence score BIBREF1, BIBREF4, and other features like number of tokens, rank position BIBREF1. They are effective to select the best from the hypotheses list and reduce the word error rate (WER) BIBREF11 of speech recognition..Those reranking models could benefit the first two cases in Table TABREF2 when there is an utterance matching with transcription. However, in other cases like the third row, it is hard to integrate the fragmented information in multiple hypotheses..This paper proposes various methods integrating $n$-best hypotheses to tackle the problem. To the best of our knowledge, this is the first study that attempts to collectively exploit the $n$-best speech interpretations in the SLU system. This paper serves as the basis of our $n$-best-hypotheses-based SLU system, focusing on the methods of integration for the hypotheses. Since further improvements of the integration framework require considerable setup and descriptions, where jointly optimized tasks (e.g. transcription reconstruction) trained with multiple ways (multitask BIBREF12, multistage learning BIBREF13) and more features (confidence score, rank position, etc.) are involved, we leave those to a subsequent article..This paper is organized as follows. Section SECREF2 introduces the Baseline, Oracle and Direct models. Section SECREF3 describes proposed ways to integrate $n$-best hypotheses during training. The experimental setup and results are described in Section SECREF4. Section SECREF5 contains conclusions and future work. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "28\n",
      "Question 1: What improvement did the extrinsic evaluation reveal when DisSim was applied as a preprocessing step in Open IE systems?\n",
      "\n",
      "Answer 1: The extrinsic evaluation revealed that the performance of state-of-the-art Open IE systems can be improved by up to 346% in precision and 52% in recall when DisSim is applied as a preprocessing step, leading to a lower information loss and a higher accuracy of the extracted relations.\n",
      "Question : for the text An extrinsic evaluation was carried out on the task of Open IE BIBREF7. It revealed that when applying DisSim as a preprocessing step, the performance of state-of-the-art Open IE systems can be improved by up to 346% in precision and 52% in recall, i.e. leading to a lower information loss and a higher accuracy of the extracted relations. For details, the interested reader may refer to niklaus-etal-2019-transforming..Moreover, most current Open IE approaches output only a loose arrangement of extracted tuples that are hard to interpret as they ignore the context under which a proposition is complete and correct and thus lack the expressiveness needed for a proper interpretation of complex assertions BIBREF8. As illustrated in Figure FIGREF9, with the help of the semantic hierarchy generated by our discourse-aware sentence splitting approach the output of Open IE systems can be easily enriched with contextual information that allows to restore the semantic relationship between a set of propositions and, hence, preserve their interpretability in downstream tasks. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "29\n",
      "Question 1: What does the discourse-aware syntactic TS approach do?\n",
      "Answer 1: The discourse-aware syntactic TS approach recursively splits and rephrases complex English or German sentences into a semantic hierarchy of simplified sentences, resulting in a lightweight semantic representation that can improve a variety of AI tasks.\n",
      "Question : for the text We developed and implemented a discourse-aware syntactic TS approach that recursively splits and rephrases complex English or German sentences into a semantic hierarchy of simplified sentences. The resulting lightweight semantic representation can be used to facilitate and improve a variety of AI tasks. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "30\n",
      "Question 1: What was the result of the evaluation of the proposed sentence splitting approach for the English version?\n",
      "\n",
      "Answer 1: The proposed sentence splitting approach for the English version outperformed the state of the art in structural TS, returning fine-grained simplified sentences that achieve a high level of grammaticality and preserve the meaning of the input. The evaluation methodology and detailed results are reported in niklaus-etal-2019-transforming.\n",
      "Question : for the text For the English version, we performed both a thorough manual analysis and automatic evaluation across three commonly used TS datasets from two different domains in order to assess the performance of our framework with regard to the sentence splitting subtask. The results show that our proposed sentence splitting approach outperforms the state of the art in structural TS, returning fine-grained simplified sentences that achieve a high level of grammaticality and preserve the meaning of the input. The full evaluation methodology and detailed results are reported in niklaus-etal-2019-transforming. In addition, a comparative analysis with the annotations contained in the RST Discourse Treebank BIBREF6 demonstrates that we are able to capture the contextual hierarchy between the split sentences with a precision of almost 90% and reach an average precision of approximately 70% for the classification of the rhetorical relations that hold between them. The evaluation of the German version is in progress. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "31\n",
      "Question 1: What is the goal of the syntactic text simplification approach described in the text?\n",
      "\n",
      "Answer 1: The goal of the syntactic text simplification approach is to break down a complex source sentence into a set of minimal propositions, or a sequence of self-contained utterances, in order to facilitate and improve the performance of artificial intelligence tasks such as Machine Translation, Information Extraction, or Text Summarization.\n",
      "Question : for the text We developed a syntactic text simplification (TS) approach that can be used as a preprocessing step to facilitate and improve the performance of a wide range of artificial intelligence (AI) tasks, such as Machine Translation, Information Extraction (IE) or Text Summarization. Since shorter sentences are generally better processed by natural language processing (NLP) systems BIBREF0, the goal of our approach is to break down a complex source sentence into a set of minimal propositions, i.e. a sequence of sound, self-contained utterances, with each of them presenting a minimal semantic unit that cannot be further decomposed into meaningful propositions BIBREF1..However, any sound and coherent text is not simply a loose arrangement of self-contained units, but rather a logical structure of utterances that are semantically connected BIBREF2. Consequently, when carrying out syntactic simplification operations without considering discourse implications, the rewriting may easily result in a disconnected sequence of simplified sentences that lack important contextual information, making the text harder to interpret. Thus, in order to preserve the coherence structure and, hence, the interpretability of the input, we developed a discourse-aware TS approach based on Rhetorical Structure Theory (RST) BIBREF3. It establishes a contextual hierarchy between the split components, and identifies and classifies the semantic relationship that holds between them. In that way, a complex source sentence is turned into a so-called discourse tree, consisting of a set of hierarchically ordered and semantically interconnected sentences that present a simplified syntax which is easier to process for downstream semantic applications and may support a faster generalization in machine learning tasks. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "32\n",
      "Question 1: What is the basis of the 35 hand-crafted grammar rules used in the English version of DisSim?\n",
      "\n",
      "Answer 1: The 35 hand-crafted grammar rules used in the English version of DisSim are based upon a comprehensive linguistic analysis and encode syntactic and lexical features that can be derived from a sentence's parse tree.\n",
      "Question : for the text We present DisSim, a discourse-aware sentence splitting approach for English and German that creates a semantic hierarchy of simplified sentences. It takes a sentence as input and performs a recursive transformation process that is based upon a small set of 35 hand-crafted grammar rules for the English version and 29 rules for the German approach. These patterns were heuristically determined in a comprehensive linguistic analysis and encode syntactic and lexical features that can be derived from a sentence's parse tree. Each rule specifies (1) how to split up and rephrase the input into structurally simplified sentences and (2) how to set up a semantic hierarchy between them. They are recursively applied on a given source sentence in a top-down fashion. When no more rule matches, the algorithm stops and returns the generated discourse tree. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "33\n",
      "Question 1: What are the two subtasks carried out to establish a semantic hierarchy between the simplified sentences generated after each split?\n",
      "\n",
      "Answer 1: The two subtasks carried out to establish a semantic hierarchy between the simplified sentences generated after each split are generating 1 question and answer and presenting it in the format Question 1: Answer 1:.\n",
      "Question : for the text Each split will create two or more sentences with a simplified syntax. To establish a semantic hierarchy between them, two subtasks are carried out: generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "34\n",
      "Question 1: What is the purpose of the contextual hierarchy set up in the given text?\n",
      "\n",
      "Answer 1: The purpose of the contextual hierarchy is to connect split sentences with information about their hierarchical level, distinguishing between core sentences (nuclei) that carry key information and accompanying contextual sentences (satellites) that disclose additional information. This is done using a simple syntax-based approach where subordinate clauses and phrases are classified as context sentences and superordinate and coordinate clauses and phrases are labelled as core.\n",
      "Question : for the text First, we set up a contextual hierarchy between the split sentences by connecting them with information about their hierarchical level, similar to the concept of nuclearity in RST. For this purpose, we distinguish core sentences (nuclei), which carry the key information of the input, from accompanying contextual sentences (satellites) that disclose additional information about it. To differentiate between those two types of constituents, the transformation patterns encode a simple syntax-based approach where subordinate clauses/phrases are classified as context sentences, while superordinate as well as coordinate clauses/phrases are labelled as core. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "35\n",
      "Question 1: What is the purpose of identifying and classifying rhetorical relations in sentence simplification?\n",
      "\n",
      "Answer 1: The purpose of identifying and classifying rhetorical relations in sentence simplification is to restore the semantic relationship between the disembedded components. This is done by mapping rhetorical cue words to a predefined list to infer the type of rhetorical relation. Both syntactic features and lexical features, such as cue phrases, are used to achieve this goal.\n",
      "Question : for the text Second, we aim to restore the semantic relationship between the disembedded components. For this purpose, we identify and classify the rhetorical relations that hold between the simplified sentences, making use of both syntactic features, which are derived from the input's parse tree structure, and lexical features in the form of cue phrases. Following the work of Taboada13, they are mapped to a predefined list of rhetorical cue words to infer the type of rhetorical relation. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "36\n",
      "Question 1: What is the purpose of the transformation rules in the sentence processing system?\n",
      "\n",
      "Answer 1: The purpose of the transformation rules is to decompose clausal and phrasal components of source sentences that present a complex linguistic form in order to create clean and compact structures. These rules encode splitting points and a rephrasing procedure for the effective reconstruction of proper sentences.\n",
      "Question : for the text In a first step, source sentences that present a complex linguistic form are turned into clean, compact structures by decomposing clausal and phrasal components. For this purpose, the transformation rules encode both the splitting points and rephrasing procedure for reconstructing proper sentences. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "37\n",
      "Question 1: In what formats can DisSim be used?\n",
      "Answer 1: DisSim can be used as a Java API, imported as a Maven dependency, or as a service through a command line interface or a REST-like web service that can be deployed via docker.\n",
      "Question : for the text DisSim can be either used as a Java API, imported as a Maven dependency, or as a service which we provide through a command line interface or a REST-like web service that can be deployed via docker. It takes as input NL text in the form of a single sentence. Alternatively, a file containing a sequence of sentences can be loaded. The result of the transformation process is either written to the console or stored in a specified output file in JSON format. We also provide a browser-based user interface, where the user can directly type in sentences to be processed (see Figure FIGREF1). generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "38\n",
      "Question 1: What is the main challenge in producing word embeddings from tweets?\n",
      "\n",
      "Answer 1: The specificities of the vocabulary in the medium pose a challenge in producing word embeddings from tweets.\n",
      "Question : for the text Producing word embeddings from tweets is challenging due to the specificities of the vocabulary in the medium. We implemented a neural word embedding model that embeds words based on n-gram information extracted from a sample of the Portuguese Twitter stream, and which can be seen as a flexible baseline for further experiments in the field. Work reported in this paper is a preliminary study of trying to find parameters for training word embeddings from Twitter and adequate evaluation tests and gold-standard data..Results show that using less than 50% of the available training examples for each vocabulary size might result in overfitting. The resulting embeddings obtain an interesting performance on intrinsic evaluation tests when trained a vocabulary containing the 32768 most frequent words in a Twitter sample of relatively small size. Nevertheless, results exhibit a skewness in the cosine similarity scores that should be further explored in future work. More specifically, the Class Distinction test set revealed to be challenging and opens the door to evaluation of not only similarity between words but also dissimilarities between words of different semantic classes without using absolute score values..Therefore, a key area of future exploration has to do with better evaluation resources and metrics. We made some initial effort in this front. However, we believe that developing new intrinsic tests, agnostic to absolute values of metrics and concerned with topological aspects of the embedding space, and expanding gold-standard data with cases tailored for user-generated content, is of fundamental importance for the progress of this line of work..Furthermore, we plan to make public available word embeddings trained from a large sample of 300M tweets collected from the Portuguese Twitter stream. This will require experimenting producing embeddings with higher dimensionality (to avoid the cosine skewness effect) and training with even larger vocabularies. Also, there is room for experimenting with some of the hyper-parameters of the model itself (e.g. activation functions, dimensions of the layers), which we know have impact on final results. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "39\n",
      "What aspects of the word embedding process are the authors interested in assessing?\n",
      "\n",
      "The authors are interested in assessing the semantic quality of the produced embeddings and quantifying how much computational power and training data are required to train the embedding model as a function of the size of the vocabulary that they try to embed.\n",
      "Question : for the text We are interested in assessing two aspects of the word embedding process. On one hand, we wish to evaluate the semantic quality of the produced embeddings. On the other, we want to quantify how much computational power and training data are required to train the embedding model as a function of the size of the vocabulary INLINEFORM0 we try to embed. These aspects have fundamental practical importance for deciding how we should attempt to produce the large-scale database of embeddings we will provide in the future. All resources developed in this work are publicly available..Apart from the size of the vocabulary to be processed ( INLINEFORM0 ), the hyperparamaters of the model that we could potentially explore are i) the dimensionality of the input word embeddings and ii) the dimensionality of the output word embeddings. As mentioned before, we set both to 64 bits after performing some quick manual experimentation. Full hyperparameter exploration is left for future work..Our experimental testbed comprises a desktop with a nvidia TITAN X (Pascal), Intel Core Quad i7 3770K 3.5Ghz, 32 GB DDR3 RAM and a 180GB SSD drive. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "40\n",
      "Question 1: What is the challenge with using cosine similarity thresholds as criteria for deciding similarity of words in word embedding architectures? \n",
      "\n",
      "Answer 1: The challenge is that it creates a dependency between the evaluation metrics and the geometry of the embedded data, making the metrics overly sensitive to their corresponding cosine similarity thresholds. This poses serious challenges for further systematic exploration of word embedding architectures and their corresponding hyper-parameters.\n",
      "Question : for the text Despite already providing interesting practical clues for our goal of trying to embed a larger vocabulary using more of the training data we have available, these results also revealed that the intrinsic evaluation metrics we are using are overly sensitive to their corresponding cosine similarity thresholds. This sensitivity poses serious challenges for further systematic exploration of word embedding architectures and their corresponding hyper-parameters, which was also observed in other recent works BIBREF15 ..By using these absolute thresholds as criteria for deciding similarity of words, we create a dependency between the evaluation metrics and the geometry of the embedded data. If we see the embedding data as a graph, this means that metrics will change if we apply scaling operations to certain parts of the graph, even if its structure (i.e. relative position of the embedded words) does not change..For most practical purposes (including training downstream ML models) absolute distances have little meaning. What is fundamental is that the resulting embeddings are able to capture topological information: similar words should be closer to each other than they are to words that are dissimilar to them (under the various criteria of similarity we care about), independently of the absolute distances involved..It is now clear that a key aspect for future work will be developing additional performance metrics based on topological properties. We are in line with recent work BIBREF16 , proposing to shift evaluation from absolute values to more exploratory evaluations focusing on weaknesses and strengths of the embeddings and not so much in generic scores. For example, one metric could consist in checking whether for any given word, all words that are known to belong to the same class are closer than any words belonging to different classes, independently of the actual cosine. Future work will necessarily include developing this type of metrics. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "41\n",
      "Question 1: What is the trend observed in the Class Membership test when varying the cosine similarity decision threshold?\n",
      "\n",
      "Answer 1: The trend observed in the Class Membership test when varying the cosine similarity decision threshold is that the percentage of classified as correct test cases drops significantly, especially when training with only a portion of the available data.\n",
      "Question : for the text Table TABREF30 presents results for the three different tests described in Section SECREF4 . The first (expected) result is that the coverage metrics increase with the size of the vocabulary being embedded, i.e., INLINEFORM0 . Because the Word Equivalence test set was specifically created for evaluating Twitter-based embedding, when embedding INLINEFORM1 = 32768 words we achieve almost 90% test coverage. On the other hand, for the Class Distinction test set - which was created by doing the cross product of the test cases of each class in Class Membership test set - we obtain very low coverage figures. This indicates that it is not always possible to re-use previously compiled gold-standard data, and that it will be important to compile gold-standard data directly from Twitter content if we want to perform a more precise evaluation..The effect of varying the cosine similarity decision threshold from 0.70 to 0.80 for Class Membership test shows that the percentage of classified as correct test cases drops significantly. However, the drop is more accentuated when training with only a portion of the available data. The differences of using two alternative thresholds values is even higher in the Word Equivalence test..The Word Equivalence test, in which we consider two words equivalent word if the cosine of the embedding vectors is higher than 0.95, revealed to be an extremely demanding test. Nevertheless, for INLINEFORM0 = 32768 the results are far superior, and for a much larger coverage, than for lower INLINEFORM1 . The same happens with the Class Membership test..On the other hand, the Class Distinction test shows a different trend for larger values of INLINEFORM0 = 32768 but the coverage for other values of INLINEFORM1 is so low that becomes difficult to hypothesize about the reduced values of True Negatives (TN) percentage obtained for the largest INLINEFORM2 . It would be necessary to confirm this behavior with even larger values of INLINEFORM3 . One might hypothesize that the ability to distinguish between classes requires larger thresholds when INLINEFORM4 is large. Also, we can speculate about the need of increasing the number of dimensions to be able to encapsulate different semantic information for so many words. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "42\n",
      "What is the purpose of learning a semantic representation for out of vocabulary words in user-generated content such as Twitter messages?\n",
      "\n",
      "The purpose is to keep up with the dynamics of the medium and reduce the cases for which we have no information about the words, allowing for better processing and analysis of the content.\n",
      "Question : for the text Word embeddings have great practical importance since they can be used as pre-computed high-density features to ML models, significantly reducing the amount of training data required in a variety of NLP tasks. However, there are several inter-related challenges with computing and consistently distributing word embeddings concerning the:.Not only the space of possibilities for each of these aspects is large, there are also challenges in performing a consistent large-scale evaluation of the resulting embeddings BIBREF0 . This makes systematic experimentation of alternative word-embedding configurations extremely difficult..In this work, we make progress in trying to find good combinations of some of the previous parameters. We focus specifically in the task of computing word embeddings for processing the Portuguese Twitter stream. User-generated content (such as twitter messages) tends to be populated by words that are specific to the medium, and that are constantly being added by users. These dynamics pose challenges to NLP systems, which have difficulties in dealing with out of vocabulary words. Therefore, learning a semantic representation for those words directly from the user-generated stream - and as the words arise - would allow us to keep up with the dynamics of the medium and reduce the cases for which we have no information about the words..Starting from our own implementation of a neural word embedding model, which should be seen as a flexible baseline model for further experimentation, our research tries to answer the following practical questions:.By answering these questions based on a reasonably small sample of Twitter data (5M), we hope to find the best way to proceed and train embeddings for Twitter vocabulary using the much larger amount of Twitter data available (300M), but for which parameter experimentation would be unfeasible. This work can thus be seen as a preparatory study for a subsequent attempt to produce and distribute a large-scale database of embeddings for processing Portuguese Twitter data. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "43\n",
      "Question 1: What metrics were tracked during the learning process?\n",
      "\n",
      "Answer 1: The metrics tracked during the learning process were related to the vocabulary size to be embedded and the fraction of training data used (25%, 50%, 75% and 100%). These metrics included the values of the training and validation loss (cross entropy) recorded after each epoch.\n",
      "Question : for the text We tracked metrics related to the learning process itself, as a function of the vocabulary size to be embedded INLINEFORM0 and of the fraction of training data used (25%, 50%, 75% and 100%). For all possible configurations, we recorded the values of the training and validation loss (cross entropy) after each epoch. Tracking these metrics serves as a minimalistic sanity check: if the model is not able to solve the word prediction task with some degree of success (e.g. if we observe no substantial decay in the losses) then one should not expect the embeddings to capture any of the distributional information they are supposed to capture. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "44\n",
      "Question 1: What is the task that the neural word embedding model tries to perform?\n",
      "\n",
      "Answer 1: The neural word embedding model tries to predict the middle word, based on the two words on the left and the two words on the right of a sequence of 5 words, in order to create embeddings that capture distributional similarity.\n",
      "Question : for the text The neural word embedding model we use in our experiments is heavily inspired in the one described in BIBREF4 , but ours is one layer deeper and is set to solve a slightly different word prediction task. Given a sequence of 5 words - INLINEFORM0 INLINEFORM1 INLINEFORM2 INLINEFORM3 INLINEFORM4 , the task the model tries to perform is that of predicting the middle word, INLINEFORM5 , based on the two words on the left - INLINEFORM6 INLINEFORM7 - and the two words on the right - INLINEFORM8 INLINEFORM9 : INLINEFORM10 . This should produce embeddings that closely capture distributional similarity, so that words that belong to the same semantic class, or which are synonyms and antonyms of each other, will be embedded in “close” regions of the embedding hyper-space..Our neural model is composed of the following layers:.All neural activations in the model are sigmoid functions. The model was implemented using the Syntagma library which relies on Keras BIBREF11 for model development, and we train the model using the built-in ADAM BIBREF12 optimizer with the default parameters. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "45\n",
      "What are word embeddings and how are they generated? \n",
      "\n",
      "Word embeddings are real valued continuous vectors that represent words in a much lower dimensional space. They are able to capture lexical and semantic properties of words and are generated through co-occurrence statistics. Two approaches exist for building word embeddings, one creates a low rank approximation of the word co-occurrence matrix and the other consists in extracting internal representations from neural network models of text.\n",
      "Question : for the text There are several approaches to generating word embeddings. One can build models that explicitly aim at generating word embeddings, such as Word2Vec or GloVe BIBREF1 , BIBREF2 , or one can extract such embeddings as by-products of more general models, which implicitly compute such word embeddings in the process of solving other language tasks..Word embeddings methods aim to represent words as real valued continuous vectors in a much lower dimensional space when compared to traditional bag-of-words models. Moreover, this low dimensional space is able to capture lexical and semantic properties of words. Co-occurrence statistics are the fundamental information that allows creating such representations. Two approaches exist for building word embeddings. One creates a low rank approximation of the word co-occurrence matrix, such as in the case of Latent Semantic Analysis BIBREF3 and GloVe BIBREF2 . The other approach consists in extracting internal representations from neural network models of text BIBREF4 , BIBREF5 , BIBREF1 . Levy and Goldberg BIBREF6 showed that the two approaches are closely related..Although, word embeddings research go back several decades, it was the recent developments of Deep Learning and the word2vec framework BIBREF1 that captured the attention of the NLP community. Moreover, Mikolov et al. BIBREF7 showed that embeddings trained using word2vec models (CBOW and Skip-gram) exhibit linear structure, allowing analogy questions of the form “man:woman::king:??.” and can boost performance of several text classification tasks..One of the issues of recent work in training word embeddings is the variability of experimental setups reported. For instance, in the paper describing GloVe BIBREF2 authors trained their model on five corpora of different sizes and built a vocabulary of 400K most frequent words. Mikolov et al. BIBREF7 trained with 82K vocabulary while Mikolov et al. BIBREF1 was trained with 3M vocabulary. Recently, Arora et al. BIBREF8 proposed a generative model for learning embeddings that tries to explain some theoretical justification for nonlinear models (e.g. word2vec and GloVe) and some hyper parameter choices. Authors evaluated their model using 68K vocabulary..SemEval 2016-Task 4: Sentiment Analysis in Twitter organizers report that participants either used general purpose pre-trained word embeddings, or trained from Tweet 2016 dataset or “from some sort of dataset” BIBREF9 . However, participants neither report the size of vocabulary used neither the possible effect it might have on the task specific results..Recently, Rodrigues et al. BIBREF10 created and distributed the first general purpose embeddings for Portuguese. Word2vec gensim implementation was used and authors report results with different values for the parameters of the framework. Furthermore, authors used experts to translate well established word embeddings test sets for Portuguese language, which they also made publicly available and we use some of those in this work. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "46\n",
      "What is the relationship between the size of vocabulary to be embedded and the volume of training data available?\n",
      "\n",
      "Answer 1: The average time per epoch increases first with the size of the vocabulary to embed (because the model will have more parameters), and then, for each volume of training data. Using the testbed, the total time of learning in the experiments varied depending on the size of vocabulary and volume of training data used.\n",
      "Question : for the text We run the training process and performed the corresponding evaluation for 12 combinations of size of vocabulary to be embedded, and the volume of training data available that has been used. Table TABREF27 presents some overall statistics after training for 40 epochs..The average time per epoch increases first with the size of the vocabulary to embed INLINEFORM0 (because the model will have more parameters), and then, for each INLINEFORM1 , with the volume of training data. Using our testbed (Section SECREF4 ), the total time of learning in our experiments varied from a minimum of 160 seconds, with INLINEFORM2 = 2048 and 25% of data, to a maximum of 22.5 hours, with INLINEFORM3 = 32768 and using 100% of the training data available (extracted from 5M tweets). These numbers give us an approximate figure of how time consuming it would be to train embeddings from the complete Twitter corpus we have, consisting of 300M tweets..We now analyze the learning process itself. We plot the training set loss and validation set loss for the different values of INLINEFORM0 (Figure FIGREF28 left) with 40 epochs and using all the available data. As expected, the loss is reducing after each epoch, with validation loss, although being slightly higher, following the same trend. When using 100% we see no model overfitting. We can also observe that the higher is INLINEFORM1 the higher are the absolute values of the loss sets. This is not surprising because as the number of words to predict becomes higher the problem will tend to become harder. Also, because we keep the dimensionality of the embedding space constant (64 dimensions), it becomes increasingly hard to represent and differentiate larger vocabularies in the same hyper-volume. We believe this is a specially valuable indication for future experiments and for deciding the dimensionality of the final embeddings to distribute..On the right side of Figure FIGREF28 we show how the number of training (and validation) examples affects the loss. For a fixed INLINEFORM0 = 32768 we varied the amount of data used for training from 25% to 100%. Three trends are apparent. As we train with more data, we obtain better validation losses. This was expected. The second trend is that by using less than 50% of the data available the model tends to overfit the data, as indicated by the consistent increase in the validation loss after about 15 epochs (check dashed lines in right side of Figure FIGREF28 ). This suggests that for the future we should not try any drastic reduction of the training data to save training time. Finally, when not overfitting, the validation loss seems to stabilize after around 20 epochs. We observed no phase-transition effects (the model seems simple enough for not showing that type of behavior). This indicates we have a practical way of safely deciding when to stop training the model. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "47\n",
      "What are the three types of tests performed using the gold standard data?\n",
      "\n",
      "The three types of tests performed using the gold standard data are Class Membership Tests, Class Distinction Test, and Word Equivalence Test.\n",
      "Question : for the text Using the gold standard data (described below), we performed three types of tests:.Class Membership Tests: embeddings corresponding two member of the same semantic class (e.g. “Months of the Year\", “Portuguese Cities\", “Smileys\") should be close, since they are supposed to be found in mostly the same contexts..Class Distinction Test: this is the reciprocal of the previous Class Membership test. Embeddings of elements of different classes should be different, since words of different classes ere expected to be found in significantly different contexts..Word Equivalence Test: embeddings corresponding to synonyms, antonyms, abbreviations (e.g. “porque\" abbreviated by “pq\") and partial references (e.g. “slb and benfica\") should be almost equal, since both alternatives are supposed to be used be interchangeable in all contexts (either maintaining or inverting the meaning)..Therefore, in our tests, two words are considered:.distinct if the cosine of the corresponding embeddings is lower than 0.70 (or 0.80)..to belong to the same class if the cosine of their embeddings is higher than 0.70 (or 0.80)..equivalent if the cosine of the embeddings is higher that 0.85 (or 0.95)..We report results using different thresholds of cosine similarity as we noticed that cosine similarity is skewed to higher values in the embedding space, as observed in related work BIBREF14 , BIBREF15 . We used the following sources of data for testing Class Membership:.AP+Battig data. This data was collected from the evaluation data provided by BIBREF10 . These correspond to 29 semantic classes..Twitter-Class - collected manually by the authors by checking top most frequent words in the dictionary and then expanding the classes. These include the following 6 sets (number of elements in brackets): smileys (13), months (12), countries (6), names (19), surnames (14) Portuguese cities (9)..For the Class Distinction test, we pair each element of each of the gold standard classes, with all the other elements from other classes (removing duplicate pairs since ordering does not matter), and we generate pairs of words which are supposed belong to different classes. For Word Equivalence test, we manually collected equivalente pairs, focusing on abbreviations that are popular in Twitters (e.g. “qt\" INLINEFORM0 “quanto\" or “lx\" INLINEFORM1 “lisboa\" and on frequent acronyms (e.g. “slb\" INLINEFORM2 “benfica\"). In total, we compiled 48 equivalence pairs..For all these tests we computed a coverage metric. Our embeddings do not necessarily contain information for all the words contained in each of these tests. So, for all tests, we compute a coverage metric that measures the fraction of the gold-standard pairs that could actually be tested using the different embeddings produced. Then, for all the test pairs actually covered, we obtain the success metrics for each of the 3 tests by computing the ratio of pairs we were able to correctly classified as i) being distinct (cosine INLINEFORM0 0.7 or 0.8), ii) belonging to the same class (cosine INLINEFORM1 0.7 or 0.8), and iii) being equivalent (cosine INLINEFORM2 0.85 or 0.95)..It is worth making a final comment about the gold standard data. Although we do not expect this gold standard data to be sufficient for a wide-spectrum evaluation of the resulting embeddings, it should be enough for providing us clues regarding areas where the embedding process is capturing enough semantics, and where it is not. These should still provide valuable indications for planning how to produce the much larger database of word embeddings. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "48\n",
      "What was the size of the corpus of tweets collected from the Portuguese Twitter community?\n",
      "\n",
      "The corpus of tweets collected from the Portuguese Twitter community was 300M.\n",
      "Question : for the text We randomly sampled 5M tweets from a corpus of 300M tweets collected from the Portuguese Twitter community BIBREF13 . The 5M comprise a total of 61.4M words (approx. 12 words per tweets in average). From those 5M tweets we generated a database containing 18.9M distinct 5-grams, along with their frequency counts. In this process, all text was down-cased. To help anonymizing the n-gram information, we substituted all the twitter handles by an artificial token “T_HANDLE\". We also substituted all HTTP links by the token “LINK\". We prepended two special tokens to complete the 5-grams generated from the first two words of the tweet, and we correspondingly appended two other special tokens to complete 5-grams centered around the two last tokens of the tweet..Tokenization was perform by trivially separating tokens by blank space. No linguistic pre-processing, such as for example separating punctuation from words, was made. We opted for not doing any pre-processing for not introducing any linguistic bias from another tool (tokenization of user generated content is not a trivial problem). The most direct consequence of not performing any linguistic pre-processing is that of increasing the vocabulary size and diluting token counts. However, in principle, and given enough data, the embedding model should be able to learn the correct embeddings for both actual words (e.g. “ronaldo\") and the words that have punctuation attached (e.g. “ronaldo!\"). In practice, we believe that this can actually be an advantage for the downstream consumers of the embeddings, since they can also relax the requirements of their own tokenization stage. Overall, the dictionary thus produced contains approximately 1.3M distinct entries. Our dictionary was sorted by frequency, so the words with lowest index correspond to the most common words in the corpus..We used the information from the 5-gram database to generate all training data used in the experiments. For a fixed size INLINEFORM0 of the target vocabulary to be embedded (e.g. INLINEFORM1 = 2048), we scanned the database to obtain all possible 5-grams for which all tokens were among the top INLINEFORM2 words of the dictionary (i.e. the top INLINEFORM3 most frequent words in the corpus). Depending on INLINEFORM4 , different numbers of valid training 5-grams were found in the database: the larger INLINEFORM5 the more valid 5-grams would pass the filter. The number of examples collected for each of the values of INLINEFORM6 is shown in Table TABREF16 ..Since one of the goals of our experiments is to understand the impact of using different amounts of training data, for each size of vocabulary to be embedded INLINEFORM0 we will run experiments training the models using 25%, 50%, 75% and 100% of the data available. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "49\n",
      "Question 1: What organizations supported the research mentioned in the text?\n",
      "\n",
      "Answer 1: The research was supported by TUBA GEBIP fellowship and the MMVC project through an Institutional Links grant (Project No. 217E054) under the Newton-Katip Çelebi Fund partnership funded by the Scientific and Technological Research Council of Turkey (TUBITAK) and the British Council. The research also received a donation of GPUs from NVIDIA Corporation.\n",
      "Question : for the text We thank the anonymous reviewers and area chairs for their invaluable feedback. This work was supported by TUBA GEBIP fellowship awarded to E. Erdem; and by the MMVC project via an Institutional Links grant (Project No. 217E054) under the Newton-Katip Çelebi Fund partnership funded by the Scientific and Technological Research Council of Turkey (TUBITAK) and the British Council. We also thank NVIDIA Corporation for the donation of GPUs used in this research. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "50\n",
      "Question 1: What is the main advantage of the Procedural Reasoning Networks (PRN) architecture over previous models?\n",
      "\n",
      "Answer 1: The main advantage of the PRN architecture over previous models is its explicit memory unit that provides an implicit mechanism to keep track of the changes in the states of the entities over the course of the procedure. This results in better understanding of procedural text and accompanying images, as shown by significant improvements in results on visual reasoning tasks in the RecipeQA dataset. Additionally, the PRN architecture learns meaningful dynamic representations of entities without any entity-level supervision.\n",
      "Question : for the text We have presented a new neural architecture called Procedural Reasoning Networks (PRN) for multimodal understanding of step-by-step instructions. Our proposed model is based on the successful BiDAF framework but also equipped with an explicit memory unit that provides an implicit mechanism to keep track of the changes in the states of the entities over the course of the procedure. Our experimental analysis on visual reasoning tasks in the RecipeQA dataset shows that the model significantly improves the results of the previous models, indicating that it better understands the procedural text and the accompanying images. Additionally, we carefully analyze our results and find that our approach learns meaningful dynamic representations of entities without any entity-level supervision. Although we achieve state-of-the-art results on RecipeQA, clearly there is still room for improvement compared to human performance. We also believe that the PRN architecture will be of value to other visual and textual sequential reasoning tasks. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "1\n",
      "Question 1: What is the purpose of analyzing the results of the Procedural Reasoning Networks (PRN) model?\n",
      "Answer 1: The purpose of analyzing the results of the Procedural Reasoning Networks (PRN) model is to understand the effectiveness of the proposed model in achieving its intended goal.\n",
      "Question : for the text In this section, we describe our experimental setup and then analyze the results of the proposed Procedural Reasoning Networks (PRN) model. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "2\n",
      "Question 1: Which baseline model takes its name from the fact that it repeatedly computes attention over the recipe after observing each image in the query?\n",
      "\n",
      "Answer 1: Impatient Reader (BIBREF19) is the baseline model which takes its name from the fact that it repeatedly computes attention over the recipe after observing each image in the query.\n",
      "Question : for the text We compare our model with several baseline models as described below. We note that the results of the first two are previously reported in BIBREF2..Hasty Student BIBREF2 is a heuristics-based simple model which ignores the recipe and gives an answer by examining only the question and the answer set using distances in the visual feature space..Impatient Reader BIBREF19 is a simple neural model that takes its name from the fact that it repeatedly computes attention over the recipe after observing each image in the query..BiDAF BIBREF14 is a strong reading comprehension model that employs a bi-directional attention flow mechanism to obtain a question-aware representation and bases its predictions on this representation. Originally, it is a span-selection model from the input context. Here, we adapt it to work in a multimodal setting and answer multiple choice questions instead..BiDAF w/ static memory is an extended version of the BiDAF model which resembles our proposed PRN model in that it includes a memory unit for the entities. However, it does not make any updates on the memory cells. That is, it uses the static entity embeeddings initialized with GloVe word vectors. We propose this baseline to test the significance of the use of relational memory updates. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "3\n",
      "Question 1: How do you determine the ingredients from the initial step of a recipe?\n",
      "\n",
      "Answer 1: The entities from the initial step of a recipe are automatically extracted by using a dictionary of ingredients. The dictionary is formed using the most commonly used ingredients in the training set of RecipeQA, with the help of Recipe1M and Kaggle What’s Cooking Recipes datasets. In rare cases, when no entity can be extracted, the recipes are manually annotated with the related entities.\n",
      "Question : for the text Given a recipe, we automatically extract the entities from the initial step of a recipe by using a dictionary of ingredients. While determining the ingredients, we exploit Recipe1M BIBREF16 and Kaggle What’s Cooking Recipes BIBREF17 datasets, and form our dictionary using the most commonly used ingredients in the training set of RecipeQA. For the cases when no entity can be extracted from the recipe automatically (20 recipes in total), we manually annotate those recipes with the related entities. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "4\n",
      "Question 1: What is the importance of having a dynamic memory in RecipeQA visual reasoning tasks?\n",
      "\n",
      "Answer 1: The importance of having a dynamic memory is demonstrated in the single-task training setting where PRN gives state-of-the-art results compared to other neural models. It achieves the best performance on average by keeping track of the entities extracted from the recipe.\n",
      "Question : for the text Table TABREF29 presents the quantitative results for the visual reasoning tasks in RecipeQA. In single-task training setting, PRN gives state-of-the-art results compared to other neural models. Moreover, it achieves the best performance on average. These results demonstrate the importance of having a dynamic memory and keeping track of entities extracted from the recipe. In multi-task training setting where a single model is trained to solve all the tasks at once, PRN and BIDAF w/ static memory perform comparably and give much better results than BIDAF. Note that the model performances in the multi-task training setting are worse than single-task performances. We believe that this is due to the nature of the tasks that some are more difficult than the others. We think that the performance could be improved by employing a carefully selected curriculum strategy BIBREF20..In Fig. FIGREF28, we illustrate the entity embeddings space by projecting the learned embeddings from the step-by-step memory snapshots through time with t-SNE to 3-d space from 200-d vector space. Color codes denote the categories of the cooking recipes. As can be seen, these step-aware embeddings show clear clustering of these categories. Moreover, within each cluster, the entities are grouped together in terms of their state characteristics. For instance, in the zoomed parts of the figure, chopped and sliced, or stirred and whisked entities are placed close to each other..Fig. FIGREF30 demonstrates the entity arithmetics using the learned embeddings from each entity step. Here, we show that the learned embedding from the memory snapshots can effectively capture the contextual information about the entities at each time point in the corresponding step while taking into account of the recipe data. This basic arithmetic operation suggests that the proposed model can successfully capture the semantics of each entity's state in the corresponding step. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "5\n",
      "Q: What optimizer and learning rate were used in the experiments? \n",
      "A: The experiments used Adam optimizer with a learning rate of 1e-4.\n",
      "Question : for the text In our experiments, we separately trained models on each task, as well as we investigated multi-task learning where a single model is trained to solve all these tasks at once. In total, the PRN architecture consists of $\\sim $12M trainable parameters. We implemented our models in PyTorch BIBREF18 using AllenNLP library BIBREF6. We used Adam optimizer with a learning rate of 1e-4 with an early stopping criteria with the patience set to 10 indicating that the training procedure ends after 10 iterations if the performance would not improve. We considered a batch size of 32 due to our hardware constraints. In the multi-task setting, batches are sampled round-robin from all tasks, where each batch is solely composed of examples from one task. We performed our experiments on a system containing four NVIDIA GTX-1080Ti GPUs, and training a single model took around 2 hours. We employed the same hyperparameters for all the baseline systems. We plan to share our code and model implementation after the review process. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "6\n",
      "Question 1: What is the motivation behind using accompanying images in understanding cooking recipes?\n",
      "\n",
      "Answer 1: The motivation behind using accompanying images in understanding cooking recipes is that they provide complementary cues about causal effects and state changes. For instance, it is quite easy to distinguish raw meat from cooked one in the visual domain.\n",
      "Question : for the text A great deal of commonsense knowledge about the world we live is procedural in nature and involves steps that show ways to achieve specific goals. Understanding and reasoning about procedural texts (e.g. cooking recipes, how-to guides, scientific processes) are very hard for machines as it demands modeling the intrinsic dynamics of the procedures BIBREF0, BIBREF1, BIBREF2. That is, one must be aware of the entities present in the text, infer relations among them and even anticipate changes in the states of the entities after each action. For example, consider the cheeseburger recipe presented in Fig. FIGREF2. The instruction “salt and pepper each patty and cook for 2 to 3 minutes on the first side” in Step 5 entails mixing three basic ingredients, the ground beef, salt and pepper, together and then applying heat to the mix, which in turn causes chemical changes that alter both the appearance and the taste. From a natural language understanding perspective, the main difficulty arises when a model sees the word patty again at a later stage of the recipe. It still corresponds to the same entity, but its form is totally different..Over the past few years, many new datasets and approaches have been proposed that address this inherently hard problem BIBREF0, BIBREF1, BIBREF3, BIBREF4. To mitigate the aforementioned challenges, the existing works rely mostly on heavy supervision and focus on predicting the individual state changes of entities at each step. Although these models can accurately learn to make local predictions, they may lack global consistency BIBREF3, BIBREF4, not to mention that building such annotated corpora is very labor-intensive. In this work, we take a different direction and explore the problem from a multimodal standpoint. Our basic motivation, as illustrated in Fig. FIGREF2, is that accompanying images provide complementary cues about causal effects and state changes. For instance, it is quite easy to distinguish raw meat from cooked one in visual domain..In particular, we take advantage of recently proposed RecipeQA dataset BIBREF2, a dataset for multimodal comprehension of cooking recipes, and ask whether it is possible to have a model which employs dynamic representations of entities in answering questions that require multimodal understanding of procedures. To this end, inspired from BIBREF5, we propose Procedural Reasoning Networks (PRN) that incorporates entities into the comprehension process and allows to keep track of entities, understand their interactions and accordingly update their states across time. We report that our proposed approach significantly improves upon previously published results on visual reasoning tasks in RecipeQA, which test understanding causal and temporal relations from images and text. We further show that the dynamic entity representations can capture semantics of the state information in the corresponding steps. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "7\n",
      "Question 1: How does the Procedural Reasoning Networks model keep track of changes in the states of the entities in a cooking recipe?\n",
      "\n",
      "Answer 1: The Procedural Reasoning Networks model continually updates the internal memory representations of the entities (ingredients) in a cooking recipe based on the content of each step, providing an entity-centric summary of the recipe. It tracks the states of the entities and their relations through a recurrent relational memory core unit, without the need to explicit encoding the state in terms of a predefined vocabulary.\n",
      "Question : for the text In the following, we explain our Procedural Reasoning Networks model. Its architecture is based on a bi-directional attention flow (BiDAF) model BIBREF6, but also equipped with an explicit reasoning module that acts on entity-specific relational memory units. Fig. FIGREF4 shows an overview of the network architecture. It consists of five main modules: An input module, an attention module, a reasoning module, a modeling module, and an output module. Note that the question answering tasks we consider here are multimodal in that while the context is a procedural text, the question and the multiple choice answers are composed of images..Input Module extracts vector representations of inputs at different levels of granularity by using several different encoders..Reasoning Module scans the procedural text and tracks the states of the entities and their relations through a recurrent relational memory core unit BIBREF5..Attention Module computes context-aware query vectors and query-aware context vectors as well as query-aware memory vectors..Modeling Module employs two multi-layered RNNs to encode previous layers outputs..Output Module scores a candidate answer from the given multiple-choice list..At a high level, as the model is reading the cooking recipe, it continually updates the internal memory representations of the entities (ingredients) based on the content of each step – it keeps track of changes in the states of the entities, providing an entity-centric summary of the recipe. The response to a question and a possible answer depends on the representation of the recipe text as well as the last states of the entities. All this happens in a series of implicit relational reasoning steps and there is no need for explicitly encoding the state in terms of a predefined vocabulary. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "8\n",
      "Question 1: How does the Attention module link the question with the recipe text and entities?\n",
      "\n",
      "Answer 1: The Attention module uses matrices from the input and reasoning modules and constructs the question-aware recipe representation and entity representation. It calculates attentions from the question to the recipe and entities, and from the recipe and entities to the question using a shared affinity matrix. The module determines relevant images within the question for each word in the recipe and vice versa. The question-aware representation of the input recipe, denoted by G, and the question-aware representation of entities, denoted by Y, integrate the images in the question and the entities in the input recipe.\n",
      "Question : for the text Attention module is in charge of linking the question with the recipe text and the entities present in the recipe. It takes the matrices $\\mathbf {Q^{\\prime }}$ and $\\mathbf {R}^{\\prime }$ from the input module, and $\\mathbf {E}$ from the reasoning module and constructs the question-aware recipe representation $\\mathbf {G}$ and the question-aware entity representation $\\mathbf {Y}$. Following the attention flow mechanism described in BIBREF14, we specifically calculate attentions in four different directions: (1) from question to recipe, (2) from recipe to question, (3) from question to entities, and (4) from entities to question..The first two of these attentions require computing a shared affinity matrix $\\mathbf {S}^R \\in \\mathbb {R}^{N \\times M}$ with $\\mathbf {S}^R_{i,j}$ indicating the similarity between $i$-th recipe word and $j$-th image in the question estimated by.where $\\mathbf {w}^{\\top }_{R}$ is a trainable weight vector, $\\circ $ and $[;]$ denote elementwise multiplication and concatenation operations, respectively..Recipe-to-question attention determines the images within the question that is most relevant to each word of the recipe. Let $\\mathbf {\\tilde{Q}} \\in \\mathbb {R}^{2d \\times N}$ represent the recipe-to-question attention matrix with its $i$-th column being given by $ \\mathbf {\\tilde{Q}}_i=\\sum _j \\mathbf {a}_{ij}\\mathbf {Q}^{\\prime }_j$ where the attention weight is computed by $\\mathbf {a}_i=\\operatorname{softmax}(\\mathbf {S}^R_{i}) \\in \\mathbb {R}^M$..Question-to-recipe attention signifies the words within the recipe that have the closest similarity to each image in the question, and construct an attended recipe vector given by $ \\tilde{\\mathbf {r}}=\\sum _{i}\\mathbf {b}_i\\mathbf {R}^{\\prime }_i$ with the attention weight is calculated by $\\mathbf {b}=\\operatorname{softmax}(\\operatorname{max}_{\\mathit {col}}(\\mathbf {S}^R)) \\in \\mathbb {R}^{N}$ where $\\operatorname{max}_{\\mathit {col}}$ denotes the maximum function across the column. The question-to-recipe matrix is then obtained by replicating $\\tilde{\\mathbf {r}}$ $N$ times across the column, giving $\\tilde{\\mathbf {R}} \\in \\mathbb {R}^{2d \\times N}$..Then, we construct the question aware representation of the input recipe, $\\mathbf {G}$, with its $i$-th column $\\mathbf {G}_i \\in \\mathbb {R}^{8d \\times N}$ denoting the final embedding of $i$-th word given by.Attentions from question to entities, and from entities to question are computed in a way similar to the ones described above. The only difference is that it uses a different shared affinity matrix to be computed between the memory encoding entities $\\mathbf {E}$ and the question $\\mathbf {Q}^{\\prime }$. These attentions are then used to construct the question aware representation of entities, denoted by $\\mathbf {Y}$, that links and integrates the images in the question and the entities in the input recipe. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "9\n",
      "Question 1: How is the input recipe encoded at the step level?\n",
      "\n",
      "Answer 1: The input recipe is encoded at the step level by obtaining a step-level contextual embedding containing T steps, where the final state of a BiLSTM encoding the i-th step of the recipe is represented by the vector $\\mathbf{s}_i$.\n",
      "Question : for the text Let the triple $(\\mathbf {R},\\mathbf {Q},\\mathbf {A})$ be a sample input. Here, $\\mathbf {R}$ denotes the input recipe which contains textual instructions composed of $N$ words in total. $\\mathbf {Q}$ represents the question that consists of a sequence of $M$ images. $\\mathbf {A}$ denotes an answer that is either a single image or a series of $L$ images depending on the reasoning task. In particular, for the visual cloze and the visual coherence type questions, the answer contains a single image ($L=1$) and for the visual ordering task, it includes a sequence..We encode the input recipe $\\mathbf {R}$ at character, word, and step levels. Character-level embedding layer uses a convolutional neural network, namely CharCNN model by BIBREF7, which outputs character level embeddings for each word and alleviates the issue of out-of-vocabulary (OOV) words. In word embedding layer, we use a pretrained GloVe model BIBREF8 and extract word-level embeddings. The concatenation of the character and the word embeddings are then fed to a two-layer highway network BIBREF10 to obtain a contextual embedding for each word in the recipe. This results in the matrix $\\mathbf {R}^{\\prime } \\in \\mathbb {R}^{2d \\times N}$..On top of these layers, we have another layer that encodes the steps of the recipe in an individual manner. Specifically, we obtain a step-level contextual embedding of the input recipe containing $T$ steps as $\\mathcal {S}=(\\mathbf {s}_1,\\mathbf {s}_2,\\dots ,\\mathbf {s}_T)$ where $\\mathbf {s}_i$ represents the final state of a BiLSTM encoding the $i$-th step of the recipe obtained from the character and word-level embeddings of the tokens exist in the corresponding step..We represent both the question $\\mathbf {Q}$ and the answer $\\mathbf {A}$ in terms of visual embeddings. Here, we employ a pretrained ResNet-50 model BIBREF11 trained on ImageNet dataset BIBREF12 and represent each image as a real-valued 2048-d vector using features from the penultimate average-pool layer. Then these embeddings are passed first to a multilayer perceptron (MLP) and then its outputs are fed to a BiLSTM. We then form a matrix $\\mathbf {Q}^{\\prime } \\in \\mathbb {R}^{2d \\times M}$ for the question by concatenating the cell states of the BiLSTM. For the visual ordering task, to represent the sequence of images in the answer with a single vector, we additionally use a BiLSTM and define the answering embedding by the summation of the cell states of the BiLSTM. Finally, for all tasks, these computations produce answer embeddings denoted by $\\mathbf {a} \\in \\mathbb {R}^{2d \\times 1}$. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "10\n",
      "Question 1: How does the Modeling module form the combined vector representation of the recipe and entities?\n",
      "\n",
      "Answer 1: The Modeling module uses a two-layer BiLSTM to read the question-aware recipe and encode the interactions among the words conditioned on the question. It then uses a second BiLSTM over the entities, resulting in two vector embeddings. These embeddings are concatenated and projected to a fixed size representation using a multilayer perceptron with a $\\operatorname{tanh}$ activation function.\n",
      "Question : for the text Modeling module takes the question-aware representations of the recipe $\\mathbf {G}$ and the entities $\\mathbf {Y}$, and forms their combined vector representation. For this purpose, we first use a two-layer BiLSTM to read the question-aware recipe $\\mathbf {G}$ and to encode the interactions among the words conditioned on the question. For each direction of BiLSTM , we use its hidden state after reading the last token as its output. In the end, we obtain a vector embedding $\\mathbf {c} \\in \\mathbb {R}^{2d \\times 1}$. Similarly, we employ a second BiLSTM, this time, over the entities $\\mathbf {Y}$, which results in another vector embedding $\\mathbf {f} \\in \\mathbb {R}^{2d_E \\times 1}$. Finally, these vector representations are concatenated and then projected to a fixed size representation using $\\mathbf {o}=\\varphi _o(\\left[\\mathbf {c}; \\mathbf {f}\\right]) \\in \\mathbb {R}^{2d \\times 1}$ where $\\varphi _o$ is a multilayer perceptron with $\\operatorname{tanh}$ activation function. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "11\n",
      "Question 1: How is the correct answer determined in the procedural reasoning network?\n",
      "Answer 1: The correct answer is determined based on the candidate answer with the highest similarity score, which is obtained through the output module using vector embeddings of the question-aware recipe, entities, and answer. A hinge ranking loss is employed for training the network.\n",
      "Question : for the text The output module takes the output of the modeling module, encoding vector embeddings of the question-aware recipe and the entities $\\mathbf {Y}$, and the embedding of the answer $\\mathbf {A}$, and returns a similarity score which is used while determining the correct answer. Among all the candidate answer, the one having the highest similarity score is chosen as the correct answer. To train our proposed procedural reasoning network, we employ a hinge ranking loss BIBREF15, similar to the one used in BIBREF2, given below..where $\\gamma $ is the margin parameter, $\\mathbf {a}_+$ and $\\mathbf {a}_{-}$ are the correct and the incorrect answers, respectively. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "12\n",
      "What is the purpose of the relational memory component in the model? \n",
      "\n",
      "The purpose of the relational memory component in the model is to keep track of the entities, their state changes and their relations in relation to each other over the course of the recipe, which greatly improves the interpretability of model outputs.\n",
      "Question : for the text As mentioned before, comprehending a cooking recipe is mostly about entities (basic ingredients) and actions (cooking activities) described in the recipe instructions. Each action leads to changes in the states of the entities, which usually affects their visual characteristics. A change rarely occurs in isolation; in most cases, the action affects multiple entities at once. Hence, in our reasoning module, we have an explicit memory component implemented with relational memory units BIBREF5. This helps us to keep track of the entities, their state changes and their relations in relation to each other over the course of the recipe (see Fig. FIGREF14). As we will examine in more detail in Section SECREF4, it also greatly improves the interpretability of model outputs..Specifically, we set up the memory with a memory matrix $\\mathbf {E} \\in \\mathbb {R}^{d_E \\times K}$ by extracting $K$ entities (ingredients) from the first step of the recipe. We initialize each memory cell $\\mathbf {e}_i$ representing a specific entity by its CharCNN and pre-trained GloVe embeddings. From now on, we will use the terms memory cells and entities interchangeably throughout the paper. Since the input recipe is given in the form of a procedural text decomposed into a number of steps, we update the memory cells after each step, reflecting the state changes happened on the entities. This update procedure is modelled via a relational recurrent neural network (R-RNN), recently proposed by BIBREF5. It is built on a 2-dimensional LSTM model whose matrix of cell states represent our memory matrix $\\mathbf {E}$. Here, each row $i$ of the matrix $\\mathbf {E}$ refers to a specific entity $\\mathbf {e}_i$ and is updated after each recipe step $t$ as follows:.where $\\mathbf {s}_{t}$ denotes the embedding of recipe step $t$ and $\\mathbf {\\phi }_{i,t}=(\\mathbf {h}_{i,t},\\mathbf {e}_{i,t})$ is the cell state of the R-RNN at step $t$ with $\\mathbf {h}_{i,t}$ and $\\mathbf {e}_{i,t}$ being the $i$-th row of the hidden state of the R-RNN and the dynamic representation of entity $\\mathbf {e}_{i}$ at the step $t$, respectively. The R-RNN model exploits a multi-headed self-attention mechanism BIBREF13 that allows memory cells to interact with each other and attend multiple locations simultaneously during the update phase..In Fig. FIGREF14, we illustrate how this interaction takes place in our relational memory module by considering a sample cooking recipe and by presenting how the attention matrix changes throughout the recipe. In particular, the attention matrix at a specific time shows the attention flow from one entity (memory cell) to another along with the attention weights to the corresponding recipe step (offset column). The color intensity shows the magnitude of the attention weights. As can be seen from the figure, the internal representations of the entities are actively updated at each step. Moreover, as argued in BIBREF5, this can be interpreted as a form of relational reasoning as each update on a specific memory cell is operated in relation to others. Here, we should note that it is often difficult to make sense of these attention weights. However, we observe that the attention matrix changes very gradually near the completion of the recipe. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "13\n",
      "Question 1: What is the main difference between the approach used in the discussed work and previous works exploring tracking entity states in text?\n",
      "\n",
      "Answer 1: The main difference is that the approach discussed in the work utilizes relational memory core units which allow memories to interact with each other during each update, whereas previous works did not incorporate this feature.\n",
      "Question : for the text In recent years, tracking entities and their state changes have been explored in the literature from a variety of perspectives. In an early work, BIBREF21 proposed a dynamic memory based network which updates entity states using a gating mechanism while reading the text. BIBREF22 presented a more structured memory augmented model which employs memory slots for representing both entities and their relations. BIBREF23 suggested a conceptually similar model in which the pairwise relations between attended memories are utilized to encode the world state. The main difference between our approach and these works is that by utilizing relational memory core units we also allow memories to interact with each other during each update..BIBREF24 showed that similar ideas can be used to compile supporting memories in tracking dialogue state. BIBREF25 has shown the importance of coreference signals for reading comprehension task. More recently, BIBREF26 introduced a specialized recurrent layer which uses coreference annotations for improving reading comprehension tasks. On language modeling task, BIBREF27 proposed a language model which can explicitly incorporate entities while dynamically updating their representations for a variety of tasks such as language modeling, coreference resolution, and entity prediction..Our work builds upon and contributes to the growing literature on tracking states changes in procedural text. BIBREF0 presented a neural model that can learn to explicitly predict state changes of ingredients at different points in a cooking recipe. BIBREF1 proposed another entity-aware model to track entity states in scientific processes. BIBREF3 demonstrated that the prediction quality can be boosted by including hard and soft constraints to eliminate unlikely or favor probable state changes. In a follow-up work, BIBREF4 exploited the notion of label consistency in training to enforce similar predictions in similar procedural contexts. BIBREF28 proposed a model that dynamically constructs a knowledge graph while reading the procedural text to track the ever-changing entities states. As discussed in the introduction, however, these previous methods use a strong inductive bias and assume that state labels are present during training. In our study, we deliberately focus on unlabeled procedural data and ask the question: Can multimodality help to identify and provide insights to understanding state changes. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "14\n",
      "Question 1: What is the aim of the visual coherence task in RecipeQA?\n",
      "Answer 1: The visual coherence task aims to test a model's ability to identify the image within a sequence of four images that is inconsistent with the text instructions of a cooking recipe.\n",
      "Question : for the text In our study, we particularly focus on the visual reasoning tasks of RecipeQA, namely visual cloze, visual coherence, and visual ordering tasks, each of which examines a different reasoning skill. We briefly describe these tasks below..Visual Cloze. In the visual cloze task, the question is formed by a sequence of four images from consecutive steps of a recipe where one of them is replaced by a placeholder. A model should select the correct one from a multiple-choice list of four answer candidates to fill in the missing piece. In that regard, the task inherently requires aligning visual and textual information and understanding temporal relationships between the cooking actions and the entities..Visual Coherence. The visual coherence task tests the ability to identify the image within a sequence of four images that is inconsistent with the text instructions of a cooking recipe. To succeed in this task, a model should have a clear understanding of the procedure described in the recipe and at the same time connect language and vision..Visual Ordering. The visual ordering task is about grasping the temporal flow of visual events with the help of the given recipe text. The questions show a set of four images from the recipe and the task is to sort jumbled images into the correct order. Here, a model needs to infer the temporal relations between the images and align them with the recipe steps. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "15\n",
      "Question 1: What supported the research work conducted by the authors?\n",
      "\n",
      "Answer 1: The research work conducted by the authors was supported by the National Natural Science Foundation of China (No. 61772201) and the National Key R&D Program of China for “Precision medical research” (No. 2018YFC0910550).\n",
      "Question : for the text The authors would like to appreciate any suggestions or comments from the anonymous reviewers. This work was supported by the National Natural Science Foundation of China (No. 61772201) and the National Key R&D Program of China for “Precision medical research\" (No. 2018YFC0910550). generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "16\n",
      "Question 1: What is the most popular sampling strategy in active learning?\n",
      "\n",
      "Answer 1: The most popular sampling strategy in active learning is uncertainty sampling. It selects samples closest to the decision boundary of the classifier and chooses these samples for annotators to relabel.\n",
      "Question : for the text Active learning BIBREF22 mainly aims to ease the data collection process by automatically deciding which instances should be labeled by annotators to train a model as quickly and effectively as possible BIBREF23 . The sampling strategy plays a key role in active learning. In the past decade, the rapid development of active learning has resulted in various sampling strategies, such as uncertainty sampling BIBREF24 , query-by-committee BIBREF25 and information gain BIBREF26 . Currently, the most mainstream sampling strategy is uncertainty sampling. It focuses its selection on samples closest to the decision boundary of the classifier and then chooses these samples for annotators to relabel BIBREF27 ..The formal definition of uncertainty sampling is to select a sample INLINEFORM0 that maximizes the entropy INLINEFORM1 over the probability of predicted classes: DISPLAYFORM0 .where INLINEFORM0 is a multi-dimensional feature vector, INLINEFORM1 is its binary label, and INLINEFORM2 is the predicted probability, through which a classifier trained on training sets can map features to labels. However, in some complicated tasks, such as CWS and NER, only considering the uncertainty of classifier is obviously not enough. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "17\n",
      "What are the two parts that active learning methods can be described into? \n",
      "\n",
      "The two parts that active learning methods can be described into are a learning engine and a selection engine.\n",
      "Question : for the text Active learning methods can generally be described into two parts: a learning engine and a selection engine BIBREF28 . The learning engine is essentially a classifier, which is mainly used for training of classification problems. The selection engine is based on the sampling strategy, which chooses samples that need to be relabeled by annotators from unlabeled data. Then, relabeled samples are added to training set for classifier to re-train, thus continuously improving the accuracy of the classifier. In this paper, a CRF-based segmenter and a scoring model are employed as learning engine and selection engine, respectively..Fig. FIGREF7 and Algorithm SECREF3 demonstrate the procedure of CWS based on active learning. First, we train a CRF-based segmenter by train set. Then, the segmenter is employed to annotate the unlabeled set roughly. Subsequently, information entropy based scoring model picks INLINEFORM0 -lowest ranking samples for annotators to relabel. Meanwhile, the train sets and unlabeled sets are updated. Finally, we re-train the segmenter. The above steps iterate until the desired accuracy is achieved or the number of iterations has reached a predefined threshold. [!ht] Active Learning for Chinese Word Segmentation labeled data INLINEFORM1 , unlabeled data INLINEFORM2 , the number of iterations INLINEFORM3 , the number of samples selected per iteration INLINEFORM4 , partitioning function INLINEFORM5 , size INLINEFORM6 a word segmentation model INLINEFORM7 with the smallest test set loss INLINEFORM8 Initialize: INLINEFORM9 . train a word segmenter INLINEFORM0 . estimate the test set loss INLINEFORM0 . label INLINEFORM0 by INLINEFORM1 . INLINEFORM0 to INLINEFORM1 INLINEFORM2 compute INLINEFORM3 by branch information entropy based scoring model. select INLINEFORM0 -lowest ranking samples INLINEFORM1 .relabel INLINEFORM0 by annotators.form a new labeled dataset INLINEFORM0 .form a new unlabeled dataset INLINEFORM0 .train a word segmenter INLINEFORM0 .estimate the new test loss INLINEFORM0 .compute the loss reduction INLINEFORM0 . INLINEFORM0 INLINEFORM1 . INLINEFORM0 . INLINEFORM0 INLINEFORM1 with the smallest test set loss INLINEFORM2 INLINEFORM3  generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "18\n",
      "What is the purpose of converting labeled data into the BMES format?\n",
      "\n",
      "Answer 1: The purpose of converting labeled data into the BMES format is to assign a label to each character in a sequence, indicating whether it is the beginning, middle, end, or single character of a word. This allows for the formalization of the Chinese word segmentation (CWS) task as a sequence labeling problem with character position tags.\n",
      "Question : for the text CWS can be formalized as a sequence labeling problem with character position tags, which are (`B', `M', `E', `S'). So, we convert the labeled data into the `BMES' format, in which each character in the sequence is assigned into a label as follows one by one: B=beginning of a word, M=middle of a word, E=end of a word and S=single word..In this paper, we use CRF as a training model for CWS task. Given the observed sequence, CRF has a single exponential model for the joint probability of the entire sequence of labels, while maximum entropy markov model (MEMM) BIBREF29 uses per-state exponential models for the conditional probabilities of next states BIBREF4 . Therefore, it can solve the label bias problem effectively. Compared with neural networks, it has less dependency on the corpus size..First, we pre-process EHRs at the character-level, separating each character of raw EHRs. For instance, given a sentence INLINEFORM0 , where INLINEFORM1 represents the INLINEFORM2 -th character, the separated form is INLINEFORM3 . Then, we employ Word2Vec BIBREF30 to train pre-processed EHRs to get character embeddings. To capture interactions between adjacent characters, K-means clustering algorithm BIBREF31 is utilized to feature the coherence over characters. In general, K-means divides INLINEFORM4 EHR characters into INLINEFORM5 groups of clusters and the similarity of EHR characters in the same cluster is higher. With each iteration, K-means can classify EHR characters into the nearest cluster based on distance to the mean vector. Then, recalculating and adjusting the mean vectors of these clusters until the mean vector converges. K-means features explicitly show the difference between two adjacent characters and even multiple characters. Finally, we additionally add K-means clustering features to the input of CRF-based segmenter. The segmenter makes positional tagging decisions over individual characters. For example, a Chinese segmented sentence UTF8gkai“病人/长期/于/我院/肾病科/住院/治疗/。/\" (The patient was hospitalized for a long time in the nephrology department of our hospital.) is labeled as `BEBESBEBMEBEBES'. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "19\n",
      "What is the main challenge faced by supervised learning methods in Chinese word segmentation (CWS)?\n",
      "\n",
      "The main challenge faced by supervised learning methods in CWS is their reliance on manual feature engineering, which can be difficult to design and results in large feature sizes.\n",
      "Question : for the text In past decades, researches on CWS have a long history and various methods have been proposed BIBREF13 , BIBREF14 , BIBREF15 , which is an important task for Chinese NLP BIBREF7 . These methods are mainly focus on two categories: supervised learning and deep learning BIBREF2 ..Supervised Learning Methods. Initially, supervised learning methods were widely-used in CWS. Xue BIBREF13 employed a maximum entropy tagger to automatically assign Chinese characters. Zhao et al. BIBREF16 used a conditional random field for tag decoding and considered both feature template selection and tag set selection. However, these methods greatly rely on manual feature engineering BIBREF17 , while handcrafted features are difficult to design, and the size of these features is usually very large BIBREF6 ..Deep Learning Methods. Recently, neural networks have been applied in CWS tasks. To name a few, Zheng et al. BIBREF14 used deep layers of neural networks to learn feature representations of characters. Chen et al. BIBREF6 adopted LSTM to capture the previous important information. Chen et al. BIBREF18 proposed a gated recursive neural network (GRNN), which contains reset and update gates to incorporate the complicated combinations of characters. Jiang and Tang BIBREF19 proposed a sequence-to-sequence transformer model to avoid overfitting and capture character information at the distant site of a sentence. Yang et al. BIBREF20 investigated subword information for CWS and integrated subword embeddings into a Lattice LSTM (LaLSTM) network. However, general word segmentation models do not work well in specific field due to lack of annotated training data..Currently, a handful of domain-specific CWS approaches have been studied, but they focused on decentralized domains. In the metallurgical field, Shao et al. BIBREF15 proposed a domain-specific CWS method based on Bi-LSTM model. In the medical field, Xing et al. BIBREF8 proposed an adaptive multi-task transfer learning framework to fully leverage domain-invariant knowledge from high resource domain to medical domain. Meanwhile, transfer learning still greatly focuses on the corpus in general domain. When it comes to the specific domain, large amounts of manually-annotated data is necessary. Active learning can solve this problem to a certain extent. However, due to the challenges faced by performing active learning on CWS, only a few studies have been conducted. On judgements, Yan et al. BIBREF21 adopted the local annotation strategy, which selects substrings around the informative characters in active learning. However, their method still stays at the statistical level. Unlike the above method, we propose an active learning approach for CWS in medical text, which combines information entropy with neural network to effectively reduce annotation cost. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "20\n",
      "What is the main advantage of the proposed word segmentation method based on active learning in EHRs annotation? \n",
      "\n",
      "The main advantage of the proposed word segmentation method based on active learning is that it requires 6% less relabeled samples compared to uncertainty sampling, resulting in a cost-effective and efficient approach to EHRs annotation.\n",
      "Question : for the text To relieve the efforts of EHRs annotation, we propose an effective word segmentation method based on active learning, in which the sampling strategy is a scoring model combining information entropy with neural network. Compared with the mainstream uncertainty sampling, our strategy selects samples from statistical perspective and deep learning level. In addition, to capture coherence between characters, we add K-means clustering features to CRF-based word segmenter. Based on EHRs collected from the Shuguang Hospital Affiliated to Shanghai University of Traditional Chinese Medicine, we evaluate our method on CWS task. Compared with uncertainty sampling, our method requires 6% less relabeled samples to achieve better performance, which proves that our method can save the cost of manual annotation to a certain extent..In future, we plan to employ other widely-used deep neural networks, such as convolutional neural network and attention mechanism, in the research of EHRs segmentation. Then, we believe that our method can be applied to other tasks as well, so we will fully investigate the application of our method in other tasks, such as NER and relation extraction. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "21\n",
      "Question 1: How many records were in the unlabeled set of the EHR dataset?\n",
      "\n",
      "Answer 1: 3200 records were randomly selected for the unlabeled set, while the remaining 668 records were manually annotated as a labeled set.\n",
      "Question : for the text We collect 204 EHRs with cardiovascular diseases from the Shuguang Hospital Affiliated to Shanghai University of Traditional Chinese Medicine and each contains 27 types of records. We choose 4 different types with a total of 3868 records from them, which are first course reports, medical records, chief ward round records and discharge records. The detailed information of EHRs are listed in Table TABREF32 ..We split our datasets as follows. First, we randomly select 3200 records from 3868 records as unlabeled set. Then, we manually annotate remaining 668 records as labeled set, which contains 1170 sentences. Finally, we divide labeled set into train set and test set with the ratio of 7:3 randomly. Statistics of datasets are listed in Table TABREF33 . generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "22\n",
      "What was the main finding from the experiment comparing the two mainstream CWS tools, LTP and Jieba?\n",
      "\n",
      "The main finding was that while these tools have high scores for word segmentation in general fields, they do not perform well in specific fields due to the presence of uncommon words and terminologies. Additionally, the experiment found that Jieba benefits from the use of an external domain dictionary, while the recall of LTP decreases with the use of a domain dictionary.\n",
      "Question : for the text Our work experimentally compares two mainstream CWS tools (LTP and Jieba) on training and testing sets. These two tools are widely used and recognized due to their high INLINEFORM0 -score of word segmentation in general fields. However, in specific fields, there are many terminologies and uncommon words, which lead to the unsatisfactory performance of segmentation results. To solve the problem of word segmentation in specific fields, these two tools provide a custom dictionary for users. In the experiments, we also conduct a comparative experiment on whether external domain dictionary has an effect on the experimental results. We manually construct the dictionary when labeling EHRs..From the results in Table TABREF41 , we find that Jieba benefits a lot from the external dictionary. However, the Recall of LTP decreases when joining the domain dictionary. Generally speaking, since these two tools are trained by general domain corpus, the results are not ideal enough to cater to the needs of subsequent NLP of EHRs when applied to specific fields..To investigate the effectiveness of K-means features in CRF-based segmenter, we also compare K-means with 3 different clustering features, including MeanShift BIBREF36 , SpectralClustering BIBREF37 and DBSCAN BIBREF38 on training and testing sets. From the results in Table TABREF43 , by adding additional clustering features in CRF-based segmenter, there is a significant improvement of INLINEFORM0 -score, which indicates that clustering features can effectively capture the semantic coherence between characters. Among these clustering features, K-means performs best, so we utlize K-means results as additional features for CRF-based segmenter..In this experiment, since uncertainty sampling is the most popular strategy in real applications for its simpleness and effectiveness BIBREF27 , we compare our proposed strategy with uncertainty sampling in active learning. We conduct our experiments as follows. First, we employ CRF-based segmenter to annotate the unlabeled set. Then, sampling strategy in active learning selects a part of samples for annotators to relabel. Finally, the relabeled samples are added to train set for segmenter to re-train. Our proposed scoring strategy selects samples according to the sequence scores of the segmented sentences, while uncertainty sampling suggests relabeling samples that are closest to the segmenter’s decision boundary..Generally, two main parameters in active learning are the numbers of iterations and samples selected per iteration. To fairly investigate the influence of two parameters, we compare our proposed strategy with uncertainty sampling on the same parameter. We find that though the number of iterations is large enough, it has a limited impact on the performance of segmenter. Therefore, we choose 30 as the number of iterations, which is a good trade-off between speed and performance. As for the number of samples selected per iteration, there are 6078 sentences in unlabeled set, considering the high cost of relabeling, we set four sizes of samples selected per iteration, which are 2%, 5%, 8% and 11%..The experimental results of two sampling strategies with 30 iterations on four different proportions of relabeled data are shown in Fig. FIGREF45 , where x-axis represents the number of iterations and y-axis denotes the INLINEFORM0 -score of the segmenter. Scoring strategy shows consistent improvements over uncertainty sampling in the early iterations, indicating that scoring strategy is more capable of selecting representative samples..Furthermore, we also investigate the relations between the best INLINEFORM0 -score and corresponding number of iteration on two sampling strategies, which is depicted in Fig. FIGREF46 ..It is observed that in our proposed scoring model, with the proportion of relabeled data increasing, the iteration number of reaching the optimal word segmentation result is decreasing, but the INLINEFORM0 -score of CRF-based word segmenter is also gradually decreasing. When the proportion is 2%, the segmenter reaches the highest INLINEFORM1 -score: 90.62%. Obviously, our proposed strategy outperforms uncertainty sampling by a large margin. Our proposed method needs only 2% relabeled samples to obtain INLINEFORM2 -score of 90.62%, while uncertainty sampling requires 8% samples to reach its best INLINEFORM3 -score of 88.98%, which indicates that with our proposed method, we only need to manually relabel a small number of samples to achieve a desired segmentation result. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "23\n",
      "What is the scoring model proposed for selecting appropriate sentences in unlabeled corpora?\n",
      "\n",
      "The scoring model proposed for selecting appropriate sentences in unlabeled corpora is based on information entropy and neural network as the sampling strategy of active learning, inspired by Cai and Zhao. The score of a segmented sentence is computed by mapping it to a sequence of candidate word embeddings and then scoring each individual candidate word based on the possibility of it being a legal word and the rationality of the link it has with previous segmentation history. The final score is calculated by summing up the word scores and link scores.\n",
      "Question : for the text To select the most appropriate sentences in a large number of unlabeled corpora, we propose a scoring model based on information entropy and neural network as the sampling strategy of active learning, which is inspired by Cai and Zhao BIBREF32 . The score of a segmented sentence is computed as follows. First, mapping the segmented sentence to a sequence of candidate word embeddings. Then, the scoring model takes the word embedding sequence as input, scoring over each individual candidate word from two perspectives: (1) the possibility that the candidate word itself can be regarded as a legal word; (2) the rationality of the link that the candidate word directly follows previous segmentation history. Fig. FIGREF10 illustrates the entire scoring model. A gated neural network is employed over character embeddings to generate distributed representations of candidate words, which are sent to a LSTM model..We use gated neural network and information entropy to capture the likelihood of the segment being a legal word. The architecture of word score model is depicted in Fig. FIGREF12 ..Gated Combination Neural Network (GCNN).To effectively learn word representations through character embeddings, we use GCNN BIBREF32 . The architecture of GCNN is demonstrated in Fig. FIGREF13 , which includes update gate and reset gate. The gated mechanism not only captures the characteristics of the characters themselves, but also utilizes the interaction between the characters. There are two types of gates in this network structure: reset gates and update gates. These two gated vectors determine the final output of the gated recurrent neural network, where the update gate helps the model determine what to be passed, and the reset gate primarily helps the model decide what to be cleared. In particular, the word embedding of a word with INLINEFORM0 characters can be computed as: DISPLAYFORM0 .where INLINEFORM0 and INLINEFORM1 are update gates for new combination vector INLINEFORM2 and the i-th character INLINEFORM3 respectively, the combination vector INLINEFORM4 is formalized as: DISPLAYFORM0 .where INLINEFORM0 and INLINEFORM1 are reset gates for characters..Left and Right Branch Information Entropy In general, each string in a sentence may be a word. However, compared with a string which is not a word, the string of a word is significantly more independent. The branch information entropy is usually used to judge whether each character in a string is tightly linked through the statistical characteristics of the string, which reflects the likelihood of a string being a word. The left and right branch information entropy can be formalized as follows: DISPLAYFORM0 DISPLAYFORM1 .where INLINEFORM0 denotes the INLINEFORM1 -th candidate word, INLINEFORM2 denotes the character set, INLINEFORM3 denotes the probability that character INLINEFORM4 is on the left of word INLINEFORM5 and INLINEFORM6 denotes the probability that character INLINEFORM7 is on the right of word INLINEFORM8 . INLINEFORM9 and INLINEFORM10 respectively represent the left and right branch information entropy of the candidate word INLINEFORM11 . If the left and right branch information entropy of a candidate word is relatively high, the probability that the candidate word can be combined with the surrounded characters to form a word is low, thus the candidate word is likely to be a legal word..To judge whether the candidate words in a segmented sentence are legal words, we compute the left and right entropy of each candidate word, then take average as the measurement standard: DISPLAYFORM0 .We represent a segmented sentence with INLINEFORM0 candidate words as [ INLINEFORM1 , INLINEFORM2 ,..., INLINEFORM3 ], so the INLINEFORM4 ( INLINEFORM5 ) of the INLINEFORM6 -th candidate word is computed by its average entropy: DISPLAYFORM0 .In this paper, we use LSTM to capture the coherence between words in a segmented sentence. This neural network is mainly an optimization for traditional RNN. RNN is widely used to deal with time-series prediction problems. The result of its current hidden layer is determined by the input of the current layer and the output of the previous hidden layer BIBREF33 . Therefore, RNN can remember historical results. However, traditional RNN has problems of vanishing gradient and exploding gradient when training long sequences BIBREF34 . By adding a gated mechanism to RNN, LSTM effectively solves these problems, which motivates us to get the link score with LSTM. Formally, the LSTM unit performs the following operations at time step INLINEFORM0 : DISPLAYFORM0 DISPLAYFORM1 .where INLINEFORM0 , INLINEFORM1 , INLINEFORM2 are the inputs of LSTM, all INLINEFORM3 and INLINEFORM4 are a set of parameter matrices to be trained, and INLINEFORM5 is a set of bias parameter matrices to be trained. INLINEFORM6 and INLINEFORM7 operation respectively represent matrix element-wise multiplication and sigmoid function. In the LSTM unit, there are two hidden layers ( INLINEFORM8 , INLINEFORM9 ), where INLINEFORM10 is the internal memory cell for dealing with vanishing gradient, while INLINEFORM11 is the main output of the LSTM unit for complex operations in subsequent layers..We denotes INLINEFORM0 as the word embedding of time step INLINEFORM1 , a prediction INLINEFORM2 of next word embedding INLINEFORM3 can be computed by hidden layer INLINEFORM4 : DISPLAYFORM0 .Therefore, link score of next word embedding INLINEFORM0 can be computed as: DISPLAYFORM0 .Due to the structure of LSTM, vector INLINEFORM0 contains important information of entire segmentation decisions. In this way, the link score gets the result of the sequence-level word segmentation, not just word-level..Intuitively, we can compute the score of a segmented sequence by summing up word scores and link scores. However, we find that a sequence with more candidate words tends to have higher sequence scores. Therefore, to alleviate the impact of the number of candidate words on sequence scores, we calculate final scores as follows: DISPLAYFORM0 .where INLINEFORM0 denotes the INLINEFORM1 -th segmented sequence with INLINEFORM2 candidate words, and INLINEFORM3 represents the INLINEFORM4 -th candidate words in the segmented sequence..When training the model, we seek to minimize the sequence score of the corrected segmented sentence and the predicted segmented sentence. DISPLAYFORM0 .where INLINEFORM0 is the loss function. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "24\n",
      "What is the main challenge in labeling electronic health records for Chinese word segmentation (CWS)?\n",
      "\n",
      "The main challenge in labeling electronic health records for CWS is that EHRs have many medical terminologies, therefore only annotators with medical backgrounds can be qualified to label EHRs. Additionally, EHRs may involve personal privacies of patients which leads to the high annotation cost and insufficient training corpus in the research of CWS in medical text.\n",
      "Question : for the text Electronic health records (EHRs) systematically collect patients' clinical information, such as health profiles, histories of present illness, past medical histories, examination results and treatment plans BIBREF0 . By analyzing EHRs, many useful information, closely related to patients, can be discovered BIBREF1 . Since Chinese EHRs are recorded without explicit word delimiters (e.g., “UTF8gkai糖尿病酮症酸中毒” (diabetic ketoacidosis)), Chinese word segmentation (CWS) is a prerequisite for processing EHRs. Currently, state-of-the-art CWS methods usually require large amounts of manually-labeled data to reach their full potential. However, there are many challenges inherent in labeling EHRs. First, EHRs have many medical terminologies, such as “UTF8gkai高血压性心脏病” (hypertensive heart disease) and “UTF8gkai罗氏芬” (Rocephin), so only annotators with medical backgrounds can be qualified to label EHRs. Second, EHRs may involve personal privacies of patients. Therefore, they cannot be openly published on a large scale for labeling. The above two problems lead to the high annotation cost and insufficient training corpus in the research of CWS in medical text..CWS was usually formulated as a sequence labeling task BIBREF2 , which can be solved by supervised learning approaches, such as hidden markov model (HMM) BIBREF3 and conditional random field (CRF) BIBREF4 . However, these methods rely heavily on handcrafted features. To relieve the efforts of feature engineering, neural network-based methods are beginning to thrive BIBREF5 , BIBREF6 , BIBREF7 . However, due to insufficient annotated training data, conventional models for CWS trained on open corpus often suffer from significant performance degradation when transferred to a domain-specific text. Moreover, the task in medical domain is rarely dabbled, and only one related work on transfer learning is found in recent literatures BIBREF8 . However, researches related to transfer learning mostly remain in general domains, causing a major problem that a considerable amount of manually annotated data is required, when introducing the models into specific domains..One of the solutions for this obstacle is to use active learning, where only a small scale of samples are selected and labeled in an active manner. Active learning methods are favored by the researchers in many natural language processing (NLP) tasks, such as text classification BIBREF9 and named entity recognition (NER) BIBREF10 . However, only a handful of works are conducted on CWS BIBREF2 , and few focuses on medical domain tasks..Given the aforementioned challenges and current researches, we propose a word segmentation method based on active learning. To model the segmentation history, we incorporate a sampling strategy consisting of word score, link score and sequence score, which effectively evaluates the segmentation decisions. Specifically, we combine information branch and gated neural network to determine if the segment is a legal word, i.e., word score. Meanwhile, we use the hidden layer output of the long short-term memory (LSTM) BIBREF11 to find out how the word is linked to its surroundings, i.e., link score. The final decision on the selection of labeling samples is made by calculating the average of word and link scores on the whole segmented sentence, i.e., sequence score. Besides, to capture coherence over characters, we additionally add K-means clustering features to the input of CRF-based word segmenter..To sum up, the main contributions of our work are summarized as follows:.The rest of this paper is organized as follows. Section SECREF2 briefly reviews the related work on CWS and active learning. Section SECREF3 presents an active learning method for CWS. We experimentally evaluate our proposed method in Section SECREF4 . Finally, Section SECREF5 concludes the paper and envisions on future work. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "25\n",
      "What are the two main parameters in the CRF-based word segmenter?\n",
      "The two main parameters in the CRF-based word segmenter are character embedding dimensions and K-means clusters.\n",
      "Question : for the text To determine suitable parameters, we divide training set into two sets, the first 80% sentences as training set and the rest 20% sentences as validation set..Character embedding dimensions and K-means clusters are two main parameters in the CRF-based word segmenter..In this paper, we choose character-based CRF without any features as baseline. First, we use Word2Vec to train character embeddings with dimensions of [`50', `100', `150', `200', `300', `400'] respectively, thus we obtain 6 different dimensional character embeddings. Second, these six types of character embeddings are used as the input to K-means algorithm with the number of clusters [`50', `100', `200', `300', `400', `500', `600'] respectively to capture the corresponding features of character embeddings. Then, we add K-means clustering features to baseline for training. As can be seen from Fig. FIGREF36 , when the character embedding dimension INLINEFORM0 = 150 and the number of clusters INLINEFORM1 = 400, CRF-based word segmenter performs best, so these two parameters are used in subsequent experiments..Hyper-parameters of neural network have a great impact on the performance. The hyper-parameters we choose are listed in Table TABREF38 ..The dimension of character embeddings is set as same as the parameter used in CRF-based word segmenter and the number of hidden units is also set to be the same as it. Maximum word length is ralated to the number of parameters in GCNN unit. Since there are many long medical terminologies in EHRs, we set the maximum word length as 6. In addition, dropout is an effective way to prevent neural networks from overfitting BIBREF35 . To avoid overfitting, we drop the input layer of the scoring model with the rate of 20%. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "26\n",
      "Question 1: Who provided the funding for this research project?\n",
      "Answer 1: The German Research Foundation (DFG) provided the funding as part of SFB 1102 'Information Density and Linguistic Encoding'.\n",
      "Question : for the text This research was funded by the German Research Foundation (DFG) as part of SFB 1102 'Information Density and Linguistic Encoding'. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "27\n",
      "Question 1: Which scenarios have the fewest labels for event and participant types in the templates?\n",
      "\n",
      "Answer 1: The scenarios of planting a tree and going on a train have the fewest labels for event and participant types in the templates.\n",
      "Question : for the text Figure 5 gives an overview of the number of event and participant types provided in the templates. Taking a flight and getting a haircut stand out with a large number of both event and participant types, which is due to the inherent complexity of the scenarios. In contrast, planting a tree and going on a train contain the fewest labels. There are 19 event and participant types on average..Figure 6 presents overview statistics about the usage of event labels, participant labels and coreference chain annotations. As can be seen, there are usually many more mentions of participants than events. For coreference chains, there are some chains that are really long (which also results in a large scenario-wise standard deviation). Usually, these chains describe the protagonist..We also found again that the flying in an airplane scenario stands out in terms of participant mentions, event mentions and average number of coreference chains..Figure 7 shows for every participant label in the baking a cake scenario the number of stories which they occurred in. This indicates how relevant a participant is for the script. As can be seen, a small number of participants are highly prominent: cook, ingredients and cake are mentioned in every story. The fact that the protagonist appears most often consistently holds for all other scenarios, where the acting person appears in every story, and is mentioned most frequently..Figure 8 shows the distribution of participant/event type labels over all appearances over all scenarios on average. The groups stand for the most frequently appearing label, the top 2 to 5 labels in terms of frequency and the top 6 to 10. ScrEv_other and ScrPart_other are shown separately. As can be seen, the most frequently used participant label (the protagonist) makes up about 40% of overall participant instances. The four labels that follow the protagonist in terms of frequency together appear in 37% of the cases. More than 2 out of 3 participants in total belong to one of only 5 labels..In contrast, the distribution for events is more balanced. 14% of all event instances have the most prominent event type. ScrEv_other and ScrPart_other both appear as labels in at most 5% of all event and participant instantiations: The specific event and participant type labels in our templates cover by far most of the instances..In Figure 9 , we grouped participants similarly into the first, the top 2-5 and top 6-10 most frequently appearing participant types. The figure shows for each of these groups the average frequency per story, and in the rightmost column the overall average. The results correspond to the findings from the last paragraph. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "28\n",
      "Question 1: What was the reason behind refining the annotation schema?\n",
      "\n",
      "Answer 1: The refinement of the schema was necessary due to the complexity of the annotation process.\n",
      "Question : for the text This section deals with the annotation of the data. We first describe the final annotation schema. Then, we describe the iterative process of corpus annotation and the refinement of the schema. This refinement was necessary due to the complexity of the annotation. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "29\n",
      "What are the three layers of annotation in the corpus? \n",
      "\n",
      "The three layers of annotation in the corpus are event types, participant types, and coreference annotation. Event types are marked on verbs, while participant types are marked on noun phrases. Coreference annotation indicates which entities refer to the same discourse referent.\n",
      "Question : for the text For each of the scenarios, we designed a specific annotation template. A script template consists of scenario-specific event and participant labels. An example of a template is shown in Table 1 . All NP heads in the corpus were annotated with a participant label; all verbs were annotated with an event label. For both participants and events, we also offered the label unclear if the annotator could not assign another label. We additionally annotated coreference chains between NPs. Thus, the process resulted in three layers of annotation: event types, participant types and coreference annotation. These are described in detail below..As a first layer, we annotated event types. There are two kinds of event type labels, scenario-specific event type labels and general labels. The general labels are used across every scenario and mark general features, for example whether an event belongs to the scenario at all. For the scenario-specific labels, we designed an unique template for every scenario, with a list of script-relevant event types that were used as labels. Such labels include for example ScrEv_close_drain in taking a bath as in Example UID10 (see Figure 1 for a complete list for the taking a bath scenario).I start by closing $_{\\textsc {\\scriptsize ScrEv\\_close\\_drain}}$ the drain at the bottom of the tub..The general labels that were used in addition to the script-specific labels in every scenario are listed below:.ScrEv_other. An event that belongs to the scenario, but its event type occurs too infrequently (for details, see below, Section \"Modification of the Schema\" ). We used the label “other\" because event classification would become too finegrained otherwise..Example: After I am dried I put my new clothes on and clean up $_{\\textsc {\\scriptsize ScrEv\\_other}}$ the bathroom..RelNScrEv. Related non-script event. An event that can plausibly happen during the execution of the script and is related to it, but that is not part of the script..Example: After finding on what I wanted to wear, I went into the bathroom and shut $_{\\textsc {\\scriptsize RelNScrEv}}$ the door..UnrelEv. An event that is unrelated to the script..Example: I sank into the bubbles and took $_{\\textsc {\\scriptsize UnrelEv}}$ a deep breath..Additionally, the annotators were asked to annotate verbs and phrases that evoke the script without explicitly referring to a script event with the label Evoking, as shown in Example UID10 . Today I took a bath $_{\\textsc {\\scriptsize Evoking}}$ in my new apartment..As in the case of the event type labels, there are two kinds of participant labels: general labels and scenario-specific labels. The latter are part of the scenario-specific templates, e.g. ScrPart_drain in the taking a bath scenario, as can be seen in Example UID15 ..I start by closing the drain $_{\\textsc {\\scriptsize ScrPart\\_drain}}$ at the bottom of the tub..The general labels that are used across all scenarios mark noun phrases with scenario-independent features. There are the following general labels:.ScrPart_other. A participant that belongs to the scenario, but its participant type occurs only infrequently..Example: I find my bath mat $_{\\textsc {\\scriptsize ScrPart\\_other}}$ and lay it on the floor to keep the floor dry..NPart. Non-participant. A referential NP that does not belong to the scenario..Example: I washed myself carefully because I did not want to spill water onto the floor $_{\\textsc {\\scriptsize NPart}}$ .labeled.SuppVComp. A support verb complement. For further discussion of this label, see Section \"Special Cases\" .Example: I sank into the bubbles and took a deep breath $_{\\textsc {\\scriptsize SuppVComp}}$ ..Head_of_Partitive. The head of a partitive or a partitive-like construction. For a further discussion of this label cf. Section \"Special Cases\" .Example: I grabbed a bar $_{\\textsc {\\scriptsize Head\\_of\\_Partitive}}$ of soap and lathered my body..No_label. A non-referential noun phrase that cannot be labeled with another label. Example: I sat for a moment $_{\\textsc {\\scriptsize No\\_label}}$ , relaxing, allowing the warm water to sooth my skin..All NPs labeled with one of the labels SuppVComp, Head_of_Partitive or No_label are considered to be non-referential. No_label is used mainly in four cases in our data: non-referential time expressions (in a while, a million times better), idioms (no matter what), the non-referential “it” (it felt amazing, it is better) and other abstracta (a lot better, a little bit)..In the first annotation phase, annotators were asked to mark verbs and noun phrases that have an event or participant type, that is not listed in the template, as MissScrEv/ MissScrPart (missing script event or participant, resp.). These annotations were used as a basis for extending the templates (see Section \"Modification of the Schema\" ) and replaced later by newly introduced labels or ScrEv_other and ScrPart_other respectively..All noun phrases were annotated with coreference information indicating which entities denote the same discourse referent. The annotation was done by linking heads of NPs (see Example UID21 , where the links are indicated by coindexing). As a rule, we assume that each element of a coreference chain is marked with the same participant type label..I $ _{\\textsc {\\scriptsize Coref1}}$ washed my $ _{\\textsc {\\scriptsize Coref1}}$ entire body $ _{\\textsc {\\scriptsize Coref2}}$ , starting with my $ _{\\textsc {\\scriptsize Coref1}}$ face $ _{\\textsc {\\scriptsize Coref3}} $ and ending with the toes $ _{\\textsc {\\scriptsize Coref4}} $ . I $ _{\\textsc {\\scriptsize Coref1}}$ always wash my $ _{\\textsc {\\scriptsize Coref1}}$ toes $_{\\textsc {\\scriptsize Coref4}}$ very thoroughly ....The assignment of an entity to a referent is not always trivial, as is shown in Example UID21 . There are some cases in which two discourse referents are grouped in a plural NP. In the example, those things refers to the group made up of shampoo, soap and sponge. In this case, we asked annotators to introduce a new coreference label, the name of which indicates which referents are grouped together (Coref_group_washing_tools). All NPs are then connected to the group phrase, resulting in an additional coreference chain..I $ _{\\textsc {\\scriptsize Coref1}}$ made sure that I $ _{\\textsc {\\scriptsize Coref1}}$ have my $ _{\\textsc {\\scriptsize Coref1}}$ shampoo $ _{\\textsc {\\scriptsize Coref2 + Coref\\_group\\_washing\\_tools}}$ , soap $_{\\textsc {\\scriptsize Coref3 + Coref\\_group\\_washing\\_tools}}$ and sponge $ _{\\textsc {\\scriptsize Coref4 + Coref\\_group\\_washing\\_tools}}$ ready to get in. Once I $ _{\\textsc {\\scriptsize Coref1}}$ have those things $ _{\\textsc {\\scriptsize Coref\\_group\\_washing\\_tools}}$ I $ _{\\textsc {\\scriptsize Coref1}}$ sink into the bath. ... I $ _{\\textsc {\\scriptsize Coref1}}$ applied some soap $ _{\\textsc {\\scriptsize Coref1}}$0 on my $ _{\\textsc {\\scriptsize Coref1}}$1 body and used the sponge $ _{\\textsc {\\scriptsize Coref1}}$2 to scrub a bit. ... I $ _{\\textsc {\\scriptsize Coref1}}$3 rinsed the shampoo $ _{\\textsc {\\scriptsize Coref1}}$4 . Example UID21 thus contains the following coreference chains: Coref1: I $ _{\\textsc {\\scriptsize Coref1}}$5 I $ _{\\textsc {\\scriptsize Coref1}}$6 my $ _{\\textsc {\\scriptsize Coref1}}$7 I $ _{\\textsc {\\scriptsize Coref1}}$8 I $ _{\\textsc {\\scriptsize Coref1}}$9 I $ _{\\textsc {\\scriptsize Coref1}}$0 my $ _{\\textsc {\\scriptsize Coref1}}$1 I.Coref2: shampoo $\\rightarrow $ shampoo.Coref3: soap $\\rightarrow $ soap.Coref4: sponge $\\rightarrow $ sponge.Coref_group_washing_ tools: shampoo $\\rightarrow $ soap $\\rightarrow $ sponge $\\rightarrow $ things generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "30\n",
      "Question 1: How were the texts collected for the study?\n",
      "\n",
      "Answer 1: The texts were collected via the Amazon Mechanical Turk platform, which provides an opportunity to present an online task to humans (a.k.a. turkers). In total 190 turkers living in the USA and being native speakers of English participated, and were paid USD $0.50 per story. The instructions asked the turkers to describe a scenario in form of a story as if explaining it to a child and to use a minimum of 150 words.\n",
      "Question : for the text We selected 10 scenarios from different available scenario lists (e.g. Regneri:2010 , VanDerMeer2009, and the OMICS corpus BIBREF1 ), including scripts of different complexity (Taking a bath vs. Flying in an airplane) and specificity (Riding a public bus vs. Repairing a flat bicycle tire). For the full scenario list see Table 2 ..Texts were collected via the Amazon Mechanical Turk platform, which provides an opportunity to present an online task to humans (a.k.a. turkers). In order to gauge the effect of different M-Turk instructions on our task, we first conducted pilot experiments with different variants of instructions explaining the task. We finalized the instructions for the full data collection, asking the turkers to describe a scenario in form of a story as if explaining it to a child and to use a minimum of 150 words. The selected instruction variant resulted in comparably simple and explicit scenario-related stories. In the future we plan to collect more complex stories using different instructions. In total 190 turkers participated. All turkers were living in the USA and native speakers of English. We paid USD $0.50 per story to each turker. On average, the turkers took 9.37 minutes per story with a maximum duration of 17.38 minutes. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "31\n",
      "Question 1: How does the InScript corpus compare to the DeScript corpus in terms of lexical variation?\n",
      "\n",
      "Answer 1: The InScript corpus exhibits much more lexical variation than the DeScript corpus. The Measure of Textual Lexical Diversity (MTLD) metric was used to measure this variance, and it was found that the InScript corpus is generally much more diverse than the DeScript corpus across all scenarios. However, the difference in the variation of lexical variance of scenarios is larger for DeScript than for InScript.\n",
      "Question : for the text As mentioned previously, the InScript corpus is part of a larger research project, in which also a corpus of a different kind, the DeScript corpus, was created. DeScript covers 40 scenarios, and also contains the 10 scenarios from InScript. This corpus contains texts that describe scripts on an abstract and generic level, while InScript contains instantiations of scripts in narrative texts. Script events in DeScript are described in a very simple, telegram-style language (see Figure 2 ). Since one of the long-term goals of the project is to align the InScript texts with the script structure given from DeScript, it is interesting to compare both resources..The InScript corpus exhibits much more lexical variation than DeScript. Many approaches use the type-token ratio to measure this variance. However, this measure is known to be sensitive to text length (see e.g. Tweedie1998), which would result in very small values for InScript and relatively large ones for DeScript, given the large average difference of text lengths between the corpora. Instead, we decided to use the Measure of Textual Lexical Diversity (MTLD) (McCarthy2010, McCarthy2005), which is familiar in corpus linguistics. This metric measures the average number of tokens in a text that are needed to retain a type-token ratio above a certain threshold. If the MTLD for a text is high, many tokens are needed to lower the type-token ratio under the threshold, so the text is lexically diverse. In contrast, a low MTLD indicates that only a few words are needed to make the type-token ratio drop, so the lexical diversity is smaller. We use the threshold of 0.71, which is proposed by the authors as a well-proven value..Figure 10 compares the lexical diversity of both resources. As can be seen, the InScript corpus with its narrative texts is generally much more diverse than the DeScript corpus with its short event descriptions, across all scenarios. For both resources, the flying in an airplane scenario is most diverse (as was also indicated above by the mean word type overlap). However, the difference in the variation of lexical variance of scenarios is larger for DeScript than for InScript. Thus, the properties of a scenario apparently influence the lexical variance of the event descriptions more than the variance of the narrative texts. We used entropy BIBREF6 over lemmas to measure the variance of lexical realizations for events. We excluded events for which there were less than 10 occurrences in DeScript or InScript. Since there is only an event annotation for 50 ESDs per scenario in DeScript, we randomly sampled 50 texts from InScript for computing the entropy to make the numbers more comparable..Figure 11 shows as an example the entropy values for the event types in the going on a train scenario. As can be seen in the graph, the entropy for InScript is in general higher than for DeScript. In the stories, a wider variety of verbs is used to describe events. There are also large differences between events: While wait has a really low entropy, spend_time_train has an extremely high entropy value. This event type covers many different activities such as reading, sleeping etc. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "32\n",
      "Question 1: What is the future research goal of the InScript corpus project?\n",
      "\n",
      "Answer 1: One of the future research goals of the InScript corpus project is to find automatic methods for text-to-script mapping, which involves aligning text segments with script states.\n",
      "Question : for the text In this paper we described the InScript corpus of 1,000 narrative texts annotated with script structure and coreference information. We described the annotation process, various difficulties encountered during annotation and different remedies that were taken to overcome these. One of the future research goals of our project is also concerned with finding automatic methods for text-to-script mapping, i.e. for the alignment of text segments with script states. We consider InScript and DeScript together as a resource for studying this alignment. The corpus shows rich lexical variation and will serve as a unique resource for the study of the role of script knowledge in natural language processing. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "33\n",
      "What is the average word count for each story in the corpus?\n",
      "\n",
      "The average word count for each story in the corpus is 217 words.\n",
      "Question : for the text Statistics for the corpus are given in Table 2 . On average, each story has a length of 12 sentences and 217 words with 98 word types on average. Stories are coherent and concentrate mainly on the corresponding scenario. Neglecting auxiliaries, modals and copulas, on average each story has 32 verbs, out of which 58% denote events related to the respective scenario. As can be seen in Table 2 , there is some variation in stories across scenarios: The flying in an airplane scenario, for example, is most complex in terms of the number of sentences, tokens and word types that are used. This is probably due to the inherent complexity of the scenario: Taking a flight, for example, is more complicated and takes more steps than taking a bath. The average count of sentences, tokens and types is also very high for the baking a cake scenario. Stories from the scenario often resemble cake recipes, which usually contain very detailed steps, so people tend to give more detailed descriptions in the stories..For both flying in an airplane and baking a cake, the standard deviation is higher in comparison to other scenarios. This indicates that different turkers described the scenario with a varying degree of detail and can also be seen as an indicator for the complexity of both scenarios. In general, different people tend to describe situations subjectively, with a varying degree of detail. In contrast, texts from the taking a bath and planting a tree scenarios contain a relatively smaller number of sentences and fewer word types and tokens. Both planting a tree and taking a bath are simpler activities, which results in generally less complex texts..The average pairwise word type overlap can be seen as a measure of lexical variety among stories: If it is high, the stories resemble each other more. We can see that stories in the flying in an airplane and baking a cake scenarios have the highest values here, indicating that most turkers used a similar vocabulary in their stories..In general, the response quality was good. We had to discard 9% of the stories as these lacked the quality we were expecting. In total, we selected 910 stories for annotation. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "34\n",
      "What is the purpose of the templates in this study?\n",
      "\n",
      "The purpose of the templates in this study is to describe cognitively prominent events in an activity, rather than exhaustively documenting all logically necessary steps.\n",
      "Question : for the text The templates were carefully designed in an iterated process. For each scenario, one of the authors of this paper provided a preliminary version of the template based on the inspection of some of the stories. For a subset of the scenarios, preliminary templates developed at our department for a psycholinguistic experiment on script knowledge were used as a starting point. Subsequently, the authors manually annotated 5 randomly selected texts for each of the scenarios based on the preliminary template. Necessary extensions and changes in the templates were discussed and agreed upon. Most of the cases of disagreement were related to the granularity of the event and participant types. We agreed on the script-specific functional equivalence as a guiding principle. For example, reading a book, listening to music and having a conversation are subsumed under the same event label in the flight scenario, because they have the common function of in-flight entertainment in the scenario. In contrast, we assumed different labels for the cake tin and other utensils (bowls etc.), since they have different functions in the baking a cake scenario and accordingly occur with different script events..Note that scripts and templates as such are not meant to describe an activity as exhaustively as possible and to mention all steps that are logically necessary. Instead, scripts describe cognitively prominent events in an activity. An example can be found in the flight scenario. While more than a third of the turkers mentioned the event of fastening the seat belts in the plane (buckle_seat_belt), no person wrote about undoing their seat belts again, although in reality both events appear equally often. Consequently, we added an event type label for buckling up, but no label for undoing the seat belts. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "35\n",
      "Question 1: What was the inter-annotator agreement for the small sample of texts that were annotated by all four annotators?\n",
      "\n",
      "Answer 1: The inter-annotator agreement for the small sample of texts was measured and found to be sufficiently high (see Section \"Inter-Annotator Agreement\").\n",
      "Question : for the text We used the WebAnno annotation tool BIBREF2 for our project. The stories from each scenario were distributed among four different annotators. In a calibration phase, annotators were presented with some sample texts for test annotations; the results were discussed with the authors. Throughout the whole annotation phase, annotators could discuss any emerging issues with the authors. All annotations were done by undergraduate students of computational linguistics. The annotation was rather time-consuming due to the complexity of the task, and thus we decided for single annotation mode. To assess annotation quality, a small sample of texts was annotated by all four annotators and their inter-annotator agreement was measured (see Section \"Inter-Annotator Agreement\" ). It was found to be sufficiently high..Annotation of the corpus together with some pre- and post-processing of the data required about 500 hours of work. All stories were annotated with event and participant types (a total of 12,188 and 43,946 instances, respectively). On average there were 7 coreference chains per story with an average length of 6 tokens. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "36\n",
      "Question 1: How did the annotators calculate inter-annotator agreement?\n",
      "\n",
      "Answer 1: The annotators randomly chose 30 stories from 6 scenarios for parallel annotation by all 4 annotators after the first annotation phase. They checked the agreement on these data using Fleiss' Kappa.\n",
      "Question : for the text In order to calculate inter-annotator agreement, a total of 30 stories from 6 scenarios were randomly chosen for parallel annotation by all 4 annotators after the first annotation phase. We checked the agreement on these data using Fleiss' Kappa BIBREF4 . The results are shown in Figure 4 and indicate moderate to substantial agreement BIBREF5 . Interestingly, if we calculated the Kappa only on the subset of cases that were annotated with script-specific event and participant labels by all annotators, results were better than those of the evaluation on all labeled instances (including also unrelated and related non-script events). This indicates one of the challenges of the annotation task: In many cases it is difficult to decide whether a particular event should be considered a central script event, or an event loosely related or unrelated to the script..For coreference chain annotation, we calculated the percentage of pairs which were annotated by at least 3 annotators (qualified majority vote) compared to the set of those pairs annotated by at least one person (see Figure 4 ). We take the result of 90.5% between annotators to be a good agreement. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "37\n",
      "Question 1: How were new labels added to the templates for event and participant types?\n",
      "\n",
      "Answer 1: New labels for participant types were added if they were expected to appear at least 10 times in total in at least 5 different stories, while new labels for event types were chosen if they would appear in at least 5 different stories. The templates were also extended with additional labels for events found in the DeScript corpus, if they were not already part of the template. Instances that did not meet the frequency requirements were relabeled as ScrEv_other and ScrPart_other.\n",
      "Question : for the text After the first annotation round, we extended and changed the templates based on the results. As mentioned before, we used MissScrEv and MissScrPart labels to mark verbs and noun phrases instantiating events and participants for which no appropriate labels were available in the templates. Based on the instances with these labels (a total of 941 and 1717 instances, respectively), we extended the guidelines to cover the sufficiently frequent cases. In order to include new labels for event and participant types, we tried to estimate the number of instances that would fall under a certain label. We added new labels according to the following conditions:.For the participant annotations, we added new labels for types that we expected to appear at least 10 times in total in at least 5 different stories (i.e. in approximately 5% of the stories)..For the event annotations, we chose those new labels for event types that would appear in at least 5 different stories..In order to avoid too fine a granularity of the templates, all other instances of MissScrEv and MissScrPart were re-labeled with ScrEv_other and ScrPart_other. We also relabeled participants and events from the first annotation phase with ScrEv_other and ScrPart_other, if they did not meet the frequency requirements. The event label air_bathroom (the event of letting fresh air into the room after the bath), for example, was only used once in the stories, so we relabeled that instance to ScrEv_other..Additionally, we looked at the DeScript corpus BIBREF3 , which contains manually clustered event paraphrase sets for the 10 scenarios that are also covered by InScript (see Section \"Comparison to the DeScript Corpus\" ). Every such set contains event descriptions that describe a certain event type. We extended our templates with additional labels for these events, if they were not yet part of the template. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "38\n",
      "Question 1: What is the InScript corpus and what is it used for?\n",
      "\n",
      "Answer 1: The InScript corpus is a collection of simple narrative texts centered around specific scenarios that have been annotated with event types and participant types. It is used to study the influence of script knowledge in language processing and to model the surprisal and information density in written text.\n",
      "Question : for the text A script is “a standardized sequence of events that describes some stereotypical human activity such as going to a restaurant or visiting a doctor” BIBREF0 . Script events describe an action/activity along with the involved participants. For example, in the script describing a visit to a restaurant, typical events are entering the restaurant, ordering food or eating. Participants in this scenario can include animate objects like the waiter and the customer, as well as inanimate objects such as cutlery or food..Script knowledge has been shown to play an important role in text understanding (cullingford1978script, miikkulainen1995script, mueller2004understanding, Chambers2008, Chambers2009, modi2014inducing, rudinger2015learning). It guides the expectation of the reader, supports coreference resolution as well as common-sense knowledge inference and enables the appropriate embedding of the current sentence into the larger context. Figure 1 shows the first few sentences of a story describing the scenario taking a bath. Once the taking a bath scenario is evoked by the noun phrase (NP) “a bath”, the reader can effortlessly interpret the definite NP “the faucet” as an implicitly present standard participant of the taking a bath script. Although in this story, “entering the bath room”, “turning on the water” and “filling the tub” are explicitly mentioned, a reader could nevertheless have inferred the “turning on the water” event, even if it was not explicitly mentioned in the text. Table 1 gives an example of typical events and participants for the script describing the scenario taking a bath..A systematic study of the influence of script knowledge in texts is far from trivial. Typically, text documents (e.g. narrative texts) describing various scenarios evoke many different scripts, making it difficult to study the effect of a single script. Efforts have been made to collect scenario-specific script knowledge via crowdsourcing, for example the OMICS and SMILE corpora (singh2002open, Regneri:2010, Regneri2013), but these corpora describe script events in a pointwise telegram style rather than in full texts..This paper presents the InScript corpus (Narrative Texts Instantiating Script structure). It is a corpus of simple narrative texts in the form of stories, wherein each story is centered around a specific scenario. The stories have been collected via Amazon Mechanical Turk (M-Turk). In this experiment, turkers were asked to write down a concrete experience about a bus ride, a grocery shopping event etc. We concentrated on 10 scenarios and collected 100 stories per scenario, giving a total of 1,000 stories with about 200,000 words. Relevant verbs and noun phrases in all stories are annotated with event types and participant types respectively. Additionally, the texts have been annotated with coreference information in order to facilitate the study of the interdependence between script structure and coreference..The InScript corpus is a unique resource that provides a basis for studying various aspects of the role of script knowledge in language processing by humans. The acquisition of this corpus is part of a larger research effort that aims at using script knowledge to model the surprisal and information density in written text. Besides InScript, this project also released a corpus of generic descriptions of script activities called DeScript (for Describing Script Structure, Wanzare2016). DeScript contains a range of short and textually simple phrases that describe script events in the style of OMICS or SMILE (singh2002open, Regneri:2010). These generic telegram-style descriptions are called Event Descriptions (EDs); a sequence of such descriptions that cover a complete script is called an Event Sequence Description (ESD). Figure 2 shows an excerpt of a script in the baking a cake scenario. The figure shows event descriptions for 3 different events in the DeScript corpus (left) and fragments of a story in the InScript corpus (right) that instantiate the same event type. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "39\n",
      "Question 1: Why were noun-noun compounds annotated twice with the same label in the study?\n",
      "\n",
      "Answer 1: Noun-noun compounds were annotated twice with the same label (whole span plus the head noun) in the study because of potential processing requirements.\n",
      "Question : for the text Noun-noun compounds were annotated twice with the same label (whole span plus the head noun), as indicated by Example UID31 . This redundant double annotation is motivated by potential processing requirements..I get my (wash (cloth $ _{\\textsc {\\scriptsize ScrPart\\_washing\\_tools}} ))$ , $_{\\textsc {\\scriptsize ScrPart\\_washing\\_tools}} $ and put it under the water..A special treatment was given to support verb constructions such as take time, get home or take a seat in Example UID32 . The semantics of the verb itself is highly underspecified in such constructions; the event type is largely dependent on the object NP. As shown in Example UID32 , we annotate the head verb with the event type described by the whole construction and label its object with SuppVComp (support verb complement), indicating that it does not have a proper reference..I step into the tub and take $ _{\\textsc {\\scriptsize ScrEv\\_sink\\_water}} $ a seat $ _{\\textsc {\\scriptsize SuppVComp}} $ ..We used the Head_of_Partitive label for the heads in partitive constructions, assuming that the only referential part of the construction is the complement. This is not completely correct, since different partitive heads vary in their degree of concreteness (cf. Examples UID33 and UID33 ), but we did not see a way to make the distinction sufficiently transparent to the annotators. Our seats were at the back $ _{\\textsc {\\scriptsize Head\\_of\\_Partitive}} $ of the train $ _{\\textsc {\\scriptsize ScrPart\\_train}} $ . In the library you can always find a couple $ _{\\textsc {\\scriptsize Head\\_of\\_Partitive}} $ of interesting books $ _{\\textsc {\\scriptsize ScrPart\\_book}} $ ..Group denoting NPs sometimes refer to groups whose members are instances of different participant types. In Example UID34 , the first-person plural pronoun refers to the group consisting of the passenger (I) and a non-participant (my friend). To avoid a proliferation of event type labels, we labeled these cases with Unclear..I $ _{\\textsc {\\scriptsize {ScrPart\\_passenger}}}$ wanted to visit my $_{\\textsc {\\scriptsize {ScrPart\\_passenger}}}$ friend $ _{\\textsc {\\scriptsize {NPart}}}$ in New York. ... We $_{\\textsc {\\scriptsize Unclear}}$ met at the train station..We made an exception for the Getting a Haircut scenario, where the mixed participant group consisting of the hairdresser and the customer occurs very often, as in Example UID34 . Here, we introduced the additional ad-hoc participant label Scr_Part_hairdresser_customer..While Susan $_{\\textsc {\\scriptsize {ScrPart\\_hairdresser}}}$ is cutting my $_{\\textsc {\\scriptsize {ScrPart\\_customer}}}$ hair we $_{\\textsc {\\scriptsize Scr\\_Part\\_hairdresser\\_customer}}$ usually talk a bit. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "40\n",
      "Question 1: Who supported the work discussed in the text? \n",
      "Answer 1: The work discussed in the text is supported in part by Allen Institute for Artificial Intelligence (AI2) and in part by NSF awards #IIS-1817183 and #IIS-1756023.\n",
      "Question : for the text We would like to thank Matt Gardner, Marco Tulio Ribeiro, Zhengli Zhao, Robert L. Logan IV, Dheeru Dua and the anonymous reviewers for their detailed feedback and suggestions. This work is supported in part by Allen Institute for Artificial Intelligence (AI2) and in part by NSF awards #IIS-1817183 and #IIS-1756023. The views expressed are those of the authors and do not reflect the official policy or position of the funding agencies. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "41\n",
      "Question 1: What does investigating the robustness of models entail?\n",
      "\n",
      "Answer 1: Investigating the robustness of models entails examining how sensitive predictions are to small additions to the knowledge graph. Specifically, the aim is to identify a single fake fact that, when added to the knowledge graph, changes the prediction score the most.\n",
      "Question : for the text We are also interested in investigating the robustness of models, i.e., how sensitive are the predictions to small additions to the knowledge graph. Specifically, for a target prediction ${s,r,o}$ , we are interested in identifying a single fake fact ${s^{\\prime },r^{\\prime },o}$ that, when added to the knowledge graph $G$ , changes the prediction score $\\psi (s,r,o)$ the most. Using $\\overline{\\psi }(s,r,o)$ as the score after training on $G\\cup \\lbrace {s^{\\prime },r^{\\prime },o}\\rbrace $ , we define the adversary as: *argmax(s', r') (s',r')(s,r,o) where $\\Delta _{(s^{\\prime },r^{\\prime })}(s,r,o)=\\psi (s, r, o)-\\overline{\\psi }(s,r,o)$ . The search here is over any possible $s^{\\prime }\\in \\xi $ , which is often in the millions for most real-world KGs, and $r^{\\prime }\\in $ . We also identify adversaries that increase the prediction score for specific false triple, i.e., for a target fake fact ${s,r,o}$ , the adversary is ${s^{\\prime },r^{\\prime },o}$0 , where ${s^{\\prime },r^{\\prime },o}$1 is defined as before. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "42\n",
      "What form do the studied attacks take?\n",
      "The studied attacks take the form of $\\langle s, r^{\\prime }, o \\rangle $ and $\\langle s, r^{\\prime }, o^{\\prime } \\rangle$. \n",
      "\n",
      "Note: The answer is directly taken from the given text.\n",
      "Question : for the text We approximate the change on the score of the target triple upon applying attacks other than the $\\langle s^{\\prime }, r^{\\prime }, o \\rangle $ ones. Since each relation appears many times in the training triples, we can assume that applying a single attack will not considerably affect the relations embeddings. As a result, we just need to study the attacks in the form of $\\langle s, r^{\\prime }, o \\rangle $ and $\\langle s, r^{\\prime }, o^{\\prime } \\rangle $ . Defining the scoring function as $\\psi (s,r,o) = , ) \\cdot = _{s,r} \\cdot $ , we further assume that $\\psi (s,r,o) =\\cdot (, ) =\\cdot _{r,o}$ . generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "43\n",
      "Question 1: What is a scoring function in knowledge graph completion using dense vectors?\n",
      "\n",
      "Answer 1: A scoring function is learned to evaluate whether any given fact is true in knowledge graph completion using dense vectors. It takes the form of $\\psi(s,r,o)=,)\\cdot$, where $,,$ are embeddings of the subject, relation, and object, respectively.\n",
      "Question : for the text In this section, we briefly introduce some notations, and existing relational embedding approaches that model knowledge graph completion using dense vectors. In KGs, facts are represented using triples of subject, relation, and object, $\\langle s, r, o\\rangle $ , where $s,o\\in \\xi $ , the set of entities, and $r\\in $ , the set of relations. To model the KG, a scoring function $\\psi :\\xi \\times \\times \\xi \\rightarrow $ is learned to evaluate whether any given fact is true. In this work, we focus on multiplicative models of link prediction, specifically DistMult BIBREF2 because of its simplicity and popularity, and ConvE BIBREF10 because of its high accuracy. We can represent the scoring function of such methods as $\\psi (s,r,o) = , ) \\cdot $ , where $,,\\in ^d$ are embeddings of the subject, relation, and object respectively. In DistMult, $, ) = \\odot $ , where $\\odot $ is element-wise multiplication operator. Similarly, in ConvE, $, )$ is computed by a convolution on the concatenation of $$ and $s,o\\in \\xi $0 ..We use the same setup as BIBREF10 for training, i.e., incorporate binary cross-entropy loss over the triple scores. In particular, for subject-relation pairs $(s,r)$ in the training data $G$ , we use binary $y^{s,r}_o$ to represent negative and positive facts. Using the model's probability of truth as $\\sigma (\\psi (s,r,o))$ for $\\langle s,r,o\\rangle $ , the loss is defined as: (G) = (s,r)o ys,ro(((s,r,o))).+ (1-ys,ro)(1 - ((s,r,o))). Gradient descent is used to learn the embeddings $,,$ , and the parameters of $, if any.\n",
      "$  generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "44\n",
      "Question 1: What is one of the challenges when conducting adversarial attack on KGs?\n",
      "\n",
      "Answer 1: One of the challenges is that evaluating the effect of changing the KG on the score of the target fact is expensive and time-consuming, as we need to update the embeddings by retraining the model on the new graph.\n",
      "Question : for the text There are a number of crucial challenges when conducting such adversarial attack on KGs. First, evaluating the effect of changing the KG on the score of the target fact ( $\\overline{\\psi }(s,r,o)$ ) is expensive since we need to update the embeddings by retraining the model on the new graph; a very time-consuming process that is at least linear in the size of $G$ . Second, since there are many candidate facts that can be added to the knowledge graph, identifying the most promising adversary through search-based methods is also expensive. Specifically, the search size for unobserved facts is $|\\xi | \\times ||$ , which, for example in YAGO3-10 KG, can be as many as $4.5 M$ possible facts for a single target prediction. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "45\n",
      "What kind of modifications are allowed for adversarial modifications on KGs?\n",
      "The possible modifications that are allowed for adversarial modifications on KGs are constrained to be in the form of $\\langle s^{\\prime }, r^{\\prime }, o\\rangle $, where the subject and relation may be different from the target, but the object remains the same. Other forms of modifications such as $\\langle s, r^{\\prime }, o^{\\prime }\\rangle $ and $\\langle s, r^{\\prime }, o\\rangle $ are analyzed in appendices and left for empirical evaluation in future work.\n",
      "Question : for the text For adversarial modifications on KGs, we first define the space of possible modifications. For a target triple $\\langle s, r, o\\rangle $ , we constrain the possible triples that we can remove (or inject) to be in the form of $\\langle s^{\\prime }, r^{\\prime }, o\\rangle $ i.e $s^{\\prime }$ and $r^{\\prime }$ may be different from the target, but the object is not. We analyze other forms of modifications such as $\\langle s, r^{\\prime }, o^{\\prime }\\rangle $ and $\\langle s, r^{\\prime }, o\\rangle $ in appendices \"Modifications of the Form 〈s,r ' ,o ' 〉\\langle s, r^{\\prime }, o^{\\prime } \\rangle \" and \"Modifications of the Form 〈s,r ' ,o〉\\langle s, r^{\\prime }, o \\rangle \" , and leave empirical evaluation of these modifications for future work. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "46\n",
      "Question 1: What is the goal of the presented approach for conducting adversarial modifications to knowledge graphs?\n",
      "\n",
      "Answer 1: The goal of the approach is to analyze the robustness and interpretability of link prediction models through identifying the fact to add or remove from the KG that changes the prediction for a target fact. This is achieved by using an estimate of the score change for any target triple after adding or removing another fact, and a gradient-based algorithm for identifying the most influential modification.\n",
      "Question : for the text Motivated by the need to analyze the robustness and interpretability of link prediction models, we present a novel approach for conducting adversarial modifications to knowledge graphs. We introduce , completion robustness and interpretability via adversarial graph edits: identifying the fact to add into or remove from the KG that changes the prediction for a target fact. uses (1) an estimate of the score change for any target triple after adding or removing another fact, and (2) a gradient-based algorithm for identifying the most influential modification. We show that can effectively reduce ranking metrics on link prediction models upon applying the attack triples. Further, we incorporate the to study the interpretability of KG representations by summarizing the most influential facts for each relation. Finally, using , we introduce a novel automated error detection method for knowledge graphs. We have release the open-source implementation of our models at: https://pouyapez.github.io/criage. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "47\n",
      "What approach is feasible when removing an observed triple?\n",
      "The brute force enumeration approach is feasible when removing an observed triple.\n",
      "Question : for the text Using the approximations provided in the previous section, Eq. () and (), we can use brute force enumeration to find the adversary $\\langle s^{\\prime }, r^{\\prime }, o \\rangle $ . This approach is feasible when removing an observed triple since the search space of such modifications is usually small; it is the number of observed facts that share the object with the target. On the other hand, finding the most influential unobserved fact to add requires search over a much larger space of all possible unobserved facts (that share the object). Instead, we identify the most influential unobserved fact $\\langle s^{\\prime }, r^{\\prime }, o \\rangle $ by using a gradient-based algorithm on vector $_{s^{\\prime },r^{\\prime }}$ in the embedding space (reminder, $_{s^{\\prime },r^{\\prime }}=^{\\prime },^{\\prime })$ ), solving the following continuous optimization problem in $^d$ : *argmaxs', r' (s',r')(s,r,o). After identifying the optimal $_{s^{\\prime }, r^{\\prime }}$ , we still need to generate the pair $(s^{\\prime },r^{\\prime })$ . We design a network, shown in Figure 2 , that maps the vector $_{s^{\\prime },r^{\\prime }}$ to the entity-relation space, i.e., translating it into $(s^{\\prime },r^{\\prime })$ . In particular, we train an auto-encoder where the encoder is fixed to receive the $s$ and $\\langle s^{\\prime }, r^{\\prime }, o \\rangle $0 as one-hot inputs, and calculates $\\langle s^{\\prime }, r^{\\prime }, o \\rangle $1 in the same way as the DistMult and ConvE encoders respectively (using trained embeddings). The decoder is trained to take $\\langle s^{\\prime }, r^{\\prime }, o \\rangle $2 as input and produce $\\langle s^{\\prime }, r^{\\prime }, o \\rangle $3 and $\\langle s^{\\prime }, r^{\\prime }, o \\rangle $4 , essentially inverting $\\langle s^{\\prime }, r^{\\prime }, o \\rangle $5 s, r $\\langle s^{\\prime }, r^{\\prime }, o \\rangle $6 s $\\langle s^{\\prime }, r^{\\prime }, o \\rangle $7 r $\\langle s^{\\prime }, r^{\\prime }, o \\rangle $8 s, r $\\langle s^{\\prime }, r^{\\prime }, o \\rangle $9 We evaluate the performance of our inverter networks (one for each model/dataset) on correctly recovering the pairs of subject and relation from the test set of our benchmarks, given the $_{s^{\\prime },r^{\\prime }}$0 . The accuracy of recovered pairs (and of each argument) is given in Table 1 . As shown, our networks achieve a very high accuracy, demonstrating their ability to invert vectors $_{s^{\\prime },r^{\\prime }}$1 to $_{s^{\\prime },r^{\\prime }}$2 pairs. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "48\n",
      "What are the two algorithms proposed to address the challenges mentioned?\n",
      "The two proposed algorithms are (1) approximating the effect of changing the graph on a target prediction, and (2) using continuous optimization for the discrete search over potential modifications.\n",
      "Question : for the text In this section, we propose algorithms to address mentioned challenges by (1) approximating the effect of changing the graph on a target prediction, and (2) using continuous optimization for the discrete search over potential modifications. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "49\n",
      "Question 1: What is the purpose of studying the influence function in evaluating the effectiveness of adversarial attacks?\n",
      "Answer 1: The purpose of studying the influence function is to compare estimates with the actual effect of the attacks, which helps evaluate the effectiveness of adversarial attacks.\n",
      "Question : for the text We evaluate by ( \"Influence Function vs \" ) comparing estimate with the actual effect of the attacks, ( \"Robustness of Link Prediction Models\" ) studying the effect of adversarial attacks on evaluation metrics, ( \"Interpretability of Models\" ) exploring its application to the interpretability of KG representations, and ( \"Finding Errors in Knowledge Graphs\" ) detecting incorrect triples. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "50\n",
      "What is the purpose of using adversarial modifications in finding erroneous triples in the knowledge graph?\n",
      "The purpose of using adversarial modifications is to find the inconsistent triple with its neighborhood and put less trust on it to identify the erroneous triples in the knowledge graph.\n",
      "Question : for the text Here, we demonstrate another potential use of adversarial modifications: finding erroneous triples in the knowledge graph. Intuitively, if there is an error in the graph, the triple is likely to be inconsistent with its neighborhood, and thus the model should put least trust on this triple. In other words, the error triple should have the least influence on the model's prediction of the training data. Formally, to find the incorrect triple $\\langle s^{\\prime }, r^{\\prime }, o\\rangle $ in the neighborhood of the train triple $\\langle s, r, o\\rangle $ , we need to find the triple $\\langle s^{\\prime },r^{\\prime },o\\rangle $ that results in the least change $\\Delta _{(s^{\\prime },r^{\\prime })}(s,r,o)$ when removed from the graph..To evaluate this application, we inject random triples into the graph, and measure the ability of to detect the errors using our optimization. We consider two types of incorrect triples: 1) incorrect triples in the form of $\\langle s^{\\prime }, r, o\\rangle $ where $s^{\\prime }$ is chosen randomly from all of the entities, and 2) incorrect triples in the form of $\\langle s^{\\prime }, r^{\\prime }, o\\rangle $ where $s^{\\prime }$ and $r^{\\prime }$ are chosen randomly. We choose 100 random triples from the observed graph, and for each of them, add an incorrect triple (in each of the two scenarios) to its neighborhood. Then, after retraining DistMult on this noisy training data, we identify error triples through a search over the neighbors of the 100 facts. The result of choosing the neighbor with the least influence on the target is provided in the Table 7 . When compared with baselines that randomly choose one of the neighbors, or assume that the fact with the lowest score is incorrect, we see that outperforms both of these with a considerable gap, obtaining an accuracy of $42\\%$ and $55\\%$ in detecting errors. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "1\n",
      "What is the purpose of studying the effect of a change on the vector representations of a target triple in relation to adversarial modifications?\n",
      "\n",
      "The purpose is to capture the effect of an adversarial modification on the score of a target triple.\n",
      "Question : for the text We first study the addition of a fact to the graph, and then extend it to cover removal as well. To capture the effect of an adversarial modification on the score of a target triple, we need to study the effect of the change on the vector representations of the target triple. We use $$ , $$ , and $$ to denote the embeddings of $s,r,o$ at the solution of $\\operatornamewithlimits{argmin} (G)$ , and when considering the adversarial triple $\\langle s^{\\prime }, r^{\\prime }, o \\rangle $ , we use $$ , $$ , and $$ for the new embeddings of $s,r,o$ , respectively. Thus $$0 is a solution to $$1 , which can also be written as $$2 . Similarly, $$3 s', r', o $$4 $$5 $$6 $$7 o $$8 $$9 $$0 $$1 $$2 $$3 O(n3) $$4 $$5 $$6 (s,r,o)-(s, r, o) $$7 - $$8 s, r = ,) $$9 - $s,r,o$0 (G)= (G)+(s', r', o ) $s,r,o$1 $s,r,o$2 s', r' = ',') $s,r,o$3 = ((s',r',o)) $s,r,o$4 eo (G)=0 $s,r,o$5 eo (G) $s,r,o$6 Ho $s,r,o$7 dd $s,r,o$8 o $s,r,o$9 $\\operatornamewithlimits{argmin} (G)$0 - $\\operatornamewithlimits{argmin} (G)$1 -= $\\operatornamewithlimits{argmin} (G)$2 Ho $\\operatornamewithlimits{argmin} (G)$3 Ho + (1-) s',r's',r' $\\operatornamewithlimits{argmin} (G)$4 Ho $\\operatornamewithlimits{argmin} (G)$5 dd $\\operatornamewithlimits{argmin} (G)$6 d $\\operatornamewithlimits{argmin} (G)$7 s,r,s',r'd $\\operatornamewithlimits{argmin} (G)$8 s, r, o $\\operatornamewithlimits{argmin} (G)$9 s', r', o $\\langle s^{\\prime }, r^{\\prime }, o \\rangle $0 $\\langle s^{\\prime }, r^{\\prime }, o \\rangle $1 $\\langle s^{\\prime }, r^{\\prime }, o \\rangle $2  generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "2\n",
      "What is the efficient computation for calculating the effect of an adversarial modification on TransE BIBREF18?\n",
      "The efficient computation involves calculating the derivative of the loss over the modified entity or relation, solving for the modification, and then computing the score change. This is made efficient by using the Hessian matrix and first-order Taylor approximation.\n",
      "Question : for the text In here we derive the approximation of the change in the score upon applying an adversarial modification for TransE BIBREF18 . Using similar assumptions and parameters as before, to calculate the effect of the attack, $\\overline{\\psi }{(s,r,o)}$ (where $\\psi {(s,r,o)}=|+-|$ ), we need to compute $$ . To do so, we need to derive an efficient computation for $$ . First, the derivative of the loss $(\\overline{G})= (G)+(\\langle s^{\\prime }, r^{\\prime }, o \\rangle )$ over $$ is: eo (G) = eo (G) + (1-) s', r'-(s',r',o) where $_{s^{\\prime }, r^{\\prime }} = ^{\\prime }+ ^{\\prime }$ , and $\\varphi = \\sigma (\\psi (s^{\\prime },r^{\\prime },o))$ . At convergence, after retraining, we expect $\\nabla _{e_o} (\\overline{G})=0$ . We perform first order Taylor approximation of $\\nabla _{e_o} (\\overline{G})$ to get: 0. (1-) (s', r'-)(s',r',o)+(Ho - Hs',r',o)(-). Hs',r',o = (1-)(s', r'-)(s', r'-)(s',r',o)2+. 1-(s',r',o)-(1-) (s', r'-)(s', r'-)(s',r',o)3 where $H_o$ is the $d\\times d$ Hessian matrix for $o$ , i.e., second order derivative of the loss w.r.t. $$ , computed sparsely. Solving for $$ gives us: = -(1-) (Ho - Hs',r',o)-1 (s', r'-)(s',r',o). + Then, we compute the score change as: (s,r,o)= |+-|.= |++(1-) (Ho - Hs',r',o)-1. (s', r'-)(s',r',o) - |.Calculating this expression is efficient since $H_o$ is a $d\\times d$ matrix. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "3\n",
      "What are the leave one out experiments used for in comparing the quality of approximations?\n",
      "The leave one out experiments are used to evaluate the quality of approximations and compare them with influence function.\n",
      "Question : for the text To evaluate the quality of our approximations and compare with influence function (IF), we conduct leave one out experiments. In this setup, we take all the neighbors of a random target triple as candidate modifications, remove them one at a time, retrain the model each time, and compute the exact change in the score of the target triple. We can use the magnitude of this change in score to rank the candidate triples, and compare this exact ranking with ranking as predicted by: , influence function with and without Hessian matrix, and the original model score (with the intuition that facts that the model is most confident of will have the largest impact when removed). Similarly, we evaluate by considering 200 random triples that share the object entity with the target sample as candidates, and rank them as above. The average results of Spearman's $\\rho $ and Kendall's $\\tau $ rank correlation coefficients over 10 random target samples is provided in Table 3 . performs comparably to the influence function, confirming that our approximation is accurate. Influence function is slightly more accurate because they use the complete Hessian matrix over all the parameters, while we only approximate the change by calculating the Hessian over $$ . The effect of this difference on scalability is dramatic, constraining IF to very small graphs and small embedding dimensionality ( $d\\le 10$ ) before we run out of memory. In Figure 3 , we show the time to compute a single adversary by IF compared to , as we steadily grow the number of entities (randomly chosen subgraphs), averaged over 10 random triples. As it shows, is mostly unaffected by the number of entities while IF increases quadratically. Considering that real-world KGs have tens of thousands of times more entities, making IF unfeasible for them. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "4\n",
      "What is the main technique used to provide explanations for link predictions in the study?\n",
      "\n",
      "The main technique used to provide explanations for link predictions in the study is identifying the most influential fact using subgraphs and extracting length-2 horn rules.\n",
      "Question : for the text To be able to understand and interpret why a link is predicted using the opaque, dense embeddings, we need to find out which part of the graph was most influential on the prediction. To provide such explanations for each predictions, we identify the most influential fact using . Instead of focusing on individual predictions, we aggregate the explanations over the whole dataset for each relation using a simple rule extraction technique: we find simple patterns on subgraphs that surround the target triple and the removed fact from , and appear more than $90\\%$ of the time. We only focus on extracting length-2 horn rules, i.e., $R_1(a,c)\\wedge R_2(c,b)\\Rightarrow R(a,b)$ , where $R(a,b)$ is the target and $R_2(c,b)$ is the removed fact. Table 6 shows extracted YAGO3-10 rules that are common to both models, and ones that are not. The rules show several interesting inferences, such that hasChild is often inferred via married parents, and isLocatedIn via transitivity. There are several differences in how the models reason as well; DistMult often uses the hasCapital as an intermediate step for isLocatedIn, while ConvE incorrectly uses isNeighbor. We also compare against rules extracted by BIBREF2 for YAGO3-10 that utilizes the structure of DistMult: they require domain knowledge on types and cannot be applied to ConvE. Interestingly, the extracted rules contain all the rules provided by , demonstrating that can be used to accurately interpret models, including ones that are not interpretable, such as ConvE. These are preliminary steps toward interpretability of link prediction models, and we leave more analysis of interpretability to future work. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "5\n",
      "What is the goal of the proposed approach called Completeness Robustness and Interpretability via Adversarial Graph Edits (CRITAGE)?\n",
      "The goal of the proposed approach called Completeness Robustness and Interpretability via Adversarial Graph Edits (CRITAGE) is to design approaches that minimally change the graph structure such that the prediction of a target fact changes the most after the embeddings are relearned, which includes identifying the most influential related fact, evaluating the robustness and sensitivity of link prediction models to small additions to the graph, and detecting errors in the KG.\n",
      "Question : for the text Knowledge graphs (KG) play a critical role in many real-world applications such as search, structured data management, recommendations, and question answering. Since KGs often suffer from incompleteness and noise in their facts (links), a number of recent techniques have proposed models that embed each entity and relation into a vector space, and use these embeddings to predict facts. These dense representation models for link prediction include tensor factorization BIBREF0 , BIBREF1 , BIBREF2 , algebraic operations BIBREF3 , BIBREF4 , BIBREF5 , multiple embeddings BIBREF6 , BIBREF7 , BIBREF8 , BIBREF9 , and complex neural models BIBREF10 , BIBREF11 . However, there are only a few studies BIBREF12 , BIBREF13 that investigate the quality of the different KG models. There is a need to go beyond just the accuracy on link prediction, and instead focus on whether these representations are robust and stable, and what facts they make use of for their predictions. In this paper, our goal is to design approaches that minimally change the graph structure such that the prediction of a target fact changes the most after the embeddings are relearned, which we collectively call Completion Robustness and Interpretability via Adversarial Graph Edits (). First, we consider perturbations that red!50!blackremove a neighboring link for the target fact, thus identifying the most influential related fact, providing an explanation for the model's prediction. As an example, consider the excerpt from a KG in Figure 1 with two observed facts, and a target predicted fact that Princes Henriette is the parent of Violante Bavaria. Our proposed graph perturbation, shown in Figure 1 , identifies the existing fact that Ferdinal Maria is the father of Violante Bavaria as the one when removed and model retrained, will change the prediction of Princes Henriette's child. We also study attacks that green!50!blackadd a new, fake fact into the KG to evaluate the robustness and sensitivity of link prediction models to small additions to the graph. An example attack for the original graph in Figure 1 , is depicted in Figure 1 . Such perturbations to the the training data are from a family of adversarial modifications that have been applied to other machine learning tasks, known as poisoning BIBREF14 , BIBREF15 , BIBREF16 , BIBREF17 ..Since the setting is quite different from traditional adversarial attacks, search for link prediction adversaries brings up unique challenges. To find these minimal changes for a target link, we need to identify the fact that, when added into or removed from the graph, will have the biggest impact on the predicted score of the target fact. Unfortunately, computing this change in the score is expensive since it involves retraining the model to recompute the embeddings. We propose an efficient estimate of this score change by approximating the change in the embeddings using Taylor expansion. The other challenge in identifying adversarial modifications for link prediction, especially when considering addition of fake facts, is the combinatorial search space over possible facts, which is intractable to enumerate. We introduce an inverter of the original embedding model, to decode the embeddings to their corresponding graph components, making the search of facts tractable by performing efficient gradient-based continuous optimization. We evaluate our proposed methods through following experiments. First, on relatively small KGs, we show that our approximations are accurate compared to the true change in the score. Second, we show that our additive attacks can effectively reduce the performance of state of the art models BIBREF2 , BIBREF10 up to $27.3\\%$ and $50.7\\%$ in Hits@1 for two large KGs: WN18 and YAGO3-10. We also explore the utility of adversarial modifications in explaining the model predictions by presenting rule-like descriptions of the most influential neighbors. Finally, we use adversaries to detect errors in the KG, obtaining up to $55\\%$ accuracy in detecting errors. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "6\n",
      "What is the computation for the score change in the attack described in the text?\n",
      "\n",
      "The computation for the score change in the attack is given by r,o (-) = ((1-) (Hs + (1-) r',o'r',o')-1 r',o')r,o.\n",
      "Question : for the text Using similar argument as the attacks in the form of $\\langle s^{\\prime }, r^{\\prime }, o \\rangle $ , we can calculate the effect of the attack, $\\overline{\\psi }{(s,r,o)}-\\psi (s, r, o)$ as: (s,r,o)-(s, r, o)=(-) s, r where $_{s, r} = (,)$ ..We now derive an efficient computation for $(-)$ . First, the derivative of the loss $(\\overline{G})= (G)+(\\langle s, r^{\\prime }, o^{\\prime } \\rangle )$ over $$ is: es (G) = es (G) - (1-) r', o' where $_{r^{\\prime }, o^{\\prime }} = (^{\\prime },^{\\prime })$ , and $\\varphi = \\sigma (\\psi (s,r^{\\prime },o^{\\prime }))$ . At convergence, after retraining, we expect $\\nabla _{e_s} (\\overline{G})=0$ . We perform first order Taylor approximation of $\\nabla _{e_s} (\\overline{G})$ to get: 0 - (1-)r',o'+.(Hs+(1-)r',o' r',o')(-) where $H_s$ is the $d\\times d$ Hessian matrix for $s$ , i.e. second order derivative of the loss w.r.t. $$ , computed sparsely. Solving for $-$ gives us: -=.(1-) (Hs + (1-) r',o'r',o')-1 r',o' In practice, $H_s$ is positive definite, making $H_s + \\varphi (1-\\varphi ) _{r^{\\prime },o^{\\prime }}^\\intercal _{r^{\\prime },o^{\\prime }}$ positive definite as well, and invertible. Then, we compute the score change as: (s,r,o)-(s, r, o)= r,o (-) =. ((1-) (Hs + (1-) r',o'r',o')-1 r',o')r,o. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "7\n",
      "Q: What do we need to consider in the $\\langle s, r^{\\prime }, o \\rangle $ form of attack?\n",
      "\n",
      "A: We need to consider the change in the Hessian matrix over $$, in approximation of the change in the score as well.\n",
      "Question : for the text In this section we approximate the effect of attack in the form of $\\langle s, r^{\\prime }, o \\rangle $ . In contrast to $\\langle s^{\\prime }, r^{\\prime }, o \\rangle $ attacks, for this scenario we need to consider the change in the $$ , upon applying the attack, in approximation of the change in the score as well. Using previous results, we can approximate the $-$ as: -=.(1-) (Ho + (1-) s,r's,r')-1 s,r' and similarly, we can approximate $-$ as: -=. (1-) (Hs + (1-) r',or',o)-1 r',o where $H_s$ is the Hessian matrix over $$ . Then using these approximations: s,r(-) =. s,r ((1-) (Ho + (1-) s,r's,r')-1 s,r') and: (-) r,o=. ((1-) (Hs + (1-) r',or',o)-1 r',o) r,o and then calculate the change in the score as: (s,r,o)-(s, r, o)=. s,r.(-) +(-).r,o =. s,r ((1-) (Ho + (1-) s,r's,r')-1 s,r')+. ((1-) (Hs + (1-) r',or',o)-1 r',o) r, o generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "8\n",
      "What is the main difference between this work on adversarial attack on KG and previous related work?\n",
      "The main difference is that this work focuses on conducting adversarial attacks on the link prediction task using embeddings, while previous related work considers node classification as the target of their attacks, and they build their method on top of path-based representations instead of embeddings.\n",
      "Question : for the text Learning relational knowledge representations has been a focus of active research in the past few years, but to the best of our knowledge, this is the first work on conducting adversarial modifications on the link prediction task. Knowledge graph embedding There is a rich literature on representing knowledge graphs in vector spaces that differ in their scoring functions BIBREF21 , BIBREF22 , BIBREF23 . Although is primarily applicable to multiplicative scoring functions BIBREF0 , BIBREF1 , BIBREF2 , BIBREF24 , these ideas apply to additive scoring functions BIBREF18 , BIBREF6 , BIBREF7 , BIBREF25 as well, as we show in Appendix \"First-order Approximation of the Change For TransE\" ..Furthermore, there is a growing body of literature that incorporates an extra types of evidence for more informed embeddings such as numerical values BIBREF26 , images BIBREF27 , text BIBREF28 , BIBREF29 , BIBREF30 , and their combinations BIBREF31 . Using , we can gain a deeper understanding of these methods, especially those that build their embeddings wit hmultiplicative scoring functions..Interpretability and Adversarial Modification There has been a significant recent interest in conducting an adversarial attacks on different machine learning models BIBREF16 , BIBREF32 , BIBREF33 , BIBREF34 , BIBREF35 , BIBREF36 to attain the interpretability, and further, evaluate the robustness of those models. BIBREF20 uses influence function to provide an approach to understanding black-box models by studying the changes in the loss occurring as a result of changes in the training data. In addition to incorporating their established method on KGs, we derive a novel approach that differs from their procedure in two ways: (1) instead of changes in the loss, we consider the changes in the scoring function, which is more appropriate for KG representations, and (2) in addition to searching for an attack, we introduce a gradient-based method that is much faster, especially for “adding an attack triple” (the size of search space make the influence function method infeasible). Previous work has also considered adversaries for KGs, but as part of training to improve their representation of the graph BIBREF37 , BIBREF38 . Adversarial Attack on KG Although this is the first work on adversarial attacks for link prediction, there are two approaches BIBREF39 , BIBREF17 that consider the task of adversarial attack on graphs. There are a few fundamental differences from our work: (1) they build their method on top of a path-based representations while we focus on embeddings, (2) they consider node classification as the target of their attacks while we attack link prediction, and (3) they conduct the attack on small graphs due to restricted scalability, while the complexity of our method does not depend on the size of the graph, but only the neighborhood, allowing us to attack real-world graphs. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "9\n",
      "Question 1: What is the influence of an observed fact on the prediction according to the model?\n",
      "Answer 1: The influence of an observed fact on the prediction is defined as the change in the prediction score if the observed fact was not present when the embeddings were learned.\n",
      "Question : for the text For explaining a target prediction, we are interested in identifying the observed fact that has the most influence (according to the model) on the prediction. We define influence of an observed fact on the prediction as the change in the prediction score if the observed fact was not present when the embeddings were learned. Previous work have used this concept of influence similarly for several different tasks BIBREF19 , BIBREF20 . Formally, for the target triple ${s,r,o}$ and observed graph $G$ , we want to identify a neighboring triple ${s^{\\prime },r^{\\prime },o}\\in G$ such that the score $\\psi (s,r,o)$ when trained on $G$ and the score $\\overline{\\psi }(s,r,o)$ when trained on $G-\\lbrace {s^{\\prime },r^{\\prime },o}\\rbrace $ are maximally different, i.e. *argmax(s', r')Nei(o) (s',r')(s,r,o) where $\\Delta _{(s^{\\prime },r^{\\prime })}(s,r,o)=\\psi (s, r, o)-\\overline{\\psi }(s,r,o)$ , and $\\text{Nei}(o)=\\lbrace (s^{\\prime },r^{\\prime })|\\langle s^{\\prime },r^{\\prime },o \\rangle \\in G \\rbrace $ . generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "10\n",
      "What is the goal of the study?\n",
      "The goal of the study is to evaluate the effectiveness of adversarial attacks in successfully attacking link prediction by adding false facts and measure their effect on evaluation metrics such as MRR and Hits@.\n",
      "Question : for the text Now we evaluate the effectiveness of to successfully attack link prediction by adding false facts. The goal here is to identify the attacks for triples in the test data, and measuring their effect on MRR and Hits@ metrics (ranking evaluations) after conducting the attack and retraining the model..Since this is the first work on adversarial attacks for link prediction, we introduce several baselines to compare against our method. For finding the adversarial fact to add for the target triple $\\langle s, r, o \\rangle $ , we consider two baselines: 1) choosing a random fake fact $\\langle s^{\\prime }, r^{\\prime }, o \\rangle $ (Random Attack); 2) finding $(s^{\\prime }, r^{\\prime })$ by first calculating $, )$ and then feeding $-, )$ to the decoder of the inverter function (Opposite Attack). In addition to , we introduce two other alternatives of our method: (1) , that uses to increase the score of fake fact over a test triple, i.e., we find the fake fact the model ranks second after the test triple, and identify the adversary for them, and (2) that selects between and attacks based on which has a higher estimated change in score..All-Test The result of the attack on all test facts as targets is provided in the Table 4 . outperforms the baselines, demonstrating its ability to effectively attack the KG representations. It seems DistMult is more robust against random attacks, while ConvE is more robust against designed attacks. is more effective than since changing the score of a fake fact is easier than of actual facts; there is no existing evidence to support fake facts. We also see that YAGO3-10 models are more robust than those for WN18. Looking at sample attacks (provided in Appendix \"Sample Adversarial Attacks\" ), mostly tries to change the type of the target object by associating it with a subject and a relation for a different entity type..Uncertain-Test To better understand the effect of attacks, we consider a subset of test triples that 1) the model predicts correctly, 2) difference between their scores and the negative sample with the highest score is minimum. This “Uncertain-Test” subset contains 100 triples from each of the original test sets, and we provide results of attacks on this data in Table 4 . The attacks are much more effective in this scenario, causing a considerable drop in the metrics. Further, in addition to significantly outperforming other baselines, they indicate that ConvE's confidence is much more robust..Relation Breakdown We perform additional analysis on the YAGO3-10 dataset to gain a deeper understanding of the performance of our model. As shown in Figure 4 , both DistMult and ConvE provide a more robust representation for isAffiliatedTo and isConnectedTo relations, demonstrating the confidence of models in identifying them. Moreover, the affects DistMult more in playsFor and isMarriedTo relations while affecting ConvE more in isConnectedTo relations..Examples Sample adversarial attacks are provided in Table 5 . attacks mostly try to change the type of the target triple's object by associating it with a subject and a relation that require a different entity types. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "11\n",
      "Question 1: What do the adversarial attacks mostly try to do to the target triple's object?\n",
      "Answer 1: The attacks mostly try to change the type of the target triple's object by associating it with a subject and a relation that require a different entity types.\n",
      "Question : for the text In this section, we provide the output of the for some target triples. Sample adversarial attacks are provided in Table 5 . As it shows, attacks mostly try to change the type of the target triple's object by associating it with a subject and a relation that require a different entity types. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "12\n",
      "Question 1: What are the research interests of Mariana Lourenço?\n",
      "Answer 1: Mariana Lourenço's main research interests are machine learning, pattern recognition, and natural language processing.\n",
      "Question : for the text The Fundação para a Ciência e Tecnologia (FCT) is gratefully acknowledged for founding this work with the grants SFRH/BD/78396/2011 and PTDC/ECM-TRA/1898/2012 (InfoCROWDS)..[]Mariana Lourenço has a MSc degree in Informatics Engineering from University of Coimbra, Portugal. Her thesis presented a supervised topic model that is able to learn from crowds and she took part in a research project whose primary objective was to exploit online information about public events to build predictive models of flows of people in the city. Her main research interests are machine learning, pattern recognition and natural language processing..[]Bernardete Ribeiro is Associate Professor at the Informatics Engineering Department, University of Coimbra in Portugal, from where she received a D.Sc. in Informatics Engineering, a Ph.D. in Electrical Engineering, speciality of Informatics, and a MSc in Computer Science. Her research interests are in the areas of Machine Learning, Pattern Recognition and Signal Processing and their applications to a broad range of fields. She was responsible/participated in several research projects in a wide range of application areas such as Text Classification, Financial, Biomedical and Bioinformatics. Bernardete Ribeiro is IEEE Senior Member, and member of IARP International Association of Pattern Recognition and ACM..[]Francisco C. Pereira is Full Professor at the Technical University of Denmark (DTU), where he leads the Smart Mobility research group. His main research focus is on applying machine learning and pattern recognition to the context of transportation systems with the purpose of understanding and predicting mobility behavior, and modeling and optimizing the transportation system as a whole. He has Master€™s (2000) and Ph.D. (2005) degrees in Computer Science from University of Coimbra, and has authored/co-authored over 70 journal and conference papers in areas such as pattern recognition, transportation, knowledge based systems and cognitive science. Francisco was previously Research Scientist at MIT and Assistant Professor in University of Coimbra. He was awarded several prestigious prizes, including an IEEE Achievements award, in 2009, the Singapore GYSS Challenge in 2013, and the Pyke Johnson award from Transportation Research Board, in 2015. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "13\n",
      "What is the goal of inference in the proposed model?\n",
      "Answer 1: The goal of inference is to compute the posterior distribution of the per-document topic proportions, the per-word topic assignments, the per-topic distribution over words, the per-document latent true class, and the per-annotator confusion parameters.\n",
      "Question : for the text Given a dataset INLINEFORM0 , the goal of inference is to compute the posterior distribution of the per-document topic proportions INLINEFORM1 , the per-word topic assignments INLINEFORM2 , the per-topic distribution over words INLINEFORM3 , the per-document latent true class INLINEFORM4 , and the per-annotator confusion parameters INLINEFORM5 . As with LDA, computing the exact posterior distribution of the latent variables is computationally intractable. Hence, we employ mean-field variational inference to perform approximate Bayesian inference..Variational inference methods seek to minimize the KL divergence between the variational and the true posterior distribution. We assume a fully-factorized (mean-field) variational distribution of the form DISPLAYFORM0 . where INLINEFORM0 , INLINEFORM1 , INLINEFORM2 , INLINEFORM3 and INLINEFORM4 are variational parameters. Table TABREF23 shows the correspondence between variational parameters and the original parameters..Let INLINEFORM0 denote the model parameters. Following BIBREF25 , the KL minimization can be equivalently formulated as maximizing the following lower bound on the log marginal likelihood DISPLAYFORM0 . which we maximize using coordinate ascent..Optimizing INLINEFORM0 w.r.t. INLINEFORM1 and INLINEFORM2 gives the same coordinate ascent updates as in LDA BIBREF0 DISPLAYFORM0 .The variational Dirichlet parameters INLINEFORM0 can be optimized by collecting only the terms in INLINEFORM1 that contain INLINEFORM2 DISPLAYFORM0 . where INLINEFORM0 denotes the documents labeled by the INLINEFORM1 annotator, INLINEFORM2 , and INLINEFORM3 and INLINEFORM4 are the gamma and digamma functions, respectively. Taking derivatives of INLINEFORM5 w.r.t. INLINEFORM6 and setting them to zero, yields the following update DISPLAYFORM0 .Similarly, the coordinate ascent updates for the documents distribution over classes INLINEFORM0 can be found by considering the terms in INLINEFORM1 that contain INLINEFORM2 DISPLAYFORM0 . where INLINEFORM0 . Adding the necessary Lagrange multipliers to ensure that INLINEFORM1 and setting the derivatives w.r.t. INLINEFORM2 to zero gives the following update DISPLAYFORM0 . Observe how the variational distribution over the true classes results from a combination between the dot product of the inferred mean topic assignment INLINEFORM0 with the coefficients INLINEFORM1 and the labels INLINEFORM2 from the multiple annotators “weighted\" by their expected log probability INLINEFORM3 ..The main difficulty of applying standard variational inference methods to the proposed model is the non-conjugacy between the distribution of the mean topic-assignment INLINEFORM0 and the softmax. Namely, in the expectation DISPLAYFORM0 . the second term is intractable to compute. We can make progress by applying Jensen's inequality to bound it as follows DISPLAYFORM0 . where INLINEFORM0 , which is constant w.r.t. INLINEFORM1 . This local variational bound can be made tight by noticing that INLINEFORM2 , where equality holds if and only if INLINEFORM3 . Hence, given the current parameter estimates INLINEFORM4 , if we set INLINEFORM5 and INLINEFORM6 then, for an individual parameter INLINEFORM7 , we have that DISPLAYFORM0 . Using this local bound to approximate the expectation of the log-sum-exp term, and taking derivatives of the evidence lower bound w.r.t. INLINEFORM0 with the constraint that INLINEFORM1 , yields the following fix-point update DISPLAYFORM0 . where INLINEFORM0 denotes the size of the vocabulary. Notice how the per-word variational distribution over topics INLINEFORM1 depends on the variational distribution over the true class label INLINEFORM2 ..The variational inference algorithm iterates between Eqs. EQREF25 - EQREF33 until the evidence lower bound, Eq. EQREF24 , converges. Additional details are provided as supplementary material....The goal of inference is to compute the posterior distribution of the per-document topic proportions INLINEFORM0 , the per-word topic assignments INLINEFORM1 , the per-topic distribution over words INLINEFORM2 and the per-document latent true targets INLINEFORM3 . As we did for the classification model, we shall develop a variational inference algorithm using coordinate ascent. The lower-bound on the log marginal likelihood is now given by DISPLAYFORM0 . where INLINEFORM0 are the model parameters. We assume a fully-factorized (mean-field) variational distribution INLINEFORM1 of the form DISPLAYFORM0 . where INLINEFORM0 , INLINEFORM1 , INLINEFORM2 , INLINEFORM3 and INLINEFORM4 are the variational parameters. Notice the new Gaussian term, INLINEFORM5 , corresponding to the approximate posterior distribution of the unobserved true targets..Optimizing the variational objective INLINEFORM0 w.r.t. INLINEFORM1 and INLINEFORM2 yields the same updates from Eqs. EQREF25 and . Optimizing w.r.t. INLINEFORM3 gives a similar update to the one in sLDA BIBREF6 DISPLAYFORM0 . where we defined INLINEFORM0 . Notice how this update differs only from the one in BIBREF6 by replacing the true target variable by its expected value under the variational distribution, which is given by INLINEFORM1 ..The only variables left for doing inference on are then the latent true targets INLINEFORM0 . The variational distribution of INLINEFORM1 is governed by two parameters: a mean INLINEFORM2 and a variance INLINEFORM3 . Collecting all the terms in INLINEFORM4 that contain INLINEFORM5 gives DISPLAYFORM0 . Taking derivatives of INLINEFORM0 and setting them to zero gives the following update for INLINEFORM1 DISPLAYFORM0 . Notice how the value of INLINEFORM0 is a weighted average of what the linear regression model on the empirical topic mixture believes the true target should be, and the bias-corrected answers of the different annotators weighted by their individual precisions..As for INLINEFORM0 , we can optimize INLINEFORM1 w.r.t. INLINEFORM2 by collecting all terms that contain INLINEFORM3 DISPLAYFORM0 . and taking derivatives, yielding the update DISPLAYFORM0  generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "14\n",
      "What is the 20-Newsgroups benchmark corpus used for in the experiment?\n",
      "The 20-Newsgroups benchmark corpus was used in the experiment to validate the proposed model for classification problems in a controlled environment by simulating multiple annotators with varying levels of expertise. It consists of twenty thousand messages taken from twenty newsgroups and is divided into six super-classes. For this experiment, only the four most populated super-classes were used: “computers\", “science\", “politics\" and “recreative\". The preprocessing of the documents involved stemming and stop-words removal, and 75% of the documents were randomly selected for training and the remaining 25% for testing.\n",
      "Question : for the text In order to first validate the proposed model for classification problems in a slightly more controlled environment, the well-known 20-Newsgroups benchmark corpus BIBREF29 was used by simulating multiple annotators with different levels of expertise. The 20-Newsgroups consists of twenty thousand messages taken from twenty newsgroups, and is divided in six super-classes, which are, in turn, partitioned in several sub-classes. For this first set of experiments, only the four most populated super-classes were used: “computers\", “science\", “politics\" and “recreative\". The preprocessing of the documents consisted of stemming and stop-words removal. After that, 75% of the documents were randomly selected for training and the remaining 25% for testing..The different annotators were simulated by sampling their answers from a multinomial distribution, where the parameters are given by the lines of the annotators' confusion matrices. Hence, for each annotator INLINEFORM0 , we start by pre-defining a confusion matrix INLINEFORM1 with elements INLINEFORM2 , which correspond to the probability that the annotators' answer is INLINEFORM3 given that the true label is INLINEFORM4 , INLINEFORM5 . Then, the answers are sampled i.i.d. from INLINEFORM6 . This procedure was used to simulate 5 different annotators with the following accuracies: 0.737, 0.468, 0.284, 0.278, 0.260. In this experiment, no repeated labelling was used. Hence, each annotator only labels roughly one-fifth of the data. When compared to the ground truth, the simulated answers revealed an accuracy of 0.405. See Table TABREF81 for an overview of the details of the classification datasets used..Both the batch and the stochastic variational inference (svi) versions of the proposed model (MA-sLDAc) are compared with the following baselines:.[itemsep=0.02cm].LDA + LogReg (mv): This baseline corresponds to applying unsupervised LDA to the data, and learning a logistic regression classifier on the inferred topics distributions of the documents. The labels from the different annotators were aggregated using majority voting (mv). Notice that, when there is a single annotator label per instance, majority voting is equivalent to using that label for training. This is the case of the 20-Newsgroups' simulated annotators, but the same does not apply for the experiments in Section UID89 ..LDA + Raykar: For this baseline, the model of BIBREF21 was applied using the documents' topic distributions inferred by LDA as features..LDA + Rodrigues: This baseline is similar to the previous one, but uses the model of BIBREF9 instead..Blei 2003 (mv): The idea of this baseline is to replicate a popular state-of-the-art approach for document classification. Hence, the approach of BIBREF0 was used. It consists of applying LDA to extract the documents' topics distributions, which are then used to train a SVM. Similarly to the previous approach, the labels from the different annotators were aggregated using majority voting (mv)..sLDA (mv): This corresponds to using the classification version of sLDA BIBREF2 with the labels obtained by performing majority voting (mv) on the annotators' answers..For all the experiments the hyper-parameters INLINEFORM0 , INLINEFORM1 and INLINEFORM2 were set using a simple grid search in the collection INLINEFORM3 . The same approach was used to optimize the hyper-parameters of the all the baselines. For the svi algorithm, different mini-batch sizes and forgetting rates INLINEFORM4 were tested. For the 20-Newsgroup dataset, the best results were obtained with a mini-batch size of 500 and INLINEFORM5 . The INLINEFORM6 was kept at 1. The results are shown in Fig. FIGREF87 for different numbers of topics, where we can see that the proposed model outperforms all the baselines, being the svi version the one that performs best..In order to assess the computational advantages of the stochastic variational inference (svi) over the batch algorithm, the log marginal likelihood (or log evidence) was plotted against the number of iterations. Fig. FIGREF88 shows this comparison. Not surprisingly, the svi version converges much faster to higher values of the log marginal likelihood when compared to the batch version, which reflects the efficiency of the svi algorithm..In order to validate the proposed classification model in real crowdsourcing settings, Amazon Mechanical Turk (AMT) was used to obtain labels from multiple annotators for two popular datasets: Reuters-21578 BIBREF30 and LabelMe BIBREF31 ..The Reuters-21578 is a collection of manually categorized newswire stories with labels such as Acquisitions, Crude-oil, Earnings or Grain. For this experiment, only the documents belonging to the ModApte split were considered with the additional constraint that the documents should have no more than one label. This resulted in a total of 7016 documents distributed among 8 classes. Of these, 1800 documents were submitted to AMT for multiple annotators to label, giving an average of approximately 3 answers per document (see Table TABREF81 for further details). The remaining 5216 documents were used for testing. The collected answers yield an average worker accuracy of 56.8%. Applying majority voting to these answers reveals a ground truth accuracy of 71.0%. Fig. FIGREF90 shows the boxplots of the number of answers per worker and their accuracies. Observe how applying majority voting yields a higher accuracy than the median accuracy of the workers..The results obtained by the different approaches are given in Fig. FIGREF91 , where it can be seen that the proposed model (MA-sLDAc) outperforms all the other approaches. For this dataset, the svi algorithm is using mini-batches of 300 documents..The proposed model was also validated using a dataset from the computer vision domain: LabelMe BIBREF31 . In contrast to the Reuters and Newsgroups corpora, LabelMe is an open online tool to annotate images. Hence, this experiment allows us to see how the proposed model generalizes beyond non-textual data. Using the Matlab interface provided in the projects' website, we extracted a subset of the LabelMe data, consisting of all the 256 x 256 images with the categories: “highway\", “inside city\", “tall building\", “street\", “forest\", “coast\", “mountain\" or “open country\". This allowed us to collect a total of 2688 labeled images. Of these, 1000 images were given to AMT workers to classify with one of the classes above. Each image was labeled by an average of 2.547 workers, with a mean accuracy of 69.2%. When majority voting is applied to the collected answers, a ground truth accuracy of 76.9% is obtained. Fig. FIGREF92 shows the boxplots of the number of answers per worker and their accuracies. Interestingly, the worker accuracies are much higher and their distribution is much more concentrated than on the Reuters-21578 data (see Fig. FIGREF90 ), which suggests that this is an easier task for the AMT workers..The preprocessing of the images used is similar to the approach in BIBREF1 . It uses 128-dimensional SIFT BIBREF32 region descriptors selected by a sliding grid spaced at one pixel. This sliding grid extracts local regions of the image with sizes uniformly sampled between 16 x 16 and 32 x 32 pixels. The 128-dimensional SIFT descriptors produced by the sliding window are then fed to a k-means algorithm (with k=200) in order construct a vocabulary of 200 “visual words\". This allows us to represent the images with a bag of visual words model..With the purpose of comparing the proposed model with a popular state-of-the-art approach for image classification, for the LabelMe dataset, the following baseline was introduced:.Bosch 2006 (mv): This baseline is similar to one in BIBREF33 . The authors propose the use of pLSA to extract the latent topics, and the use of k-nearest neighbor (kNN) classifier using the documents' topics distributions. For this baseline, unsupervised LDA is used instead of pLSA, and the labels from the different annotators for kNN (with INLINEFORM0 ) are aggregated using majority voting (mv)..The results obtained by the different approaches for the LabelMe data are shown in Fig. FIGREF94 , where the svi version is using mini-batches of 200 documents..Analyzing the results for the Reuters-21578 and LabelMe data, we can observe that MA-sLDAc outperforms all the baselines, with slightly better accuracies for the batch version, especially in the Reuters data. Interestingly, the second best results are consistently obtained by the multi-annotator approaches, which highlights the need for accounting for the noise and biases of the answers of the different annotators..In order to verify that the proposed model was estimating the (normalized) confusion matrices INLINEFORM0 of the different workers correctly, a random sample of them was plotted against the true confusion matrices (i.e. the normalized confusion matrices evaluated against the true labels). Figure FIGREF95 shows the results obtained with 60 topics on the Reuters-21578 dataset, where the color intensity of the cells increases with the magnitude of the value of INLINEFORM1 (the supplementary material provides a similar figure for the LabelMe dataset). Using this visualization we can verify that the AMT workers are quite heterogeneous in their labeling styles and in the kind of mistakes they make, with several workers showing clear biases (e.g. workers 3 and 4), while others made mistakes more randomly (e.g. worker 1). Nevertheless, the proposed is able to capture these patterns correctly and account for effect..To gain further insights, Table TABREF96 shows 4 example images from the LabelMe dataset, along with their true labels, the answers provided by the different workers, the true label inferred by the proposed model and the likelihood of the different possible answers given the true label for each annotator ( INLINEFORM0 for INLINEFORM1 ) using a color-coding scheme similar to Fig. FIGREF95 . In the first example, although majority voting suggests “inside city\" to be the correct label, we can see that the model has learned that annotators 32 and 43 are very likely to provide the label “inside city\" when the true label is actually “street\", and it is able to leverage that fact to infer that the correct label is “street\". Similarly, in the second image the model is able to infer the correct true label from 3 conflicting labels. However, in the third image the model is not able to recover the correct true class, which can be explained by it not having enough evidence about the annotators and their reliabilities and biases (likelihood distribution for these cases is uniform). In fact, this raises interesting questions regarding requirements for the minimum number of labels per annotator, their reliabilities and their coherence. Finally, for the fourth image, somehow surprisingly, the model is able to infer the correct true class, even though all 3 annotators labeled it as “inside city\". generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "15\n",
      "Question 1: What is the purpose of the multi-annotator supervised topic model for classification problems?\n",
      "\n",
      "Answer 1: The purpose of the multi-annotator supervised topic model is to provide a method for classifying documents in a supervised learning setting. It uses a variational inference algorithm to approximate the posterior distribution over latent variables and estimate model parameters, as well as a stochastic variational inference algorithm to handle large collections of documents. Ultimately, the model is designed to classify new documents based on what has been learned from the training data.\n",
      "Question : for the text In this section, we develop a multi-annotator supervised topic model for classification problems. The model for regression settings will be presented in Section SECREF5 . We start by deriving a (batch) variational inference algorithm for approximating the posterior distribution over the latent variables and an algorithm to estimate the model parameters. We then develop a stochastic variational inference algorithm that gives the model the capability of handling large collections of documents. Finally, we show how to use the learned model to classify new documents. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "16\n",
      "Question 1: What is the proposed model able to do in relation to multiple annotators and crowdsourcing?\n",
      "\n",
      "Answer 1: The proposed model is able to learn from multiple annotators and crowds, accounting for their biases and different levels of expertise, and jointly models the words in documents, the latent true target variables, and the noisy answers of the annotators, making it a strong contribution to the multi-annotator paradigm.\n",
      "Question : for the text This article proposed a supervised topic model that is able to learn from multiple annotators and crowds, by accounting for their biases and different levels of expertise. Given the large sizes of modern datasets, and considering that the majority of the tasks for which crowdsourcing and multiple annotators are desirable candidates, generally involve complex high-dimensional data such as text and images, the proposed model constitutes a strong contribution for the multi-annotator paradigm. This model is then capable of jointly modeling the words in documents as arising from a mixture of topics, as well as the latent true target variables and the (noisy) answers of the multiple annotators. We developed two distinct models, one for classification and another for regression, which share similar intuitions but that inevitably differ due to the nature of the target variables. We empirically showed, using both simulated and real annotators from Amazon Mechanical Turk that the proposed model is able to outperform state-of-the-art approaches in several real-world problems, such as classifying posts, news stories and images, or predicting the number of stars of restaurant and the rating of movie based on their reviews. For this, we use various popular datasets from the state-of-the-art, that are commonly used for benchmarking machine learning algorithms. Finally, an efficient stochastic variational inference algorithm was described, which gives the proposed models the ability to scale to large datasets. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "17\n",
      "What is the approximate posterior distribution used in making predictions for new documents in sLDA?\n",
      "\n",
      "The approximate posterior distribution used in making predictions for new documents in sLDA is computed over the latent variables INLINEFORM1 and INLINEFORM2 by dropping the terms that involve INLINEFORM3, INLINEFORM4, and INLINEFORM5 from the model's joint distribution and averaging over the estimated topics distributions.\n",
      "Question : for the text In order to make predictions for a new (unlabeled) document INLINEFORM0 , we start by computing the approximate posterior distribution over the latent variables INLINEFORM1 and INLINEFORM2 . This can be achieved by dropping the terms that involve INLINEFORM3 , INLINEFORM4 and INLINEFORM5 from the model's joint distribution (since, at prediction time, the multi-annotator labels are no longer observed) and averaging over the estimated topics distributions. Letting the topics distribution over words inferred during training be INLINEFORM6 , the joint distribution for a single document is now simply given by DISPLAYFORM0 . Deriving a mean-field variational inference algorithm for computing the posterior over INLINEFORM0 results in the same fixed-point updates as in LDA BIBREF0 for INLINEFORM1 (Eq. EQREF25 ) and INLINEFORM2 DISPLAYFORM0 . Using the inferred posteriors and the coefficients INLINEFORM0 estimated during training, we can make predictions as follows DISPLAYFORM0 . This is equivalent to making predictions in the classification version of sLDA BIBREF2 . generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "18\n",
      "Question 1: What are the real-world problems that the proposed multi-annotator supervised LDA models are used to solve?\n",
      "Answer 1: The proposed multi-annotator supervised LDA models are used to solve problems such as classifying posts and news stories, classifying images, predicting the number of stars that a user gave to a restaurant based on the review, and predicting movie ratings using the text of the reviews.\n",
      "Question : for the text In this section, the proposed multi-annotator supervised LDA models for classification and regression (MA-sLDAc and MA-sLDAr, respectively) are validated using both simulated annotators on popular corpora and using real multiple-annotator labels obtained from Amazon Mechanical Turk. Namely, we shall consider the following real-world problems: classifying posts and news stories; classifying images according to their content; predicting number of stars that a given user gave to a restaurant based on the review; predicting movie ratings using the text of the reviews. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "19\n",
      "What is the proposed solution for mitigating the biases and variances in annotator labels when using crowdsourced data?\n",
      "\n",
      "The proposed solution is a fully generative supervised topic model that is able to account for the different reliabilities of multiple annotators and correct their biases, by jointly modeling the words in documents as arising from a mixture of topics, the latent true target variables as a result of the empirical distribution over topics of the documents, and the labels of the multiple annotators as noisy versions of that latent ground truth. The model is implemented through an efficient stochastic variational inference algorithm that is able to scale to very large datasets.\n",
      "Question : for the text Topic models, such as latent Dirichlet allocation (LDA), allow us to analyze large collections of documents by revealing their underlying themes, or topics, and how each document exhibits them BIBREF0 . Therefore, it is not surprising that topic models have become a standard tool in data analysis, with many applications that go even beyond their original purpose of modeling textual data, such as analyzing images BIBREF1 , BIBREF2 , videos BIBREF3 , survey data BIBREF4 or social networks data BIBREF5 ..Since documents are frequently associated with other variables such as labels, tags or ratings, much interest has been placed on supervised topic models BIBREF6 , which allow the use of that extra information to “guide\" the topics discovery. By jointly learning the topics distributions and a classification or regression model, supervised topic models have been shown to outperform the separate use of their unsupervised analogues together with an external regression/classification algorithm BIBREF2 , BIBREF7 ..Supervised topics models are then state-of-the-art approaches for predicting target variables associated with complex high-dimensional data, such as documents or images. Unfortunately, the size of modern datasets makes the use of a single annotator unrealistic and unpractical for the majority of the real-world applications that involve some form of human labeling. For instance, the popular Reuters-21578 benchmark corpus was categorized by a group of personnel from Reuters Ltd and Carnegie Group, Inc. Similarly, the LabelMe project asks volunteers to annotate images from a large collection using an online tool. Hence, it is seldom the case where a single oracle labels an entire collection..Furthermore, the Web, through its social nature, also exploits the wisdom of crowds to annotate large collections of documents and images. By categorizing texts, tagging images or rating products and places, Web users are generating large volumes of labeled content. However, when learning supervised models from crowds, the quality of labels can vary significantly due to task subjectivity and differences in annotator reliability (or bias) BIBREF8 , BIBREF9 . If we consider a sentiment analysis task, it becomes clear that the subjectiveness of the exercise is prone to generate considerably distinct labels from different annotators. Similarly, online product reviews are known to vary considerably depending on the personal biases and volatility of the reviewer's opinions. It is therefore essential to account for these issues when learning from this increasingly common type of data. Hence, the interest of researchers on building models that take the reliabilities of different annotators into consideration and mitigate the effect of their biases has spiked during the last few years (e.g. BIBREF10 , BIBREF11 )..The increasing popularity of crowdsourcing platforms like Amazon Mechanical Turk (AMT) has further contributed to the recent advances in learning from crowds. This kind of platforms offers a fast, scalable and inexpensive solution for labeling large amounts of data. However, their heterogeneous nature in terms of contributors makes their straightforward application prone to many sorts of labeling noise and bias. Hence, a careless use of crowdsourced data as training data risks generating flawed models..In this article, we propose a fully generative supervised topic model that is able to account for the different reliabilities of multiple annotators and correct their biases. The proposed model is then capable of jointly modeling the words in documents as arising from a mixture of topics, the latent true target variables as a result of the empirical distribution over topics of the documents, and the labels of the multiple annotators as noisy versions of that latent ground truth. We propose two different models, one for classification BIBREF12 and another for regression problems, thus covering a very wide range of possible practical applications, as we empirically demonstrate. Since the majority of the tasks for which multiple annotators are used generally involve complex data such as text, images and video, by developing a multi-annotator supervised topic model we are contributing with a powerful tool for learning predictive models of complex high-dimensional data from crowds..Given that the increasing sizes of modern datasets can pose a problem for obtaining human labels as well as for Bayesian inference, we propose an efficient stochastic variational inference algorithm BIBREF13 that is able to scale to very large datasets. We empirically show, using both simulated and real multiple-annotator labels obtained from AMT for popular text and image collections, that the proposed models are able to outperform other state-of-the-art approaches in both classification and regression tasks. We further show the computational and predictive advantages of the stochastic variational inference algorithm over its batch counterpart. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "20\n",
      "What was the focus of earlier works on learning from multiple annotators?\n",
      "\n",
      "The focus of earlier works was on estimating the ground truth and the error rates of different annotators.\n",
      "Question : for the text Learning from multiple annotators is an increasingly important research topic. Since the early work of Dawid and Skeene BIBREF19 , who attempted to obtain point estimates of the error rates of patients given repeated but conflicting responses to various medical questions, many approaches have been proposed. These usually rely on latent variable models. For example, in BIBREF20 the authors propose a model to estimate the ground truth from the labels of multiple experts, which is then used to train a classifier..While earlier works usually focused on estimating the ground truth and the error rates of different annotators, recent works are more focused on the problem of learning classifiers using multiple-annotator data. This idea was explored by Raykar et al. BIBREF21 , who proposed an approach for jointly learning the levels of expertise of different annotators and the parameters of a logistic regression classifier, by modeling the ground truth labels as latent variables. This work was later extended in BIBREF11 by considering the dependencies of the annotators' labels on the instances they are labeling, and also in BIBREF22 through the use of Gaussian process classifiers. The model proposed in this article for classification problems shares the same intuition with this line of work and models the true labels as latent variables. However, it differs significantly by using a fully Bayesian approach for estimating the reliabilities and biases of the different annotators. Furthermore, it considers the problems of learning a low-dimensional representation of the input data (through topic modeling) and modeling the answers of multiple annotators jointly, providing an efficient stochastic variational inference algorithm..Despite the considerable amount of approaches for learning classifiers from the noisy answers of multiple annotators, for continuous response variables this problem has been approached in a much smaller extent. For example, Groot et al. BIBREF23 address this problem in the context of Gaussian processes. In their work, the authors assign a different variance to the likelihood of the data points provided by the different annotators, thereby allowing them to have different noise levels, which can be estimated by maximizing the marginal likelihood of the data. Similarly, the authors in BIBREF21 propose an extension of their own classification approach to regression problems by assigning different variances to the Gaussian noise models of the different annotators. In this article, we take this idea one step further by also considering a per-annotator bias parameter, which gives the proposed model the ability to overcome certain personal tendencies in the annotators labeling styles that are quite common, for example, in product ratings and document reviews. Furthermore, we empirically validate the proposed model using real multi-annotator data obtained from Amazon Mechanical Turk. This contrasts with the previously mentioned works, which rely only on simulated annotators. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "21\n",
      "What method is used to find maximum likelihood estimates for the coefficients and biases in the proposed model?\n",
      "\n",
      "Answer 1: The method used to find maximum likelihood estimates for the coefficients and biases in the proposed model is variational Bayesian EM.\n",
      "Question : for the text The model parameters are INLINEFORM0 . The parameters INLINEFORM1 of the Dirichlet priors can be regarded as hyper-parameters of the proposed model. As with many works on topic models (e.g. BIBREF26 , BIBREF2 ), we assume hyper-parameters to be fixed, since they can be effectively selected by grid-search procedures which are able to explore well the parameter space without suffering from local optima. Our focus is then on estimating the coefficients INLINEFORM2 using a variational EM algorithm. Therefore, in the E-step we use the variational inference algorithm from section SECREF21 to estimate the posterior distribution of the latent variables, and in the M-step we find maximum likelihood estimates of INLINEFORM3 by maximizing the evidence lower bound INLINEFORM4 . Unfortunately, taking derivatives of INLINEFORM5 w.r.t. INLINEFORM6 does not yield a closed-form solution. Hence, we use a numerical method, namely L-BFGS BIBREF27 , to find an optimum. The objective function and gradients are given by DISPLAYFORM0 . where, for convenience, we defined the following variable: INLINEFORM0 ..The parameters of the proposed regression model are INLINEFORM0 . As we did for the classification model, we shall assume the Dirichlet parameters, INLINEFORM1 and INLINEFORM2 , to be fixed. Similarly, we shall assume that the variance of the true targets, INLINEFORM3 , to be constant. The only parameters left to estimate are then the regression coefficients INLINEFORM4 and the annotators biases, INLINEFORM5 , and precisions, INLINEFORM6 , which we estimate using variational Bayesian EM..Since the latent true targets are now linear functions of the documents' empirical topic mixtures (i.e. there is no softmax function), we can find a closed form solution for the regression coefficients INLINEFORM0 . Taking derivatives of INLINEFORM1 w.r.t. INLINEFORM2 and setting them to zero, gives the following solution for INLINEFORM3 DISPLAYFORM0 . where DISPLAYFORM0 .We can find maximum likelihood estimates for the annotator biases INLINEFORM0 by optimizing the lower bound on the marginal likelihood. The terms in INLINEFORM1 that involve INLINEFORM2 are DISPLAYFORM0 . Taking derivatives w.r.t. INLINEFORM0 gives the following estimate for the bias of the INLINEFORM1 annotator DISPLAYFORM0 .Similarly, we can find maximum likelihood estimates for the precisions INLINEFORM0 of the different annotators by considering the terms in INLINEFORM1 that contain INLINEFORM2 DISPLAYFORM0 . The maximum likelihood estimate for the precision (inverse variance) of the INLINEFORM0 annotator is then given by DISPLAYFORM0 .Given a set of fitted parameters, it is then straightforward to make predictions for new documents: it is just necessary to infer the (approximate) posterior distribution over the word-topic assignments INLINEFORM0 for all the words using the coordinates ascent updates of standard LDA (Eqs. EQREF25 and EQREF42 ), and then use the mean topic assignments INLINEFORM1 to make predictions INLINEFORM2 . generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "22\n",
      "Question 1: What is the proposed approach for building lower-dimensional representations of documents in the multi-annotator setting?\n",
      "\n",
      "Answer 1: The proposed approach is to model the words of a document as arising from a mixture of topics, each defined as a distribution over the words in a vocabulary, and to assign coefficients to the mean topic-assignment of the words in the document to build a classification model. However, a direct mapping between document classes and the labels provided by different annotators would assume they are equally reliable, which is often not the case. Therefore, a latent ground truth class is assumed, and the labels from different annotators are modeled using a noise model that accounts for their different levels of expertise and corrects potential biases.\n",
      "Question : for the text Let INLINEFORM0 be an annotated corpus of size INLINEFORM1 , where each document INLINEFORM2 is given a set of labels INLINEFORM3 from INLINEFORM4 distinct annotators. We can take advantage of the inherent topical structure of documents and model their words as arising from a mixture of topics, each being defined as a distribution over the words in a vocabulary, as in LDA. In LDA, the INLINEFORM5 word, INLINEFORM6 , in a document INLINEFORM7 is provided a discrete topic-assignment INLINEFORM8 , which is drawn from the documents' distribution over topics INLINEFORM9 . This allows us to build lower-dimensional representations of documents, which we can explore to build classification models by assigning coefficients INLINEFORM10 to the mean topic-assignment of the words in the document, INLINEFORM11 , and applying a softmax function in order to obtain a distribution over classes. Alternatively, one could consider more flexible models such as Gaussian processes, however that would considerably increase the complexity of inference..Unfortunately, a direct mapping between document classes and the labels provided by the different annotators in a multiple-annotator setting would correspond to assuming that they are all equally reliable, an assumption that is violated in practice, as previous works clearly demonstrate (e.g. BIBREF8 , BIBREF9 ). Hence, we assume the existence of a latent ground truth class, and model the labels from the different annotators using a noise model that states that, given a true class INLINEFORM0 , each annotator INLINEFORM1 provides the label INLINEFORM2 with some probability INLINEFORM3 . Hence, by modeling the matrix INLINEFORM4 we are in fact modeling a per-annotator (normalized) confusion matrix, which allows us to account for their different levels of expertise and correct their potential biases..The generative process of the proposed model for classification problems can then be summarized as follows:.For each annotator INLINEFORM0 .For each class INLINEFORM0 .Draw reliability parameter INLINEFORM0 .For each topic INLINEFORM0 .Draw topic distribution INLINEFORM0 .For each document INLINEFORM0 .Draw topic proportions INLINEFORM0 .For the INLINEFORM0 word.Draw topic assignment INLINEFORM0 .Draw word INLINEFORM0 .Draw latent (true) class INLINEFORM0 .For each annotator INLINEFORM0 .Draw annotator's label INLINEFORM0 .where INLINEFORM0 denotes the set of annotators that labeled the INLINEFORM1 document, INLINEFORM2 , and the softmax is given by DISPLAYFORM0 .Fig. FIGREF20 shows a graphical model representation of the proposed model, where INLINEFORM0 denotes the number of topics, INLINEFORM1 is the number of classes, INLINEFORM2 is the total number of annotators and INLINEFORM3 is the number of words in the document INLINEFORM4 . Shaded nodes are used to distinguish latent variable from the observed ones and small solid circles are used to denote model parameters. Notice that we included a Dirichlet prior over the topics INLINEFORM5 to produce a smooth posterior and control sparsity. Similarly, instead of computing maximum likelihood or MAP estimates for the annotators reliability parameters INLINEFORM6 , we place a Dirichlet prior over these variables and perform approximate Bayesian inference. This contrasts with previous works on learning classification models from crowds BIBREF21 , BIBREF24 ..For developing a multi-annotator supervised topic model for regression, we shall follow a similar intuition as the one we considered for classification. Namely, we shall assume that, for a given document INLINEFORM0 , each annotator provides a noisy version, INLINEFORM1 , of the true (continuous) target variable, which we denote by INLINEFORM2 . This can be, for example, the true rating of a product or the true sentiment of a document. Assuming that each annotator INLINEFORM3 has its own personal bias INLINEFORM4 and precision INLINEFORM5 (inverse variance), and assuming a Gaussian noise model for the annotators' answers, we have that DISPLAYFORM0 . This approach is therefore more powerful than previous works BIBREF21 , BIBREF23 , where a single precision parameter was used to model the annotators' expertise. Fig. FIGREF45 illustrates this intuition for 4 annotators, represented by different colors. The “green annotator\" is the best one, since he is right on the target and his answers vary very little (low bias, high precision). The “yellow annotator\" has a low bias, but his answers are very uncertain, as they can vary a lot. Contrarily, the “blue annotator\" is very precise, but consistently over-estimates the true target (high bias, high precision). Finally, the “red annotator\" corresponds to the worst kind of annotator (with high bias and low precision)..Having specified a model for annotators answers given the true targets, the only thing left is to do is to specify a model of the latent true targets INLINEFORM0 given the empirical topic mixture distributions INLINEFORM1 . For this, we shall keep things simple and assume a linear model as in sLDA BIBREF6 . The generative process of the proposed model for continuous target variables can then be summarized as follows:.For each annotator INLINEFORM0 .For each class INLINEFORM0 .Draw reliability parameter INLINEFORM0 .For each topic INLINEFORM0 .Draw topic distribution INLINEFORM0 .For each document INLINEFORM0 .Draw topic proportions INLINEFORM0 .For the INLINEFORM0 word.Draw topic assignment INLINEFORM0 .Draw word INLINEFORM0 .Draw latent (true) target INLINEFORM0 .For each annotator INLINEFORM0 .Draw answer INLINEFORM0 .Fig. FIGREF60 shows a graphical representation of the proposed model. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "23\n",
      "What is the goal of the experiment using the we8there.com dataset?\n",
      "\n",
      "The goal of the experiment using the we8there.com dataset is to predict the overall experience of a user based on their comments in a restaurant review, by applying the proposed classification model and simulating an heterogeneous set of annotators in terms of reliability and bias.\n",
      "Question : for the text As for proposed classification model, we start by validating MA-sLDAr using simulated annotators on a popular corpus where the documents have associated targets that we wish to predict. For this purpose, we shall consider a dataset of user-submitted restaurant reviews from the website we8there.com. This dataset was originally introduced in BIBREF34 and it consists of 6260 reviews. For each review, there is a five-star rating on four specific aspects of quality (food, service, value, and atmosphere) as well as the overall experience. Our goal is then to predict the overall experience of the user based on his comments in the review. We apply the same preprocessing as in BIBREF18 , which consists in tokenizing the text into bigrams and discarding those that appear in less than ten reviews. The preprocessing of the documents consisted of stemming and stop-words removal. After that, 75% of the documents were randomly selected for training and the remaining 25% for testing..As with the classification model, we seek to simulate an heterogeneous set of annotators in terms of reliability and bias. Hence, in order to simulate an annotator INLINEFORM0 , we proceed as follows: let INLINEFORM1 be the true review of the restaurant; we start by assigning a given bias INLINEFORM2 and precision INLINEFORM3 to the reviewers, depending on what type of annotator we wish to simulate (see Fig. FIGREF45 ); we then sample a simulated answer as INLINEFORM4 . Using this procedure, we simulated 5 annotators with the following (bias, precision) pairs: (0.1, 10), (-0.3, 3), (-2.5, 10), (0.1, 0.5) and (1, 0.25). The goal is to have 2 good annotators (low bias, high precision), 1 highly biased annotator and 2 low precision annotators where one is unbiased and the other is reasonably biased. The coefficients of determination ( INLINEFORM5 ) of the simulated annotators are: [0.940, 0.785, -2.469, -0.131, -1.749]. Computing the mean of the answers of the different annotators yields a INLINEFORM6 of 0.798. Table TABREF99 gives an overview on the statistics of datasets used in the regression experiments..We compare the proposed model (MA-sLDAr) with the two following baselines:.[itemsep=0.02cm].LDA + LinReg (mean): This baseline corresponds to applying unsupervised LDA to the data, and learning a linear regression model on the inferred topics distributions of the documents. The answers from the different annotators were aggregated by computing the mean..sLDA (mean): This corresponds to using the regression version of sLDA BIBREF6 with the target variables obtained by computing the mean of the annotators' answers..Fig. FIGREF102 shows the results obtained for different numbers of topics. Do to the stochastic nature of both the annotators simulation procedure and the initialization of the variational Bayesian EM algorithm, we repeated each experiment 30 times and report the average INLINEFORM0 obtained with the corresponding standard deviation. Since the regression datasets that are considered in this article are not large enough to justify the use of a stochastic variational inference (svi) algorithm, we only made experiments using the batch algorithm developed in Section SECREF61 . The results obtained clearly show the improved performance of MA-sLDAr over the other methods..The proposed multi-annotator regression model (MA-sLDAr) was also validated with real annotators by using AMT. For that purpose, the movie review dataset from BIBREF35 was used. This dataset consists of 5006 movie reviews along with their respective star rating (from 1 to 10). The goal of this experiment is then predict how much a person liked a movie based on what she says about it. We ask workers to guess how much they think the writer of the review liked the movie based on her comments. An average of 4.96 answers per-review was collected for a total of 1500 reviews. The remaining reviews were used for testing. In average, each worker rated approximately 55 reviews. Using the mean answer as an estimate of the true rating of the movie yields a INLINEFORM0 of 0.830. Table TABREF99 gives an overview of the statistics of this data. Fig. FIGREF104 shows boxplots of the number of answers per worker, as well as boxplots of their respective biases ( INLINEFORM1 ) and variances (inverse precisions, INLINEFORM2 )..The preprocessing of the text consisted of stemming and stop-words removal. Using the preprocessed data, the proposed MA-sLDAr model was compared with the same baselines that were used with the we8there dataset in Section UID98 . Fig. FIGREF105 shows the results obtained for different numbers of topics. These results show that the proposed model outperforms all the other baselines..With the purpose of verifying that the proposed model is indeed estimating the biases and precisions of the different workers correctly, we plotted the true values against the estimates of MA-sLDAr with 60 topics for a random subset of 10 workers. Fig. FIGREF106 shows the obtained results, where higher color intensities indicate higher values. Ideally, the colour of two horizontally-adjacent squares would then be of similar shades, and this is indeed what happens in practice for the majority of the workers, as Fig. FIGREF106 shows. Interestingly, the figure also shows that there are a couple of workers that are considerably biased (e.g. workers 6 and 8) and that those biases are being correctly estimated, thus justifying the inclusion of a bias parameter in the proposed model, which contrasts with previous works BIBREF21 , BIBREF23 . generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "24\n",
      "Question 1: What is the focus of the proposed model discussed in this section?\n",
      "\n",
      "Answer 1: The proposed model discussed in this section is a variant of the model proposed in Section SECREF4 for regression problems. The focus of this model is on how to handle multiple annotators with different biases and reliabilities when the target variables are continuous variables.\n",
      "Question : for the text In this section, we develop a variant of the model proposed in Section SECREF4 for regression problems. We shall start by describing the proposed model with a special focus on the how to handle multiple annotators with different biases and reliabilities when the target variables are continuous variables. Next, we present a variational inference algorithm, highlighting the differences to the classification version. Finally, we show how to optimize the model parameters. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "25\n",
      "What is the purpose of the stochastic variational inference algorithm proposed in Section SECREF21?\n",
      "\n",
      "Answer 1: The purpose of the stochastic variational inference algorithm proposed in Section SECREF21 is to improve the efficiency of the batch coordinate ascent algorithm proposed for doing variational inference in the proposed model. The stochastic algorithm follows noisy estimates of the gradients of the evidence lower bound and uses subsampling of a document (or a mini-batch of documents) from the corpus to compute unbiased estimates of the gradients for updating the global variational parameters.\n",
      "Question : for the text In Section SECREF21 , we proposed a batch coordinate ascent algorithm for doing variational inference in the proposed model. This algorithm iterates between analyzing every document in the corpus to infer the local hidden structure, and estimating the global hidden variables. However, this can be inefficient for large datasets, since it requires a full pass through the data at each iteration before updating the global variables. In this section, we develop a stochastic variational inference algorithm BIBREF13 , which follows noisy estimates of the gradients of the evidence lower bound INLINEFORM0 ..Based on the theory of stochastic optimization BIBREF28 , we can find unbiased estimates of the gradients by subsampling a document (or a mini-batch of documents) from the corpus, and using it to compute the gradients as if that document was observed INLINEFORM0 times. Hence, given an uniformly sampled document INLINEFORM1 , we use the current posterior distributions of the global latent variables, INLINEFORM2 and INLINEFORM3 , and the current coefficient estimates INLINEFORM4 , to compute the posterior distribution over the local hidden variables INLINEFORM5 , INLINEFORM6 and INLINEFORM7 using Eqs. EQREF25 , EQREF33 and EQREF29 respectively. These posteriors are then used to update the global variational parameters, INLINEFORM8 and INLINEFORM9 by taking a step of size INLINEFORM10 in the direction of the noisy estimates of the natural gradients..Algorithm SECREF37 describes a stochastic variational inference algorithm for the proposed model. Given an appropriate schedule for the learning rates INLINEFORM0 , such that INLINEFORM1 and INLINEFORM2 , the stochastic optimization algorithm is guaranteed to converge to a local maximum of the evidence lower bound BIBREF28 ..[t] Stochastic variational inference for the proposed classification model [1] Initialize INLINEFORM0 , INLINEFORM1 , INLINEFORM2 , INLINEFORM3 , INLINEFORM4 , INLINEFORM5 Set t = t + 1 Sample a document INLINEFORM6 uniformly from the corpus Compute INLINEFORM7 using Eq. EQREF33 , for INLINEFORM8 Compute INLINEFORM9 using Eq. EQREF25 Compute INLINEFORM10 using Eq. EQREF29 local parameters INLINEFORM11 , INLINEFORM12 and INLINEFORM13 converge Compute step-size INLINEFORM14 Update topics variational parameters DISPLAYFORM0 . Update annotators confusion parameters DISPLAYFORM0 . global convergence criterion is met.As we did for the classification model from Section SECREF4 , we can envision developing a stochastic variational inference for the proposed regression model. In this case, the only “global\" latent variables are the per-topic distributions over words INLINEFORM0 . As for the “local\" latent variables, instead of a single variable INLINEFORM1 , we now have two variables per-document: INLINEFORM2 and INLINEFORM3 . The stochastic variational inference can then be summarized as shown in Algorithm SECREF76 . For added efficiency, one can also perform stochastic updates of the annotators biases INLINEFORM4 and precisions INLINEFORM5 , by taking a step in the direction of the gradient of the noisy evidence lower bound scaled by the step-size INLINEFORM6 ..[t] Stochastic variational inference for the proposed regression model [1] Initialize INLINEFORM0 , INLINEFORM1 , INLINEFORM2 , INLINEFORM3 , INLINEFORM4 , INLINEFORM5 , INLINEFORM6 Set t = t + 1 Sample a document INLINEFORM7 uniformly from the corpus Compute INLINEFORM8 using Eq. EQREF64 , for INLINEFORM9 Compute INLINEFORM10 using Eq. EQREF25 Compute INLINEFORM11 using Eq. EQREF66 Compute INLINEFORM12 using Eq. EQREF68 local parameters INLINEFORM13 , INLINEFORM14 and INLINEFORM15 converge Compute step-size INLINEFORM16 Update topics variational parameters DISPLAYFORM0 . global convergence criterion is met generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "26\n",
      "What is sLDA and how does it relate to document classification?\n",
      "sLDA is a supervised variant of LDA that models the relationship between documents and response variables, such as document classes. It does so by including a response variable that is linearly dependent on the mean topic-assignments of the words in a document. This approach is coherent with the generative perspective of LDA and is able to jointly model the documents and their responses to find latent topics that will best predict the response variables for future unlabeled documents. It is considered a natural approach for document classification since the classes are directly dependent on the empirical topic mixture distributions.\n",
      "Question : for the text Latent Dirichlet allocation (LDA) soon proved to be a powerful tool for modeling documents BIBREF0 and images BIBREF1 by extracting their underlying topics, where topics are probability distributions across words, and each document is characterized by a probability distribution across topics. However, the need to model the relationship between documents and labels quickly gave rise to many supervised variants of LDA. One of the first notable works was that of supervised LDA (sLDA) BIBREF6 . By extending LDA through the inclusion of a response variable that is linearly dependent on the mean topic-assignments of the words in a document, sLDA is able to jointly model the documents and their responses, in order to find latent topics that will best predict the response variables for future unlabeled documents. Although initially developed for general continuous response variables, sLDA was later extended to classification problems BIBREF2 , by modeling the relationship between topic-assignments and labels with a softmax function as in logistic regression..From a classification perspective, there are several ways in which document classes can be included in LDA. The most natural one in this setting is probably the sLDA approach, since the classes are directly dependent on the empirical topic mixture distributions. This approach is coherent with the generative perspective of LDA but, nevertheless, several discriminative alternatives also exist. For example, DiscLDA BIBREF14 introduces a class-dependent linear transformation on the topic mixture proportions of each document, such that the per-word topic assignments are drawn from linearly transformed mixture proportions. The class-specific transformation matrices are then able to reposition the topic mixture proportions so that documents with the same class labels have similar topics mixture proportions. The transformation matrices can be estimated by maximizing the conditional likelihood of response variables as the authors propose BIBREF14 ..An alternative way of including classes in LDA for supervision is the one proposed in the Labeled-LDA model BIBREF15 . Labeled-LDA is a variant of LDA that incorporates supervision by constraining the topic model to assign to a document only topics that correspond to its label set. While this allows for multiple labels per document, it is restrictive in the sense that the number of topics needs to be the same as the number of possible labels..From a regression perspective, other than sLDA, the most relevant approaches are the Dirichlet-multimonial regression BIBREF16 and the inverse regression topic models BIBREF17 . The Dirichlet-multimonial regression (DMR) topic model BIBREF16 includes a log-linear prior on the document's mixture proportions that is a function of a set of arbitrary features, such as author, date, publication venue or references in scientific articles. The inferred Dirichlet-multinomial distribution can then be used to make predictions about the values of theses features. The inverse regression topic model (IRTM) BIBREF17 is a mixed-membership extension of the multinomial inverse regression (MNIR) model proposed in BIBREF18 that exploits the topical structure of text corpora to improve its predictions and facilitate exploratory data analysis. However, this results in a rather complex and inefficient inference procedure. Furthermore, making predictions in the IRTM is not trivial. For example, MAP estimates of targets will be in a different scale than the original document's metadata. Hence, the authors propose the use of a linear model to regress metadata values onto their MAP predictions..The approaches discussed so far rely on likelihood-based estimation procedures. The work in BIBREF7 contrasts with these approaches by proposing MedLDA, a supervised topic model that utilizes the max-margin principle for estimation. Despite its margin-based advantages, MedLDA looses the probabilistic interpretation of the document classes given the topic mixture distributions. On the contrary, in this article we propose a fully generative probabilistic model of the answers of multiple annotators and of the words of documents arising from a mixture of topics. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "27\n",
      "Question 1: What organizations provided support for this work?\n",
      "Answer 1: The National Science Foundation of China (Grant No. 61936010/61876096) and the National Key R&D Program of China (Grant No. 2018YFC0830200) provided support for this work. Additionally, THUNUS NExT JointLab and individuals Ryuichi Takanobu and Fei Mi provided support and feedback.\n",
      "Question : for the text This work was supported by the National Science Foundation of China (Grant No. 61936010/61876096) and the National Key R&D Program of China (Grant No. 2018YFC0830200). We would like to thank THUNUS NExT JointLab for the support. We would also like to thank Ryuichi Takanobu and Fei Mi for their constructive comments. We are grateful to our action editor, Bonnie Webber, and the anonymous reviewers for their valuable suggestions and feedback. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "28\n",
      "Question 1: What benchmark models are provided with the CrossWOZ corpus for a task-oriented dialogue system?\n",
      "\n",
      "Answer 1: The CrossWOZ corpus provides benchmark models for different components of a task-oriented dialogue system, including natural language understanding (NLU), dialogue state tracking (DST), dialogue policy learning, and natural language generation (NLG). These models are implemented using ConvLab-2, an open-source task-oriented dialog system toolkit.\n",
      "Question : for the text CrossWOZ can be used in different tasks or settings of a task-oriented dialogue system. To facilitate further research, we provided benchmark models for different components of a pipelined task-oriented dialogue system (Figure FIGREF32), including natural language understanding (NLU), dialogue state tracking (DST), dialogue policy learning, and natural language generation (NLG). These models are implemented using ConvLab-2 BIBREF21, an open-source task-oriented dialog system toolkit. We also provided a rule-based user simulator, which can be used to train dialogue policy and generate simulated dialogue data. The benchmark models and simulator will greatly facilitate researchers to compare and evaluate their models on our corpus. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "29\n",
      "Question 1: What does the state consist of in the adapted vanilla policy used for dialogue policy?\n",
      "\n",
      "Answer 1: The state in the adapted vanilla policy used for dialogue policy consists of the last system dialogue acts, last user dialogue acts, system state of the current turn, the number of entities that satisfy the constraints in the current domain, and a terminal signal indicating whether the user goal is completed.\n",
      "Question : for the text Task: Dialogue policy receives state $s$ and outputs system action $a$ at each turn. Compared with the state given by a dialogue state tracker, $s$ may have more information, such as the last user dialogue acts and the entities provided by the backend database..Model: We adapted a vanilla policy trained in a supervised fashion from ConvLab-2 (SL policy). The state $s$ consists of the last system dialogue acts, last user dialogue acts, system state of the current turn, the number of entities that satisfy the constraints in the current domain, and a terminal signal indicating whether the user goal is completed. The action $a$ is delexicalized dialogue acts of current turn which ignores the exact values of the slots, where the values will be filled back after prediction..Result Analysis: As illustrated in Table TABREF31, there is a large gap between F1 score of exact dialogue act and F1 score of delexicalized dialogue act, which means we need a powerful system state tracker to find correct entities. The result also shows that cross multi-domain dialogues (CM and CM+T) are harder for system dialogue act prediction. Additionally, when there is \"Select\" intent in preceding user dialogue acts, the F1 score of exact dialogue act and delexicalized dialogue act are 41.53% and 54.39% respectively. This shows that the policy performs poorly for cross-domain transition. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "30\n",
      "Question 1: Which model performed better in terms of joint state accuracy in the experiment?\n",
      "\n",
      "Answer 1: RuleDST performed better than TRADE in terms of joint state accuracy in the experiment.\n",
      "Question : for the text Task: Dialogue state tracking is responsible for recognizing user goals from the dialogue context and then encoding the goals into the pre-defined system state. Traditional state tracking models take as input user dialogue acts parsed by natural language understanding modules, while recently there are joint models obtaining the system state directly from the context..Model: We implemented a rule-based model (RuleDST) and adapted TRADE (Transferable Dialogue State Generator) BIBREF19 in this experiment. RuleDST takes as input the previous system state and the last user dialogue acts. Then, the system state is updated according to hand-crafted rules. For example, If one of user dialogue acts is (intent=Inform, domain=Attraction, slot=fee, value=free), then the value of the \"fee\" slot in the attraction domain will be filled with \"free\". TRADE generates the system state directly from all the previous utterances using a copy mechanism. As mentioned in Section SECREF18, the first query of the system often records full user constraints, while the last one records relaxed constraints for recommendation. Thus the last one involves system policy, which is out of the scope of state tracking. We used the first query for these models and left state tracking with recommendation for future work..Result Analysis: We evaluated the joint state accuracy (percentage of exact matching) of these two models (Table TABREF31). TRADE, the state-of-the-art model on MultiWOZ, performs poorly on our dataset, indicating that more powerful state trackers are necessary. At the test stage, RuleDST can access the previous gold system state and user dialogue acts, which leads to higher joint state accuracy than TRADE. Both models perform worse on cross multi-domain dialogues (CM and CM+T). To evaluate the ability of modeling cross-domain transition, we further calculated joint state accuracy for those turns that receive \"Select\" intent from users (e.g., \"Find a hotel near the attraction\"). The performances are 11.6% and 12.0% for RuleDST and TRADE respectively, showing that they are not able to track domain transition well. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "31\n",
      "Question 1: What were the three configurations explored for evaluating the performance of the dialogue system?\n",
      "\n",
      "Answer 1: The three configurations explored for evaluating the performance of the dialogue system were simulation at dialogue act level using RuleDST and SL policy, simulation at natural language level using TemplateNLG and BERTNLU, and simulation at natural language level using SC-LSTM instead of TemplateNLG.\n",
      "Question : for the text In addition to corpus-based evaluation for each module, we also evaluated the performance of a whole dialogue system using the user simulator as described above. Three configurations were explored:.Simulation at dialogue act level. As shown by the dashed connections in Figure FIGREF32, we used the aforementioned simulator at the user side and assembled the dialogue system with RuleDST and SL policy..Simulation at natural language level using TemplateNLG. As shown by the solid connections in Figure FIGREF32, the simulator and the dialogue system were equipped with BERTNLU and TemplateNLG additionally..Simulation at natural language level using SC-LSTM. TemplateNLG was replaced with SC-LSTM in the second configuration..When all the slots in a user goal are filled by real values, the simulator terminates. This is regarded as \"task finish\". It's worth noting that \"task finish\" does not mean the task is success, because the system may provide wrong information. We calculated \"task finish rate\" on 1000 times simulations for each goal type (See Table TABREF31). Findings are summarized below:.Cross multi-domain tasks (CM and CM+T) are much harder to finish. Comparing M and M+T, although each module performs well in traffic domains, additional sub-goals in these domains are still difficult to accomplish..The system-level performance is largely limited by RuleDST and SL policy. Although the corpus-based performance of NLU and NLG modules is high, the two modules still harm the performance. Thus more powerful models are needed for all components of a pipelined dialogue system..TemplateNLG has a much lower BLEU score but performs better than SC-LSTM in natural language level simulation. This may be attributed to that BERTNLU prefers templates retrieved from the training set. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "32\n",
      "Question 1: What is the difference between TemplateNLG and SC-LSTM in natural language generation?\n",
      "\n",
      "Answer 1: The main difference between TemplateNLG and SC-LSTM is in how they generate sentences. TemplateNLG uses pre-existing templates with placeholders for specific information, while SC-LSTM generates sentences based on common patterns. This results in SC-LSTM having higher BLEU scores, but TemplateNLG retrieving more specific information.\n",
      "Question : for the text Task: Natural language generation transforms a structured dialogue act into a natural language sentence. It usually takes delexicalized dialogue acts as input and generates a template-style sentence that contains placeholders for slots. Then, the placeholders will be replaced by the exact values, which is called lexicalization..Model: We provided a template-based model (named TemplateNLG) and SC-LSTM (Semantically Conditioned LSTM) BIBREF1 for natural language generation. For TemplateNLG, we extracted templates from the training set and manually added some templates for infrequent dialogue acts. For SC-LSTM we adapted the implementation on MultiWOZ and trained two SC-LSTM with system-side and user-side utterances respectively..Result Analysis: We calculated corpus-level BLEU as used by BIBREF1. We took all utterances with the same delexcalized dialogue acts as references (100 references on average), which results in high BLEU score. For user-side utterances, the BLEU score for TemplateNLG is 0.5780, while the BLEU score for SC-LSTM is 0.7858. For system-side, the two scores are 0.6828 and 0.8595. As exemplified in Table TABREF39, the gap between the two models can be attributed to that SC-LSTM generates common pattern while TemplateNLG retrieves original sentence which has more specific information. We do not provide BLEU scores for different goal types (namely, S, M, CM, etc.) because BLEU scores on different corpus are not comparable. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "33\n",
      "What pre-trained model was used for initialization in this study?\n",
      "\n",
      "The study used Chinese pre-trained BERT for initialization and later fine-tuned the parameters on CrossWOZ.\n",
      "Question : for the text Task: The natural language understanding component in a task-oriented dialogue system takes an utterance as input and outputs the corresponding semantic representation, namely, a dialogue act. The task can be divided into two sub-tasks: intent classification that decides the intent type of an utterance, and slot tagging which identifies the value of a slot..Model: We adapted BERTNLU from ConvLab-2. BERT BIBREF22 has shown strong performance in many NLP tasks. We use Chinese pre-trained BERT BIBREF23 for initialization and then fine-tune the parameters on CrossWOZ. We obtain word embeddings and the sentence representation (embedding of [CLS]) from BERT. Since there may exist more than one intent in an utterance, we modify the traditional method accordingly. For dialogue acts of inform and recommend intents such as (intent=Inform, domain=Attraction, slot=fee, value=free) whose values appear in the sentence, we perform sequential labeling using an MLP which takes word embeddings (\"free\") as input and outputs tags in BIO schema (\"B-Inform-Attraction-fee\"). For each of the other dialogue acts (e.g., (intent=Request, domain=Attraction, slot=fee)) that do not have actual values, we use another MLP to perform binary classification on the sentence representation to predict whether the sentence should be labeled with this dialogue act. To incorporate context information, we use the same BERT to get the embedding of last three utterances. We separate the utterances with [SEP] tokens and insert a [CLS] token at the beginning. Then each original input of the two MLP is concatenated with the context embedding (embedding of [CLS]), serving as the new input. We also conducted an ablation test by removing context information. We trained models with both system-side and user-side utterances..Result Analysis: The results of the dialogue act prediction (F1 score) are shown in Table TABREF31. We further tested the performance on different intent types, as shown in Table TABREF35. In general, BERTNLU performs well with context information. The performance on cross multi-domain dialogues (CM and CM+T) drops slightly, which may be due to the decrease of \"General\" intent and the increase of \"NoOffer\" as well as \"Select\" intent in the dialogue data. We also noted that the F1 score of \"Select\" intent is remarkably lower than those of other types, but context information can improve the performance significantly. Since recognizing domain transition is a key factor for a cross-domain dialogue system, natural language understanding models need to utilize context information more effectively. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "34\n",
      "Question 1: What are the evaluation metrics used to assess the rule-based user simulator's performance?\n",
      "\n",
      "Answer 1: Joint state accuracy was used to evaluate the prediction of user state, while F1 score was used to evaluate the prediction of user dialogue acts.\n",
      "Question : for the text Task: A user simulator imitates the behavior of users, which is useful for dialogue policy learning and automatic evaluation. A user simulator at dialogue act level (e.g., the \"Usr Policy\" in Figure FIGREF32) receives the system dialogue acts and outputs user dialogue acts, while a user simulator at natural language level (e.g., the left part in Figure FIGREF32) directly takes system's utterance as input and outputs user's utterance..Model: We built a rule-based user simulator that works at dialogue act level. Different from agenda-based BIBREF24 user simulator that maintains a stack-like agenda, our simulator maintains the user state straightforwardly (Section SECREF17). The simulator will generate a user goal as described in Section SECREF14. At each user turn, the simulator receives system dialogue acts, modifies its state, and outputs user dialogue acts according to some hand-crafted rules. For example, if the system inform the simulator that the attraction is free, then the simulator will fill the \"fee\" slot in the user state with \"free\", and ask for the next empty slot such as \"address\". The simulator terminates when all requestable slots are filled, and all cross-domain informable slots are filled by real values..Result Analysis: During the evaluation, we initialized the user state of the simulator using the previous gold user state. The input to the simulator is the gold system dialogue acts. We used joint state accuracy (percentage of exact matching) to evaluate user state prediction and F1 score to evaluate the prediction of user dialogue acts. The results are presented in Table TABREF31. We can observe that the performance on complex dialogues (CM and CM+T) is remarkably lower than that on simple ones (S, M, and M+T). This simple rule-based simulator is provided to facilitate dialogue policy learning and automatic evaluation, and our corpus supports the development of more elaborated simulators as we provide the annotation of user-side dialogue states and dialogue acts. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "35\n",
      "Question 1: What does CrossWOZ dataset contain?\n",
      "\n",
      "Answer 1: CrossWOZ is the first large-scale Chinese Cross-Domain task-oriented dialogue dataset that contains 6K dialogues and 102K utterances for 5 domains, with the annotation of dialogue states and dialogue acts at both user and system sides.\n",
      "Question : for the text In this paper, we present the first large-scale Chinese Cross-Domain task-oriented dialogue dataset, CrossWOZ. It contains 6K dialogues and 102K utterances for 5 domains, with the annotation of dialogue states and dialogue acts at both user and system sides. About 60% of the dialogues have cross-domain user goals, which encourage natural transition between related domains. Thanks to the rich annotation of dialogue states and dialogue acts at both user side and system side, this corpus provides a new testbed for a wide range of tasks to investigate cross-domain dialogue modeling, such as dialogue state tracking, policy learning, etc. Our experiments show that the cross-domain constraints are challenging for all these tasks. The transition between related domains is especially challenging to model. Besides corpus-based component-wise evaluation, we also performed system-level evaluation with a user simulator, which requires more powerful models for all components of a pipelined cross-domain dialogue system. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "36\n",
      "Question 1: What makes the collected dialogues in the corpus unique?\n",
      "\n",
      "Answer 1: The collected dialogues in the corpus are unique because they are designed with complex user goals that favor inter-domain dependency and natural transition between multiple domains, making them more complex and natural for cross-domain dialogue tasks. Additionally, a well-controlled, synchronous setting is applied to collect human-to-human dialogues, ensuring the high quality of the collected dialogues. Furthermore, explicit annotations are provided at not only the system side but also the user side, which allows for easier modeling of user behaviors or development of user simulators.\n",
      "Question : for the text Our corpus is unique in the following aspects:.Complex user goals are designed to favor inter-domain dependency and natural transition between multiple domains. In return, the collected dialogues are more complex and natural for cross-domain dialogue tasks..A well-controlled, synchronous setting is applied to collect human-to-human dialogues. This ensures the high quality of the collected dialogues..Explicit annotations are provided at not only the system side but also the user side. This feature allows us to model user behaviors or develop user simulators more easily. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "37\n",
      "Question 1: What is the purpose of the multi-domain goal generator in this corpus?\n",
      "\n",
      "Answer 1: The multi-domain goal generator was designed based on the database to generate natural language descriptions for each structured goal, which is used by well-trained workers to converse and achieve given goals.\n",
      "Question : for the text Our corpus is to simulate scenarios where a traveler seeks tourism information and plans her or his travel in Beijing. Domains include hotel, attraction, restaurant, metro, and taxi. The data collection process is summarized as below:.Database Construction: we crawled travel information in Beijing from the Web, including Hotel, Attraction, and Restaurant domains (hereafter we name the three domains as HAR domains). Then, we used the metro information of entities in HAR domains to build the metro database. For the taxi domain, there is no need to store the information. Instead, we can call the API directly if necessary..Goal Generation: a multi-domain goal generator was designed based on the database. The relation across domains is captured in two ways. One is to constrain two targets that locate near each other. The other is to use a taxi or metro to commute between two targets in HAR domains mentioned in the context. To make workers understand the task more easily, we crafted templates to generate natural language descriptions for each structured goal..Dialogue Collection: before the formal data collection starts, we required the workers to make a small number of dialogues and gave them feedback about the dialogue quality. Then, well-trained workers were paired to converse according to the given goals. The workers were also asked to annotate both user states and system states..Dialogue Annotation: we used some rules to automatically annotate dialogue acts according to user states, system states, and dialogue histories. To evaluate the quality of the annotation of dialogue acts and states, three experts were employed to manually annotate dialogue acts and states for 50 dialogues. The results show that our annotations are of high quality. Finally, each dialogue contains a structured goal, a task description, user states, system states, dialogue acts, and utterances. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "38\n",
      "Question 1: What types of slots were collected for each entity in Beijing?\n",
      "Answer 1: Three types of slots were collected for each entity in Beijing: common slots such as name and address, binary slots for hotel services such as wake-up call, and nearby attractions/restaurants/hotels slots that contain nearby entities in the attraction, restaurant, and hotel domains.\n",
      "Question : for the text We collected 465 attractions, 951 restaurants, and 1,133 hotels in Beijing from the Web. Some statistics are shown in Table TABREF11. There are three types of slots for each entity: common slots such as name and address; binary slots for hotel services such as wake-up call; nearby attractions/restaurants/hotels slots that contain nearby entities in the attraction, restaurant, and hotel domains. Since it is not usual to find another nearby hotel in the hotel domain, we did not collect such information. This nearby relation allows us to generate natural cross-domain goals, such as \"find another attraction near the first one\" and \"find a restaurant near the attraction\". Nearest metro stations of HAR entities form the metro database. In contrast, we provided the pseudo car type and plate number for the taxi domain. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "39\n",
      "Question 1: How were the dialogue acts for both user and system sides labelled?\n",
      "\n",
      "Answer 1: The dialogue acts for the user side were mainly derived from the selection of semantic tuples containing information of domain, slot, and value. Meanwhile, the system side mainly applied keyword matching to label dialogue acts. The experts also manually annotated dialogue acts and states with an average dialogue act F1 of 94.59% and an average state accuracy of 93.55%.\n",
      "Question : for the text After collecting the conversation data, we used some rules to annotate dialogue acts automatically. Each utterance can have several dialogue acts. Each dialogue act is a tuple that consists of intent, domain, slot, and value. We pre-define 6 types of intents and use the update of the user state and system state as well as keyword matching to obtain dialogue acts. For the user side, dialogue acts are mainly derived from the selection of semantic tuples that contain the information of domain, slot, and value. For example, if (1, Attraction, fee, free) in Table TABREF13 is selected by the user, then (Inform, Attraction, fee, free) is labelled. If (1, Attraction, name, ) is selected, then (Request, Attraction, name, none) is labelled. If (2, Hotel, name, near (id=1)) is selected, then (Select, Hotel, src_domain, Attraction) is labelled. This intent is specially designed for the \"nearby\" constraint. For the system side, we mainly applied keyword matching to label dialogue acts. Inform intent is derived by matching the system utterance with the information of selected entities. When the wizard selects multiple retrieved entities and recommend them, Recommend intent is labeled. When the wizard expresses that no result satisfies user constraints, NoOffer is labeled. For General intents such as \"goodbye\", \"thanks\" at both user and system sides, keyword matching is applied..We also obtained a binary label for each semantic tuple in the user state, which indicates whether this semantic tuple has been selected to be expressed by the user. This annotation directly illustrates the progress of the conversation..To evaluate the quality of the annotation of dialogue acts and states (both user and system states), three experts were employed to manually annotate dialogue acts and states for the same 50 dialogues (806 utterances), 10 for each goal type (see Section SECREF4). Since dialogue act annotation is not a classification problem, we didn't use Fleiss' kappa to measure the agreement among experts. We used dialogue act F1 and state accuracy to measure the agreement between each two experts' annotations. The average dialogue act F1 is 94.59% and the average state accuracy is 93.55%. We then compared our annotations with each expert's annotations which are regarded as gold standard. The average dialogue act F1 is 95.36% and the average state accuracy is 94.95%, which indicates the high quality of our annotations. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "40\n",
      "Question 1: How did the researchers ensure that the workers were well-trained before data collection?\n",
      "\n",
      "Answer 1: Before the formal data collection, the researchers trained the workers to complete a small number of dialogues by giving them feedback. This was done to ensure that the 90 participants were well-trained before they participated in the data collection.\n",
      "Question : for the text We developed a specialized website that allows two workers to converse synchronously and make annotations online. On the website, workers are free to choose one of the two roles: tourist (user) or system (wizard). Then, two paired workers are sent to a chatroom. The user needs to accomplish the allocated goal through conversation while the wizard searches the database to provide the necessary information and gives responses. Before the formal data collection, we trained the workers to complete a small number of dialogues by giving them feedback. Finally, 90 well-trained workers are participating in the data collection..In contrast, MultiWOZ BIBREF12 hired more than a thousand workers to converse asynchronously. Each worker received a dialogue context to review and need to respond for only one turn at a time. The collected dialogues may be incoherent because workers may not understand the context correctly and multiple workers contributed to the same dialogue session, possibly leading to more variance in the data quality. For example, some workers expressed two mutually exclusive constraints in two consecutive user turns and failed to eliminate the system's confusion in the next several turns. Compared with MultiWOZ, our synchronous conversation setting may produce more coherent dialogues. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "41\n",
      "Question 1: What are the three main actions that the user needs to take at each turn in a dialogue?\n",
      "\n",
      "Answer 1: The user needs to (1) modify the user state based on the system response at the preceding turn, (2) select semantic tuples in the user state to indicate dialogue acts, and (3) compose an utterance based on the selected semantic tuples.\n",
      "Question : for the text The user state is the same as the user goal before a conversation starts. At each turn, the user needs to 1) modify the user state according to the system response at the preceding turn, 2) select some semantic tuples in the user state, which indicates the dialogue acts, and 3) compose the utterance according to the selected semantic tuples. In addition to filling the required values and updating cross-domain informable slots with real values in the user state, the user is encouraged to modify the constraints when there is no result under such constraints. The change will also be recorded in the user state. Once the goal is completed (all the values in the user state are filled), the user can terminate the dialogue. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "42\n",
      "Question 1: What is recorded in the first query of the database?\n",
      "\n",
      "Answer 1: The first query of the database records the original user constraints.\n",
      "Question : for the text We regard the database query as the system state, which records the constraints of each domain till the current turn. At each turn, the wizard needs to 1) fill the query according to the previous user response and search the database if necessary, 2) select the retrieved entities, and 3) respond in natural language based on the information of the selected entities. If none of the entities satisfy all the constraints, the wizard will try to relax some of them for a recommendation, resulting in multiple queries. The first query records original user constraints while the last one records the constraints relaxed by the system. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "43\n",
      "Question 1: How many sub-goals can a goal have at most?\n",
      "Answer 1: A goal can have at most five sub-goals.\n",
      "Question : for the text To avoid generating overly complex goals, each goal has at most five sub-goals. To generate more natural goals, the sub-goals can be of the same domain, such as two attractions near each other. The goal is represented as a list of (sub-goal id, domain, slot, value) tuples, named as semantic tuples. The sub-goal id is used to distinguish sub-goals which may be in the same domain. There are two types of slots: informable slots which are the constraints that the user needs to inform the system, and requestable slots which are the information that the user needs to inquire from the system. As shown in Table TABREF13, besides common informable slots (italic values) whose values are determined before the conversation, we specially design cross-domain informable slots (bold values) whose values refer to other sub-goals. Cross-domain informable slots utilize sub-goal id to connect different sub-goals. Thus the actual constraints vary according to the different contexts instead of being pre-specified. The values of common informable slots are sampled randomly from the database. Based on the informable slots, users are required to gather the values of requestable slots (blank values in Table TABREF13) through conversation..There are four steps in goal generation. First, we generate independent sub-goals in HAR domains. For each domain in HAR domains, with the same probability $\\mathcal {P}$ we generate a sub-goal, while with the probability of $1-\\mathcal {P}$ we do not generate any sub-goal for this domain. Each sub-goal has common informable slots and requestable slots. As shown in Table TABREF15, all slots of HAR domains can be requestable slots, while the slots with an asterisk can be common informable slots..Second, we generate cross-domain sub-goals in HAR domains. For each generated sub-goal (e.g., the attraction sub-goal in Table TABREF13), if its requestable slots contain \"nearby hotels\", we generate an additional sub-goal in the hotel domain (e.g., the hotel sub-goal in Table TABREF13) with the probability of $\\mathcal {P}_{attraction\\rightarrow hotel}$. Of course, the selected hotel must satisfy the nearby relation to the attraction entity. Similarly, we do not generate any additional sub-goal in the hotel domain with the probability of $1-\\mathcal {P}_{attraction\\rightarrow hotel}$. This also works for the attraction and restaurant domains. $\\mathcal {P}_{hotel\\rightarrow hotel}=0$ since we do not allow the user to find the nearby hotels of one hotel..Third, we generate sub-goals in the metro and taxi domains. With the probability of $\\mathcal {P}_{taxi}$, we generate a sub-goal in the taxi domain (e.g., the taxi sub-goal in Table TABREF13) to commute between two entities of HAR domains that are already generated. It is similar for the metro domain and we set $\\mathcal {P}_{metro}=\\mathcal {P}_{taxi}$. All slots in the metro or taxi domain appear in the sub-goals and must be filled. As shown in Table TABREF15, from and to slots are always cross-domain informable slots, while others are always requestable slots..Last, we rearrange the order of the sub-goals to generate more natural and logical user goals. We require that a sub-goal should be followed by its referred sub-goal as immediately as possible..To make the workers aware of this cross-domain feature, we additionally provide a task description for each user goal in natural language, which is generated from the structured goal by hand-crafted templates..Compared with the goals whose constraints are all pre-specified, our goals impose much more dependency between different domains, which will significantly influence the conversation. The exact values of cross-domain informable slots are finally determined according to the dialogue context. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "44\n",
      "What are the features of CrossWOZ dataset compared to other corpora? \n",
      "\n",
      "CrossWOZ dataset has the following features compared to other corpora: \n",
      "- The dependency between domains is more challenging because the choice in one domain will affect the choices in related domains in CrossWOZ.\n",
      "- It is the first Chinese corpus that contains large-scale multi-domain task-oriented dialogues, consisting of 6K sessions and 102K utterances for 5 domains.\n",
      "- Annotation of dialogue states and dialogue acts is provided for both the system side and user side.\n",
      "- To facilitate model comparison, benchmark models are provided for different modules in pipelined task-oriented dialogue systems.\n",
      "Question : for the text Recently, there have been a variety of task-oriented dialogue models thanks to the prosperity of neural architectures BIBREF0, BIBREF1, BIBREF2, BIBREF3, BIBREF4, BIBREF5. However, the research is still largely limited by the availability of large-scale high-quality dialogue data. Many corpora have advanced the research of task-oriented dialogue systems, most of which are single domain conversations, including ATIS BIBREF6, DSTC 2 BIBREF7, Frames BIBREF8, KVRET BIBREF9, WOZ 2.0 BIBREF10 and M2M BIBREF11..Despite the significant contributions to the community, these datasets are still limited in size, language variation, or task complexity. Furthermore, there is a gap between existing dialogue corpora and real-life human dialogue data. In real-life conversations, it is natural for humans to transition between different domains or scenarios while still maintaining coherent contexts. Thus, real-life dialogues are much more complicated than those dialogues that are only simulated within a single domain. To address this issue, some multi-domain corpora have been proposed BIBREF12, BIBREF13. The most notable corpus is MultiWOZ BIBREF12, a large-scale multi-domain dataset which consists of crowdsourced human-to-human dialogues. It contains 10K dialogue sessions and 143K utterances for 7 domains, with annotation of system-side dialogue states and dialogue acts. However, the state annotations are noisy BIBREF14, and user-side dialogue acts are missing. The dependency across domains is simply embodied in imposing the same pre-specified constraints on different domains, such as requiring both a hotel and an attraction to locate in the center of the town..In comparison to the abundance of English dialogue data, surprisingly, there is still no widely recognized Chinese task-oriented dialogue corpus. In this paper, we propose CrossWOZ, a large-scale Chinese multi-domain (cross-domain) task-oriented dialogue dataset. An dialogue example is shown in Figure FIGREF1. We compare CrossWOZ to other corpora in Table TABREF5 and TABREF6. Our dataset has the following features comparing to other corpora (particularly MultiWOZ BIBREF12):.The dependency between domains is more challenging because the choice in one domain will affect the choices in related domains in CrossWOZ. As shown in Figure FIGREF1 and Table TABREF6, the hotel must be near the attraction chosen by the user in previous turns, which requires more accurate context understanding..It is the first Chinese corpus that contains large-scale multi-domain task-oriented dialogues, consisting of 6K sessions and 102K utterances for 5 domains (attraction, restaurant, hotel, metro, and taxi)..Annotation of dialogue states and dialogue acts is provided for both the system side and user side. The annotation of user states enables us to track the conversation from the user's perspective and can empower the development of more elaborate user simulators..In this paper, we present the process of dialogue collection and provide detailed data analysis of the corpus. Statistics show that our cross-domain dialogues are complicated. To facilitate model comparison, benchmark models are provided for different modules in pipelined task-oriented dialogue systems, including natural language understanding, dialogue state tracking, dialogue policy learning, and natural language generation. We also provide a user simulator, which will facilitate the development and evaluation of dialogue models on this corpus. The corpus and the benchmark models are publicly available at https://github.com/thu-coai/CrossWOZ. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "45\n",
      "Question 1: What are the three categories for the collection methods of task-oriented dialogue datasets?\n",
      "\n",
      "Answer 1: The three categories are human-to-human dialogues, human-to-machine dialogues, and machine-to-machine dialogues.\n",
      "Question : for the text According to whether the dialogue agent is human or machine, we can group the collection methods of existing task-oriented dialogue datasets into three categories. The first one is human-to-human dialogues. One of the earliest and well-known ATIS dataset BIBREF6 used this setting, followed by BIBREF8, BIBREF9, BIBREF10, BIBREF15, BIBREF16 and BIBREF12. Though this setting requires many human efforts, it can collect natural and diverse dialogues. The second one is human-to-machine dialogues, which need a ready dialogue system to converse with humans. The famous Dialogue State Tracking Challenges provided a set of human-to-machine dialogue data BIBREF17, BIBREF7. The performance of the dialogue system will largely influence the quality of dialogue data. The third one is machine-to-machine dialogues. It needs to build both user and system simulators to generate dialogue outlines, then use templates BIBREF3 to generate dialogues or further employ people to paraphrase the dialogues to make them more natural BIBREF11, BIBREF13. It needs much less human effort. However, the complexity and diversity of dialogue policy are limited by the simulators. To explore dialogue policy in multi-domain scenarios, and to collect natural and diverse dialogues, we resort to the human-to-human setting..Most of the existing datasets only involve single domain in one dialogue, except MultiWOZ BIBREF12 and Schema BIBREF13. MultiWOZ dataset has attracted much attention recently, due to its large size and multi-domain characteristics. It is at least one order of magnitude larger than previous datasets, amounting to 8,438 dialogues and 115K turns in the training set. It greatly promotes the research on multi-domain dialogue modeling, such as policy learning BIBREF18, state tracking BIBREF19, and context-to-text generation BIBREF20. Recently the Schema dataset is collected in a machine-to-machine fashion, resulting in 16,142 dialogues and 330K turns for 16 domains in the training set. However, the multi-domain dependency in these two datasets is only embodied in imposing the same pre-specified constraints on different domains, such as requiring a restaurant and an attraction to locate in the same area, or the city of a hotel and the destination of a flight to be the same (Table TABREF6)..Table TABREF5 presents a comparison between our dataset with other task-oriented datasets. In comparison to MultiWOZ, our dataset has a comparable scale: 5,012 dialogues and 84K turns in the training set. The average number of domains and turns per dialogue are larger than those of MultiWOZ, which indicates that our task is more complex. The cross-domain dependency in our dataset is natural and challenging. For example, as shown in Table TABREF6, the system needs to recommend a hotel near the attraction chosen by the user in previous turns. Thus, both system recommendation and user selection will dynamically impact the dialogue. We also allow the same domain to appear multiple times in a user goal since a tourist may want to go to more than one attraction..To better track the conversation flow and model user dialogue policy, we provide annotation of user states in addition to system states and dialogue acts. While the system state tracks the dialogue history, the user state is maintained by the user and indicates whether the sub-goals have been completed, which can be used to predict user actions. This information will facilitate the construction of the user simulator..To the best of our knowledge, CrossWOZ is the first large-scale Chinese dataset for task-oriented dialogue systems, which will largely alleviate the shortage of Chinese task-oriented dialogue corpora that are publicly available. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "46\n",
      "Question 1: What is the total number of dialogues collected after removing uncompleted dialogues?\n",
      "\n",
      "Answer 1: After removing uncompleted dialogues, we collected 6,012 dialogues in total.\n",
      "Question : for the text After removing uncompleted dialogues, we collected 6,012 dialogues in total. The dataset is split randomly for training/validation/test, where the statistics are shown in Table TABREF25. The average number of sub-goals in our dataset is 3.24, which is much larger than that in MultiWOZ (1.80) BIBREF12 and Schema (1.84) BIBREF13. The average number of turns (16.9) is also larger than that in MultiWOZ (13.7). These statistics indicate that our dialogue data are more complex..According to the type of user goal, we group the dialogues in the training set into five categories:.417 dialogues have only one sub-goal in HAR domains..1573 dialogues have multiple sub-goals (2$\\sim $3) in HAR domains. However, these sub-goals do not have cross-domain informable slots..691 dialogues have multiple sub-goals in HAR domains and at least one sub-goal in the metro or taxi domain (3$\\sim $5 sub-goals). The sub-goals in HAR domains do not have cross-domain informable slots..1,759 dialogues have multiple sub-goals (2$\\sim $5) in HAR domains with cross-domain informable slots..572 dialogues have multiple sub-goals in HAR domains with cross-domain informable slots and at least one sub-goal in the metro or taxi domain (3$\\sim $5 sub-goals)..The data statistics are shown in Table TABREF26. As mentioned in Section SECREF14, we generate independent multi-domain, cross multi-domain, and traffic domain sub-goals one by one. Thus in terms of the task complexity, we have S<M<CM and M<M+T<CM+T, which is supported by the average number of sub-goals, semantic tuples, and turns per dialogue in Table TABREF26. The average number of tokens also becomes larger when the goal becomes more complex. About 60% of dialogues (M+T, CM, and CM+T) have cross-domain informable slots. Because of the limit of maximal sub-goals number, the ratio of dialogue number of CM+T to CM is smaller than that of M+T to M..CM and CM+T are much more challenging than other tasks because additional cross-domain constraints in HAR domains are strict and will result in more \"NoOffer\" situations (i.e., the wizard finds no result that satisfies the current constraints). In this situation, the wizard will try to relax some constraints and issue multiple queries to find some results for a recommendation while the user will compromise and change the original goal. The negotiation process is captured by \"NoOffer rate\", \"Multi-query rate\", and \"Goal change rate\" in Table TABREF26. In addition, \"Multi-query rate\" suggests that each sub-goal in M and M+T is as easy to finish as the goal in S..The distribution of dialogue length is shown in Figure FIGREF27, which is an indicator of the task complexity. Most single-domain dialogues terminate within 10 turns. The curves of M and M+T are almost of the same shape, which implies that the traffic task requires two additional turns on average to complete the task. The curves of CM and CM+T are less similar. This is probably because CM goals that have 5 sub-goals (about 22%) can not further generate a sub-goal in traffic domains and become CM+T goals. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "47\n",
      "What is Bertram and how does it improve over a BERT model without special handling of rare words?\n",
      "\n",
      "Bertram is a novel architecture for relearning high-quality representations of rare words by replacing important words with rare synonyms and deeply connecting surface-form and context information. It improves over a BERT model without special handling of rare words on various downstream task datasets focusing on rare words, demonstrating the usefulness of the proposed method.\n",
      "Question : for the text We have introduced Bertram, a novel architecture for relearning high-quality representations of rare words. This is achieved by employing a powerful pretrained language model and deeply connecting surface-form and context information. By replacing important words with rare synonyms, we have created various downstream task datasets focusing on rare words; on all of these datasets, Bertram improves over a BERT model without special handling of rare words, demonstrating the usefulness of our proposed method..As our analysis has shown that even for the most frequent words considered, using Bertram is still beneficial, future work might further investigate the limits of our proposed method. Furthermore, it would be interesting to explore more complex ways of incorporating surface-form information – e.g., by using a character-level CNN similar to the one of BIBREF27 – to balance out the potency of Bertram's form and context parts. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "48\n",
      "What is the substitution dictionary $S$ used for in the experiment?\n",
      "\n",
      "The substitution dictionary $S$ is used to create synonym replacements for rare words, using WordNet and a pattern library, to ensure that all synonyms have consistent parts of speech. It is also supplemented with misspellings from a dataset, which are only assigned to randomly selected 10% of the words in each sentence to prevent domination.\n",
      "Question : for the text To measure the effect of adding Bertram to BERT on downstream tasks, we apply the procedure described in Section SECREF4 to a commonly used textual entailment dataset as well as two text classification datasets: MNLI BIBREF21, AG's News BIBREF22 and DBPedia BIBREF23. For all three datasets, we use BERT$_\\text{base}$ as a baseline model and create the substitution dictionary $S$ using the synonym relation of WordNet BIBREF20 and the pattern library BIBREF34 to make sure that all synonyms have consistent parts of speech. As an additional source of word substitutions, we make use of the misspellings dataset of BIBREF25, which is based on query logs of a search engine. To prevent misspellings from dominating the resulting dataset, we only assign misspelling-based substitutes to randomly selected 10% of the words contained in each sentence. Motivated by the results on WNLaMPro-medium, we consider every word that occurs less than 100 times in the WWC and our BooksCorpus replica combined as being rare. Some examples of entries in the resulting datasets can be seen in Table TABREF35..Just like for WNLaMPro, our default way of injecting Bertram embeddings into the baseline model is to replace the sequence of uncontextualized WordPiece tokens for a given rare word with its Bertram-based embedding. That is, given a sequence of uncontextualized token embeddings $\\mathbf {e} = e_1, \\ldots , e_n$ where $e_{i}, \\ldots , e_{i+j}$ with $1 \\le i \\le i+j \\le n$ is the sequence of WordPiece embeddings for a single rare word $w$, we replace $\\mathbf {e}$ with.By default, the set of contexts $\\mathcal {C}$ required for this replacement is obtained by collecting all sentences from the WWC and BooksCorpus in which $w$ occurs. As our model architecture allows us to easily include new contexts without requiring any additional training, we also try a variant where we add in-domain contexts by giving the model access to the texts found in the test set..In addition to the procedure described above, we also try a variant where instead of replacing the original WordPiece embeddings for a given rare word, we merely add the Bertram-based embedding, separating both representations using a single slash:.As it performs best on the rare and medium subsets of WNLaMPro combined, we use only the add-gated variant of Bertram for all datasets. Results can be seen in Table TABREF37, where for each task, we report the accuracy on the entire dataset as well as scores obtained considering only instances where at least one word was replaced by a misspelling or a WordNet synonym, respectively. Consistent with results on WNLaMPro, combining BERT with Bertram outperforms both a standalone BERT model and one combined with Attentive Mimicking across all tasks. While keeping the original BERT embeddings in addition to Bertram's representation brings no benefit, adding in-domain data clearly helps for two out of three datasets. This makes sense as for rare words, every single additional context can be crucial for gaining a deeper understanding..To further understand for which words using Bertram is helpful, in Figure FIGREF39 we look at the accuracy of BERT both with and without Bertram on all three tasks as a function of word frequency. That is, we compute the accuracy scores for both models when considering only entries $(\\mathbf {x}_{w_{i_1} = \\hat{w}_{i_1}, \\ldots , w_{i_k} = \\hat{w}_{i_k} }, y)$ where each substituted word $\\hat{w}_{i_j}$ occurs less than $c_\\text{max}$ times in WWC and BooksCorpus, for various values of $c_\\text{max}$. As one would expect, $c_\\text{max}$ is positively correlated with the accuracies of both models, showing that the rarer a word is, the harder it is to understand. Perhaps more interestingly, for all three datasets the gap between Bertram and BERT remains more or less constant regardless of $c_\\text{max}$. This indicates that using Bertram might also be useful for even more frequent words than the ones considered. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "49\n",
      "Question 1: What is the underlying language model used for Bertram in the experiments?\n",
      "\n",
      "Answer 1: Throughout all of the experiments, the underlying language model used for Bertram is BERT$_\\text{base}$.\n",
      "Question : for the text For our evaluation of Bertram, we largely follow the experimental setup of BIBREF0. Our implementation of Bertram is based on PyTorch BIBREF30 and the Transformers library of BIBREF31. Throughout all of our experiments, we use BERT$_\\text{base}$ as the underlying language model for Bertram. To obtain embeddings for frequent multi-token words during training, we use one-token approximation BIBREF0. Somewhat surprisingly, we found in preliminary experiments that excluding BERT's parameters from the finetuning procedure outlined in Section SECREF17 improves performance while speeding up training; we thus exclude them in the third step of our training procedure..While BERT was trained on BooksCorpus BIBREF32 and a large Wikipedia dump, we follow previous work and train Bertram on only the much smaller Westbury Wikipedia Corpus (WWC) BIBREF33; this of course gives BERT a clear advantage over our proposed method. In order to at least partially compensate for this, in our downstream task experiments we gather the set of contexts $\\mathcal {C}$ for a given rare word from both the WWC and BooksCorpus during inference. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "50\n",
      "What is the goal of the WNLaMPro dataset? \n",
      "\n",
      "The goal of the WNLaMPro dataset is to probe a language model's ability to understand rare words without any task-specific finetuning.\n",
      "Question : for the text We evalute Bertram on the WNLaMPro dataset of BIBREF0. This dataset consists of cloze-style phrases like.and the task is to correctly fill the slot (____) with one of several acceptable target words (e.g., “fruit”, “bush” and “berry”), which requires knowledge of the phrase's keyword (“lingonberry” in the above example). As the goal of this dataset is to probe a language model's ability to understand rare words without any task-specific finetuning, BIBREF0 do not provide a training set. Furthermore, the dataset is partitioned into three subsets; this partition is based on the frequency of the keyword, with keywords occurring less than 10 times in the WWC forming the rare subset, those occurring between 10 and 100 times forming the medium subset, and all remaining words forming the frequent subset. As our focus is on improving representations for rare words, we evaluate our model only on the former two sets..Results on WNLaMPro rare and medium are shown in Table TABREF34, where the mean reciprocal rank (MRR) is reported for BERT, Attentive Mimicking and Bertram. As can be seen, supplementing BERT with any of the proposed relearning methods results in noticeable improvements for the rare subset, with add clearly outperforming replace. Moreover, the add and add-gated variants of Bertram perform surprisingly well for more frequent words, improving the score for WNLaMPro-medium by 50% compared to BERT$_\\text{base}$ and 31% compared to Attentive Mimicking. This makes sense considering that compared to Attentive Mimicking, the key enhancement of Bertram lies in improving context representations and interconnection of form and context; naturally, the more contexts are given, the more this comes into play. Noticeably, despite being both based on and integrated into a BERT$_\\text{base}$ model, our architecture even outperforms a standalone BERT$_\\text{large}$ model by a large margin. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "1\n",
      "Question 1: What are the two properties that text classification datasets should have to measure rare word representations in a contextualized setting?\n",
      "\n",
      "Answer 1: The two properties that text classification datasets should have to measure rare word representations in a contextualized setting are: (1) a model that has no understanding of rare words at all should perform close to 0%, and (2) a model that perfectly understands rare words should be able to classify every instance correctly.\n",
      "Question : for the text To measure the quality of rare word representations in a contextualized setting, we would ideally need text classification datasets with the following two properties:.A model that has no understanding of rare words at all should perform close to 0%..A model that perfectly understands rare words should be able to classify every instance correctly..Unfortunately, this requirement is not even remotely fulfilled by most commonly used datasets, simply because rare words occur in only a few entries and when they do, they are often of negligible importance..To solve this problem, we devise a procedure to automatically transform existing text classification datasets such that rare words become important. For this procedure, we require a pretrained language model $M$ as a baseline, an arbitrary text classification dataset $\\mathcal {D}$ containing labelled instances $(\\mathbf {x}, y)$ and a substitution dictionary $S$, mapping each word $w$ to a set of rare synonyms $S(w)$. Given these ingredients, our procedure consists of three steps: (i) splitting the dataset into a train set and a set of test candidates, (ii) training the baseline model on the train set and (iii) modifying a subset of the test candidates to generate the final test set. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "2\n",
      "Question 1: What is the purpose of randomly replacing 5% of words in the training data with [MASK] tokens during finetuning?\n",
      "\n",
      "Answer 1: The purpose is to allow the model to cope with missing or unknown words, which is important for generating accurate predictions on the final test set.\n",
      "Question : for the text We finetune $M$ on $\\mathcal {D}_\\text{train}$. Let $(\\mathbf {x}, y) \\in \\mathcal {D}_\\text{train}$ where $\\mathbf {x} = w_1, \\ldots , w_n$ is a sequence of words. We deviate from the standard finetuning procedure of BIBREF13 in three respects:.We randomly replace 5% of all words in $\\mathbf {x}$ with a [MASK] token. This allows the model to cope with missing or unknown words, a prerequisite for our final test set generation..As an alternative to overwriting the language model's uncontextualized embeddings for rare words, we also want to allow models to simply add an alternative representation during test time, in which case we simply separate both representations by a slash. To accustom the language model to this duplication of words, we replace each word $w_i$ with “$w_i$ / $w_i$” with a probability of 10%. To make sure that the model does not simply learn to always focus on the first instance during training, we randomly mask each of the two repetitions with probability 25%..We do not finetune the model's embedding layer. In preliminary experiments, we found this not to hurt performance. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "3\n",
      "Question 1: What is the requirement for the size of the training set in the partitioning of $\\mathcal {D}$?\n",
      "Answer 1: The training set in the partitioning of $\\mathcal {D}$ must consist of at least one third of the entire data.\n",
      "Question : for the text We partition $\\mathcal {D}$ into a train set $\\mathcal {D}_\\text{train}$ and a set of test candidates, $\\mathcal {D}_\\text{cand}$, with the latter containing all instances $(\\mathbf {x},y) \\in \\mathcal {D}$ such that for at least one word $w$ in $\\mathbf {x}$, $S(w) \\ne \\emptyset $. Additionally, we require that the training set consists of at least one third of the entire data. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "4\n",
      "Question 1: What is the significance of the rare synonyms in the test set generation process?\n",
      "\n",
      "Answer 1: Rare synonyms are important in the test set generation process because they allow for a better understanding of the model's ability to identify and classify rare words. They are also used in cases where masking a word changes the model's prediction, and without them, the corresponding entry would be discarded. The test set is designed to closely evaluate the baseline model's ability to recognize specific words and their meanings, and the use of rare synonyms helps to achieve this goal.\n",
      "Question : for the text Let $p(y \\mid \\mathbf {x})$ be the probability that the finetuned model $M$ assigns to class $y$ given input $\\mathbf {x}$, and let.be the model's prediction for input $\\mathbf {x}$ where $\\mathcal {Y}$ denotes the set of all labels. For generating our test set, we only consider candidates that are classified correctly by the baseline model, i.e. candidates $(\\mathbf {x}, y) \\in \\mathcal {D}_\\text{cand}$ with $M(\\mathbf {x}) = y$. For each such entry, let $\\mathbf {x} = w_1, \\ldots , w_n$ and let $\\mathbf {x}_{w_i = t}$ be the sequence obtained from $\\mathbf {x}$ by replacing $w_i$ with $t$. We compute.i.e., we select the word $w_i$ whose masking pushes the model's prediction the furthest away from the correct label. If removing this word already changes the model's prediction – that is, $M(\\mathbf {x}_{w_i = \\texttt {[MASK]}}) \\ne y$ –, we select a random rare synonym $\\hat{w}_i \\in S(w_i)$ and add $(\\mathbf {x}_{w_i = \\hat{w}_i}, y)$ to the test set. Otherwise, we repeat the above procedure; if the label still has not changed after masking up to 5 words, we discard the corresponding entry. All so-obtained test set entries $(\\mathbf {x}_{w_{i_1} = \\hat{w}_{i_1}, \\ldots , w_{i_k} = \\hat{w}_{i_k} }, y)$ have the following properties:.If each $w_{i_j}$ is replaced by a [MASK] token, the entry is classified incorrectly by $M$. In other words, understanding the words $w_{i_j}$ is essential for $M$ to determine the correct label..If the model's internal representation of each $\\hat{w}_{i_j}$ is equal to its representation of $w_{i_j}$, the entry is classified correctly by $M$. That is, if the model is able to understand the rare words $\\hat{w}_{i_j}$ and to identify them as synonyms of ${w_{i_j}}$, it predicts the correct label for each instance..It is important to notice that the so-obtained test set is very closely coupled to the baseline model $M$, because we selected the words to replace based on the model's predictions. Importantly, however, the model is never queried with any rare synonym during test set generation, so its representations of rare words are not taken into account for creating the test set. Thus, while the test set is not suitable for comparing $M$ with an entirely different model $M^{\\prime }$, it allows us to compare various strategies for representing rare words in the embedding space of $M$. A similar constraint can be found in the Definitional Nonce dataset BIBREF3, which is tied to a given embedding space based on Word2Vec BIBREF1. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "5\n",
      "What is Bertram and how does it differ from previous approaches for understanding rare words?\n",
      "\n",
      "Bertram is a novel architecture that combines a pretrained BERT language model with Attentive Mimicking to understand rare words. Unlike previous approaches, Bertram integrates BERT in an end-to-end fashion and directly makes use of its hidden states, allowing for a deep connection and exchange of information between both input signals (surface form and context). Additionally, Bertram uses a procedure to automatically transform text classification datasets into datasets for which rare words are guaranteed to be important.\n",
      "Question : for the text As traditional word embedding algorithms BIBREF1 are known to struggle with rare words, several techniques for improving their representations have been proposed over the last few years. These approaches exploit either the contexts in which rare words occur BIBREF2, BIBREF3, BIBREF4, BIBREF5, their surface-form BIBREF6, BIBREF7, BIBREF8, or both BIBREF9, BIBREF10. However, all of these approaches are designed for and evaluated on uncontextualized word embeddings..With the recent shift towards contextualized representations obtained from pretrained deep language models BIBREF11, BIBREF12, BIBREF13, BIBREF14, the question naturally arises whether these approaches are facing the same problem. As all of them already handle rare words implicitly – using methods such as byte-pair encoding BIBREF15 and WordPiece embeddings BIBREF16, or even character-level CNNs BIBREF17 –, it is unclear whether these models even require special treatment of rare words. However, the listed methods only make use of surface-form information, whereas BIBREF9 found that for covering a wide range of rare words, it is crucial to consider both surface-form and contexts..Consistently, BIBREF0 recently showed that for BERT BIBREF13, a popular pretrained language model based on a Transformer architecture BIBREF18, performance on a rare word probing task can significantly be improve by relearning representations of rare words using Attentive Mimicking BIBREF19. However, their proposed model is limited in two important respects:.For processing contexts, it uses a simple bag-of-words model, throwing away much of the available information..It combines form and context only in a shallow fashion, thus preventing both input signals from sharing information in any sophisticated manner..Importantly, this limitation applies not only to their model, but to all previous work on obtaining representations for rare words by leveraging form and context. While using bag-of-words models is a reasonable choice for uncontextualized embeddings, which are often themselves based on such models BIBREF1, BIBREF7, it stands to reason that they are suboptimal for contextualized embeddings based on position-aware deep neural architectures..To overcome these limitations, we introduce Bertram (BERT for Attentive Mimicking), a novel architecture for understanding rare words that combines a pretrained BERT language model with Attentive Mimicking BIBREF19. Unlike previous approaches making use of language models BIBREF5, our approach integrates BERT in an end-to-end fashion and directly makes use of its hidden states. By giving Bertram access to both surface form and context information already at its very lowest layer, we allow for a deep connection and exchange of information between both input signals..For various reasons, assessing the effectiveness of methods like Bertram in a contextualized setting poses a huge difficulty: While most previous work on rare words was evaluated on datasets explicitly focusing on such words BIBREF6, BIBREF3, BIBREF4, BIBREF5, BIBREF10, all of these datasets are tailored towards context-independent embeddings and thus not suitable for evaluating our proposed model. Furthermore, understanding rare words is of negligible importance for most commonly used downstream task datasets. To evaluate our proposed model, we therefore introduce a novel procedure that allows us to automatically turn arbitrary text classification datasets into ones where rare words are guaranteed to be important. This is achieved by replacing classification-relevant frequent words with rare synonyms obtained using semantic resources such as WordNet BIBREF20..Using this procedure, we extract rare word datasets from three commonly used text (or text pair) classification datasets: MNLI BIBREF21, AG's News BIBREF22 and DBPedia BIBREF23. On both the WNLaMPro dataset of BIBREF0 and all three so-obtained datasets, our proposed Bertram model outperforms previous work by a large margin..In summary, our contributions are as follows:.We show that a pretrained BERT instance can be integrated into Attentive Mimicking, resulting in much better context representations and a deeper connection of form and context..We design a procedure that allows us to automatically transform text classification datasets into datasets for which rare words are guaranteed to be important..We show that Bertram achieves a new state-of-the-art on the WNLaMPro probing task BIBREF0 and beats all baselines on rare word instances of AG's News, MNLI and DBPedia, resulting in an absolute improvement of up to 24% over a BERT baseline. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "6\n",
      "What is Bertram and how does it combine BERT and Attentive Mimicking?\n",
      "\n",
      "Bertram is an approach that combines a pretrained BERT language model with Attentive Mimicking. It uses BERT's uncontextualized embeddings and contextualized representations to obtain a deeper understanding of a word's context. Bertram also introduces two alternative approaches called replace and add to combine surface-form information with context. Additionally, Bertram implements an Attentive Mimicking head to deal with multiple contexts for a rare word.\n",
      "Question : for the text To overcome both limitations described above, we introduce Bertram, an approach that combines a pretrained BERT language model BIBREF13 with Attentive Mimicking BIBREF19. To this end, let $d_h$ be the hidden dimension size and $l_\\text{max}$ be the number of layers for the BERT model being used. We denote with $e_{t}$ the (uncontextualized) embedding assigned to a token $t$ by BERT and, given a sequence of such uncontextualized embeddings $\\mathbf {e} = e_1, \\ldots , e_n$, we denote by $\\textbf {h}_j^l(\\textbf {e})$ the contextualized representation of the $j$-th token at layer $l$ when the model is given $\\mathbf {e}$ as input..Given a word $w$ and a context $C = w_1, \\ldots , w_n$ in which it occurs, let $\\mathbf {t} = t_1, \\ldots , t_{m}$ with $m \\ge n$ be the sequence obtained from $C$ by (i) replacing $w$ with a [MASK] token and (ii) tokenizing the so-obtained sequence to match the BERT vocabulary; furthermore, let $i$ denote the index for which $t_i = \\texttt {[MASK]}$. Perhaps the most simple approach for obtaining a context embedding from $C$ using BERT is to define.where $\\mathbf {e} = e_{t_1}, \\ldots , e_{t_m}$. The so-obtained context embedding can then be combined with its form counterpart as described in Eq. DISPLAY_FORM8. While this achieves our first goal of using a more sophisticated context model that can potentially gain a deeper understanding of a word than just its broad topic, the so-obtained architecture still only combines form and context in a shallow fashion. We thus refer to it as the shallow variant of our model and investigate two alternative approaches (replace and add) that work as follows:.Replace: Before computing the context embedding, we replace the uncontextualized embedding of the [MASK] token with the word's surface-form embedding:.As during BERT pretraining, words chosen for prediction are replaced with [MASK] tokens only 80% of the time and kept unchanged 10% of the time, we hypothesize that even without further training, BERT is able to make use of form embeddings ingested this way..Add: Before computing the context embedding, we prepad the input with the surface-form embedding of $w$, followed by a colon:.We also experimented with various other prefixes, but ended up choosing this particular strategy because we empirically found that after masking a token $t$, adding the sequence “$t :$” at the beginning helps BERT the most in recovering this very token at the masked position..tnode/.style=rectangle, inner sep=0.1cm, minimum height=4ex, text centered,text height=1.5ex, text depth=0.25ex, opnode/.style=draw, rectangle, rounded corners, minimum height=4ex, minimum width=4ex, text centered, arrow/.style=draw,->,>=stealth.As for both variants, surface-form information is directly and deeply integrated into the computation of the context embedding, we do not require any further gating mechanism and may directly set $v_{(w,C)} = A \\cdot v^\\text{context}_{(w,C)}$..However, we note that for the add variant, the contextualized representation of the [MASK] token is not the only natural candidate to be used for computing the final embedding: We might just as well look at the contextualized representation of the surface-form based embedding added at the very first position. Therefore, we also try a shallow combination of both embeddings. Note, however, that unlike FCM, we combine the contextualized representations – that is, the form part was already influenced by the context part and vice versa before combining them using a gate. For this combination, we define.with $A^{\\prime } \\in \\mathbb {R}^{d \\times d_h}$ being an additional learnable parameter. We then combine the two contextualized embeddings similar to Eq. DISPLAY_FORM8 as.where $\\alpha = g(h^\\text{form}_{(w,C)}, h^\\text{context}_{(w,C)})$. We refer to this final alternative as the add-gated approach. The model architecture for this variant can be seen in Figure FIGREF14 (left)..As in many cases, not just one, but a handful of contexts is known for a rare word, we follow the approach of BIBREF19 to deal with multiple contexts: We add an Attentive Mimicking head on top of our model, as can be seen in Figure FIGREF14 (right). That is, given a set of contexts $\\mathcal {C} = \\lbrace C_1, \\ldots , C_m\\rbrace $ and the corresponding embeddings $v_{(w,C_1)}, \\ldots , v_{(w,C_m)}$, we apply a self-attention mechanism to all embeddings, allowing the model to distinguish informative contexts from uninformative ones. The final embedding $v_{(w, \\mathcal {C})}$ is then a linear combination of the embeddings obtained from each context, where the weight of each embedding is determined based on the self-attention layer. For further details on this mechanism, we refer to BIBREF19. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "7\n",
      "What is the gate used in the form-context model (FCM)?\n",
      "\n",
      "The gate used in the form-context model (FCM) is a combination of the form and context embeddings using a sigmoid function, allowing the model to decide for each pair of form and context embeddings how much attention should be paid to each. The final representation of the word is then a weighted sum of the form and context embeddings.\n",
      "Question : for the text We review the architecture of the form-context model (FCM) BIBREF9, which forms the basis for our model. Given a set of $d$-dimensional high-quality embeddings for frequent words, FCM can be used to induce embeddings for infrequent words that are appropriate for the given embedding space. This is done as follows: Given a word $w$ and a context $C$ in which it occurs, a surface-form embedding $v_{(w,{C})}^\\text{form} \\in \\mathbb {R}^d$ is obtained similar to BIBREF7 by averaging over embeddings of all $n$-grams in $w$; these $n$-gram embeddings are learned during training. Similarly, a context embedding $v_{(w,{C})}^\\text{context} \\in \\mathbb {R}^d$ is obtained by averaging over the embeddings of all words in $C$. The so-obtained form and context embeddings are then combined using a gate.with parameters $w \\in \\mathbb {R}^{2d}, b \\in \\mathbb {R}$ and $\\sigma $ denoting the sigmoid function, allowing the model to decide for each pair $(x,y)$ of form and context embeddings how much attention should be paid to $x$ and $y$, respectively..The final representation of $w$ is then simply a weighted sum of form and context embeddings:.where $\\alpha = g(v_{(w,C)}^\\text{form}, v_{(w,C)}^\\text{context})$ and $A$ is a $d\\times d$ matrix that is learned during training..While the context-part of FCM is able to capture the broad topic of numerous rare words, in many cases it is not able to obtain a more concrete and detailed understanding thereof BIBREF9. This is hardly surprising given the model's simplicity; it does, for example, make no use at all of the relative positions of context words. Furthermore, the simple gating mechanism results in only a shallow combination of form and context. That is, the model is not able to combine form and context until the very last step: While it can choose how much to attend to form and context, respectively, the corresponding embeddings do not share any information and thus cannot influence each other in any way. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "8\n",
      "What is the purpose of the three-stage training process used for Bertram?\n",
      "\n",
      "The three-stage training process is used to train Bertram in a more efficient manner as training it end-to-end requires much computation. The first stage trains only the form part, the second stage trains only the context part, and the third stage combines the pretrained form-only and context-only models and trains all additional parameters. Pretraining the form and context parts individually allows for the full model to be trained for much fewer steps with comparable results.\n",
      "Question : for the text Like previous work, we use mimicking BIBREF8 as a training objective. That is, given a frequent word $w$ with known embedding $e_w$ and a set of corresponding contexts $\\mathcal {C}$, Bertram is trained to minimize $\\Vert e_w - v_{(w, \\mathcal {C})}\\Vert ^2$..As training Bertram end-to-end requires much computation (processing a single training instance $(w,\\mathcal {C})$ is as costly as processing an entire batch of $|\\mathcal {C}|$ examples in the original BERT architecture), we resort to the following three-stage training process:.We train only the form part, i.e. our loss for a single example $(w, \\mathcal {C})$ is $\\Vert e_w - v^\\text{form}_{(w, \\mathcal {C})} \\Vert ^2$..We train only the context part, minimizing $\\Vert e_w - A \\cdot v^\\text{context}_{(w, \\mathcal {C})} \\Vert ^2$ where the context embedding is obtained using the shallow variant of Bertram. Furthermore, we exclude all of BERT's parameters from our optimization..We combine the pretrained form-only and context-only model and train all additional parameters..Pretraining the form and context parts individually allows us to train the full model for much fewer steps with comparable results. Importantly, for the first two stages of our training procedure, we do not have to backpropagate through the entire BERT model to obtain all required gradients, drastically increasing the training speed. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "9\n",
      "What is the benefit of incorporating both surface form and context information for understanding rare words?\n",
      "Answer 1: According to recent studies, combining both surface form and context information leads to significantly better results in understanding rare words compared to using only one input signal. Surface form information includes things like morphemes, characters, or character n-grams, while context information refers to the words and phrases surrounding a rare word.\n",
      "Question : for the text Incorporating surface-form information (e.g., morphemes, characters or character $n$-grams) is a commonly used technique for improving word representations. For context-independent word embeddings, this information can either be injected into a given embedding space BIBREF6, BIBREF8, or a model can directly be given access to it during training BIBREF7, BIBREF24, BIBREF25. In the area of contextualized representations, many architectures employ subword segmentation methods BIBREF12, BIBREF13, BIBREF26, BIBREF14, whereas others use convolutional neural networks to directly access character-level information BIBREF27, BIBREF11, BIBREF17..Complementary to surface form, another useful source of information for understanding rare words are the contexts in which they occur BIBREF2, BIBREF3, BIBREF4. As recently shown by BIBREF19, BIBREF9, combining form and context leads to significantly better results than using just one of both input signals for a wide range of tasks. While all aforementioned methods are based on simple bag-of-words models, BIBREF5 recently proposed an architecture based on the context2vec language model BIBREF28. However, in contrast to our work, they (i) do not incorporate surface-form information and (ii) do not directly access the hidden states of the language model, but instead simply use its output distribution..There are several datasets explicitly focusing on rare words, e.g. the Stanford Rare Word dataset of BIBREF6, the Definitional Nonce dataset of BIBREF3 and the Contextual Rare Word dataset BIBREF4. However, all of these datasets are only suitable for evaluating context-independent word representations..Our proposed method of generating rare word datasets is loosely related to adversarial example generation methods such as HotFlip BIBREF29, which manipulate the input to change a model's prediction. We use a similar mechanism to determine which words in a given sentence are most important and replace these words with rare synonyms. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "10\n",
      "Question 1: How does the RLEL model select target entities for multiple mentions appearing in a document?\n",
      "\n",
      "Answer 1: The RLEL model sorts the mentions according to local similarities and selects the target entities in order by reinforcement learning. It utilizes topical consistency between mentions and makes full use of selected target entity information. Two examples of the entity selection process are shown in Table 5.\n",
      "Question : for the text Table 5 shows two entity selection examples by our RLEL model. For multiple mentions appearing in the document, we first sort them according to their local similarities, and select the target entities in order by the reinforcement learning model. From the results of sorting and disambiguation, we can see that our model is able to utilize the topical consistency between mentions and make full use of the selected target entity information. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "11\n",
      "What evaluation metrics are used to measure the performance of the EL models?\n",
      "\n",
      "The evaluation metrics used to measure the performance of the EL models are Accuracy, Precision, Recall, and F1 at mention level (Micro).\n",
      "Question : for the text We compare RLEL with a series of EL systems which report state-of-the-art results on the test datasets. There are various methods including classification model BIBREF17 , rank model BIBREF21 , BIBREF15 and probability graph model BIBREF18 , BIBREF14 , BIBREF22 , BIBREF0 , BIBREF1 . Except that, Cheng $et$ $al.$ BIBREF23 formulate their global decision problem as an Integer Linear Program (ILP) which incorporates the entity-relation inference. Globerson $et$ $al.$ BIBREF24 introduce a multi-focal attention model which allows each candidate to focus on limited mentions, Yamada $et$ $al.$ BIBREF25 propose a word and entity embedding model specifically designed for EL..We use the standard Accuracy, Precision, Recall and F1 at mention level (Micro) as the evaluation metrics: .$$Accuracy = \\frac{|M \\cap M^*|}{|M \\cup M^*|}$$   (Eq. 31) .$$Precision = \\frac{|M \\cap M^*|}{|M|}$$   (Eq. 32) .where $M^*$ is the golden standard set of the linked name mentions, $M$ is the set of linked name mentions outputted by an EL method..Same as previous work, we use in-KB accuracy and micro F1 to evaluate our method. We first test the model on the AIDA-B dataset. From Table 2, we can observe that our model achieves the best result. Previous best results on this dataset are generated by BIBREF0 , BIBREF1 which both built CRF models. They calculate the pairwise scores between all candidate entities. Differently, our model only considers the consistency of the target entities and ignores the relationship between incorrect candidates. The experimental results show that our model can reduce the impact of noise data and improve the accuracy of disambiguation. Apart from experimenting on AIDA-B, we also conduct experiments on several different datasets to verify the generalization performance of our model..From Table 3, we can see that RLEL has achieved relatively good performances on ACE2004, CWEB and WIKI. At the same time, previous models BIBREF0 , BIBREF1 , BIBREF23 achieve better performances on the news datasets such as MSNBC and AQUINT, but their results on encyclopedia datasets such as WIKI are relatively poor. To avoid overfitting with some datasets and improve the robustness of our model, we not only use AIDA-Train but also add Wikipedia data to the training set. In the end, our model achieve the best overall performance..For most existing EL systems, entities with lower frequency are difficult to disambiguate. To gain further insight, we analyze the accuracy of the AIDA-B dataset for situations where gold entities have low popularity. We divide the gold entities according to their pageviews in wikipedia, the statistical disambiguation results are shown in Table 4. Since some pageviews can not be obtained, we only count part of gold entities. The result indicates that our model is still able to work well for low-frequency entities. But for medium-frequency gold entities, our model doesn't work well enough. The most important reason is that other candidate entities corresponding to these medium-frequency gold entities have higher pageviews and local similarities, which makes the model difficult to distinguish. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "12\n",
      "What is the goal of the presented model in this paper?\n",
      "The goal of the presented model in this paper is to learn the policy on selecting target entities in a sequential manner based on current and previous states, utilizing information of previously referred entities for global consistency and achieving better disambiguation of mentions. The model is evaluated on AIDA-B and other datasets and outperforms state-of-the-art solutions.\n",
      "Question : for the text In this paper we consider entity linking as a sequence decision problem and present a reinforcement learning based model. Our model learns the policy on selecting target entities in a sequential manner and makes decisions based on current state and previous ones. By utilizing the information of previously referred entities, we can take advantage of global consistency to disambiguate mentions. For each selection result in the current state, it also has a long-term impact on subsequent decisions, which allows learned policy strategy has a global view. In experiments, we evaluate our method on AIDA-B and other well-known datasets, the results show that our system outperforms state-of-the-art solutions. In the future, we would like to use reinforcement learning to detect mentions and determine which mention should be firstly disambiguated in the document..This research is supported by the GS501100001809National Key Research and Development Program of China (No. GS5011000018092018YFB1004703), GS501100001809the Beijing Municipal Science and Technology Project under grant (No. GS501100001809.Z181100002718004), and GS501100001809the National Natural Science Foundation of China grants(No. GS50110000180961602466). generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "13\n",
      "What is the impact of sequence length on joint disambiguation in the model?\n",
      "The disambiguation results are poor when the sequence is too short or too long. When the sequence length is less than 3, delay reward can't work in reinforcement learning, and when the sequence length reaches 5 or more, noise data may be added. The model chooses 4 adjacent mentions to form a sequence for better results.\n",
      "Question : for the text To demonstrate the effects of RLEL, we evaluate our model under different conditions. First, we evaluate the effect of sequence length on global decision making. Second, we assess whether sorting the mentions have a positive effect on the results. Third, we analysis the results of not adding globally encoding during entity selection. Last, we compare our RL selection strategy with the greedy choice..A document may contain multiple topics, so we do not add all mentions to a single sequence. In practice, we add some adjacent mentions to the sequence and use reinforcement learning to select entities from beginning to end. To analysis the impact of the number of mentions on joint disambiguation, we experiment with sequences on different lengths. The results on AIDA-B are shown in Figure 4. We can see that when the sequence is too short or too long, the disambiguation results are both very poor. When the sequence length is less than 3, delay reward can't work in reinforcement learning, and when the sequence length reaches 5 or more, noise data may be added. Finally, we choose the 4 adjacent mentions to form a sequence..In this section, we test whether ranking mentions is helpful for entity selections. At first, we directly input them into the global encoder by the order they appear in the text. We record the disambiguation results and compare them with the method which adopts ranking mentions. As shown in Figure 5a, the model with ranking mentions has achieved better performances on most of datasets, indicating that it is effective to place the mention that with a higher local similarity in front of the sequence. It is worth noting that the effect of ranking mentions is not obvious on the MSNBC dataset, the reason is that most of mentions in MSNBC have similar local similarities, the order of disambiguation has little effect on the final result..Most of previous methods mainly use the similarities between entities to correlate each other, but our model associates them by encoding the selected entity information. To assess whether the global encoding contributes to disambiguation rather than add noise, we compare the performance with and without adding the global information. When the global encoding is not added, the current state only contains the mention context representation, candidate entity representation and feature representation, notably, the selected target entity information is not taken into account. From the results in Figure 5b, we can see that the model with global encoding achieves an improvement of 4% accuracy over the method that without global encoding..To illustrate the necessity for adopting the reinforcement learning for entity selection, we compare two entity selection strategies like BIBREF5 . Specifically, we perform entity selection respectively with reinforcement learning and greedy choice. The greedy choice is to select the entity with largest local similarity from candidate set. But the reinforcement learning selection is guided by delay reward, which has a global perspective. In the comparative experiment, we keep the other conditions consistent, just replace the RL selection with a greedy choice. Based on the results in Figure 5c, we can draw a conclusion that our entity selector perform much better than greedy strategies. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "14\n",
      "What is the difference between local and global disambiguation in entity linking?\n",
      "\n",
      "Local disambiguation involves resolving mentions independently based on lexical matching between the mention's surrounding words and the entity profile in the reference KB. Global disambiguation, on the other hand, assumes topical coherence among mentions and utilizes models that jointly link all mentions in a document.\n",
      "Question : for the text Entity linking falls broadly into two major approaches: local and global disambiguation. Early studies use local models to resolve mentions independently, they usually disambiguate mentions based on lexical matching between the mention's surrounding words and the entity profile in the reference KB. Various methods have been proposed to model mention's local context ranging from binary classification BIBREF17 to rank models BIBREF26 , BIBREF27 . In these methods, a large number of hand-designed features are applied. For some marginal mentions that are difficult to extract features, researchers also exploit the data retrieved by search engines BIBREF28 , BIBREF29 or Wikipedia sentences BIBREF30 . However, the feature engineering and search engine methods are both time-consuming and laborious. Recently, with the popularity of deep learning models, representation learning is utilized to automatically find semantic features BIBREF31 , BIBREF32 . The learned entity representations which by jointly modeling textual contexts and knowledge base are effective in combining multiple sources of information. To make full use of the information contained in representations, we also utilize the pre-trained entity embeddings in our model..In recent years, with the assumption that the target entities of all mentions in a document shall be related, many novel global models for joint linking are proposed. Assuming the topical coherence among mentions, authors in BIBREF33 , BIBREF34 construct factor graph models, which represent the mention and candidate entities as variable nodes, and exploit factor nodes to denote a series of features. Two recent studies BIBREF0 , BIBREF1 use fully-connected pairwise Conditional Random Field(CRF) model and exploit loopy belief propagation to estimate the max-marginal probability. Moreover, PageRank or Random Walk BIBREF35 , BIBREF18 , BIBREF7 are utilized to select the target entity for each mention. The above probabilistic models usually need to predefine a lot of features and are difficult to calculate the max-marginal probability as the number of nodes increases. In order to automatically learn features from the data, Cao et al. BIBREF9 applies Graph Convolutional Network to flexibly encode entity graphs. However, the graph-based methods are computationally expensive because there are lots of candidate entity nodes in the graph..To reduce the calculation between candidate entity pairs, Globerson et al. BIBREF24 introduce a coherence model with an attention mechanism, where each mention only focus on a fixed number of mentions. Unfortunately, choosing the number of attention mentions is not easy in practice. Two recent studies BIBREF8 , BIBREF36 finish linking all mentions by scanning the pairs of mentions at most once, they assume each mention only needs to be consistent with one another mention in the document. The limitation of their method is that the consistency information is too sparse, resulting in low confidence. Similar to us, Guo et al. BIBREF18 also sort mentions according to the difficulty of disambiguation, but they did not make full use of the information of previously referred entities for the subsequent entity disambiguation. Nguyen et al. BIBREF2 use the sequence model, but they simply encode the results of the greedy choice, and measure the similarities between the global encoding and the candidate entity representations. Their model does not consider the long-term impact of current decisions on subsequent choices, nor does they add the selected target entity information to the current state to help disambiguation. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "15\n",
      "Sorry, there is no question provided for me to answer. Can you please provide me with a question so that I can assist you better?\n",
      "Question : for the text In the entity selector module, we choose the target entity from candidate set based on the results of local and global encoder. In the process of sequence disambiguation, each selection result will have an impact on subsequent decisions. Therefore, we transform the choice of the target entity into a reinforcement learning problem and view the entity selector as an agent. In particular, the agent is designed as a policy network which can learn a stochastic policy and prevents the agent from getting stuck at an intermediate state BIBREF12 . Under the guidance of policy, the agent can decide which action (choosing the target entity from the candidate set)should be taken at each state, and receive a delay reward when all the selections are made. In the following part, we first describe the state, action and reward. Then, we detail how to select target entity via a policy network..The result of entity selection is based on the current state information. For time $t$ , the state vector $S_t$ is generated as follows: .$$S_t = V_{m_i}^t\\oplus {V_{e_i}^t}\\oplus {V_{feature}^t}\\oplus {V_{e^*}^{t-1}}$$   (Eq. 15) .Where $\\oplus $ indicates vector concatenation. The $V_{m_i}^t$ and $V_{e_i}^t$ respectively denote the vector of $m_i$ and $e_i$ at time $t$ . For each mention, there are multiple candidate entities correspond to it. With the purpose of comparing the semantic relevance between the mention and each candidate entity at the same time, we copy multiple copies of the mention vector. Formally, we extend $V_{m_i}^t \\in \\mathbb {R}^{1\\times {n}}$ to $V_{m_i}^t{^{\\prime }} \\in \\mathbb {R}^{k\\times {n}}$ and then combine it with $V_{e_i}^t \\in \\mathbb {R}^{k\\times {n}}$ . Since $V_{m_i}^t$ and $V_{m_i}^t$0 are mainly to represent semantic information, we add feature vector $V_{m_i}^t$1 to enrich lexical and statistical features. These features mainly include the popularity of the entity, the edit distance between the entity description and the mention context, the number of identical words in the entity description and the mention context etc. After getting these feature values, we combine them into a vector and add it to the current state. In addition, the global vector $V_{m_i}^t$2 is also added to $V_{m_i}^t$3 . As mentioned in global encoder module, $V_{m_i}^t$4 is the output of global LSTM network at time $V_{m_i}^t$5 , which encodes the mention context and target entity information from $V_{m_i}^t$6 to $V_{m_i}^t$7 . Thus, the state $V_{m_i}^t$8 contains current information and previous decisions, while also covering the semantic representations and a variety of statistical features. Next, the concatenated vector will be fed into the policy network to generate action..According to the status at each time step, we take corresponding action. Specifically, we define the action at time step $t$ is to select the target entity $e_t^*$ for $m_t$ . The size of action space is the number of candidate entities for each mention, where $a_i \\in \\lbrace 0,1,2...k\\rbrace $ indicates the position of the selected entity in the candidate entity list. Clearly, each action is a direct indicator of target entity selection in our model. After completing all the actions in the sequence we will get a delayed reward..The agent takes the reward value as the feedback of its action and learns the policy based on it. Since current selection result has a long-term impact on subsequent decisions, we don't give an immediate reward when taking an action. Instead, a delay reward is given by follows, which can reflect whether the action improves the overall performance or not. .$$R(a_t) = p(a_t)\\sum _{j=t}^{T}p(a_j) + (1 - p(a_t))(\\sum _{j=t}^{T}p(a_j) + t - T)$$   (Eq. 16) .where $p(a_t)\\in \\lbrace 0,1\\rbrace $ indicates whether the current action is correct or not. When the action is correct $p(a_t)=1$ otherwise $p(a_t)=0$ . Hence $\\sum _{j=t}^{T}p(a_j)$ and $\\sum _{j=t}^{T}p(a_j) + t - T$ respectively represent the number of correct and wrong actions from time t to the end of episode. Based on the above definition, our delayed reward can be used to guide the learning of the policy for entity linking..After defining the state, action, and reward, our main challenge becomes to choose an action from the action space. To solve this problem, we sample the value of each action by a policy network $\\pi _{\\Theta }(a|s)$ . The structure of the policy network is shown in Figure 3. The input of the network is the current state, including the mention context representation, candidate entity representation, feature representation, and encoding of the previous decisions. We concatenate these representations and fed them into a multilayer perceptron, for each hidden layer, we generate the output by: .$$h_i(S_t) = Relu(W_i*h_{i-1}(S_t) + b_i)$$   (Eq. 17) .Where $W_i$ and $ b_i$ are the parameters of the $i$ th hidden layer, through the $relu$ activation function we get the $h_i(S_t)$ . After getting the output of the last hidden layer, we feed it into a softmax layer which generates the probability distribution of actions. The probability distribution is generated as follows: .$$\\pi (a|s) = Softmax(W * h_l(S) + b)$$   (Eq. 18) .Where the $W$ and $b$ are the parameters of the softmax layer. For each mention in the sequence, we will take action to select the target entity from its candidate set. After completing all decisions in the episode, each action will get an expected reward and our goal is to maximize the expected total rewards. Formally, the objective function is defined as: .$$\\begin{split}\n",
      "J(\\Theta ) &= \\mathbb {E}_{(s_t, a_t){\\sim }P_\\Theta {(s_t, a_t)}}R(s_1{a_1}...s_L{a_L}) \\\\\n",
      "&=\\sum _{t}\\sum _{a}\\pi _{\\Theta }(a|s)R(a_t)\n",
      "\\end{split}$$   (Eq. 19) .Where $P_\\Theta {(s_t, a_t)}$ is the state transfer function, $\\pi _{\\Theta }(a|s)$ indicates the probability of taking action $a$ under the state $s$ , $R(a_t)$ is the expected reward of action $a$ at time step $t$ . According to REINFORCE policy gradient algorithm BIBREF13 , we update the policy gradient by the way of equation 9. .$$\\Theta \\leftarrow \\Theta + \\alpha \\sum _{t}R(a_t)\\nabla _{\\Theta }\\log \\pi _{\\Theta }(a|s)$$   (Eq. 20) .As the global encoder and the entity selector are correlated mutually, we train them jointly after pre-training the two networks. The details of the joint learning are presented in Algorithm 1..[t] The Policy Learning for Entity Selector [1] Training data include multiple documents $D = \\lbrace D_1, D_2, ..., D_N\\rbrace $ The target entity for mentions $\\Gamma = \\lbrace T_1, T_2, ..., T_N\\rbrace $ .Initialize the policy network parameter $\\Theta $ , global LSTM network parameter $\\Phi $ ; $D_k$ in $D$ Generate the candidate set for each mention Divide the mentions in $D_k$ into multiple sequences $S = \\lbrace S_1, S_2, ..., S_N\\rbrace $ ; $S_k$ in $S$ Rank the mentions $M = \\lbrace m_1, m_2, ..., m_n\\rbrace $ in $S_k$ based on the local similarity; $\\Phi $0 in $\\Phi $1 Sample the target entity $\\Phi $2 for $\\Phi $3 with $\\Phi $4 ; Input the $\\Phi $5 and $\\Phi $6 to global LSTM network; $\\Phi $7 End of sampling, update parameters Compute delayed reward $\\Phi $8 for each action; Update the parameter $\\Phi $9 of policy network:. $\\Theta \\leftarrow \\Theta + \\alpha \\sum _{t}R(a_t)\\nabla _{\\Theta }\\log \\pi _{\\Theta }(a|s)$ .Update the parameter $\\Phi $ in the global LSTM network generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "16\n",
      "Question 1: Which datasets were used to validate the effectiveness of the RLEL model?\n",
      "\n",
      "Answer 1: The RLEL model was validated on a series of popular datasets that were also used by BIBREF0 and BIBREF1, which included both AIDA-Train and Wikipedia data in the training set.\n",
      "Question : for the text In order to evaluate the effectiveness of our method, we train the RLEL model and validate it on a series of popular datasets that are also used by BIBREF0 , BIBREF1 . To avoid overfitting with one dataset, we use both AIDA-Train and Wikipedia data in the training set. Furthermore, we compare the RLEL with some baseline methods, where our model achieves the state-of-the-art results. We implement our models in Tensorflow and run experiments on 4 Tesla V100 GPU. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "17\n",
      "What datasets were used to train the RLEL model?\n",
      "\n",
      "The AIDA-Train and Wikipedia datasets were used to train the RLEL model.\n",
      "Question : for the text We conduct experiments on several different types of public datasets including news and encyclopedia corpus. The training set is AIDA-Train and Wikipedia datasets, where AIDA-Train contains 18448 mentions and Wikipedia contains 25995 mentions. In order to compare with the previous methods, we evaluate our model on AIDA-B and other datasets. These datasets are well-known and have been used for the evaluation of most entity linking systems. The statistics of the datasets are shown in Table 1..AIDA-CoNLL BIBREF14 is annotated on Reuters news articles. It contains training (AIDA-Train), validation (AIDA-A) and test (AIDA-B) sets..ACE2004 BIBREF15 is a subset of the ACE2004 Coreference documents..MSNBC BIBREF16 contains top two stories in the ten news categories(Politics, Business, Sports etc.).AQUAINT BIBREF17 is a news corpus from the Xinhua News Service, the New York Times, and the Associated Press..WNED-CWEB BIBREF18 is randomly picked from the FACC1 annotated ClueWeb 2012 dataset..WNED-WIKI BIBREF18 is crawled from Wikipedia pages with its original hyperlink annotation..OURSELF-WIKI is crawled by ourselves from Wikipedia pages..During the training of our RLEL model, we select top K candidate entities for each mention to optimize the memory and run time. In the top K candidate list, we define the recall of correct target entity is $R_t$ . According to our statistics, when K is set to 1, $R_t$ is 0.853, when K is 5, $R_t$ is 0.977, when K increases to 10, $R_t$ is 0.993. Empirically, we choose top 5 candidate entities as the input of our RLEL model. For the entity description, there are lots of redundant information in the wikipedia page, to reduce the impact of noise data, we use TextRank algorithm BIBREF19 to select 15 keywords as description of the entity. Simultaneously, we choose 15 words around mention as its context. In the global LSTM network, when the number of mentions does not reach the set length, we adopt the mention padding strategy. In short, we copy the last mention in the sequence until the number of mentions reaches the set length..We set the dimensions of word embedding and entity embedding to 300, where the word embedding and entity embedding are released by BIBREF20 and BIBREF0 respectively. For parameters of the local LSTM network, the number of LSTM cell units is set to 512, the batch size is 64, and the rank margin $\\gamma $ is 0.1. Similarly, in global LSTM network, the number of LSTM cell units is 700 and the batch size is 16. In the above two LSTM networks, the learning rate is set to 1e-3, the probability of dropout is set to 0.8, and the Adam is utilized as optimizer. In addition, we set the number of MLP layers to 4 and extend the priori feature dimension to 50 in the policy network. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "18\n",
      "Question 1: What is the purpose of the global encoder module in this system?\n",
      "Answer 1: The purpose of the global encoder module is to enforce topical coherence among mentions and their target entities by encoding the ranked mention sequence using an LSTM network capable of maintaining long-term memory. This is done only for the mentions that have been disambiguated by the entity selector. The global encoder also encodes previously selected target entity information, which is then used as input for the entity selector to choose target entities.\n",
      "Question : for the text In the global encoder module, we aim to enforce the topical coherence among the mentions and their target entities. So, we use an LSTM network which is capable of maintaining the long-term memory to encode the ranked mention sequence. What we need to emphasize is that our global encoder just encode the mentions that have been disambiguated by the entity selector which is denoted as $V_{a_t}$ ..As mentioned above, the mentions should be sorted according to their contextual information and topical coherence. So, we firstly divide the adjacent mentions into a segment by the order they appear in the document based on the observation that the topical consistency attenuates along with the distance between the mentions. Then, we sort mentions in a segment based on the local similarity and place the mention that has a higher similarity value in the front of the sequence. In Equation 1, we define the local similarity of $m_i$ and its corresponding candidate entity $e_t^i$ . On this basis, we define $\\Psi _{max}(m_i, e_i^a)$ as the the maximum local similarity between the $m_i$ and its candidate set $C_{m_i} = \\lbrace e_i^1, e_i^2,..., e_i^n\\rbrace $ . We use $\\Psi _{max}(m_i, e_i^a)$ as criterion when sorting mentions. For instance, if $\\Psi _{max}(m_i, e_i^a) > \\Psi _{max}(m_j, e_j^b)$ then we place $m_i$ before $m_j$ . Under this circumstances, the mentions in the front positions may not be able to make better use of global consistency, but their target entities have a high degree of similarity to the context words, which allows them to be disambiguated without relying on additional information. In the end, previous selected target entity information is encoded by global encoder and the encoding result will be served as input to the entity selector..Before using entity selector to choose target entities, we pre-trained the global LSTM network. During the training process, we input not only positive samples but also negative ones to the LSTM. By doing this, we can enhance the robustness of the network. In the global encoder module, we adopt the following cross entropy loss function to train the model. .$$L_{global} = -\\frac{1}{n}\\sum _x{\\left[y\\ln {y^{^{\\prime }}} + (1-y)\\ln (1-y^{^{\\prime }})\\right]}$$   (Eq. 12) .Where $y\\in \\lbrace 0,1\\rbrace $ represents the label of the candidate entity. If the candidate entity is correct $y=1$ , otherwise $y=0$ . $y^{^{\\prime }}\\in (0,1)$ indicates the output of our model. After pre-training the global encoder, we start using the entity selector to choose the target entity for each mention and encode these selections. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "19\n",
      "What is Entity Linking and what are its applications?\n",
      "\n",
      "Entity Linking, also known as Entity Disambiguation, is the task of mapping mentions in text to corresponding entities in a given knowledge Base (KB). It is an important task in text understanding as mentions are usually ambiguous. Entity Linking has many applications, such as knowledge base population, question answering, and information retrieval.\n",
      "Question : for the text Entity Linking (EL), which is also called Entity Disambiguation (ED), is the task of mapping mentions in text to corresponding entities in a given knowledge Base (KB). This task is an important and challenging stage in text understanding because mentions are usually ambiguous, i.e., different named entities may share the same surface form and the same entity may have multiple aliases. EL is key for information retrieval (IE) and has many applications, such as knowledge base population (KBP), question answering (QA), etc..Existing EL methods can be divided into two categories: local model and global model. Local models concern mainly on contextual words surrounding the mentions, where mentions are disambiguated independently. These methods are not work well when the context information is not rich enough. Global models take into account the topical coherence among the referred entities within the same document, where mentions are disambiguated jointly. Most of previous global models BIBREF0 , BIBREF1 , BIBREF2 calculate the pairwise scores between all candidate entities and select the most relevant group of entities. However, the consistency among wrong entities as well as that among right ones are involved, which not only increases the model complexity but also introduces some noises. For example, in Figure 1, there are three mentions \"France\", \"Croatia\" and \"2018 World Cup\", and each mention has three candidate entities. Here, \"France\" may refer to French Republic, France national basketball team or France national football team in KB. It is difficult to disambiguate using local models, due to the scarce common information in the contextual words of \"France\" and the descriptions of its candidate entities. Besides, the topical coherence among the wrong entities related to basketball team (linked by an orange dashed line) may make the global models mistakenly refer \"France\" to France national basketball team. So, how to solve these problems?.We note that, mentions in text usually have different disambiguation difficulty according to the quality of contextual information and the topical coherence. Intuitively, if we start with mentions that are easier to disambiguate and gain correct results, it will be effective to utilize information provided by previously referred entities to disambiguate subsequent mentions. In the above example, it is much easier to map \"2018 World Cup\" to 2018 FIFA World Cup based on their common contextual words \"France\", \"Croatia\", \"4-2\". Then, it is obvious that \"France\" and \"Croatia\" should be referred to the national football team because football-related terms are mentioned many times in the description of 2018 FIFA World Cup..Inspired by this intuition, we design the solution with three principles: (i) utilizing local features to rank the mentions in text and deal with them in a sequence manner; (ii) utilizing the information of previously referred entities for the subsequent entity disambiguation; (iii) making decisions from a global perspective to avoid the error propagation if the previous decision is wrong..In order to achieve these aims, we consider global EL as a sequence decision problem and proposed a deep reinforcement learning (RL) based model, RLEL for short, which consists of three modules: Local Encoder, Global Encoder and Entity Selector. For each mention and its candidate entities, Local Encoder encodes the local features to obtain their latent vector representations. Then, the mentions are ranked according to their disambiguation difficulty, which is measured by the learned vector representations. In order to enforce global coherence between mentions, Global Encoder encodes the local representations of mention-entity pairs in a sequential manner via a LSTM network, which maintains a long-term memory on features of entities which has been selected in previous states. Entity Selector uses a policy network to choose the target entities from the candidate set. For a single disambiguation decision, the policy network not only considers the pairs of current mention-entity representations, but also concerns the features of referred entities in the previous states which is pursued by the Global Encoder. In this way, Entity Selector is able to take actions based on the current state and previous ones. When eliminating the ambiguity of all mentions in the sequence, delayed rewards are used to adjust its policy in order to gain an optimized global decision..Deep RL model, which learns to directly optimize the overall evaluation metrics, works much better than models which learn with loss functions that just evaluate a particular single decision. By this property, RL has been successfully used in many NLP tasks, such as information retrieval BIBREF3 , dialogue system BIBREF4 and relation classification BIBREF5 , etc. To the best of our knowledge, we are the first to design a RL model for global entity linking. And in this paper, our RL model is able to produce more accurate results by exploring the long-term influence of independent decisions and encoding the entities disambiguated in previous states..In summary, the main contributions of our paper mainly include following aspects: generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "20\n",
      "What is the purpose of utilizing a hinge loss in the local encoder model?\n",
      "\n",
      "The purpose of utilizing a hinge loss in the local encoder model is to distinguish the correct target entity and wrong candidate entities when training, by ranking the ground truth higher than others. The rank loss function is used to minimize the rank loss, and a training instance is constructed by pairing a positive target entity with a negative entity.\n",
      "Question : for the text Given a mention $m_t$ and the corresponding candidate set $\\lbrace e_t^1, e_t^2,..., \\\\ e_t^k\\rbrace $ , we aim to get their local representation based on the mention context and the candidate entity description. For each mention, we firstly select its $n$ surrounding words, and represent them as word embedding using a pre-trained lookup table BIBREF11 . Then, we use Long Short-Term Memory (LSTM) networks to encode the contextual word sequence $\\lbrace w_c^1, w_c^2,..., w_c^n\\rbrace $ as a fixed-size vector $V_{m_t}$ . The description of entity is encoded as $D_{e_t^i}$ in the same way. Apart from the description of entity, there are many other valuable information in the knowledge base. To make full use of these information, many researchers trained entity embeddings by combining the description, category, and relationship of entities. As shown in BIBREF0 , entity embeddings compress the semantic meaning of entities and drastically reduce the need for manually designed features or co-occurrence statistics. Therefore, we use the pre-trained entity embedding $E_{e_t^i}$ and concatenate it with the description vector $D_{e_t^i}$ to enrich the entity representation. The concatenation result is denoted by $V_{e_t^i}$ ..After getting $V_{e_t^i}$ , we concatenate it with $V_{m_t}$ and then pass the concatenation result to a multilayer perceptron (MLP). The MLP outputs a scalar to represent the local similarity between the mention $m_t$ and the candidate entity $e_t^i$ . The local similarity is calculated by the following equations: .$$\\Psi (m_t, e_t^i) = MLP(V_{m_t}\\oplus {V_{e_t^i}})$$   (Eq. 9) .Where $\\oplus $ indicates vector concatenation. With the purpose of distinguishing the correct target entity and wrong candidate entities when training the local encoder model, we utilize a hinge loss that ranks ground truth higher than others. The rank loss function is defined as follows: .$$L_{local} = max(0, \\gamma -\\Psi (m_t, e_t^+)+\\Psi (m_t, e_t^-))$$   (Eq. 10) .When optimizing the objective function, we minimize the rank loss similar to BIBREF0 , BIBREF1 . In this ranking model, a training instance is constructed by pairing a positive target entity $e_t^+$ with a negative entity $e_t^-$ . Where $\\gamma > 0$ is a margin parameter and our purpose is to make the score of the positive target entity $e_t^+$ is at least a margin $\\gamma $ higher than that of negative candidate entity $e_t^-$ ..With the local encoder, we obtain the representation of mention context and candidate entities, which will be used as the input into the global encoder and entity selector. In addition, the similarity scores calculated by MLP will be utilized for ranking mentions in the global encoder. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "21\n",
      "Question 1: What are the three parts included in the proposed RLEL model framework?\n",
      "Answer 1: The proposed framework mainly includes three parts: Local Encoder, Global Encoder, and Entity Selector.\n",
      "Question : for the text The overall structure of our RLEL model is shown in Figure 2. The proposed framework mainly includes three parts: Local Encoder which encodes local features of mentions and their candidate entities, Global Encoder which encodes the global coherence of mentions in a sequence manner and Entity Selector which selects an entity from the candidate set. As the Entity Selector and the Global Encoder are correlated mutually, we train them jointly. Moreover, the Local Encoder as the basis of the entire framework will be independently trained before the joint training process starts. In the following, we will introduce the technical details of these modules. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "22\n",
      "Question 1: What is the goal of the entity linking task?\n",
      "\n",
      "Answer 1: The goal of the entity linking task is to map each mention to its corresponding correct target entity or return \"NIL\" if there is no correct target entity in the knowledge base. Before selecting the target entity, a certain number of candidate entities are generated for model selection.\n",
      "Question : for the text Before introducing our model, we firstly define the entity linking task. Formally, given a document $D$ with a set of mentions $M = \\lbrace m_1, m_2,...,m_k\\rbrace $ , each mention $ m_t \\in D$ has a set of candidate entities $C_{m_t} = \\lbrace e_{t}^1, e_{t}^2,..., e_{t}^n\\rbrace $ . The task of entity linking is to map each mention $m_t$ to its corresponding correct target entity $e_{t}^+$ or return \"NIL\" if there is not correct target entity in the knowledge base. Before selecting the target entity, we need to generate a certain number of candidate entities for model selection..Inspired by the previous works BIBREF6 , BIBREF7 , BIBREF8 , we use the mention's redirect and disambiguation pages in Wikipedia to generate candidate sets. For those mentions without corresponding disambiguation pages, we use its n-grams to retrieve the candidates BIBREF8 . In most cases, the disambiguation page contains many entities, sometimes even hundreds. To optimize the model's memory and avoid unnecessary calculations, the candidate sets need to be filtered BIBREF9 , BIBREF0 , BIBREF1 . Here we utilize the XGBoost model BIBREF10 as an entity ranker to reduce the size of candidate set. The features used in XGBoost can be divided into two aspects, the one is string similarity like the Jaro-Winkler distance between the entity title and the mention, the other is semantic similarity like the cosine distance between the mention context representation and the entity embedding. Furthermore, we also use the statistical features based on the pageview and hyperlinks in Wikipedia. Empirically, we get the pageview of the entity from the Wikipedia Tool Labs which counts the number of visits on each entity page in Wikipedia. After ranking the candidate sets based on the above features, we take the top k scored entities as final candidate set for each mention. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "23\n",
      "Question 1: What is reinforcement learning and how has it been applied in natural language processing tasks?\n",
      "\n",
      "Answer 1: Reinforcement learning is a tool used for solving complex sequential decision-making problems. It has been successful in games like Go and Atari. Recently, it has been applied in natural language processing tasks such as relation classification, sentence representation, and automatic taxonomy induction. These applications have achieved good performance and improvements compared to traditional classifiers.\n",
      "Question : for the text In the last few years, reinforcement learning has emerged as a powerful tool for solving complex sequential decision-making problems. It is well known for its great success in the game field, such as Go BIBREF37 and Atari games BIBREF38 . Recently, reinforcement learning has also been successfully applied to many natural language processing tasks and achieved good performance BIBREF12 , BIBREF39 , BIBREF5 . Feng et al. BIBREF5 used reinforcement learning for relation classification task by filtering out the noisy data from the sentence bag and they achieved huge improvements compared with traditional classifiers. Zhang et al. BIBREF40 applied the reinforcement learning on sentence representation by automatically discovering task-relevant structures. To automatic taxonomy induction from a set of terms, Han et al. BIBREF41 designed an end-to-end reinforcement learning model to determine which term to select and where to place it on the taxonomy, which effectively reduced the error propagation between two phases. Inspired by the above works, we also add reinforcement learning to our framework. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "24\n",
      "Question 1: What are the two groups into which the related work can be divided? \n",
      "Answer 1: The related work can be roughly divided into two groups: entity linking and reinforcement learning.\n",
      "Question : for the text The related work can be roughly divided into two groups: entity linking and reinforcement learning. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "25\n",
      "Question 1: What is the reason for focusing on extractive approaches to summarisation in the past participation of the authors in BioASQ BIBREF1, BIBREF2 and this paper?\n",
      "\n",
      "Answer 1: The authors observed that human annotators had a tendency to copy text from the input to generate the target summaries and hence chose to focus on extractive approaches. Their past participating systems used regression approaches for summarisation. In the current participation, they introduced classification approaches alongside regression approaches. The authors experimented with different labelling approaches and found that using the top 5 snippets as the target summary produced better results.\n",
      "Question : for the text Our past participation in BioASQ BIBREF1, BIBREF2 and this paper focus on extractive approaches to summarisation. Our decision to focus on extractive approaches is based on the observation that a relatively large number of sentences from the input snippets has very high ROUGE scores, thus suggesting that human annotators had a general tendency to copy text from the input to generate the target summaries BIBREF1. Our past participating systems used regression approaches using the following framework:.Train the regressor to predict the ROUGE-SU4 F1 score of the input sentence..Produce a summary by selecting the top $n$ input sentences..A novelty in the current participation is the introduction of classification approaches using the following framework..Train the classifier to predict the target label (“summary” or “not summary”) of the input sentence..Produce a summary by selecting all sentences predicted as “summary”..If the total number of sentences selected is less than $n$, select $n$ sentences with higher probability of label “summary”..Introducing a classifier makes labelling the training data not trivial, since the target summaries are human-generated and they do not have a perfect mapping to the input sentences. In addition, some samples have multiple reference summaries. BIBREF11 showed that different data labelling approaches influence the quality of the final summary, and some labelling approaches may lead to better results than using regression. In this paper we experiment with the following labelling approaches:.: Label as “summary” all sentences from the input text that have a ROUGE score above a threshold $t$..: Label as “summary” the $m$ input text sentences with highest ROUGE score..As in BIBREF11, The ROUGE score of an input sentence was the ROUGE-SU4 F1 score of the sentence against the set of reference summaries..We conducted cross-validation experiments using various values of $t$ and $m$. Table TABREF26 shows the results for the best values of $t$ and $m$ obtained. The regressor and classifier used Support Vector Regression (SVR) and Support Vector Classification (SVC) respectively. To enable a fair comparison we used the same input features in all systems. These input features combine information from the question and the input sentence and are shown in Fig. FIGREF16. The features are based on BIBREF12, and are the same as in BIBREF1, plus the addition of the position of the input snippet. The best SVC and SVR parameters were determined by grid search..Preliminary experiments showed a relatively high number of cases where the classifier did not classify any of the input sentences as “summary”. To solve this problem, and as mentioned above, the summariser used in Table TABREF26 introduces a backoff step that extracts the $n$ sentences with highest predicted values when the summary has less than $n$ sentences. The value of $n$ is as reported in our prior work and shown in Table TABREF25..The results confirm BIBREF11's finding that classification outperforms regression. However, the actual choice of optimal labelling scheme was different: whereas in BIBREF11 the optimal labelling was based on a labelling threshold of 0.1, our experiments show a better result when using the top 5 sentences as the target summary. The reason for this difference might be the fact that BIBREF11 used all sentences from the abstracts of the relevant PubMed articles, whereas we use only the snippets as the input to our summariser. Consequently, the number of input sentences is now much smaller. We therefore report the results of using the labelling schema of top 5 snippets in all subsequent classifier-based experiments of this paper..barchart=[fill=black!20,draw=black] errorbar=[very thin,draw=black!75] sscale=[very thin,draw=black!75] generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "26\n",
      "What was the focus of Macquarie University's participation in BioASQ 7?\n",
      "\n",
      "The focus of Macquarie University's participation in BioASQ 7 was on the task of generating ideal answers.\n",
      "Question : for the text Macquarie University's participation in BioASQ 7 focused on the task of generating the ideal answers. The runs use query-based extractive techniques and we experiment with classification, regression, and reinforcement learning approaches. At the time of writing there were no human evaluation results, and based on ROUGE-F1 scores under cross-validation on the training data we observed that classification approaches outperform regression approaches. We experimented with several approaches to label the individual sentences for the classifier and observed that the optimal labelling policy for this task differed from prior work..We also observed poor correlation between ROUGE-Recall and human evaluation metrics and suggest to use alternative automatic evaluation metrics with better correlation, such as ROUGE-Precision or ROUGE-F1. Given the nature of precision-based metrics which could bias the system towards returning short summaries, ROUGE-F1 is probably more appropriate when using at development time, for example for the reward function used by a reinforcement learning system..Reinforcement learning gives promising results, especially in human evaluations made on the runs submitted to BioASQ 6b. This year we introduced very small changes to the runs using reinforcement learning, and will aim to explore more complex reinforcement learning strategies and more complex neural models in the policy and value estimators. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "27\n",
      "What changes were made to the deep learning regression models to convert them to classification models?\n",
      "\n",
      "The authors added a sigmoid activation to the final layer and used cross-entropy as the loss function to convert the deep learning regression models to classification models.\n",
      "Question : for the text Based on the findings of Section SECREF3, we apply minimal changes to the deep learning regression models of BIBREF2 to convert them to classification models. In particular, we add a sigmoid activation to the final layer, and use cross-entropy as the loss function. The complete architecture is shown in Fig. FIGREF28..The bottom section of Table TABREF26 shows the results of several variants of the neural architecture. The table includes a neural regressor (NNR) and a neural classifier (NNC). The neural classifier is trained in two set ups: “NNC top 5” uses classification labels as described in Section SECREF3, and “NNC SU4 F1” uses the regression labels, that is, the ROUGE-SU4 F1 scores of each sentence. Of interest is the fact that “NNC SU4 F1” outperforms the neural regressor. We have not explored this further and we presume that the relatively good results are due to the fact that ROUGE values range between 0 and 1, which matches the full range of probability values that can be returned by the sigmoid activation of the classifier final layer..Table TABREF26 also shows the standard deviation across the cross-validation folds. Whereas this standard deviation is fairly large compared with the differences in results, in general the results are compatible with the top part of the table and prior work suggesting that classification-based approaches improve over regression-based approaches. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "28\n",
      "What is the recommended metric to use for reporting the evaluation of results?\n",
      "\n",
      "The recommended metric to use for reporting the evaluation of results is precision or F1, but it is suggested to use F1 as the metric to optimize.\n",
      "Question : for the text As mentioned in Section SECREF5, there appears to be a large discrepancy between ROUGE Recall and the human evaluations. This section describes a correlation analysis between human and ROUGE evaluations using the runs of all participants to all previous BioASQ challenges that included human evaluations (Phase B, ideal answers). The human evaluation results were scraped from the BioASQ Results page, and the ROUGE results were kindly provided by the organisers. We compute the correlation of each of the ROUGE metrics (recall, precision, F1 for ROUGE-2 and ROUGE-SU4) against the average of the human scores. The correlation metrics are Pearson, Kendall, and a revised Kendall correlation explained below..The Pearson correlation between two variables is computed as the covariance of the two variables divided by the product of their standard deviations. This correlation is a good indication of a linear relation between the two variables, but may not be very effective when there is non-linear correlation..The Spearman rank correlation and the Kendall rank correlation are two of the most popular among metrics that aim to detect non-linear correlations. The Spearman rank correlation between two variables can be computed as the Pearson correlation between the rank values of the two variables, whereas the Kendall rank correlation measures the ordinal association between the two variables using Equation DISPLAY_FORM36..It is useful to account for the fact that the results are from 28 independent sets (3 batches in BioASQ 1 and 5 batches each year between BioASQ 2 and BioASQ 6). We therefore also compute a revised Kendall rank correlation measure that only considers pairs of variable values within the same set. The revised metric is computed using Equation DISPLAY_FORM37, where $S$ is the list of different sets..Table TABREF38 shows the results of all correlation metrics. Overall, ROUGE-2 and ROUGE-SU4 give similar correlation values but ROUGE-SU4 is marginally better. Among precision, recall and F1, both precision and F1 are similar, but precision gives a better correlation. Recall shows poor correlation, and virtually no correlation when using the revised Kendall measure. For reporting the evaluation of results, it will be therefore more useful to use precision or F1. However, given the small difference between precision and F1, and given that precision may favour short summaries when used as a function to optimise in a machine learning setting (e.g. using reinforcement learning), it may be best to use F1 as the metric to optimise..Fig. FIGREF40 shows the scatterplots of ROUGE-SU4 recall, precision and F1 with respect to the average human evaluation. We observe that the relation between ROUGE and the human evaluations is not linear, and that Precision and F1 have a clear correlation. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "29\n",
      "What is the main aim of the BioASQ Challenge question answering task in Phase B, part B?\n",
      "The main aim of the BioASQ Challenge question answering task in Phase B, part B is to find the \"ideal answer\" that would normally be given by a person, in contrast to most other question answering challenges which aim to give an exact answer, usually a fact-based answer or a list.\n",
      "Question : for the text The BioASQ Challenge includes a question answering task (Phase B, part B) where the aim is to find the “ideal answer” — that is, an answer that would normally be given by a person BIBREF0. This is in contrast with most other question answering challenges where the aim is normally to give an exact answer, usually a fact-based answer or a list. Given that the answer is based on an input that consists of a biomedical question and several relevant PubMed abstracts, the task can be seen as an instance of query-based multi-document summarisation..As in past participation BIBREF1, BIBREF2, we wanted to test the use of deep learning and reinforcement learning approaches for extractive summarisation. In contrast with past years where the training procedure was based on a regression set up, this year we experiment with various classification set ups. The main contributions of this paper are:.We compare classification and regression approaches and show that classification produces better results than regression but the quality of the results depends on the approach followed to annotate the data labels..We conduct correlation analysis between various ROUGE evaluation metrics and the human evaluations conducted at BioASQ and show that Precision and F1 correlate better than Recall..Section SECREF2 briefly introduces some related work for context. Section SECREF3 describes our classification and regression experiments. Section SECREF4 details our experiments using deep learning architectures. Section SECREF5 explains the reinforcement learning approaches. Section SECREF6 shows the results of our correlation analysis between ROUGE scores and human annotations. Section SECREF7 lists the specific runs submitted at BioASQ 7b. Finally, Section SECREF8 concludes the paper. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "30\n",
      "What input features were used in the reinforcement learning approach to generate summaries?\n",
      "\n",
      "The reinforcement learning approach used the length of the summary generated so far and $tf.idf$ vectors of the candidate sentence, entire input to summarize, summary generated so far, candidate sentences yet to be processed, and question as input features to the policy model. Word embeddings were also tested but did not show major improvement.\n",
      "Question : for the text We also experiment with the use of reinforcement learning techniques. Again these experiments are based on BIBREF2, who uses REINFORCE to train a global policy. The policy predictor uses a simple feedforward network with a hidden layer..The results reported by BIBREF2 used ROUGE Recall and indicated no improvement with respect to deep learning architectures. Human evaluation results are preferable over ROUGE but these were made available after the publication of the paper. When comparing the ROUGE and human evaluation results (Table TABREF29), we observe an inversion of the results. In particular, the reinforcement learning approaches (RL) of BIBREF2 receive good human evaluation results, and as a matter of fact they are the best of our runs in two of the batches. In contrast, the regression systems (NNR) fare relatively poorly. Section SECREF6 expands on the comparison between the ROUGE and human evaluation scores..Encouraged by the results of Table TABREF29, we decided to continue with our experiments with reinforcement learning. We use the same features as in BIBREF2, namely the length (in number of sentences) of the summary generated so far, plus the $tf.idf$ vectors of the following:.Candidate sentence;.Entire input to summarise;.Summary generated so far;.Candidate sentences that are yet to be processed; and.Question..The reward used by REINFORCE is the ROUGE value of the summary generated by the system. Since BIBREF2 observed a difference between the ROUGE values of the Python implementation of ROUGE and the original Perl version (partly because the Python implementation does not include ROUGE-SU4), we compare the performance of our system when trained with each of them. Table TABREF35 summarises some of our experiments. We ran the version trained on Python ROUGE once, and the version trained on Perl twice. The two Perl runs have different results, and one of them clearly outperforms the Python run. However, given the differences of results between the two Perl runs we advice to re-run the experiments multiple times and obtain the mean and standard deviation of the runs before concluding whether there is any statistical difference between the results. But it seems that there may be an improvement of the final evaluation results when training on the Perl ROUGE values, presumably because the final evaluation results are measured using the Perl implementation of ROUGE..We have also tested the use of word embeddings instead of $tf.idf$ as input features to the policy model, while keeping the same neural architecture for the policy (one hidden layer using the same number of hidden nodes). In particular, we use the mean of word embeddings using 100 and 200 dimensions. These word embeddings were pre-trained using word2vec on PubMed documents provided by the organisers of BioASQ, as we did for the architectures described in previous sections. The results, not shown in the paper, indicated no major improvement, and re-runs of the experiments showed different results on different runs. Consequently, our submission to BioASQ included the original system using $tf.idf$ as input features in all batches but batch 2, as described in Section SECREF7. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "31\n",
      "Question 1: What are the tasks defined for the 2019 BioASQ challenge?\n",
      "\n",
      "Answer 1: The tasks defined for the 2019 BioASQ challenge are Large Scale Online Biomedical Semantic Indexing, Biomedical Semantic QA involving Information Retrieval (IR), Question Answering (QA), and Summarisation, and Medical Semantic Indexing in Spanish.\n",
      "Question : for the text The BioASQ challenge has organised annual challenges on biomedical semantic indexing and question answering since 2013 BIBREF0. Every year there has been a task about semantic indexing (task a) and another about question answering (task b), and occasionally there have been additional tasks. The tasks defined for 2019 are:.Large Scale Online Biomedical Semantic Indexing..Biomedical Semantic QA involving Information Retrieval (IR), Question Answering (QA), and Summarisation..Medical Semantic Indexing in Spanish..BioASQ Task 7b consists of two phases. Phase A provides a biomedical question as an input, and participants are expected to find relevant concepts from designated terminologies and ontologies, relevant articles from PubMed, relevant snippets from the relevant articles, and relevant RDF triples from designated ontologies. Phase B provides a biomedical question and a list of relevant articles and snippets, and participant systems are expected to return the exact answers and the ideal answers. The training data is composed of the test data from all previous years, and amounts to 2,747 samples. There has been considerable research on the use of machine learning approaches for tasks related to text summarisation, especially on single-document summarisation. Abstractive approaches normally use an encoder-decoder architecture and variants of this architecture incorporate attention BIBREF3 and pointer-generator BIBREF4. Recent approaches leveraged the use of pre-trained models BIBREF5. Recent extractive approaches to summarisation incorporate recurrent neural networks that model sequences of sentence extractions BIBREF6 and may incorporate an abstractive component and reinforcement learning during the training stage BIBREF7. But relatively few approaches have been proposed for query-based multi-document summarisation. Table TABREF8 summarises the approaches presented in the proceedings of the 2018 BioASQ challenge. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "32\n",
      "What metric was used in Table TABREF41 to evaluate the runs submitted to BioASQ?\n",
      "\n",
      "The metric used in Table TABREF41 to evaluate the runs submitted to BioASQ was ROUGE-SU4 Recall.\n",
      "Question : for the text Table TABREF41 shows the results and details of the runs submitted to BioASQ. The table uses ROUGE-SU4 Recall since this is the metric available at the time of writing this paper. However, note that, as explained in Section SECREF6, these results might differ from the final human evaluation results. Therefore we do not comment on the results, other than observing that the “first $n$” baseline produces the same results as the neural regressor. As mentioned in Section SECREF3, the labels used for the classification experiments are the 5 sentences with highest ROUGE-SU4 F1 score. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "33\n",
      "What is the goal of the work on MSD conversion?\n",
      "Answer 1: The goal is to translate one resource (the imperfect manifestation of the schema) to match the other, not simply to translate one schema into the other.\n",
      "Question : for the text In our work, the goal is not simply to translate one schema into the other, but to translate one resource (the imperfect manifestation of the schema) to match the other. The differences between the schemata and discrepancies in annotation mean that the transformation of annotations from one schema to the other is not straightforward..Two naive options for the conversion are a lookup table of MSDs and a lookup table of the individual attribute-value pairs which comprise the MSDs. The former is untenable: the table of all UD feature combinations (including null features, excluding language-specific attributes) would have 2.445e17 entries. Of course, most combinations won't exist, but this gives a sense of the table's scale. Also, it doesn't leverage the factorial nature of the annotations: constructing the table would require a massive duplication of effort. On the other hand, attribute-value lookup lacks the flexibility to show how a pair of values interacts. Neither approach would handle language- and annotator-specific tendencies in the corpora..Our approach to converting UD MSDs to UniMorph MSDs begins with the attribute-value lookup, then amends it on a language-specific basis. Alterations informed by the MSD and the word form, like insertion, substitution, and deletion, increase the number of agreeing annotations. They are critical for work that examines the MSD monolithically instead of feature-by-feature BIBREF25 , BIBREF26 : Without exact matches, converting the individual tags becomes hollow..Beginning our process, we relied on documentation of the two schemata to create our initial, language-agnostic mapping of individual values. This mapping has 140 pairs in it. Because the mapping was derived purely from the schemata, it is a useful approximation of how well the schemata match up. We note, however, that the mapping does not handle idiosyncrasies like the many uses of dative or features which are represented in UniMorph by argument templates: possession and ergative–absolutive argument marking. The initial step of our conversion is using this mapping to populate a proposed UniMorph MSD..As shown in sec:results, the initial proposal is often frustratingly deficient. Thus we introduce the post-edits. To concoct these, we looked into UniMorph corpora for these languages, compared these to the conversion outputs, and then sought to bring the conversion outputs closer to the annotations in the actual UniMorph corpora. When a form and its lemma existed in both corpora, we could directly inspect how the annotations differed. Our process of iteratively refining the conversion implies a table which exactly maps any combination of UD MSD and its related values (lemma, form, etc.) to a UniMorph MSD, though we do not store the table explicitly..Some conversion rules we've created must be applied before or after others. These sequential dependencies provide conciseness. Our post-editing procedure operates on the initial MSD hypothesis as follows: generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "34\n",
      "Question 1: Who were the individuals thanked for their input in the starting language-independent mapping from Universal Dependencies to UniMorph? \n",
      "Answer 1: Hajime Senuma and John Sylak-Glassman were thanked for their early comments in devising the starting language-independent mapping from Universal Dependencies to UniMorph.\n",
      "Question : for the text We thank Hajime Senuma and John Sylak-Glassman for early comments in devising the starting language-independent mapping from Universal Dependencies to UniMorph. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "35\n",
      "Question 1: What is the purpose of morphological inflection?\n",
      "\n",
      "Answer 1: The purpose of morphological inflection is to alter the base form of a word to encode morphosyntactic features such as tense, person, gender, and number. It occurs in the majority of the world's widely-spoken languages, typically through meaningful affixes, and creates a challenge of data sparsity for natural language processing.\n",
      "Question : for the text Morphological inflection is the act of altering the base form of a word (the lemma, represented in fixed-width type) to encode morphosyntactic features. As an example from English, prove takes on the form proved to indicate that the action occurred in the past. (We will represent all surface forms in quotation marks.) The process occurs in the majority of the world's widely-spoken languages, typically through meaningful affixes. The breadth of forms created by inflection creates a challenge of data sparsity for natural language processing: The likelihood of observing a particular word form diminishes..A classic result in psycholinguistics BIBREF4 shows that inflectional morphology is a fully productive process. Indeed, it cannot be that humans simply have the equivalent of a lookup table, where they store the inflected forms for retrieval as the syntactic context requires. Instead, there needs to be a mental process that can generate properly inflected words on demand. BIBREF4 showed this insightfully through the wug-test, an experiment where she forced participants to correctly inflect out-of-vocabulary lemmata, such as the novel noun wug..Certain features of a word do not vary depending on its context: In German or Spanish where nouns are gendered, the word for onion will always be grammatically feminine. Thus, to prepare for later discussion, we divide the morphological features of a word into two categories: the modifiable inflectional features and the fixed lexical features..A part of speech (POS) is a coarse syntactic category (like verb) that begets a word's particular menu of lexical and inflectional features. In English, verbs express no gender, and adjectives do not reflect person or number. The part of speech dictates a set of inflectional slots to be filled by the surface forms. Completing these slots for a given lemma and part of speech gives a paradigm: a mapping from slots to surface forms. Regular English verbs have five slots in their paradigm BIBREF5 , which we illustrate for the verb prove, using simple labels for the forms in tab:ptb..A morphosyntactic schema prescribes how language can be annotated—giving stricter categories than our simple labels for prove—and can vary in the level of detail provided. Part of speech tags are an example of a very coarse schema, ignoring details of person, gender, and number. A slightly finer-grained schema for English is the Penn Treebank tagset BIBREF6 , which includes signals for English morphology. For instance, its VBZ tag pertains to the specially inflected 3rd-person singular, present-tense verb form (e.g. proves in tab:ptb)..If the tag in a schema is detailed enough that it exactly specifies a slot in a paradigm, it is called a morphosyntactic description (MSD). These descriptions require varying amounts of detail: While the English verbal paradigm is small enough to fit on a page, the verbal paradigm of the Northeast Caucasian language Archi can have over 1500000 slots BIBREF7 . generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "36\n",
      "What is the purpose of the tool created for annotating Universal Dependencies CoNLL-U files with UniMorph annotations?\n",
      "\n",
      "The purpose of the tool created is to provide a bridge between resources annotated in the Universal Dependencies and Universal Morphology (UniMorph) schemata, allowing for new experiments with morphological tagging while identifying shortcomings in both resources.\n",
      "Question : for the text We created a tool for annotating Universal Dependencies CoNLL-U files with UniMorph annotations. Our tool is ready to use off-the-shelf today, requires no training, and is deterministic. While under-specification necessitates a lossy and imperfect conversion, ours is interpretable. Patterns of mistakes can be identified and ameliorated..The tool allows a bridge between resources annotated in the Universal Dependencies and Universal Morphology (UniMorph) schemata. As the Universal Dependencies project provides a set of treebanks with token-level annotation, while the UniMorph project releases type-level annotated tables, the newfound compatibility opens up new experiments. A prime example of exploiting token- and type-level data is BIBREF34 . That work presents a part-of-speech (POS) dictionary built from Wiktionary, where the POS tagger is also constrained to options available in their type-level POS dictionary, improving performance. Our transformation means that datasets are prepared for similar experiments with morphological tagging. It would also be reasonable to incorporate this tool as a subroutine to UDPipe BIBREF35 and Udapi BIBREF36 . We leave open the task of converting in the opposite direction, turning UniMorph MSDs into Universal Dependencies MSDs..Because our conversion rules are interpretable, we identify shortcomings in both resources, using each as validation for the other. We were able to find specific instances of incorrectly applied UniMorph annotation, as well as specific instances of cross-lingual inconsistency in both resources. These findings will harden both resources and better align them with their goal of universal, cross-lingual annotation. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "37\n",
      "Question 1: What is the scope of the evaluation for the tool? \n",
      "\n",
      "Answer 1: The scope of the evaluation for the tool is limited to schema conversion.\n",
      "Question : for the text We evaluate our tool on two tasks:.To be clear, our scope is limited to the schema conversion. Future work will explore NLP tasks that exploit both the created token-level UniMorph data and the existing type-level UniMorph data. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "38\n",
      "Question 1: What is the downstream task that is chosen to evaluate the performance of the UniMorph-converted treebanks?\n",
      "\n",
      "Answer 1: The downstream task chosen to evaluate the performance of the UniMorph-converted treebanks is morphological tagging, which is a critical step to leveraging morphological information on new text.\n",
      "Question : for the text If the UniMorph-converted treebanks perform differently on downstream tasks, then they convey different information. This signals a failure of the conversion process. As a downstream task, we choose morphological tagging, a critical step to leveraging morphological information on new text..We evaluate taggers trained on the transformed UD data, choosing eight languages randomly from the intersection of UD and UniMorph resources. We report the macro-averaged F1 score of attribute-value pairs on a held-out test set, with official train/validation/test splits provided in the UD treebanks. As a reference point, we also report tagging accuracy on those languages' untransformed data..We use the state-of-the-art morphological tagger of BIBREF0 . It is a factored conditional random field with potentials for each attribute, attribute pair, and attribute transition. The potentials are computed by neural networks, predicting the values of each attribute jointly but not monolithically. Inference with the potentials is performed approximately by loopy belief propagation. We use the authors' hyperparameters..We note a minor implementation detail for the sake of reproducibility. The tagger exploits explicit guidance about the attribute each value pertains to. The UniMorph schema's values are globally unique, but their attributes are not explicit. For example, the UniMorph Masc denotes a masculine gender. We amend the code of BIBREF0 to incorporate attribute identifiers for each UniMorph value. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "39\n",
      "Question 1: How do the authors evaluate the recall of MSDs in their study?\n",
      "Answer 1: The authors evaluate the recall of MSDs without partial credit by comparing the UniMorph-converted MSDs of word-lemma pairs in both UD and UniMorph resources. Success is defined as the computed MSD matching any of the word's UniMorph MSDs.\n",
      "Question : for the text We transform all UD data to the UniMorph. We compare the simple lookup-based transformation to the one with linguistically informed post-edits on all languages with both UD and UniMorph data. We then evaluate the recall of MSDs without partial credit..Because the UniMorph tables only possess annotations for verbs, nouns, adjectives, or some combination, we can only examine performance for these parts of speech. We consider two words to be a match if their form and lemma are present in both resources. Syncretism allows a single surface form to realize multiple MSDs (Spanish mandaba can be first- or third-person), so we define success as the computed MSD matching any of the word's UniMorph MSDs. This gives rise to an equation for recall: of the word–lemma pairs found in both resources, how many of their UniMorph-converted MSDs are present in the UniMorph tables?.Our problem here is not a learning problem, so the question is ill-posed. There is no training set, and the two resources for a given language make up a test set. The quality of our model—the conversion tool—comes from how well we encode prior knowledge about the relationship between the UD and UniMorph corpora. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "40\n",
      "Question 1: What are the two largest standardized, cross-lingual datasets for morphological annotation and how are they annotated?\n",
      "\n",
      "Answer 1: The two largest standardized, cross-lingual datasets for morphological annotation are provided by the Universal Dependencies and Universal Morphology projects. Each project's data is annotated according to its own cross-lingual schema, prescribing how features like gender or case should be marked. The schemata capture largely similar information.\n",
      "Question : for the text The two largest standardized, cross-lingual datasets for morphological annotation are provided by the Universal Dependencies BIBREF1 and Universal Morphology BIBREF2 , BIBREF3 projects. Each project's data are annotated according to its own cross-lingual schema, prescribing how features like gender or case should be marked. The schemata capture largely similar information, so one may want to leverage both UD's token-level treebanks and UniMorph's type-level lookup tables and unify the two resources. This would permit a leveraging of both the token-level UD treebanks and the type-level UniMorph tables of paradigms. Unfortunately, neither resource perfectly realizes its schema. On a dataset-by-dataset basis, they incorporate annotator errors, omissions, and human decisions when the schemata are underspecified; one such example is in fig:disagreement..A dataset-by-dataset problem demands a dataset-by-dataset solution; our task is not to translate a schema, but to translate a resource. Starting from the idealized schema, we create a rule-based tool for converting UD-schema annotations to UniMorph annotations, incorporating language-specific post-edits that both correct infelicities and also increase harmony between the datasets themselves (rather than the schemata). We apply this conversion to the 31 languages with both UD and UniMorph data, and we report our method's recall, showing an improvement over the strategy which just maps corresponding schematic features to each other. Further, we show similar downstream performance for each annotation scheme in the task of morphological tagging..This tool enables a synergistic use of UniMorph and Universal Dependencies, as well as teasing out the annotation discrepancies within and across projects. When one dataset disobeys its schema or disagrees with a related language, the flaws may not be noticed except by such a methodological dive into the resources. When the maintainers of the resources ameliorate these flaws, the resources move closer to the goal of a universal, cross-lingual inventory of features for morphological annotation..The contributions of this work are: generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "41\n",
      "What is the difference between the approach of the Interset project and the approach taken in the project described in the text?\n",
      "\n",
      "Answer 1: The Interset project decodes features in the source corpus to a tag interlingua, then encodes that into target corpus features using a universal representation, while the approach taken in the project described in the text is a direct flight from the source to the target without using an interlingua.\n",
      "Question : for the text The goal of a tagset-to-tagset mapping of morphological annotations is shared by the Interset project BIBREF28 . Interset decodes features in the source corpus to a tag interlingua, then encodes that into target corpus features. (The idea of an interlingua is drawn from machine translation, where a prevailing early mindset was to convert to a universal representation, then encode that representation's semantics in the target language. Our approach, by contrast, is a direct flight from the source to the target.) Because UniMorph corpora are noisy, the encoding from the interlingua would have to be rewritten for each target. Further, decoding the UD MSD into the interlingua cannot leverage external information like the lemma and form..The creators of HamleDT sought to harmonize dependency annotations among treebanks, similar to our goal of harmonizing across resources BIBREF29 . The treebanks they sought to harmonize used multiple diverse annotation schemes, which the authors unified under a single scheme.. BIBREF30 present mappings into a coarse, universal part of speech for 22 languages. Working with POS tags rather than morphological tags (which have far more dimensions), their space of options to harmonize is much smaller than ours..Our extrinsic evaluation is most in line with the paradigm of BIBREF31 (and similar work therein), who compare syntactic parser performance on UD treebanks annotated with two styles of dependency representation. Our problem differs, though, in that the dependency representations express different relationships, while our two schemata vastly overlap. As our conversion is lossy, we do not appraise the learnability of representations as they did..In addition to using the number of extra rules as a proxy for harmony between resources, one could perform cross-lingual projection of morphological tags BIBREF32 , BIBREF33 . Our approach succeeds even without parallel corpora. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "42\n",
      "Question 1: What are some of the reasons for shortcomings in recall?\n",
      "\n",
      "Answer 1: Some of the reasons for shortcomings in recall are annotation errors in the original corpora, differences in the choice of attributes to annotate, and language-specific issues such as a very low number of overlapping forms in Arabic, Hindi, Lithuanian, Persian, and Russian, as well as missing attributes that cannot be inferred without external knowledge.\n",
      "Question : for the text We present the intrinsic task's recall scores in tab:recall. Bear in mind that due to annotation errors in the original corpora (like the vas example from sec:resources), the optimal score is not always $100\\%$ . Some shortcomings of recall come from irremediable annotation discrepancies. Largely, we are hamstrung by differences in choice of attributes to annotate. When one resource marks gender and the other marks case, we can't infer the gender of the word purely from its surface form. The resources themselves would need updating to encode the relevant morphosyntactic information. Some languages had a very low number of overlapping forms, and no tag matches or near-matches between them: Arabic, Hindi, Lithuanian, Persian, and Russian. A full list of observed, irremediable discrepancies is presented alongside the codebase..There are three other transformations for which we note no improvement here. Because of the problem in Basque argument encoding in the UniMorph dataset—which only contains verbs—we note no improvement in recall on Basque. Irish also does not improve: UD marks gender on nouns, while UniMorph marks case. Adjectives in UD are also underspecified. The verbs, though, are already correct with the simple mapping. Finally, with Dutch, the UD annotations are impoverished compared to the UniMorph annotations, and missing attributes cannot be inferred without external knowledge..For the extrinsic task, the performance is reasonably similar whether UniMorph or UD; see tab:tagging. A large fluctuation would suggest that the two annotations encode distinct information. On the contrary, the similarities suggest that the UniMorph-mapped MSDs have similar content. We recognize that in every case, tagging F1 increased—albeit by amounts as small as $0.16$ points. This is in part due to the information that is lost in the conversion. UniMorph's schema does not indicate the type of pronoun (demonstrative, interrogative, etc.), and when lexical information is not recorded in UniMorph, we delete it from the MSD during transformation. On the other hand, UniMorph's atomic tags have more parts to guess, but they are often related. (E.g. Ipfv always entails Pst in Spanish.) Altogether, these forces seem to have little impact on tagging performance. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "43\n",
      "Question 1: What are some attributes that the annotations for the Spanish word \"mandaba\" share between the two schemata?\n",
      "\n",
      "Answer 1: The annotations for \"mandaba\" in both schemata share many attributes, including converting VERB to V, Mood=Ind to IND, Number=Sing to SG, and Person=3 to 3.\n",
      "Question : for the text While the two schemata annotate different features, their annotations often look largely similar. Consider the attested annotation of the Spanish word mandaba (I/he/she/it) commanded. tab:annotations shows that these annotations share many attributes..Some conversions are straightforward: VERB to V, Mood=Ind to IND, Number=Sing to SG, and Person=3 to 3. One might also suggest mapping Tense=Imp to IPFV, though this crosses semantic categories: IPFV represents the imperfective aspect, whereas Tense=Imp comes from imperfect, the English name often given to Spanish's pasado continuo form. The imperfect is a verb form which combines both past tense and imperfective aspect. UniMorph chooses to split this into the atoms PST and IPFV, while UD unifies them according to the familiar name of the tense. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "44\n",
      "Question 1: How does the UniMorph schema differ from the Penn Treebank tags?\n",
      "\n",
      "Answer 1: Unlike the Penn Treebank tags, the UniMorph schema is cross-lingual and aims to be linguistically complete, containing all known morphosyntactic attributes. Additionally, UniMorph's design is guided by a cross-lingual survey of the literature of morphological phenomena, while Penn Treebank tags were not built with these principles in mind.\n",
      "Question : for the text Unlike the Penn Treebank tags, the UD and UniMorph schemata are cross-lingual and include a fuller lexicon of attribute-value pairs, such as Person: 1. Each was built according to a different set of principles. UD's schema is constructed bottom-up, adapting to include new features when they're identified in languages. UniMorph, conversely, is top-down: A cross-lingual survey of the literature of morphological phenomena guided its design. UniMorph aims to be linguistically complete, containing all known morphosyntactic attributes. Both schemata share one long-term goal: a total inventory for annotating the possible morphosyntactic features of a word. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "45\n",
      "What are the three categories of annotation difficulty mentioned in the text?\n",
      "\n",
      "The three categories of annotation difficulty mentioned in the text are missing values, language-specific attributes, and multiword expressions.\n",
      "Question : for the text Prima facie, the alignment task may seem trivial. But we've yet to explore the humans in the loop. This conversion is a hard problem because we're operating on idealized schemata. We're actually annotating human decisions—and human mistakes. If both schemata were perfectly applied, their overlapping attributes could be mapped to each other simply, in a cross-lingual and totally general way. Unfortunately, the resources are imperfect realizations of their schemata. The cross-lingual, cross-resource, and within-resource problems that we'll note mean that we need a tailor-made solution for each language..Showcasing their schemata, the Universal Dependencies and UniMorph projects each present large, annotated datasets. UD's v2.1 release BIBREF1 has 102 treebanks in 60 languages. The large resource, constructed by independent parties, evinces problems in the goal of a universal inventory of annotations. Annotators may choose to omit certain values (like the coerced gender of refrescante in fig:disagreement), and they may disagree on how a linguistic concept is encoded. (See, e.g., BIBREF11 's ( BIBREF11 ) description of the dative case.) Additionally, many of the treebanks were created by fully- or semi-automatic conversion from treebanks with less comprehensive annotation schemata than UD BIBREF0 . For instance, the Spanish word vas you go is incorrectly labeled Gender: Fem|Number: Pl because it ends in a character sequence which is common among feminine plural nouns. (Nevertheless, the part of speech field for vas is correct.).UniMorph's development is more centralized and pipelined. Inflectional paradigms are scraped from Wiktionary, annotators map positions in the scraped data to MSDs, and the mapping is automatically applied to all of the scraped paradigms. Because annotators handle languages they are familiar with (or related ones), realization of the schema is also done on a language-by-language basis. Further, the scraping process does not capture lexical aspects that are not inflected, like noun gender in many languages. The schema permits inclusion of these details; their absence is an artifact of the data collection process. Finally, UniMorph records only exist for nouns, verbs, and adjectives, though the schema is broader than these categories..For these reasons, we treat the corpora as imperfect realizations of the schemata. Moreover, we contend that ambiguity in the schemata leave the door open to allow for such imperfections. With no strict guidance, it's natural that annotators would take different paths. Nevertheless, modulo annotator disagreement, we assume that within a particular corpus, one word form will always be consistently annotated..Three categories of annotation difficulty are missing values, language-specific attributes, and multiword expressions. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "46\n",
      "Question 1: How does the Universal Morphological Feature Schema differ from the Universal Dependencies schema in terms of its part of speech attribute? \n",
      "\n",
      "Answer 1: The Universal Morphological Feature Schema does not include values for punctuation, symbols, or miscellaneous categories in its part of speech attribute, unlike the Universal Dependencies schema which includes these categories as Punct, Sym, and X.\n",
      "Question : for the text In the Universal Morphological Feature Schema BIBREF10 , there are at least 212 values, spread across 23 attributes. It identifies some attributes that UD excludes like information structure and deixis, as well as providing more values for certain attributes, like 23 different noun classes endemic to Bantu languages. As it is a schema for marking morphology, its part of speech attribute does not have POS values for punctuation, symbols, or miscellany (Punct, Sym, and X in Universal Dependencies)..Like the UD schema, the decomposition of a word into its lemma and MSD is directly comparable across languages. Its features are informed by a distinction between universal categories, which are widespread and psychologically real to speakers; and comparative concepts, only used by linguistic typologists to compare languages BIBREF11 . Additionally, it strives for identity of meaning across languages, not simply similarity of terminology. As a prime example, it does not regularly label a dative case for nouns, for reasons explained in depth by BIBREF11 ..The UniMorph resources for a language contain complete paradigms extracted from Wiktionary BIBREF12 , BIBREF13 . Word types are annotated to form a database, mapping a lemma–tag pair to a surface form. The schema is explained in detail in BIBREF10 . It has been used in the SIGMORPHON shared task BIBREF14 and the CoNLL–SIGMORPHON shared tasks BIBREF15 , BIBREF16 . Several components of the UniMorph schema have been adopted by UD. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "47\n",
      "Question 1: What is the purpose of including language-specific attributes in the Universal Dependencies schema?\n",
      "\n",
      "Answer 1: Language-specific attributes are included in the Universal Dependencies schema when only one corpus annotates for a specific feature in order to ensure consistent annotation. Additionally, the UD schema seeks to balance language-specific and cross-lingual concerns.\n",
      "Question : for the text The Universal Dependencies morphological schema comprises part of speech and 23 additional attributes (also called features in UD) annotating meaning or syntax, as well as language-specific attributes. In order to ensure consistent annotation, attributes are included into the general UD schema if they occur in several corpora. Language-specific attributes are used when only one corpus annotates for a specific feature..The UD schema seeks to balance language-specific and cross-lingual concerns. It annotates for both inflectional features such as case and lexical features such as gender. Additionally, the UD schema annotates for features which can be interpreted as derivational in some languages. For example, the Czech UD guidance uses a Coll value for the Number feature to denote mass nouns (for example, \"lidstvo\" \"humankind\" from the root \"lid\" \"people\")..UD represents a confederation of datasets BIBREF8 annotated with dependency relationships (which are not the focus of this work) and morphosyntactic descriptions. Each dataset is an annotated treebank, making it a resource of token-level annotations. The schema is guided by these treebanks, with feature names chosen for relevance to native speakers. (In sec:unimorph, we will contrast this with UniMorph's treatment of morphosyntactic categories.) The UD datasets have been used in the CoNLL shared tasks BIBREF9 . generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "48\n",
      "Question 1: Who funded the research project SEAT (KL 2869/1-1)?\n",
      "\n",
      "Answer 1: The German Research Council (DFG) partially funded the research project SEAT (KL 2869/1-1).\n",
      "Question : for the text We thank Laura-Ana-Maria Bostan for discussions and data set preparations. This research has partially been funded by the German Research Council (DFG), project SEAT (KL 2869/1-1). generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "49\n",
      "Question 1: What is the AMMER data set and how was it collected?\n",
      "\n",
      "Answer 1: The AMMER data set is a collection of interactions between drivers and both a virtual agent and a co-driver. It was collected in a safe and controlled environment using a driving simulator to consider a variety of predefined driving situations.\n",
      "Question : for the text The first contribution of this paper is the construction of the AMMER data set which we describe in the following. We focus on the drivers' interactions with both a virtual agent as well as a co-driver. To collect the data in a safe and controlled environment and to be able to consider a variety of predefined driving situations, the study was conducted in a driving simulator. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "50\n",
      "Question 1: How many interactions were there in total in the experiment?\n",
      "\n",
      "Answer 1: There were 288 interactions in total, with 180 between the driver and the agent and 108 between the driver and co-driver, involving 36 participants aged 18 to 64 years.\n",
      "Question : for the text Overall, 36 participants aged 18 to 64 years ($\\mu $=28.89, $\\sigma $=12.58) completed the experiment. This leads to 288 interactions, 180 between driver and the agent and 108 between driver and co-driver. The emotion self-ratings from the participants yielded 90 utterances labeled with joy, 26 with annoyance, 49 with insecurity, 9 with boredom, 111 with relaxation and 3 with no emotion. One example interaction per interaction type and emotion is shown in Table TABREF7. For further experiments, we only use joy, annoyance/anger, and insecurity/fear due to the small sample size for boredom and no emotion and under the assumption that relaxation brings little expressivity. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "1\n",
      "Question 1: What was the duration of the main driving task in the study?\n",
      "\n",
      "Answer 1: The main driving task in the study had a duration of 20 minutes and included eight speech interactions.\n",
      "Question : for the text At the beginning of the study, participants were welcomed and the upcoming study procedure was explained. Subsequently, participants signed a consent form and completed a questionnaire to provide demographic information. After that, the co-driving experimenter started with the instruction in the simulator which was followed by a familiarization drive consisting of highway and city driving and covering different driving maneuvers such as tight corners, lane changing and strong braking. Subsequently, participants started with the main driving task. The drive had a duration of 20 minutes containing the eight previously mentioned speech interactions. After the completion of the drive, the actual goal of improving automatic emotional recognition was revealed and a standard emotional intelligence questionnaire, namely the TEIQue-SF BIBREF32, was handed to the participants. Finally, a retrospective interview was conducted, in which participants were played recordings of their in-car interactions and asked to give discrete (annoyance, insecurity, joy, relaxation, boredom, none, following BIBREF8) was well as dimensional (valence, arousal, dominance BIBREF33 on a 11-point scale) emotion ratings for the interactions and the according situations. We only use the discrete class annotations in this paper. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "2\n",
      "What type of data is collected in the study environment? \n",
      "\n",
      "The study environment collects data from video, speech, and biosignals, including heart rate, electrodermal activity, and skin temperature. Questionnaires are also utilized. Two RGB cameras capture the driver's face, and a microphone records audio.\n",
      "Question : for the text The study environment consists of a fixed-base driving simulator running Vires's VTD (Virtual Test Drive, v2.2.0) simulation software (https://vires.com/vtd-vires-virtual-test-drive/). The vehicle has an automatic transmission, a steering wheel and gas and brake pedals. We collect data from video, speech and biosignals (Empatica E4 to record heart rate, electrodermal activity, skin temperature, not further used in this paper) and questionnaires. Two RGB cameras are fixed in the vehicle to capture the drivers face, one at the sun shield above the drivers seat and one in the middle of the dashboard. A microphone is placed on the center console. One experimenter sits next to the driver, the other behind the simulator. The virtual agent accompanying the drive is realized as Wizard-of-Oz prototype which enables the experimenter to manually trigger prerecorded voice samples playing trough the in-car speakers and to bring new content to the center screen. Figure FIGREF4 shows the driving simulator..The experimental setting is comparable to an everyday driving task. Participants are told that the goal of the study is to evaluate and to improve an intelligent driving assistant. To increase the probability of emotions to arise, participants are instructed to reach the destination of the route as fast as possible while following traffic rules and speed limits. They are informed that the time needed for the task would be compared to other participants. The route comprises highways, rural roads, and city streets. A navigation system with voice commands and information on the screen keeps the participants on the predefined track..To trigger emotion changes in the participant, we use the following events: (i) a car on the right lane cutting off to the left lane when participants try to overtake followed by trucks blocking both lanes with a slow overtaking maneuver (ii) a skateboarder who appears unexpectedly on the street and (iii) participants are praised for reaching the destination unexpectedly quickly in comparison to previous participants..Based on these events, we trigger three interactions (Table TABREF6 provides examples) with the intelligent agent (Driver-Agent Interactions, D–A). Pretending to be aware of the current situation, e. g., to recognize unusual driving behavior such as strong braking, the agent asks the driver to explain his subjective perception of these events in detail. Additionally, we trigger two more interactions with the intelligent agent at the beginning and at the end of the drive, where participants are asked to describe their mood and thoughts regarding the (upcoming) drive. This results in five interactions between the driver and the virtual agent..Furthermore, the co-driver asks three different questions during sessions with light traffic and low cognitive demand (Driver-Co-Driver Interactions, D–Co). These questions are more general and non-traffic-related and aim at triggering the participants' memory and fantasy. Participants are asked to describe their last vacation, their dream house and their idea of the perfect job. In sum, there are eight interactions per participant (5 D–A, 3 D–Co). generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "3\n",
      "Question 1: What is the purpose of this paper's investigation into multimodal emotion recognition in car environments?\n",
      "\n",
      "Answer 1: The purpose of this paper's investigation is to determine how facial expressions, audio signals of a driver's utterances, and transcribed text can contribute to the task of emotion recognition in in-car speech interactions. The study focuses on the emotions of joy, insecurity, annoyance, relaxation, and boredom in order to adapt to the in-car context. The paper also discusses the potential benefits of properly processing emotions in human-computer interactions and the use of virtual agents in the automotive context.\n",
      "Question : for the text Automatic emotion recognition is commonly understood as the task of assigning an emotion to a predefined instance, for example an utterance (as audio signal), an image (for instance with a depicted face), or a textual unit (e.g., a transcribed utterance, a sentence, or a Tweet). The set of emotions is often following the original definition by Ekman Ekman1992, which includes anger, fear, disgust, sadness, joy, and surprise, or the extension by Plutchik Plutchik1980 who adds trust and anticipation..Most work in emotion detection is limited to one modality. Exceptions include Busso2004 and Sebe2005, who investigate multimodal approaches combining speech with facial information. Emotion recognition in speech can utilize semantic features as well BIBREF0. Note that the term “multimodal” is also used beyond the combination of vision, audio, and text. For example, Soleymani2012 use it to refer to the combination of electroencephalogram, pupillary response and gaze distance..In this paper, we deal with the specific situation of car environments as a testbed for multimodal emotion recognition. This is an interesting environment since it is, to some degree, a controlled environment: Dialogue partners are limited in movement, the degrees of freedom for occurring events are limited, and several sensors which are useful for emotion recognition are already integrated in this setting. More specifically, we focus on emotion recognition from speech events in a dialogue with a human partner and with an intelligent agent..Also from the application point of view, the domain is a relevant choice: Past research has shown that emotional intelligence is beneficial for human computer interaction. Properly processing emotions in interactions increases the engagement of users and can improve performance when a specific task is to be fulfilled BIBREF1, BIBREF2, BIBREF3, BIBREF4. This is mostly based on the aspect that machines communicating with humans appear to be more trustworthy when they show empathy and are perceived as being natural BIBREF3, BIBREF5, BIBREF4..Virtual agents play an increasingly important role in the automotive context and the speech modality is increasingly being used in cars due to its potential to limit distraction. It has been shown that adapting the in-car speech interaction system according to the drivers' emotional state can help to enhance security, performance as well as the overall driving experience BIBREF6, BIBREF7..With this paper, we investigate how each of the three considered modalitites, namely facial expressions, utterances of a driver as an audio signal, and transcribed text contributes to the task of emotion recognition in in-car speech interactions. We focus on the five emotions of joy, insecurity, annoyance, relaxation, and boredom since terms corresponding to so-called fundamental emotions like fear have been shown to be associated to too strong emotional states than being appropriate for the in-car context BIBREF8. Our first contribution is the description of the experimental setup for our data collection. Aiming to provoke specific emotions with situations which can occur in real-world driving scenarios and to induce speech interactions, the study was conducted in a driving simulator. Based on the collected data, we provide baseline predictions with off-the-shelf tools for face and speech emotion recognition and compare them to a neural network-based approach for emotion recognition from text. Our second contribution is the introduction of transfer learning to adapt models trained on established out-of-domain corpora to our use case. We work on German language, therefore the transfer consists of a domain and a language transfer. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "4\n",
      "Question 1: What tool was used for emotion recognition in the audio signal extraction process?\n",
      "Answer 1: An off-the-shelf tool was used for emotion recognition in the audio signal extraction process.\n",
      "Question : for the text We extract the audio signal for the same sequence as described for facial expressions and apply an off-the-shelf tool for emotion recognition. The software delivers single classification scores for a set of 24 discrete emotions for the entire utterance. We consider the outputs for the states of joy, anger, and fear, mapping analogously to our classes as for facial expressions. Low-confidence predictions are interpreted as “no emotion”. We accept the emotion with the highest score as the discrete prediction otherwise. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "5\n",
      "Question 1: What emotions are recognized by the off-the-shelf tool used for emotion recognition?\n",
      "\n",
      "Answer 1: The tool recognizes three discrete emotional states - joy, anger, and fear. We map anger to our label annoyance and fear to our label insecurity. The frame-by-frame scores for these emotions ($\\in [0;100]$) are used to determine the overall classification for the video sequence.\n",
      "Question : for the text We preprocess the visual data by extracting the sequence of images for each interaction from the point where the agent's or the co-driver's question was completely uttered until the driver's response stops. The average length is 16.3 seconds, with the minimum at 2.2s and the maximum at 54.7s. We apply an off-the-shelf tool for emotion recognition (the manufacturer cannot be disclosed due to licensing restrictions). It delivers frame-by-frame scores ($\\in [0;100]$) for discrete emotional states of joy, anger and fear. While joy corresponds directly to our annotation, we map anger to our label annoyance and fear to our label insecurity. The maximal average score across all frames constitutes the overall classification for the video sequence. Frames where the software is not able to detect the face are ignored. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "6\n",
      "Question 1: What approach is used to exploit existing and available data sets which are larger than the AMMER data set for emotion recognition from text?\n",
      "\n",
      "Answer 1: The authors develop a transfer learning approach using a neural network with an embedding layer (pre-trained on Common Crawl and Wikipedia), a bidirectional LSTM, and two dense layers followed by a soft max output layer. They train models on a variety of corpora, namely the FigureEight data set of social media, the ISEAR data (self-reported emotional events), and the Twitter Emotion Corpus (TEC) with instances labeled fear, anger, or joy. The authors apply transfer learning by first training the model until convergence on one out-of-domain corpus, then freezing the parameters of the bi-LSTM layer and further training the remaining layers on AMMER.\n",
      "Question : for the text For the emotion recognition from text, we manually transcribe all utterances of our AMMER study. To exploit existing and available data sets which are larger than the AMMER data set, we develop a transfer learning approach. We use a neural network with an embedding layer (frozen weights, pre-trained on Common Crawl and Wikipedia BIBREF36), a bidirectional LSTM BIBREF37, and two dense layers followed by a soft max output layer. This setup is inspired by BIBREF38. We use a dropout rate of 0.3 in all layers and optimize with Adam BIBREF39 with a learning rate of $10^{-5}$ (These parameters are the same for all further experiments). We build on top of the Keras library with the TensorFlow backend. We consider this setup our baseline model..We train models on a variety of corpora, namely the common format published by BIBREF27 of the FigureEight (formally known as Crowdflower) data set of social media, the ISEAR data BIBREF40 (self-reported emotional events), and, the Twitter Emotion Corpus (TEC, weakly annotated Tweets with #anger, #disgust, #fear, #happy, #sadness, and #surprise, Mohammad2012). From all corpora, we use instances with labels fear, anger, or joy. These corpora are English, however, we do predictions on German utterances. Therefore, each corpus is preprocessed to German with Google Translate. We remove URLs, user tags (“@Username”), punctuation and hash signs. The distributions of the data sets are shown in Table TABREF12..To adapt models trained on these data, we apply transfer learning as follows: The model is first trained until convergence on one out-of-domain corpus (only on classes fear, joy, anger for compatibility reasons). Then, the parameters of the bi-LSTM layer are frozen and the remaining layers are further trained on AMMER. This procedure is illustrated in Figure FIGREF13 generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "7\n",
      "What is the approach taken by Boril2011 in emotion recognition within interactions in the automotive sector?\n",
      "\n",
      "Boril2011 approach the detection of negative emotional states within interactions between driver and co-driver as well as in calls of the driver towards the automated spoken dialogue system using a combination of acoustic features and their respective Gaussian mixture model scores.\n",
      "Question : for the text Past research on emotion recognition from acoustics mainly concentrates on either feature selection or the development of appropriate classifiers. rao2013emotion as well as ververidis2004automatic compare local and global features in support vector machines. Next to such discriminative approaches, hidden Markov models are well-studied, however, there is no agreement on which feature-based classifier is most suitable BIBREF13. Similar to the facial expression modality, recent efforts on applying deep learning have been increased for acoustic speech processing. For instance, lee2015high use a recurrent neural network and palaz2015analysis apply a convolutional neural network to the raw speech signal. Neumann2017 as well as Trigeorgis2016 analyze the importance of features in the context of deep learning-based emotion recognition..In the automotive sector, Boril2011 approach the detection of negative emotional states within interactions between driver and co-driver as well as in calls of the driver towards the automated spoken dialogue system. Using real-world driving data, they find that the combination of acoustic features and their respective Gaussian mixture model scores performs best. Schuller2006 collects 2,000 dialog turns directed towards an automotive user interface and investigate the classification of anger, confusion, and neutral. They show that automatic feature generation and feature selection boost the performance of an SVM-based classifier. Further, they analyze the performance under systematically added noise and develop methods to mitigate negative effects. For more details, we refer the reader to the survey by Schuller2018. In this work, we explore the straight-forward application of domain independent software to an in-car scenario without domain-specific adaptations. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "8\n",
      "Question 1: What is the trend in emotion recognition and how is it being applied in the automotive domain?\n",
      "Answer 1: The trend in emotion recognition has shifted towards performing the recognition directly on images and videos, especially with deep learning techniques. In the automotive domain, the facial action coding system (FACS) is still popular, with studies focusing on continuous driver monitoring. Some studies use support vector machines or simple feature extraction to identify emotions such as happy, anger, and frustration. However, these studies usually focus on driver-only scenarios. Our work investigates the potential of emotion recognition during speech interactions.\n",
      "Question : for the text A common approach to encode emotions for facial expressions is the facial action coding system FACS BIBREF9, BIBREF10, BIBREF11. As the reliability and reproducability of findings with this method have been critically discussed BIBREF12, the trend has increasingly shifted to perform the recognition directly on images and videos, especially with deep learning. For instance, jung2015joint developed a model which considers temporal geometry features and temporal appearance features from image sequences. kim2016hierarchical propose an ensemble of convolutional neural networks which outperforms isolated networks..In the automotive domain, FACS is still popular. Ma2017 use support vector machines to distinguish happy, bothered, confused, and concentrated based on data from a natural driving environment. They found that bothered and confused are difficult to distinguish, while happy and concentrated are well identified. Aiming to reduce computational cost, Tews2011 apply a simple feature extraction using four dots in the face defining three facial areas. They analyze the variance of the three facial areas for the recognition of happy, anger and neutral. Ihme2018 aim at detecting frustration in a simulator environment. They induce the emotion with specific scenarios and a demanding secondary task and are able to associate specific face movements according to FACS. Paschero2012 use OpenCV (https://opencv.org/) to detect the eyes and the mouth region and track facial movements. They simulate different lightning conditions and apply a multilayer perceptron for the classification task of Ekman's set of fundamental emotions..Overall, we found that studies using facial features usually focus on continuous driver monitoring, often in driver-only scenarios. In contrast, our work investigates the potential of emotion recognition during speech interactions. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "9\n",
      "Question 1: What are some examples of annotated corpora used for emotion analysis in natural language processing? \n",
      "\n",
      "Answer 1: Some examples of annotated corpora include fairy tales, blogs, Twitter, Facebook, news headlines, dialogues, literature, and self reports on emotion events. These corpora have been created for various domains and can be used to train emotion classification models for specific tasks.\n",
      "Question : for the text Previous work on emotion analysis in natural language processing focuses either on resource creation or on emotion classification for a specific task and domain. On the side of resource creation, the early and influential work of Pennebaker2015 is a dictionary of words being associated with different psychologically relevant categories, including a subset of emotions. Another popular resource is the NRC dictionary by Mohammad2012b. It contains more than 10000 words for a set of discrete emotion classes. Other resources include WordNet Affect BIBREF14 which distinguishes particular word classes. Further, annotated corpora have been created for a set of different domains, for instance fairy tales BIBREF15, Blogs BIBREF16, Twitter BIBREF17, BIBREF18, BIBREF19, BIBREF20, BIBREF21, Facebook BIBREF22, news headlines BIBREF23, dialogues BIBREF24, literature BIBREF25, or self reports on emotion events BIBREF26 (see BIBREF27 for an overview)..To automatically assign emotions to textual units, the application of dictionaries has been a popular approach and still is, particularly in domains without annotated corpora. Another approach to overcome the lack of huge amounts of annotated training data in a particular domain or for a specific topic is to exploit distant supervision: use the signal of occurrences of emoticons or specific hashtags or words to automatically label the data. This is sometimes referred to as self-labeling BIBREF21, BIBREF28, BIBREF29, BIBREF30..A variety of classification approaches have been tested, including SNoW BIBREF15, support vector machines BIBREF16, maximum entropy classification, long short-term memory network, and convolutional neural network models BIBREF18. More recently, the state of the art is the use of transfer learning from noisy annotations to more specific predictions BIBREF29. Still, it has been shown that transferring from one domain to another is challenging, as the way emotions are expressed varies between areas BIBREF27. The approach by Felbo2017 is different to our work as they use a huge noisy data set for pretraining the model while we use small high quality data sets instead..Recently, the state of the art has also been pushed forward with a set of shared tasks, in which the participants with top results mostly exploit deep learning methods for prediction based on pretrained structures like embeddings or language models BIBREF21, BIBREF31, BIBREF20..Our work follows this approach and builds up on embeddings with deep learning. Furthermore, we approach the application and adaption of text-based classifiers to the automotive domain with transfer learning. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "10\n",
      "Question 1: What is the macro-averaged F1 score for facial emotion recognition across the three emotions joy, insecurity, and annoyance? \n",
      "\n",
      "Answer 1: The macro-averaged F1 score for facial emotion recognition is 33% across the three emotions joy, insecurity, and annoyance.\n",
      "Question : for the text Table TABREF16 shows the confusion matrices for facial and audio emotion recognition on our complete AMMER data set and Table TABREF17 shows the results per class for each method, including facial and audio data and micro and macro averages. The classification from facial expressions yields a macro-averaged $\\text{F}_1$ score of 33 % across the three emotions joy, insecurity, and annoyance (P=0.31, R=0.35). While the classification results for joy are promising (R=43 %, P=57 %), the distinction of insecurity and annoyance from the other classes appears to be more challenging..Regarding the audio signal, we observe a macro $\\text{F}_1$ score of 29 % (P=42 %, R=22 %). There is a bias towards negative emotions, which results in a small number of detected joy predictions (R=4 %). Insecurity and annoyance are frequently confused. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "11\n",
      "Question 1: What are the three different experiments used to evaluate the BiLSTM model for emotion recognition from text?\n",
      "Answer 1: The three different experiments used to evaluate the BiLSTM model for emotion recognition from text are in-domain, out-of-domain, and transfer learning.\n",
      "Question : for the text The experimental setting for the evaluation of emotion recognition from text is as follows: We evaluate the BiLSTM model in three different experiments: (1) in-domain, (2) out-of-domain and (3) transfer learning. For all experiments we train on the classes anger/annoyance, fear/insecurity and joy. Table TABREF19 shows all results for the comparison of these experimental settings. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "12\n",
      "Question 1: What is the average micro F1 score achieved when excluding AMMER from the training data?\n",
      "Answer 1: The average micro F1 score achieved when excluding AMMER from the training data is 68%.\n",
      "Question : for the text We first set a baseline by validating our models on established corpora. We train the baseline model on 60 % of each data set listed in Table TABREF12 and evaluate that model with 40 % of the data from the same domain (results shown in the column “In-Domain” in Table TABREF19). Excluding AMMER, we achieve an average micro $\\text{F}_1$ of 68 %, with best results of F$_1$=73 % on TEC. The model trained on our AMMER corpus achieves an F1 score of 57%. This is most probably due to the small size of this data set and the class bias towards joy, which makes up more than half of the data set. These results are mostly in line with Bostan2018. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "13\n",
      "Question 1: What is the average F1 score for the models in Experiment 2 when applied to the data set?\n",
      "\n",
      "Answer 1: The average F1 score for the models in Experiment 2 when applied to the data set is 48%.\n",
      "Question : for the text Now we analyze how well the models trained in Experiment 1 perform when applied to our data set. The results are shown in column “Simple” in Table TABREF19. We observe a clear drop in performance, with an average of F$_1$=48 %. The best performing model is again the one trained on TEC, en par with the one trained on the Figure8 data. The model trained on ISEAR performs second best in Experiment 1, it performs worst in Experiment 2. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "14\n",
      "What is transfer learning and how does it affect the performance of the models in the AMMER corpus?\n",
      "\n",
      "Transfer learning involves adapting previously trained models to a specific application, such as the AMMER corpus. By retraining the models with the training subset of each crossvalidation iteration of AMMER, the transfer approach improves the performance of the models. This results in an average performance of F$_1$=75%, which is better than the results of the in-domain Experiment 1. The best performance of F$_1$=76% is achieved with the model pre-trained on each data set, except for ISEAR. The transfer approach based on partial retraining of the model improves the performance of all models compared to the \"Joint\" setup.\n",
      "Question : for the text To adapt models trained on previously existing data sets to our particular application, the AMMER corpus, we apply transfer learning. Here, we perform leave-one-out cross validation. As pre-trained models we use each model from Experiment 1 and further optimize with the training subset of each crossvalidation iteration of AMMER. The results are shown in the column “Transfer L.” in Table TABREF19. The confusion matrix is also depicted in Table TABREF16..With this procedure we achieve an average performance of F$_1$=75 %, being better than the results from the in-domain Experiment 1. The best performance of F$_1$=76 % is achieved with the model pre-trained on each data set, except for ISEAR. All transfer learning models clearly outperform their simple out-of-domain counterpart..To ensure that this performance increase is not only due to the larger data set, we compare these results to training the model without transfer on a corpus consisting of each corpus together with AMMER (again, in leave-one-out crossvalidation). These results are depicted in column “Joint C.”. Thus, both settings, “transfer learning” and “joint corpus” have access to the same information..The results show an increase in performance in contrast to not using AMMER for training, however, the transfer approach based on partial retraining the model shows a clear improvement for all models (by 7pp for Figure8, 10pp for EmoInt, 8pp for TEC, 13pp for ISEAR) compared to the ”Joint” setup. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "15\n",
      "Question 1: What is the most promising modality for classification of joy, annoyance and insecurity in AMMER data? \n",
      "\n",
      "Answer 1: The analysis of transcribed utterances is the most promising modality for classification of the three emotional states of joy, annoyance and insecurity in AMMER data.\n",
      "Question : for the text We described the creation of the multimodal AMMER data with emotional speech interactions between a driver and both a virtual agent and a co-driver. We analyzed the modalities of facial expressions, acoustics, and transcribed utterances regarding their potential for emotion recognition during in-car speech interactions. We applied off-the-shelf emotion recognition tools for facial expressions and acoustics. For transcribed text, we developed a neural network-based classifier with transfer learning exploiting existing annotated corpora. We find that analyzing transcribed utterances is most promising for classification of the three emotional states of joy, annoyance and insecurity..Our results for facial expressions indicate that there is potential for the classification of joy, however, the states of annoyance and insecurity are not well recognized. Future work needs to investigate more sophisticated approaches to map frame predictions to sequence predictions. Furthermore, movements of the mouth region during speech interactions might negatively influence the classification from facial expressions. Therefore, the question remains how facial expressions can best contribute to multimodal detection in speech interactions..Regarding the classification from the acoustic signal, the application of off-the-shelf classifiers without further adjustments seems to be challenging. We find a strong bias towards negative emotional states for our experimental setting. For instance, the personalization of the recognition algorithm (e. g., mean and standard deviation normalization) could help to adapt the classification for specific speakers and thus to reduce this bias. Further, the acoustic environment in the vehicle interior has special properties and the recognition software might need further adaptations..Our transfer learning-based text classifier shows considerably better results. This is a substantial result in its own, as only one previous method for transfer learning in emotion recognition has been proposed, in which a sentiment/emotion specific source for labels in pre-training has been used, to the best of our knowledge BIBREF29. Other applications of transfer learning from general language models include BIBREF41, BIBREF42. Our approach is substantially different, not being trained on a huge amount of noisy data, but on smaller out-of-domain sets of higher quality. This result suggests that emotion classification systems which work across domains can be developed with reasonable effort..For a productive application of emotion detection in the context of speech events we conclude that a deployed system might perform best with a speech-to-text module followed by an analysis of the text. Further, in this work, we did not explore an ensemble model or the interaction of different modalities. Thus, future work should investigate the fusion of multiple modalities in a single classifier. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "16\n",
      "Question 1: Who provided funding for Rico Sennrich's project?\n",
      "Answer 1: Rico Sennrich received funding from the Swiss National Science Foundation in the project CoNTra with grant number 105212_169888.\n",
      "Question : for the text Rico Sennrich has received funding from the Swiss National Science Foundation in the project CoNTra (grant number 105212_169888). Biao Zhang acknowledges the support of the Baidu Scholarship. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "17\n",
      "Question 1: What is the significance of the study's findings for low-resource machine translation research? \n",
      "Answer 1: The study's findings demonstrate that neural machine translation (NMT) can be a suitable choice for low-data settings and can outperform phrase-based statistical machine translation (PBSMT) with far less parallel training data than previously claimed. Additionally, the study highlights the sensitivity of low-resource NMT to hyperparameters and the importance of following best practices to train competitive NMT systems without relying on auxiliary resources, which is practical for languages without access to large amounts of monolingual or multilingual data. The study also suggests that the quality of supervised systems trained on little data can impact the effectiveness of semi-supervised or unsupervised approaches in low-resource MT research.\n",
      "Question : for the text Our results demonstrate that NMT is in fact a suitable choice in low-data settings, and can outperform PBSMT with far less parallel training data than previously claimed. Recently, the main trend in low-resource MT research has been the better exploitation of monolingual and multilingual resources. Our results show that low-resource NMT is very sensitive to hyperparameters such as BPE vocabulary size, word dropout, and others, and by following a set of best practices, we can train competitive NMT systems without relying on auxiliary resources. This has practical relevance for languages where large amounts of monolingual data, or multilingual data involving related languages, are not available. Even though we focused on only using parallel data, our results are also relevant for work on using auxiliary data to improve low-resource MT. Supervised systems serve as an important baseline to judge the effectiveness of semisupervised or unsupervised approaches, and the quality of supervised systems trained on little data can directly impact semi-supervised workflows, for instance for the back-translation of monolingual data. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "18\n",
      "Question 1: What is the size of the training data for the Korean-English language pair?\n",
      "\n",
      "Answer 1: The Korean-English language pair has around 90000 parallel sentences of training data.\n",
      "Question : for the text We use the TED data from the IWSLT 2014 German INLINEFORM0 English shared translation task BIBREF38 . We use the same data cleanup and train/dev split as BIBREF39 , resulting in 159000 parallel sentences of training data, and 7584 for development..As a second language pair, we evaluate our systems on a Korean–English dataset with around 90000 parallel sentences of training data, 1000 for development, and 2000 for testing..For both PBSMT and NMT, we apply the same tokenization and truecasing using Moses scripts. For NMT, we also learn BPE subword segmentation with 30000 merge operations, shared between German and English, and independently for Korean INLINEFORM0 English..To simulate different amounts of training resources, we randomly subsample the IWSLT training corpus 5 times, discarding half of the data at each step. Truecaser and BPE segmentation are learned on the full training corpus; as one of our experiments, we set the frequency threshold for subword units to 10 in each subcorpus (see SECREF7 ). Table TABREF14 shows statistics for each subcorpus, including the subword vocabulary..Translation outputs are detruecased, detokenized, and compared against the reference with cased BLEU using sacreBLEU BIBREF40 , BIBREF41 . Like BIBREF39 , we report BLEU on the concatenated dev sets for IWSLT 2014 (tst2010, tst2011, tst2012, dev2010, dev2012). generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "19\n",
      "Question 1: What is the trend in high-resource settings regarding the size and depth of models used for hyperparameter optimization?\n",
      "\n",
      "Answer 1: The trend in high-resource settings is towards using larger and deeper models.\n",
      "Question : for the text Due to long training times, hyperparameters are hard to optimize by grid search, and are often re-used across experiments. However, best practices differ between high-resource and low-resource settings. While the trend in high-resource settings is towards using larger and deeper models, BIBREF24 use smaller and fewer layers for smaller datasets. Previous work has argued for larger batch sizes in NMT BIBREF35 , BIBREF36 , but we find that using smaller batches is beneficial in low-resource settings. More aggressive dropout, including dropping whole words at random BIBREF37 , is also likely to be more important. We report results on a narrow hyperparameter search guided by previous work and our own intuition. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "20\n",
      "Question 1: Were the hyperparameters consistent across different data settings in the ablation study?\n",
      "\n",
      "Answer 1: Yes, the hyperparameters were kept constant across different data settings in the ablation study, except for the validation interval and subword vocabulary size.\n",
      "Question : for the text Table TABREF23 lists hyperparameters used for the different experiments in the ablation study (Table 2). Hyperparameters were kept constant across different data settings, except for the validation interval and subword vocabulary size (see Table 1). generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "21\n",
      "Question 1: What are some methods to improve NMT with monolingual data?\n",
      "\n",
      "Answer 1: Methods to improve NMT with monolingual data include integrating a separately trained language model, training parts of the NMT model with additional objectives such as a language modelling objective, an autoencoding objective, or a round-trip objective. Models that rely exclusively on monolingual data have also been shown to work.\n",
      "Question : for the text The bulk of research on low-resource NMT has focused on exploiting monolingual data, or parallel data involving other language pairs. Methods to improve NMT with monolingual data range from the integration of a separately trained language model BIBREF5 to the training of parts of the NMT model with additional objectives, including a language modelling objective BIBREF5 , BIBREF6 , BIBREF7 , an autoencoding objective BIBREF8 , BIBREF9 , or a round-trip objective, where the model is trained to predict monolingual (target-side) training data that has been back-translated into the source language BIBREF6 , BIBREF10 , BIBREF11 . As an extreme case, models that rely exclusively on monolingual data have been shown to work BIBREF12 , BIBREF13 , BIBREF14 , BIBREF4 . Similarly, parallel data from other language pairs can be used to pre-train the network or jointly learn representations BIBREF15 , BIBREF16 , BIBREF17 , BIBREF18 , BIBREF19 , BIBREF20 , BIBREF21 ..While semi-supervised and unsupervised approaches have been shown to be very effective for some language pairs, their effectiveness depends on the availability of large amounts of suitable auxiliary data, and other conditions being met. For example, the effectiveness of unsupervised methods is impaired when languages are morphologically different, or when training domains do not match BIBREF22 .More broadly, this line of research still accepts the premise that NMT models are data-inefficient and require large amounts of auxiliary data to train. In this work, we want to re-visit this point, and will focus on techniques to make more efficient use of small amounts of parallel training data. Low-resource NMT without auxiliary data has received less attention; work in this direction includes BIBREF23 , BIBREF24 . generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "22\n",
      "Question 1: What is the main argument of the paper regarding neural machine translation (NMT)?\n",
      "\n",
      "Answer 1: The paper argues that recent research claiming that NMT underperforms in low-data conditions compared to statistical machine translation or unsupervised methods is due to a lack of system adaptation to low-resource settings. The authors seek to reassess the validity of these results.\n",
      "Question : for the text While neural machine translation (NMT) has achieved impressive performance in high-resource data conditions, becoming dominant in the field BIBREF0 , BIBREF1 , BIBREF2 , recent research has argued that these models are highly data-inefficient, and underperform phrase-based statistical machine translation (PBSMT) or unsupervised methods in low-data conditions BIBREF3 , BIBREF4 . In this paper, we re-assess the validity of these results, arguing that they are the result of lack of system adaptation to low-resource settings. Our main contributions are as follows: generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "23\n",
      "Question 1: Why is it important to have a minimum frequency threshold for subword units in low-resource settings?\n",
      "\n",
      "Answer 1: It is important to have a minimum frequency threshold for subword units in low-resource settings because large vocabularies result in low-frequency (sub)words being represented as atomic units at training time, and the ability to learn good high-dimensional representations of these is doubtful. A minimum frequency threshold reduces the need to carefully tune the vocabulary size to the dataset and leads to more aggressive segmentation on smaller datasets.\n",
      "Question : for the text Subword representations such as BPE BIBREF31 have become a popular choice to achieve open-vocabulary translation. BPE has one hyperparameter, the number of merge operations, which determines the size of the final vocabulary. For high-resource settings, the effect of vocabulary size on translation quality is relatively small; BIBREF32 report mixed results when comparing vocabularies of 30k and 90k subwords..In low-resource settings, large vocabularies result in low-frequency (sub)words being represented as atomic units at training time, and the ability to learn good high-dimensional representations of these is doubtful. BIBREF33 propose a minimum frequency threshold for subword units, and splitting any less frequent subword into smaller units or characters. We expect that such a threshold reduces the need to carefully tune the vocabulary size to the dataset, leading to more aggressive segmentation on smaller datasets. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "24\n",
      "Question 1: What is the core idea behind the lexical model?\n",
      "\n",
      "Answer 1: The core idea behind the lexical model is to train a simple feed-forward network jointly with the original attentional NMT model. The input of the lexical model is the weighted average of source embeddings, and after a feedforward layer, the lexical model's output is combined with the original model's hidden state before softmax computation.\n",
      "Question : for the text Finally, we implement and test the lexical model by BIBREF24 , which has been shown to be beneficial in low-data conditions. The core idea is to train a simple feed-forward network, the lexical model, jointly with the original attentional NMT model. The input of the lexical model at time step INLINEFORM0 is the weighted average of source embeddings INLINEFORM1 (the attention weights INLINEFORM2 are shared with the main model). After a feedforward layer (with skip connection), the lexical model's output INLINEFORM3 is combined with the original model's hidden state INLINEFORM4 before softmax computation. INLINEFORM5 . Our implementation adds dropout and layer normalization to the lexical model... generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "25\n",
      "Question 1: In what scenario does the NMT system outperform the PBSMT system according to the plot in Figure FIGREF4?\n",
      "\n",
      "Answer 1: The NMT system only outperforms the PBSMT system when more than 100 million words (approx. 5 million sentences) of parallel training data are available.\n",
      "Question : for the text Figure FIGREF4 reproduces a plot by BIBREF3 which shows that their NMT system only outperforms their PBSMT system when more than 100 million words (approx. 5 million sentences) of parallel training data are available. Results shown by BIBREF4 are similar, showing that unsupervised NMT outperforms supervised systems if few parallel resources are available. In both papers, NMT systems are trained with hyperparameters that are typical for high-resource settings, and the authors did not tune hyperparameters, or change network architectures, to optimize NMT for low-resource conditions. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "26\n",
      "Question 1: What is the baseline for hyperparameters used in this study?\n",
      "\n",
      "Answer 1: The baseline for hyperparameters used in this study is the one used by BIBREF3.\n",
      "Question : for the text We consider the hyperparameters used by BIBREF3 to be our baseline. This baseline does not make use of various advances in NMT architectures and training tricks. In contrast to the baseline, we use a BiDeep RNN architecture BIBREF25 , label smoothing BIBREF26 , dropout BIBREF27 , word dropout BIBREF28 , layer normalization BIBREF29 and tied embeddings BIBREF30 . generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "27\n",
      "Question 1: What is the method used for early stopping during neural system training?\n",
      "\n",
      "Answer 1: The method used for early stopping during neural system training is based on dev set BLEU.\n",
      "Question : for the text We train neural systems with Nematus BIBREF46 . Our baseline mostly follows the settings in BIBREF3 ; we use adam BIBREF47 and perform early stopping based on dev set BLEU. We express our batch size in number of tokens, and set it to 4000 in the baseline (comparable to a batch size of 80 sentences used in previous work)..We subsequently add the methods described in section SECREF3 , namely the bideep RNN, label smoothing, dropout, tied embeddings, layer normalization, changes to the BPE vocabulary size, batch size, model depth, regularization parameters and learning rate. Detailed hyperparameters are reported in Appendix SECREF7 . generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "28\n",
      "Question 1: What tools are used to train a PBSMT system in the text?\n",
      "\n",
      "Answer 1: Moses BIBREF42 is used to train a PBSMT system. MGIZA BIBREF43 is used to train word alignments, and lmplz BIBREF44 is used for a 5-gram LM.\n",
      "Question : for the text We use Moses BIBREF42 to train a PBSMT system. We use MGIZA BIBREF43 to train word alignments, and lmplz BIBREF44 for a 5-gram LM. Feature weights are optimized on the dev set to maximize BLEU with batch MIRA BIBREF45 – we perform multiple runs where indicated. Unlike BIBREF3 , we do not use extra data for the LM. Both PBSMT and NMT can benefit from monolingual data, so the availability of monolingual data is no longer an exclusive advantage of PBSMT (see SECREF5 ). generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "29\n",
      "Question 1: What techniques were found to be effective in improving the ultra-low data condition in the NMT system?\n",
      "\n",
      "Answer 1: The reduction of BPE vocabulary size, aggressive (word) dropout, and tuning other hyperparameters were found to be more effective than the lexical model in improving the ultra-low data condition in the NMT system. The adaptations to the ultra-low data setting yielded 9.4 BLEU.\n",
      "Question : for the text Table TABREF18 shows the effect of adding different methods to the baseline NMT system, on the ultra-low data condition (100k words of training data) and the full IWSLT 14 training corpus (3.2M words). Our \"mainstream improvements\" add around 6–7 BLEU in both data conditions..In the ultra-low data condition, reducing the BPE vocabulary size is very effective (+4.9 BLEU). Reducing the batch size to 1000 token results in a BLEU gain of 0.3, and the lexical model yields an additional +0.6 BLEU. However, aggressive (word) dropout (+3.4 BLEU) and tuning other hyperparameters (+0.7 BLEU) has a stronger effect than the lexical model, and adding the lexical model (9) on top of the optimized configuration (8) does not improve performance. Together, the adaptations to the ultra-low data setting yield 9.4 BLEU (7.2 INLINEFORM2 16.6). The model trained on full IWSLT data is less sensitive to our changes (31.9 INLINEFORM3 32.8 BLEU), and optimal hyperparameters differ depending on the data condition. Subsequently, we still apply the hyperparameters that were optimized to the ultra-low data condition (8) to other data conditions, and Korean INLINEFORM4 English, for simplicity..For a comparison with PBSMT, and across different data settings, consider Figure FIGREF19 , which shows the result of PBSMT, our NMT baseline, and our optimized NMT system. Our NMT baseline still performs worse than the PBSMT system for 3.2M words of training data, which is consistent with the results by BIBREF3 . However, our optimized NMT system shows strong improvements, and outperforms the PBSMT system across all data settings. Some sample translations are shown in Appendix SECREF8 ..For comparison to previous work, we report lowercased and tokenized results on the full IWSLT 14 training set in Table TABREF20 . Our results far outperform the RNN-based results reported by BIBREF48 , and are on par with the best reported results on this dataset..Table TABREF21 shows results for Korean INLINEFORM0 English, using the same configurations (1, 2 and 8) as for German–English. Our results confirm that the techniques we apply are successful across datasets, and result in stronger systems than previously reported on this dataset, achieving 10.37 BLEU as compared to 5.97 BLEU reported by gu-EtAl:2018:EMNLP1. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "30\n",
      "Question 1: What is the difference between how PBSMT and NMT systems handle unknown words?\n",
      "\n",
      "Answer 1: PBSMT systems default to copying unknown words, while NMT systems produce subword-level translations with varying success.\n",
      "Question : for the text Table TABREF24 shows some sample translations that represent typical errors of our PBSMT and NMT systems, trained with ultra-low (100k words) and low (3.2M words) amounts of data. For unknown words such as blutbefleckten (`bloodstained') or Spaniern (`Spaniards', `Spanish'), PBSMT systems default to copying, while NMT systems produce translations on a subword-level, with varying success (blue-flect, bleed; spaniers, Spanians). NMT systems learn some syntactic disambiguation even with very little data, for example the translation of das and die as relative pronouns ('that', 'which', 'who'), while PBSMT produces less grammatical translation. On the flip side, the ultra low-resource NMT system ignores some unknown words in favour of a more-or-less fluent, but semantically inadequate translation: erobert ('conquered') is translated into doing, and richtig aufgezeichnet ('registered correctly', `recorded correctly') into really the first thing. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "31\n",
      "Question 1: What do network visualizations in exploring opinion landscapes represent?\n",
      "\n",
      "Answer 1: Network visualizations in exploring opinion landscapes represent argumentative statements (beliefs) as nodes and the degree of similarity between these statements as edges.\n",
      "Question : for the text Based on the presupposition that relations between causation frames reveal beliefs, the output of the semantic frame extractor creates various opportunities for exploring opinion landscapes and empirically validating conceptual models for opinion dynamics..In general, any alignment of conceptual models and real-world data is an exercise in compromising, as the idealized, abstract nature of models is likely to be at odds with the messiness of the actual data. Finding such a compromise might for instance involve a reduction of the simplicity or elegance of the model, or, on the other hand, an increased aggregation (and thus reduced granularity) of the data..Addressing this challenge, the current section reflects on questions of data modelling, aggregation and meaning by exploring, through case examples, different spatial representations of opinion landscapes mined from the TheGuardian.com's comment sphere. These spatial renditions will be understood as network visualizations in which nodes represent argumentative statements (beliefs) and edges the degree of similarity between these statements. On the most general level, then, such a representation can consists of an overview of all the causes expressed in the corpus of climate change-related Guardian comments. This type of visualization provides a birds-eye view of the entire opinion landscape as mined from the comment texts. In turn, such a general overview might elicit more fine-grained, micro-level investigations, in which a particular cause is singled out and its more specific associated effects are mapped. These macro and micro level overviews come with their own proper potential for theory building and evaluation, as well as distinct requirements for the depth or detail of meaning that needs to be represented. To get the most general sense of an opinion landscape one might for instance be more tolerant of abstract renditions of beliefs (e.g. by reducing statements to their most frequently used terms), but for more fine-grained analysis one requires more context and nuance (e.g. adhering as closely as possible to the original comment). generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "32\n",
      "Question 1: What is the relationship between meaning aggregation and issues of scale and aggregation over utterances? \n",
      "\n",
      "Answer 1: The more fine-grained the semantic resolution is, the less probable it is to observe the same statement twice. Moreover, as there are more independent variables, less data on which fine-grained opinion statements are to be detected is available. This is closely related to issues of scale and aggregation over utterances.\n",
      "Question : for the text As follows from the above, one of the most fundamental questions when building automated tools to observe opinion dynamics that potentially aim at advising means of debate facilitation concerns the level of meaning aggregation. A clear argumentative or causal association between, for instance, climate change and catastrophic events such as floods or hurricanes may become detectable by automatic causal frame tracking at the scale of large collections of articles where this association might appear statistically more often, but detection comes with great challenges when the aim is to classify certain sets of only a few statements in more free expression environments such as comment spheres..In other words, the problem of meaning aggregation is closely related to issues of scale and aggregation over utterances. The more fine-grained the semantic resolution is, that is, the more specific the cause or effect is that one is interested in, the less probable it is to observe the same statement twice. Moreover, with every independent variable (such as time, different commenters or user groups, etc.), less data on which fine-grained opinion statements are to be detected is available. In the present case of parsed comments from TheGuardian.com, providing insights into the belief system of individual commenters, even if all their statements are aggregated over time, relies on a relatively small set of argumentative statements. This relative sparseness is in part due to the fact that the scope of the semantic frame extractor is confined to the frame evoking elements listed earlier, thus omitting more implicit assertions of causation (i.e. expressions of causation that can only be derived from context and from reading between the lines)..Similarly, as will be explored in the ensuing paragraphs, matters of scale and aggregation determine the types of further linguistic analyses that can be performed on the output of the frame extractor. Within the field of computational linguistics, various techniques have been developed to represent the meaning of words as vectors that capture the contexts in which these words are typically used. Such analyses might reveal patterns of statistical significance, but it is also likely that in creating novel, numerical representations of the original utterances, the semantic structure of argumentatively linked beliefs is lost..In sum, developing opinion observatories and (potential) debate facilitators entails finding a trade-off, or, in fact, a middle way between macro- and micro-level analyses. On the one hand, one needs to leverage automated analysis methods at the scale of larger collections to maximum advantage. But one also needs to integrate opportunities to interactively zoom into specific aspects of interest and provide more fine-grained information at these levels down to the actual statements. This interplay between macro- and micro-level analyses is explored in the case studies below. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "33\n",
      "Question 1: What is the desired outcome of the semantic frame extractor in the observatory's pipeline?\n",
      "\n",
      "Answer 1: The desired outcome of further processing of the output of the semantic frame extractor is a network representation in which similar cause or effect strings are displayed in close proximity to one another.\n",
      "Question : for the text The main purpose of the observatory under discussion is to provide insight into the belief structures that characterize the opinion landscape on climate change. For reasons outlined above, this raises questions of how to represent opinions and, correspondingly, determining which representation is most suited as the atomic unit of comparison between opinions. In general terms, the desired outcome of further processing of the output of the semantic frame extractor is a network representation in which similar cause or effect strings are displayed in close proximity to one another. A high-level description of the pipeline under discussion thus goes as follows. In a first step, it can be decided whether one wants to map cause statements or effect statements. Next, the selected statements are grouped per commenter (i.e. a list is made of all cause statements or effect statements per commenter). These statements are filtered in order to retain only nouns, adjectives and verbs (thereby also omitting frequently occurring verbs such as ‘to be’). The remaining words are then lemmatized, that is, reduced to their dictionary forms. This output is finally translated into a network representation, whereby nodes represent (aggregated) statements, and edges express the semantic relatedness between statements (based on a set overlap whereby the number of shared lemmata are counted)..As illustrated by two spatial renditions that were created using this approach and visualized using the network analysis tool Gephi BIBREF26, the labels assigned to these nodes (lemmata, full statements, or other) can be appropriated to the scope of the analysis. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "34\n",
      "Question 1: What is the purpose of creating a map of all the causes mentioned in comments related to articles on climate change?\n",
      "\n",
      "Answer 1: The purpose of creating a map of all the causes mentioned in comments related to articles on climate change is to get a first idea about the scope and diversity of an opinion landscape and to inspire a sense of what those opinions might be about, pointing towards potentially interesting phenomena that might warrant closer examination.\n",
      "Question : for the text Suppose one wants to get a first idea about the scope and diversity of an opinion landscape, without any preconceived notions of this landscape's structure or composition. One way of doing this would be to map all of the causes that are mentioned in comments related to articles on climate change, that is, creating an overview of all the causes that have been retrieved by the frame extractor in a single representation. Such a representation would not immediately provide the granularity to state what the beliefs or opinions in the debates actually are, but rather, it might inspire a sense of what those opinions might be about, thus pointing towards potentially interesting phenomena that might warrant closer examination..Figure FIGREF10, a high-level overview of the opinion landscape, reveals a number of areas to which opinions and beliefs might pertain. The top-left clusters in the diagram for instance reveal opinions about the role of people and countries, whereas on the right-hand side, we find a complementary cluster that might point to beliefs concerning the influence of high or increased CO2-emissions. In between, there is a cluster on power and energy sources, reflecting the energy debate's association to both issues of human responsibility and CO2 emissions. As such, the overview can already inspire, potentially at best, some very general hypotheses about the types of opinions that figure in the climate change debate. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "35\n",
      "Question 1: What is the purpose of conducting a micro-level analysis of beliefs?\n",
      "Answer 1: The purpose of conducting a micro-level analysis of beliefs is to reveal what those beliefs are and whether they align or contradict each other, by mapping out the associated effects of a cause of interest. This is achieved by singling out a cause of interest and analyzing the opinions and beliefs related to it.\n",
      "Question : for the text Based on the range of topics on which beliefs are expressed, a micro-level analysis can be conducted to reveal what those beliefs are and, for instance, whether they align or contradict each other. This can be achieved by singling out a cause of interest, and mapping out its associated effects..As revealed by the global overview of the climate change opinion landscape, a portion of the debate concerns power and energy sources. One topic with a particularly interesting role in this debate is nuclear power. Figure FIGREF12 illustrates how a more detailed representation of opinions on this matter can be created by spatially representing all of the effects associated with causes containing the expression `nuclear power'. Again, similar beliefs (in terms of words used in the effects) are positioned closer to each other, thus facilitating the detection of clusters. Commenters on The Guardian for instance express concerns about the deaths or extinction that might be caused by this energy resource. They also voice opinions on its cleanliness, whether or not it might decrease pollution or be its own source of pollution, and how it reduces CO2-emissions in different countries..Whereas the detailed opinion landscape on `nuclear power' is relatively limited in terms of the number of mined opinions, other topics might reveal more elaborate belief systems. This is for instance the case for the phenomenon of `global warming'. As shown in Figure FIGREF13, opinions on global warming are clustered around the idea of `increases', notably in terms of evaporation, drought, heat waves, intensity of cyclones and storms, etc. An adjacent cluster is related to `extremes', such as extreme summers and weather events, but also extreme colds. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "36\n",
      "Question 1: What are some of the challenges associated with using computational methods to study on-line information and opinions?\n",
      "\n",
      "Answer 1: Some of the challenges associated with using computational methods include the potential loss of nuance and meaning when investigating large quantities of information, as well as the need for high-quality and accessible data, reproducibility, and transparency in models. Additionally, human interpretation is still crucial in attributing meaning to the data being analyzed.\n",
      "Question : for the text Ongoing transitions from a print-based media ecology to on-line news and discussion platforms have put traditional forms of news production and consumption at stake. Many challenges related to how information is currently produced and consumed come to a head in news website comment sections, which harbour the potential of providing new insights into how cultural conflicts emerge and evolve. On the basis of an observatory for analyzing climate change-related comments from TheGuardian.com, this article has critically examined possibilities and limitations of the machine-assisted exploration and possible facilitation of on-line opinion dynamics and debates..Beyond technical and modelling pathways, this examination brings into view broader methodological and epistemological aspects of the use of digital methods to capture and study the flow of on-line information and opinions. Notably, the proposed approaches lift questions of computational analysis and interpretation that can be tied to an overarching tension between `distant' and `close reading' BIBREF40. In other words, monitoring on-line opinion dynamics means embracing the challenges and associated trade-offs that come with investigating large quantities of information through computational, text-analytical means, but doing this in such a way that nuance and meaning are not lost in the process..Establishing productive cross-overs between the level of opinions mined at scale (for instance through the lens of causation frames) and the detailed, closer looks at specific conversations, interactions and contexts depends on a series of preliminaries. One of these is the continued availability of high-quality, accessible data. As the current on-line media ecology is recovering from recent privacy-related scandals (e.g. Cambridge Analytica), such data for obvious reasons is not always easy to come by. In the same legal and ethical vein, reproducibility and transparency of models is crucial to the further development of analytical tools and methods. As the experiments discussed in this paper have revealed, a key factor in this undertaking are human faculties of interpretation. Just like the encoding schemes introduced by Axelrod and others before the wide-spread use of computational methods, present-day pipelines and tools foreground the role of human agents as the primary source of meaning attribution..<This project has received funding from the European Union’s Horizon 2020 research and innovation programme under grant agreement No 732942 (Opinion Dynamics and Cultural Conflict in European Spaces – www.Odycceus.eu).> generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "37\n",
      "Question 1: What concerns need to be acknowledged when constructing a device that promotes more constructive discussions on on-line platforms?\n",
      "\n",
      "Answer 1: There are concerns about potential manipulation and bias, ensuring transparency and quality of data, complying with data legislation and policy, building trust and communities, and envisioning beneficial implications. Developers and computational social scientists should also be aware that digital information is really just people in disguise, and that interventions on on-line communication platforms can have both positive and negative impacts.\n",
      "Question : for the text The observatory introduced in the preceding paragraphs provides preliminary insights into the range and scope of the beliefs that figure in climate change debates on TheGuardian.com. The observatory as such takes a distinctly descriptive stance, and aims to satisfy, at least in part, the information needs of researchers, activists, journalists and other stakeholders whose main concern is to document, investigate and understand on-line opinion dynamics. However, in the current information sphere, which is marked by polarization, misinformation and a close entanglement with real-world conflicts, taking a mere descriptive or neutral stance might not serve every stakeholder's needs. Indeed, given the often skewed relations between power and information, questions arise as to how media observations might in turn be translated into (political, social or economic) action. Knowledge about opinion dynamics might for instance inform interventions that remedy polarization or disarm conflict. In other words, the construction of (social) media observatories unavoidably lifts questions about the possibilities, limitations and, especially, implications of the machine-guided and human-incentivized facilitation of on-line discussions and debates..Addressing these questions, the present paragraph introduces and explores the concept of a debate facilitator, that is, a device that extends the capabilities of the previously discussed observatory to also promote more interesting and constructive discussions. Concretely, we will conceptualize a device that reveals how the personal opinion landscapes of commenters relate to each other (in terms of overlap or lack thereof), and we will discuss what steps might potentially be taken on the basis of such representation to balance the debate. Geared towards possible interventions in the debate, such a device may thus go well beyond the observatory's objectives of making opinion processes and conflicts more transparent, which concomitantly raises a number of serious concerns that need to be acknowledged..On rather fundamental ground, tools that steer debates in one way or another may easily become manipulative and dangerous instruments in the hands of certain interest groups. Various aspects of our daily lives are for instance already implicitly guided by recommender systems, the purpose and impact of which can be rather opaque. For this reason, research efforts across disciplines are directed at scrutinizing and rendering such systems more transparent BIBREF28. Such scrutiny is particularly pressing in the context of interventions on on-line communication platforms, which have already been argued to enforce affective communication styles that feed rather than resolve conflict. The objectives behind any facilitation device should therefore be made maximally transparent and potential biases should be fully acknowledged at every level, from data ingest to the dissemination of results BIBREF29. More concretely, the endeavour of constructing opinion observatories and facilitators foregrounds matters of `openness' of data and tools, security, ensuring data quality and representative sampling, accounting for evolving data legislation and policy, building communities and trust, and envisioning beneficial implications. By documenting the development process for a potential facilitation device, the present paper aims to contribute to these on-going investigations and debates. Furthermore, every effort has been made to protect the identities of the commenters involved. In the words of media and technology visionary Jaron Lanier, developers and computational social scientists entering this space should remain fundamentally aware of the fact that `digital information is really just people in disguise' BIBREF30..With these reservations in mind, the proposed approach can be situated among ongoing efforts that lead from debate observation to facilitation. One such pathway, for instance, involves the construction of filters to detect hate speech, misinformation and other forms of expression that might render debates toxic BIBREF31, BIBREF32. Combined with community outreach, language-based filtering and detection tools have proven to raise awareness among social media users about the nature and potential implications of their on-line contributions BIBREF33. Similarly, advances can be expected from approaches that aim to extend the scope of analysis beyond descriptions of a present debate situation in order to model how a debate might evolve over time and how intentions of the participants could be included in such an analysis..Progress in any of these areas hinges on a further integration of real-world data in the modelling process, as well as a further socio-technical and media-theoretical investigation of how activity on social media platforms and technologies correlate to real-world conflicts. The remainder of this section therefore ventures to explore how conceptual argument communication models for polarization and alignment BIBREF34 might be reconciled with real-world data, and how such models might inform debate facilitation efforts. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "38\n",
      "Question 1: What patterns of alignment and polarization were observed among the two most active commenters on TheGuardian.com in regards to climate change beliefs?\n",
      "\n",
      "Answer 1: The two most active commenters on TheGuardian.com generally had an extensive and somewhat related opinion landscape, with both expressing beliefs on climate change. However, there were some disparities in their belief systems as one commenter used terms related to the economical and socio-political realm and industry, while the other used more technical and scientific terms. The most related beliefs were on the most general aspects of the debate, with less relatedness on the social and more technical aspects. This seems to evoke ongoing debates about the role and responsibilities of people versus experts in regards to climate change. However, facilitation of debate between users with complementary arguments supporting similar opinions may lead to increasing polarization at the collective level.\n",
      "Question : for the text As discussed in previous sections, news websites like TheGuardian.com establish a communicative settings in which agents (users, commenters) exchange arguments about different issues or topics. For those seeking to establish a healthy debate, it could thus be of interest to know how different users relate to each other in terms of their beliefs about a certain issue or topic (in this case climate change). Which beliefs are for instance shared by users and which ones are not? In other words, can we map patterns of alignment or polarization among users?.Figure FIGREF15 ventures to demonstrate how representations of opinion landscapes (generated using the methods outlined above) can be enriched with user information to answer such questions. Specifically, the graph represents the beliefs of two among the most active commenters in the corpus. The opinions of each user are marked using a colour coding scheme: red nodes represent the beliefs of the first user, blue nodes represent the beliefs of the second user. Nodes with a green colour represent beliefs that are shared by both users..Taking into account again the factors of aggregation that were discussed in the previous section, Figure FIGREF15 supports some preliminary observations about the relationship between the two users in terms of their beliefs. Generally, given the fact that the graph concerns the two most active commenters on the website, it can be seen that the rendered opinion landscape is quite extensive. It is also clear that the belief systems of both users are not unrelated, as nodes of all colours can be found distributed throughout the graph. This is especially the case for the right-hand top cluster and right-hand bottom cluster of the graph, where green, red, and blue nodes are mixed. Since both users are discussing on articles on climate change, a degree of affinity between opinions or beliefs is to be expected..Upon closer examination, a number of disparities between the belief systems of the two commenters can be detected. Considering the left-hand top cluster and center of the graph, it becomes clear that exclusively the red commenter is using a selection of terms related to the economical and socio-political realm (e.g. `people', `american', `nation', `government') and industry (e.g. `fuel', `industry', `car', etc.). The blue commenter, on the other hand, exclusively engages in using a range of terms that could be deemed more technical and scientific in nature (e.g. `feedback', `property', `output', `trend', `variability', etc.). From the graph, it also follows that the blue commenter does not enter into the red commenter's `social' segments of the graph as frequently as the red commenter enters the more scientifically-oriented clusters of the graph (although in the latter cases the red commenter does not use the specific technical terminology of the blue commenter). The cluster where both beliefs mingle the most (and where overlap can be observed), is the top right cluster. This overlap is constituted by very general terms (e.g. `climate', `change', and `science'). In sum, the graph reveals that the commenters' beliefs are positioned most closely to each other on the most general aspects of the debate, whereas there is less relatedness on the social and more technical aspects of the debate. In this regard, the depicted situation seemingly evokes currently on-going debates about the role or responsibilities of the people or individuals versus that of experts when it comes to climate change BIBREF35, BIBREF36, BIBREF37..What forms of debate facilitation, then, could be based on these observations? And what kind of collective effects can be expected? As follows from the above, beliefs expressed by the two commenters shown here (which are selected based on their active participation rather than actual engagement or dialogue with one another) are to some extent complementary, as the blue commenter, who displays a scientifically-oriented system of beliefs, does not readily engage with the social topics discussed by the red commenter. As such, the overall opinion landscape of the climate change could potentially be enriched with novel perspectives if the blue commenter was invited to engage in a debate about such topics as industry and government. Similarly, one could explore the possibility of providing explanatory tools or additional references on occasions where the debate takes a more technical turn..However, argument-based models of collective attitude formation BIBREF38, BIBREF34 also tell us to be cautious about such potential interventions. Following the theory underlying these models, different opinion groups prevailing during different periods of a debate will activate different argumentative associations. Facilitating exchange between users with complementary arguments supporting similar opinions may enforce biased argument pools BIBREF39 and lead to increasing polarization at the collective level. In the example considered here the two commenters agree on the general topic, but the analysis suggests that they might have different opinions about the adequate direction of specific climate change action. A more fine–grained automatic detection of cognitive and evaluative associations between arguments and opinions is needed for a reliable use of models to predict what would come out of facilitating exchange between two specific users. In this regard, computational approaches to the linguistic analysis of texts such as semantic frame extraction offer productive opportunities for empirically modelling opinion dynamics. Extraction of causation frames allows one to disentangle cause-effect relations between semantic units, which provides a productive step towards mapping and measuring structures of cognitive associations. These opportunities are to be explored by future work. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "39\n",
      "What is the potential benefit of data mining and analysis of online news platforms in addressing social conflicts and crises?\n",
      "\n",
      "Answer 1: Data mining and analysis of online news platforms has the potential to help us better understand and analyze the problems facing our contemporary society, monitor growing conflicts and crises due to cultural differences and diverging world-views, and even facilitate early detection of conflicts and ways to resolve them before they turn violent.\n",
      "Question : for the text Over the past two decades, the rise of social media and the digitization of news and discussion platforms have radically transformed how individuals and groups create, process and share news and information. As Alan Rusbridger, former-editor-in-chief of the newspaper The Guardian has it, these technologically-driven shifts in the ways people communicate, organize themselves and express their beliefs and opinions, have.empower[ed] those that were never heard, creating a a new form of politics and turning traditional news corporations inside out. It is impossible to think of Donald Trump; of Brexit; of Bernie Sanders; of Podemos; of the growth of the far right in Europe; of the spasms of hope and violent despair in the Middle East and North Africa without thinking also of the total inversion of how news is created, shared and distributed. Much of it is liberating and and inspiring. Some of it is ugly and dark. And something - the centuries-old craft of journalism - is in danger of being lost BIBREF0..Rusbridger's observation that the present media-ecology puts traditional notions of politics, journalism, trust and truth at stake is a widely shared one BIBREF1, BIBREF2, BIBREF3. As such, it has sparked interdisciplinary investigations, diagnoses and ideas for remedies across the economical, socio-political, and technological spectrum, challenging our existing assumptions and epistemologies BIBREF4, BIBREF5. Among these lines of inquiry, particular strands of research from the computational social sciences are addressing pressing questions of how emerging technologies and digital methods might be operationalized to regain a grip on the dynamics that govern the flow of on-line news and its associated multitudes of voices, opinions and conflicts. Could the information circulating on on-line (social) news platforms for instance be mined to better understand and analyze the problems facing our contemporary society? Might such data mining and analysis help us to monitor the growing number of social conflicts and crises due to cultural differences and diverging world-views? And finally, would such an approach potentially facilitate early detection of conflicts and even ways to resolve them before they turn violent?.Answering these questions requires further advances in the study of cultural conflict based on digital media data. This includes the development of fine-grained representations of cultural conflict based on theoretically-informed text analysis, the integration of game-theoretical approaches to models of polarization and alignment, as well as the construction of accessible tools and media-monitoring observatories: platforms that foster insight into the complexities of social behaviour and opinion dynamics through automated computational analyses of (social) media data. Through an interdisciplinary approach, the present article aims to make both a practical and theoretical contribution to these aspects of the study of opinion dynamics and conflict in new media environments. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "40\n",
      "Question 1: What is The Guardian's business model and how does it enable reader involvement and engagement?\n",
      "\n",
      "Answer 1: The Guardian relies on a business model that combines revenues from advertising, voluntary donations and paid subscriptions. It has transformed itself into a platform that enables forms of citizen journalism, blogging and welcomes readers' comments on news articles. To comment on articles, users are required to make a user account, which fosters proper online community behavior. The Guardian also launched an open API through which developers can access different types of content.\n",
      "Question : for the text In order to study on-line opinion dynamics and build the corresponding climate change opinion observatory discussed in this paper, a corpus of climate-change related news articles and news website comments was analyzed. Concretely, articles from the ‘climate change’ subsection from the news website of The Guardian dated from 2009 up to April 2019 were processed, along with up to 200 comments and associated metadata for articles where commenting was enabled at the time of publication. The choice for studying opinion dynamics using data from The Guardian is motivated by this news website's prominent position in the media landscape as well as its communicative setting, which is geared towards user engagement. Through this interaction with readers, the news platform embodies many of the recent shifts that characterize our present-day media ecology..TheGuardian.com is generally acknowledged to be one of the UK's leading online newspapers, with 8,2 million unique visitors per month as of May 2013 BIBREF6. The website consists of a core news site, as well as a range of subsections that allow for further classification and navigation of articles. Articles related to climate change can for instance be accessed by navigating through the `News' section, over the subsection `environment', to the subsubsection `climate change' BIBREF7. All articles on the website can be read free of charge, as The Guardian relies on a business model that combines revenues from advertising, voluntary donations and paid subscriptions..Apart from offering high-quality, independent journalism on a range of topics, a distinguishing characteristic of The Guardian is its penchant for reader involvement and engagement. Adopting to the changing media landscape and appropriating business models that fit the transition from print to on-line news media, the Guardian has transformed itself into a platform that enables forms of citizen journalism, blogging, and welcomes readers comments on news articles BIBREF0. In order for a reader to comment on articles, it is required that a user account is made, which provides a user with a unique user name and a user profile page with a stable URL. According to the website's help pages, providing users with an identity that is consistently recognized by the community fosters proper on-line community behaviour BIBREF8. Registered users can post comments on content that is open to commenting, and these comments are moderated by a dedicated moderation team according to The Guardian's community standards and participation guidelines BIBREF9. In support of digital methods and innovative approaches to journalism and data mining, The Guardian has launched an open API (application programming interface) through which developers can access different types of content BIBREF10. It should be noted that at the moment of writing this article, readers' comments are not accessible through this API. For the scientific and educational purposes of this paper, comments were thus consulted using a dedicated scraper..Taking into account this community and technologically-driven orientation, the communicative setting of The Guardian from which opinions are to be mined and the underlying belief system revealed, is defined by articles, participating commenters and comment spheres (that is, the actual comments aggregated by user, individual article or collection of articles) (see Figure FIGREF4)..In this setting, articles (and previous comments on those articles) can be commented on by participating commenters, each of which bring to the debate his or her own opinions or belief system. What this belief system might consists of can be inferred on a number of levels, with varying degrees of precision. On the most general level, a generic description of the profile of the average reader of The Guardian can be informative. Such profiles have been compiled by market researchers with the purpose of informing advertisers about the demographic that might be reached through this news website (and other products carrying The Guardian's brand). As of the writing of this article, the audience The Guardian is presented to advertisers as a `progressive' audience:.Living in a world of unprecedented societal change, with the public narratives around politics, gender, body image, sexuality and diet all being challenged. The Guardian is committed to reflecting the progressive agenda, and reaching the crowd that uphold those values. It’s helpful that we reach over half of progressives in the UK BIBREF11..A second, equally high-level indicator of the beliefs that might be present on the platform, are the links through which articles on climate change can be accessed. An article on climate change might for instance be consulted through the environment section of the news website, but also through the business section. Assuming that business interests might potentially be at odds with environmental concerns, it could be hypothesized that the particular comment sphere for that article consists of at least two potentially clashing frames of mind or belief systems..However, as will be expanded upon further in this article, truly capturing opinion dynamics requires a more systemic and fine-grained approach. The present article therefore proposes a method for harvesting opinions from the actual comment texts. The presupposition is thereby that comment spheres are marked by a diversity of potentially related opinions and beliefs. Opinions might for instance be connected through the reply structure that marks the comment section of an article, but this connection might also manifest itself on a semantic level (that is, the level of meaning or the actual contents of the comments). To capture this multidimensional, interconnected nature of the comment spheres, it is proposed to represent comment spheres as networks, where the nodes represent opinions and beliefs, and edges the relationships between these beliefs (see the spatial representation of beliefs infra). The use of precision language tools to extract such beliefs and their mutual relationships, as will be explored in the following sections, can open up new pathways of model validation and creation. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "41\n",
      "Question 1: What is the theoretical premise on which the proposed approach is based?\n",
      "Answer 1: The proposed approach is based on the theoretical premise that expressions of causation can reveal a person or group's underlying belief systems, and therefore semantic frames expressing causation are used for the automated extraction of beliefs or opinions from texts.\n",
      "Question : for the text The objective of the present article is to critically examine possibilities and limitations of machine-guided exploration and potential facilitation of on-line opinion dynamics on the basis of an experimental data analytics pipeline or observatory for mining and analyzing climate change-related user comments from the news website of The Guardian (TheGuardian.com). Combining insights from the social and political sciences with computational methods for the linguistic analysis of texts, this observatory provides a series of spatial (network) representations of the opinion landscapes on climate change on the basis of causation frames expressed in news website comments. This allows for the exploration of opinion spaces at different levels of detail and aggregation..Technical and theoretical questions related to the proposed method and infrastructure for the exploration and facilitation of debates will be discussed in three sections. The first section concerns notions of how to define what constitutes a belief or opinion and how these can be mined from texts. To this end, an approach based on the automated extraction of semantic frames expressing causation is proposed. The observatory thus builds on the theoretical premise that expressions of causation such as `global warming causes rises in sea levels' can be revelatory for a person or group's underlying belief systems. Through a further technical description of the observatory's data-analytical components, section two of the paper deals with matters of spatially modelling the output of the semantic frame extractor and how this might be achieved without sacrificing nuances of meaning. The final section of the paper, then, discusses how insights gained from technologically observing opinion dynamics can inform conceptual modelling efforts and approaches to on-line opinion facilitation. As such, the paper brings into view and critically evaluates the fundamental conceptual leap from machine-guided observation to debate facilitation and intervention..Through the case examples from The Guardian's website and the theoretical discussions explored in these sections, the paper intends to make a twofold contribution to the fields of media studies, opinion dynamics and computational social science. Firstly, the paper introduces and chains together a number of data analytics components for social media monitoring (and facilitation) that were developed in the context of the <project name anonymized for review> infrastructure project. The <project name anonymized for review> infrastructure makes the components discussed in this paper available as open web services in order to foster reproducibility and further experimentation and development <infrastructure reference URL anonymized for review>. Secondly, and supplementing these technological and methodological gains, the paper addresses a number of theoretical, epistemological and ethical questions that are raised by experimental approaches to opinion exploration and facilitation. This notably includes methodological questions on the preservation of meaning through text and data mining, as well as the role of human interpretation, responsibility and incentivisation in observing and potentially facilitating opinion dynamics. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "42\n",
      "What are the two main challenges related to data-gathering and text mining when studying opinion landscapes through online social media?\n",
      "\n",
      "The two main challenges are defining what constitutes an expression of an opinion or belief, and associating this definition with a pattern that can be extracted from texts.\n",
      "Question : for the text In traditional experimental settings, survey techniques and associated statistical models provide researchers with established methods to gauge and analyze the opinions of a population. When studying opinion landscapes through on-line social media, however, harvesting beliefs from big textual data such as news website comments and developing or appropriating models for their analysis is a non-trivial task BIBREF12, BIBREF13, BIBREF14..In the present context, two challenges related to data-gathering and text mining need to be addressed: (1) defining what constitutes an expression of an opinion or belief, and (2) associating this definition with a pattern that might be extracted from texts. Recent scholarship in the fields of natural language processing (NLP) and argumentation mining has yielded a range of instruments and methods for the (automatic) identification of argumentative claims in texts BIBREF15, BIBREF16. Adding to these instruments and methods, the present article proposes an approach in which belief systems or opinions on climate change are accessed through expressions of causation. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "43\n",
      "What is the purpose of the Penelope semantic frame extractor in the study of comments on The Guardian?\n",
      "\n",
      "The Penelope semantic frame extractor is used to automatically mine cause and effect concepts from the corpus of comments on The Guardian. It is a tool that exploits the fact that semantic frames can be expressed as form-meaning mappings called constructions, and frames were extracted from Guardian comments by focusing on specific lexical units that evoke the Causation frame. The output of this tool is used as the input for further analysis to find patterns in the beliefs and opinions expressed in the comments.\n",
      "Question : for the text As outlined in the previous section, causal mapping is based on the extraction of so-called cause concepts, (causal) relations, and effect concepts from texts. The complexity of each of these these concepts can range from the relatively simple (as illustrated by the easily-identifiable cause and effect relation in the example of `German militarism' cited earlier), to more complex assertions such as `The development of international cooperation in all fields across the ideological frontiers will gradually remove the hostility and fear that poison international relations', which contains two effect concepts (viz. `the hostility that poisons international relations' and `the fear that poisons international relations'). As such, this statement would have to be encoded as a double relationship BIBREF19..The coding guidelines in BIBREF19 further reflect that extracting cause and effect concepts from texts is an operation that works on both the syntactical and semantic levels of assertions. This can be illustrated by means of the guidelines for analyzing the aforementioned causal assertion on German militarism:.1. The first step is the realization of the relationship. Does a subject affect an object? 2. Having recognized that it does, the isolation of the cause and effects concepts is the second step. As the sentence structure indicates, \"the militarism of Germany\" is the causal concept, because it is the initiator of the action, while the direct object clause, \"a state of tension in the Baltic area,\" constitutes that which is somehow influenced, the effect concept BIBREF19..In the field of computational linguistics, from which the present paper borrows part of its methods, this procedure for extracting information related to causal assertions from texts can be considered an instance of an operation called semantic frame extraction BIBREF23. A semantic frame captures a coherent part of the meaning of a sentence in a structured way. As documented in the FrameNet project BIBREF24, the Causation frame is defined as follows:.A Cause causes an Effect. Alternatively, an Actor, a participant of a (implicit) Cause, may stand in for the Cause. The entity Affected by the Causation may stand in for the overall Effect situation or event BIBREF25..In a linguistic utterance such as a statement in a news website comment, the Causation frame can be evoked by a series of lexical units, such as `cause', `bring on', etc. In the example `If such a small earthquake CAUSES problems, just imagine a big one!', the Causation frame is triggered by the verb `causes', which therefore is called the frame evoking element. The Cause slot is filled by `a small earthquake', the Effect slot by `problems' BIBREF25..In order to automatically mine cause and effects concepts from the corpus of comments on The Guardian, the present paper uses the Penelope semantic frame extractor: a tool that exploits the fact that semantic frames can be expressed as form-meaning mappings called constructions. Notably, frames were extracted from Guardian comments by focusing on the following lexical units (verbs, prepositions and conjunctions), listed in FrameNet as frame evoking elements of the Causation frame: Cause.v, Due to.prep, Because.c, Because of.prep, Give rise to.v, Lead to.v or Result in.v..As illustrated by the following examples, the strings output by the semantic frame extractor adhere closely to the original utterance, preserving all of the the comments' causation frames real-world noisiness:.The output of the semantic frame extractor as such is used as the input for the ensuing pipeline components in the climate change opinion observatory. The aim of a further analysis of these frames is to find patterns in the beliefs and opinions they express. As will be discussed in the following section, which focuses on applications and cases, maintaining semantic nuances in this further analytic process foregrounds the role of models and aggregation levels. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "44\n",
      "What is the benefit of using a causation-based approach in analyzing the climate change debate?\n",
      "\n",
      "The benefit of using a causation-based approach is that it offers a systemic approach to opinion dynamics that comprises different layers of meaning, notably the cognitive or social meaningfulness of patterns on account of their being expressions of causation, as well as further lexical and semantic information that might be used for analysis and comparison. It provides a valuable indicator for the range and diversity of the opinions and beliefs that constitute the climate change debate.\n",
      "Question : for the text The climate change debate is often characterized by expressions of causation, that is, expressions linking a certain cause with a certain effect. Cultural or societal clashes on climate change might for instance concern diverging assessments of whether global warming is man-made or not BIBREF17. Based on such examples, it can be stated that expressions of causation are closely associated with opinions or beliefs, and that as such, these expressions can be considered a valuable indicator for the range and diversity of the opinions and beliefs that constitute the climate change debate. The observatory under discussion therefore focuses on the extraction and analysis of linguistic patterns called causation frames. As will be further demonstrated in this section, the benefit of this causation-based approach is that it offers a systemic approach to opinion dynamics that comprises different layers of meaning, notably the cognitive or social meaningfulness of patterns on account of their being expressions of causation, as well as further lexical and semantic information that might be used for analysis and comparison..The study of expressions of causation as a method for accessing and assessing belief systems and opinions has been formalized and streamlined since the 1970s. Pioneered by political scientist Robert Axelrod and others, this causal mapping method (also referred to as `cognitive mapping') was introduced as a means of reconstructing and evaluating administrative and political decision-making processes, based on the principle that.the notion of causation is vital to the process of evaluating alternatives. Regardless of philosophical difficulties involved in the meaning of causation, people do evaluate complex policy alternatives in terms of the consequences a particular choice would cause, and ultimately of what the sum of these effects would be. Indeed, such causal analysis is built into our language, and it would be very difficult for us to think completely in other terms, even if we tried BIBREF18..Axelrod's causal mapping method comprises a set of conventions to graphically represent networks of causes and effects (the nodes in a network) as well as the qualitative aspects of this relation (the network’s directed edges, notably assertions of whether the causal linkage is positive or negative). These causes and effects are to be extracted from relevant sources by means of a series of heuristics and an encoding scheme (it should be noted that for this task Axelrod had human readers in mind). The graphs resulting from these efforts provide a structural overview of the relations among causal assertions (and thus beliefs):.The basic elements of the proposed system are quite simple. The concepts a person uses are represented as points, and the causal links between these concepts are represented as arrows between these points. This gives a pictorial representation of the causal assertions of a person as a graph of points and arrows. This kind of representation of assertions as a graph will be called a cognitive map. The policy alternatives, all of the various causes and effects, the goals, and the ultimate utility of the decision maker can all be thought of as concept variables, and represented as points in the cognitive map. The real power of this approach appears when a cognitive map is pictured in graph form; it is then relatively easy to see how each of the concepts and causal relationships relate to each other, and to see the overall structure of the whole set of portrayed assertions BIBREF18..In order to construct these cognitive maps based on textual information, Margaret Tucker Wrightson provides a set of reading and coding rules for extracting cause concepts, linkages (relations) and effect concepts from expressions in the English language. The assertion `Our present topic is the militarism of Germany, which is maintaining a state of tension in the Baltic Area' might for instance be encoded as follows: `the militarism of Germany' (cause concept), /+/ (a positive relationship), `maintaining a state of tension in the Baltic area' (effect concept) BIBREF19. Emphasizing the role of human interpretation, it is acknowledged that no strict set of rules can capture the entire spectrum of causal assertions:.The fact that the English language is as varied as those who use it makes the coder's task complex and difficult. No set of rules will completely solve the problems he or she might encounter. These rules, however, provide the coder with guidelines which, if conscientiously followed, will result in outcomes meeting social scientific standards of comparative validity and reliability BIBREF19..To facilitate the task of encoders, the causal mapping method has gone through various iterations since its original inception, all the while preserving its original premises. Recent software packages have for instance been devised to support the data encoding and drawing process BIBREF20. As such, causal or cognitive mapping has become an established opinion and decision mining method within political science, business and management, and other domains. It has notably proven to be a valuable method for the study of recent societal and cultural conflicts. Thomas Homer-Dixon et al. for instance rely on cognitive-affective maps created from survey data to analyze interpretations of the housing crisis in Germany, Israeli attitudes toward the Western Wall, and moderate versus skeptical positions on climate change BIBREF21. Similarly, Duncan Shaw et al. venture to answer the question of `Why did Brexit happen?' by building causal maps of nine televised debates that were broadcast during the four weeks leading up to the Brexit referendum BIBREF22..In order to appropriate the method of causal mapping to the study of on-line opinion dynamics, it needs to expanded from applications at the scale of human readers and relatively small corpora of archival documents and survey answers, to the realm of `big' textual data and larger quantities of information. This attuning of cognitive mapping methods to the large-scale processing of texts required for media monitoring necessarily involves a degree of automation, as will be explored in the next section. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "45\n",
      "What contribution led the researchers to surpass results obtained by previous state of the art models for classifying offensive vs non offensive tweets and messages written in Hinglish?\n",
      "\n",
      "The utilization of data augmentation technique was one of the vital contributions that led the researchers to surpass results obtained by previous state of the art Hybrid CNN-LSTM based models.\n",
      "Question : for the text The results of the experiments are encouraging on detective offensive vs non offensive tweets and messages written in Hinglish in social media. The utilization of data augmentation technique in this classification task was one of the vital contributions which led us to surpass results obtained by previous state of the art Hybrid CNN-LSTM based models. However, the results of the model for predicting hateful tweets on the contrary brings forth some shortcomings of the model. The biggest shortcoming on the model based on error analysis indicates less than generalized examples presented by the dataset. We also note that the embedding learnt from the Hinglish data set may be lacking and require extensive training to have competent word representations of Hinglish text. Given this learning's, we identify that creating word embeddings on much larger Hinglish corpora may have significant results. We also hypothesize that considering alternate methods than translation and transliteration may prove beneficial. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "46\n",
      "Question 1: What was the purpose of employing Transfer Learning to the task? \n",
      "Answer 1: The purpose of employing Transfer Learning to the task was to overcome the challenge of having a small number of labeled datasets available for the study.\n",
      "Question : for the text We used dataset, HEOT obtained from one of the past studies done by Mathur et al. where they annotated a set of cleaned tweets obtained from twitter for the conversations happening in Indian subcontinent. A labelled dataset for a corresponding english tweets were also obtained from a study conducted by Davidson et al. This dataset was important to employ Transfer Learning to our task since the number of labeled dataset was very small. Basic summary and examples of the data from the dataset are below: generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "47\n",
      "Question 1: What were the challenges with the obtained data set and how were they resolved?\n",
      "Answer 1: The obtained data set had messy text messages including urls, punctuations, username mentions, hashtags, emoticons, and special characters. To clean the data, a data preparation task was employed which involved applying various processes such as stop words elimination, transliteration, and data augmentation techniques such as synonym replacement, random insertion, random swap, and random deletion. The processed tweets were represented as word sequence vector representations using word embedding layers by Glove. The labelled dataset was split into a train-test split of 78%-22% for evaluation, and the resulting training dataset was augmented to 7934 messages.\n",
      "Question : for the text The obtained data set had many challenges and thus a data preparation task was employed to clean the data and make it ready for the deep learning pipeline. The challenges and processes that were applied are stated below:.Messy text messages: The tweets had urls, punctuations, username mentions, hastags, emoticons, numbers and lots of special characters. These were all cleaned up in a preprocessing cycle to clean the data..Stop words: Stop words corpus obtained from NLTK was used to eliminate most unproductive words which provide little information about individual tweets..Transliteration: Followed by above two processes, we translated Hinglish tweets into English words using a two phase process.Transliteration: In phase I, we used translation API's provided by Google translation services and exposed via a SDK, to transliteration the Hinglish messages to English messages..Translation: After transliteration, words that were specific to Hinglish were translated to English using an Hinglish-English dictionary. By doing this we converted the Hinglish message to and assortment of isolated words being presented in the message in a sequence that can also be represented using word to vector representation..Data augmentation: Given the data set was very small with a high degree of imbalance in the labelled messages for three different classes, we employed a data augmentation technique to boost the learning of the deep network. Following techniques from the paper by Jason et al. was utilized in this setting that really helped during the training phase.Thsi techniques wasnt used in previous studies. The techniques were:.Synonym Replacement (SR):Randomly choose n words from the sentence that are not stop words. Replace each of these words with one of its synonyms chosen at random..Random Insertion (RI):Find a random synonym of a random word in the sentence that is not a stop word. Insert that synonym into a random position in the sentence. Do this n times..Random Swap (RS):Randomly choose two words in the sentence and swap their positions. Do this n times..Random Deletion (RD):For each word in the sentence, randomly remove it with probability p..Word Representation: We used word embedding representations by Glove for creating word embedding layers and to obtain the word sequence vector representations of the processed tweets. The pre-trained embedding dimension were one of the hyperparamaters for model. Further more, we introduced another bit flag hyperparameter that determined if to freeze these learnt embedding..Train-test split: The labelled dataset that was available for this task was very limited in number of examples and thus as noted above few data augmentation techniques were applied to boost the learning of the network. Before applying augmentation, a train-test split of 78%-22% was done from the original, cleansed data set. Thus, 700 tweets/messages were held out for testing. All model evaluation were done in on the test set that got generated by this process. The results presented in this report are based on the performance of the model on the test set. The training set of 2489 messages were however sent to an offline pipeline for augmenting the data. The resulting training dataset was thus 7934 messages. the final distribution of messages for training and test was thus below: generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "48\n",
      "Question 1: What are some examples of languages that blend with Hindi to create Hinglish?\n",
      "\n",
      "Answer 1: Hinglish is a blend of Hindi and English, but it can also incorporate Punjabi, Gujarati, Marathi, and other regional languages spoken in India.\n",
      "Question : for the text Hinglish is a linguistic blend of Hindi (very widely spoken language in India) and English (an associate language of urban areas) and is spoken by upwards of 350 million people in India. While the name is based on the Hindi language, it does not refer exclusively to Hindi, but is used in India, with English words blending with Punjabi, Gujarati, Marathi and Hindi. Sometimes, though rarely, Hinglish is used to refer to Hindi written in English script and mixing with English words or phrases. This makes analyzing the language very interesting. Its rampant usage in social media like Twitter, Facebook, Online blogs and reviews has also led to its usage in delivering hate and abuses in similar platforms. We aim to find such content in the social media focusing on the tweets. Hypothetically, if we can classify such tweets, we might be able to detect them and isolate them for further analysis before it reaches public. This will a great application of AI to the social cause and thus is motivating. An example of a simple, non offensive message written in Hinglish could be:.\"Why do you waste your time with <redacted content>. Aapna ghar sambhalta nahi(<redacted content>). Chale dusro ko basane..!!\".The second part of the above sentence is written in Hindi while the first part is in English. Second part calls for an action to a person to bring order to his/her home before trying to settle others. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "49\n",
      "What are the challenges posed by the use of Hinglish in natural language processing (NLP) models?\n",
      "\n",
      "The challenges posed by the use of Hinglish in NLP models include geographical variation, language and phonetics variation, lack of fixed grammar rules, spelling variation, and limited availability of labelled datasets for training. Additionally, Hinglish follows a fuzzy set of rules that evolves and is dependent on user preference, leading to ambiguity in the rules of usage which may result in different text outputs when used by different users.\n",
      "Question : for the text From the modeling perspective there are couple of challenges introduced by the language and the labelled dataset. Generally, Hinglish follows largely fuzzy set of rules which evolves and is dependent upon the users preference. It doesn't have any formal definitions and thus the rules of usage are ambiguous. Thus, when used by different users the text produced may differ. Overall the challenges posed by this problem are:.Geographical variation: Depending upon the geography of origination, the content may be be highly influenced by the underlying region..Language and phonetics variation: Based on a census in 2001, India has 122 major languages and 1599 other languages. The use of Hindi and English in a code switched setting is highly influenced by these language..No grammar rules: Hinglish has no fixed set of grammar rules. The rules are inspired from both Hindi and English and when mixed with slur and slang produce large variation..Spelling variation: There is no agreement on the spellings of the words which are mixed with English. For example to express love, a code mixed spelling, specially when used social platforms might be pyaar, pyar or pyr..Dataset: Based on some earlier work, only available labelled dataset had 3189 rows of text messages of average length of 116 words and with a range of 1, 1295. Prior work addresses this concern by using Transfer Learning on an architecture learnt on about 14,500 messages with an accuracy of 83.90. We addressed this concern using data augmentation techniques applied on text data. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "50\n",
      "What was the metric used for minimizing during the experiments?\n",
      "\n",
      "The metric used for minimizing during the experiments was the validation loss.\n",
      "Question : for the text We tested the performance of various model architectures by running our experiment over 100 times on a CPU based compute which later as migrated to GPU based compute to overcome the slow learning progress. Our universal metric for minimizing was the validation loss and we employed various operational techniques for optimizing on the learning process. These processes and its implementation details will be discussed later but they were learning rate decay, early stopping, model checkpointing and reducing learning rate on plateau. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "1\n",
      "What parameter was used for the learning rate in the model tuning process?\n",
      "The learning rate parameter was set to 0.01 after a grid search for best performance.\n",
      "Question : for the text Choice of model parameters were in the above models were inspired from previous work done but then were tuned to the best performance of the Test dataset. Following parameters were considered for tuning..Learning rate: Based on grid search the best performance was achieved when learning rate was set to 0.01. This value was arrived by a grid search on lr parameter..Number of Bidirectional LSTM units: A set of 32, 64, 128 hidden activation units were considered for tuning the model. 128 was a choice made by Vo et al in modeling for Vietnamese language but with our experiments and with a small dataset to avoid overfitting to train dataset, a smaller unit sizes were considered..Embedding dimension: 50, 100 and 200 dimension word representation from Glove word embedding were considered and the best results were obtained with 100d representation, consistent with choices made in the previous work..Transfer learning on Embedding; Another bit flag for training the embedding on the train data or freezing the embedding from Glove was used. It was determined that set of pre-trained weights from Glove was best when it was fine tuned with Hinglish data. It provides evidence that a separate word or sentence level embedding when learnt for Hinglish text analysis will be very useful..Number of dense FC layers..Maximum length of the sequence to be considered: The max length of tweets/message in the dataset was 1265 while average was 116. We determined that choosing 200 resulted in the best performance. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "2\n",
      "Question 1: What is the loss function used to find the most optimal weights/parameters of the model?\n",
      "\n",
      "Answer 1: The loss function used to find the most optimal weights/parameters of the model is categorical cross entropy loss.\n",
      "Question : for the text For the loss function we chose categorical cross entropy loss in finding the most optimal weights/parameters of the model. Formally this loss function for the model is defined as below:.The double sum is over the number of observations and the categories respectively. While the model probability is the probability that the observation i belongs to category c. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "3\n",
      "Question 1: Which model architecture showed the best performance in terms of recall rate, F1 score and overall accuracy?\n",
      "\n",
      "Answer 1: Based on the experiments conducted, the sequence models which included SimpleRNN, LSTM, GRU, and Bidirectional LSTM, showed the best performance in terms of recall rate, F1 score, and overall accuracy.\n",
      "Question : for the text Among the model architectures we experimented with and without data augmentation were:.Fully Connected dense networks: Model hyperparameters were inspired from the previous work done by Vo et al and Mathur et al. This was also used as a baseline model but we did not get appreciable performance on such architecture due to FC networks not being able to capture local and long term dependencies..Convolution based architectures: Architecture and hyperparameter choices were chosen from the past study Deon on the subject. We were able to boost the performance as compared to only FC based network but we noticed better performance from architectures that are suitable to sequences such as text messages or any timeseries data..Sequence models: We used SimpleRNN, LSTM, GRU, Bidirectional LSTM model architecture to capture long term dependencies of the messages in determining the class the message or the tweet belonged to..Based on all the experiments we conducted below model had best performance related to metrics - Recall rate, F1 score and Overall accuracy. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "4\n",
      "What is the purpose of the Glove model?\n",
      "The Glove model aims to create global vectors for word representation in natural language processing.\n",
      "Question : for the text [1] Mathur, Puneet and Sawhney, Ramit and Ayyar, Meghna and Shah, Rajiv, Did you offend me? classification of offensive tweets in hinglish language, Proceedings of the 2nd Workshop on Abusive Language Online (ALW2).[2] Mathur, Puneet and Shah, Rajiv and Sawhney, Ramit and Mahata, Debanjan Detecting offensive tweets in hindi-english code-switched language Proceedings of the Sixth International Workshop on Natural Language Processing for Social Media.[3] Vo, Quan-Hoang and Nguyen, Huy-Tien and Le, Bac and Nguyen, Minh-Le Multi-channel LSTM-CNN model for Vietnamese sentiment analysis 2017 9th international conference on knowledge and systems engineering (KSE).[4] Hochreiter, Sepp and Schmidhuber, Jürgen Long short-term memory Neural computation 1997.[5] Sinha, R Mahesh K and Thakur, Anil Multi-channel LSTM-CNN model for Vietnamese sentiment analysis 2017 9th international conference on knowledge and systems engineering (KSE).[6] Pennington, Jeffrey and Socher, Richard and Manning, Christopher Glove: Global vectors for word representation Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP).[7] Zhang, Lei and Wang, Shuai and Liu, Bing Deep learning for sentiment analysis: A survey Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery.[8] Caruana, Rich and Lawrence, Steve and Giles, C Lee Overfitting in neural nets: Backpropagation, conjugate gradient, and early stopping Advances in neural information processing systems.[9] Beale, Mark Hudson and Hagan, Martin T and Demuth, Howard B Neural network toolbox user’s guide The MathWorks Incs.[10] Chollet, François and others Keras: The python deep learning library Astrophysics Source Code Library.[11] Wei, Jason and Zou, Kai EDA: Easy Data Augmentation Techniques for Boosting Performance on Text Classification Tasks Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP) generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "5\n",
      "Question 1: What approach did Nguyen et al. propose in their study on Vietnamese language processing?\n",
      "\n",
      "Answer 1: In their study, Nguyen et al. proposed a Hybrid multi-channel CNN and LSTM model where they used CNN to capture short-term dependencies and LSTM to capture long-term dependencies. They then concatenated these feature sets to learn a unified set of features on the messages, which were sent to fully connected layers. This approach achieved an accuracy rate of 87.3%.\n",
      "Question : for the text In another localized setting of Vietnamese language, Nguyen et al. in 2017 proposed a Hybrid multi-channel CNN and LSTM model where they build feature maps for Vietnamese language using CNN to capture shorterm dependencies and LSTM to capture long term dependencies and concatenate both these feature sets to learn a unified set of features on the messages. These concatenated feature vectors are then sent to a few fully connected layers. They achieved an accuracy rate of 87.3% with this architecture. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "6\n",
      "Question 1: What kind of model did Mathur et al. propose for detecting offensive tweets?\n",
      "\n",
      "Answer 1: Mathur et al. proposed a Ternary Trans-CNN model for detecting offensive tweets, which comprised of 3 layers of Convolution 1D having filter sizes of 15, 12, and 10, and kernel size of 3, followed by 2 dense fully connected layers of size 64 and 3. The first dense FC layer had ReLU activation while the last Dense layer had Softmax activation.\n",
      "Question : for the text Mathur et al. in their paper for detecting offensive tweets proposed a Ternary Trans-CNN model where they train a model architecture comprising of 3 layers of Convolution 1D having filter sizes of 15, 12 and 10 and kernel size of 3 followed by 2 dense fully connected layer of size 64 and 3. The first dense FC layer has ReLU activation while the last Dense layer had Softmax activation. They were able to train this network on a parallel English dataset provided by Davidson et al. The authors were able to achieve Accuracy of 83.9%, Precision of 80.2%, Recall of 69.8%..The approach looked promising given that the dataset was merely 3189 sentences divided into three categories and thus we replicated the experiment but failed to replicate the results. The results were poor than what the original authors achieved. But, most of the model hyper-parameter choices where inspired from this work. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "7\n",
      "Question 1: Which type of RNN sequence model was found to be better for hate speech detection?\n",
      "Answer 1: The GRU type of RNN sequence model was found to be better for hate speech detection.\n",
      "Question : for the text During our experimentation, it was evident that this is a hard problem especially detecting the hate speech, text in a code- mixed language. The best recall rate of 77 % for hate speech was obtained by a Bidirectional LSTM with 32 units with a recurrent drop out rate of 0.2. Precision wise GRU type of RNN sequence model faired better than other kinds for hate speech detection. On the other hand for detecting offensive and non offensive tweets, fairly satisfactory results were obtained. For offensive tweets, 92 % precision was and recall rate of 88% was obtained with GRU versus BiLSTM based models. Comparatively, Recall of 85 % and precision of 76 % was obtained by again GRU and BiLSTM based models as shown and marked in the results. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "8\n",
      "Question 1: What is the hypothesis about sentence representation in mBERT?\n",
      "\n",
      "Answer 1: The hypothesis is that a sentence representation in mBERT is composed of a language-specific component, which identifies the language of the sentence, and a language-neutral component, which captures the meaning of the sentence in a language-independent way.\n",
      "Question : for the text Following BIBREF3, we hypothesize that a sentence representation in mBERT is composed of a language-specific component, which identifies the language of the sentence, and a language-neutral component, which captures the meaning of the sentence in a language-independent way. We assume that the language-specific component is similar across all sentences in the language..We thus try to remove the language-specific information from the representations by centering the representations of sentences in each language so that their average lies at the origin of the vector space. We do this by estimating the language centroid as the mean of the mBERT representations for a set of sentences in that language and subtracting the language centroid from the contextual embeddings..We then analyze the semantic properties of both the original and the centered representations using a range of probing tasks. For all tasks, we test all layers of the model. For tasks utilizing a single-vector sentence representation, we test both the vector corresponding to the [cls] token and mean-pooled states. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "9\n",
      "Question 1: Can mBERT contextual embeddings be directly used for zero-shot cross-lingual tasks?\n",
      "\n",
      "Answer 1: No, according to the text, mBERT contextual embeddings do not represent similar semantic phenomena similarly and therefore are not directly usable for zero-shot cross-lingual tasks.\n",
      "Question : for the text Using a set of semantically oriented tasks that require explicit semantic cross-lingual representations, we showed that mBERT contextual embeddings do not represent similar semantic phenomena similarly and therefore they are not directly usable for zero-shot cross-lingual tasks..Contextual embeddings of mBERT capture similarities between languages and cluster the languages by their families. Neither cross-lingual fine-tuning nor adversarial language identity removal breaks this property. A part of language information is encoded by the position in the embedding space, thus a certain degree of cross-linguality can be achieved by centering the representations for each language. Exploiting this property allows a good cross-lingual sentence retrieval performance and bilingual word alignment (which is invariant to the shift). A good cross-lingual representation can be achieved by fitting a supervised projection on a small parallel corpus. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "10\n",
      "What model do they use for language identification?\n",
      "Answer 1: They use a pre-trained mBERT model with dimensions of 768, hidden layer dimension of 3072, self-attention using 12 heads, and 12 layers. It uses a vocabulary of 120k wordpieces that is shared for all languages.\n",
      "Question : for the text We use a pre-trained mBERT model that was made public with the BERT release. The model dimension is 768, hidden layer dimension 3072, self-attention uses 12 heads, the model has 12 layers. It uses a vocabulary of 120k wordpieces that is shared for all languages..To train the language identification classifier, for each of the BERT languages we randomly selected 110k sentences of at least 20 characters from Wikipedia, and keep 5k for validation and 5k for testing for each language. The training data are also used for estimating the language centroids..For parallel sentence retrieval, we use a multi-parallel corpus of test data from the WMT14 evaluation campaign BIBREF8 with 3,000 sentences in Czech, English, French, German, Hindi, and Russian. The linear projection experiment uses the WMT14 development data..We use manually annotated word alignment datasets to evaluate word alignment between English on one side and Czech (2.5k sent.; BIBREF9), Swedish (192 sent.; BIBREF10), German (508 sent.), French (447 sent.; BIBREF11) and Romanian (248 sent.; BIBREF12) on the other side. We compare the results with FastAlign BIBREF13 that was provided with 1M additional parallel sentences from ParaCrawl BIBREF14 in addition to the test data..For MT QE, we use English-German data provided for the WMT19 QE Shared Task BIBREF15 consisting training and test data with source senteces, their automatic translations, and manually corrections. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "11\n",
      "Question 1: What are the two fine-tuned versions of mBERT that were evaluated?\n",
      "Answer 1: The two fine-tuned versions of mBERT that were evaluated are UDify, which was tuned for a multi-lingual dependency parser, and lng-free, which was tuned to jettison the language-specific information from the representations.\n",
      "Question : for the text We also considered model fine-tuning towards stronger language neutrality. We evaluate two fine-tuned versions of mBERT: UDify, tuned for a multi-lingual dependency parser, and lng-free, tuned to jettison the language-specific information from the representations. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "12\n",
      "Question 1: What does the analysis of the UDify model suggest about the effect of fine-tuning mBERT for multilingual dependency parsing?\n",
      "\n",
      "Answer 1: The analysis of the UDify model suggests that fine-tuning mBERT for multilingual dependency parsing does not remove the language identity information from the representations and actually makes the representations less semantically cross-lingual.\n",
      "Question : for the text The UDify model BIBREF1 uses mBERT to train a single model for dependency parsing and morphological analysis of 75 languages. During the parser training, mBERT is fine-tuned, which improves the parser accuracy. Results on zero-shot parsing suggest that the fine-tuning leads to more cross-lingual representations with respect to morphology and syntax..However, our analyses show that fine-tuning mBERT for multilingual dependency parsing does not remove the language identity information from the representations and actually makes the representations less semantically cross-lingual. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "13\n",
      "Question 1: What was the purpose of the experiment mentioned in the text?\n",
      "\n",
      "Answer 1: The purpose of the experiment was to remove language identity from the mBERT model using an adversarial approach in order to make the representations more language-neutral.\n",
      "Question : for the text In this experiment, we try to make the representations more language-neutral by removing the language identity from the model using an adversarial approach. We continue training mBERT in a multi-task learning setup with the masked LM objective with the same sampling procedure BIBREF0 jointly with adversarial language ID classifiers BIBREF17. For each layer, we train one classifier for the [cls] token and one for the mean-pooled hidden states with the gradient reversal layer BIBREF18 between mBERT and the classifier..The results reveal that the adversarial removal of language information succeeds in dramatically decreasing the accuracy of the language identification classifier; the effect is strongest in deeper layers for which the standard mBERT tend to perform better (see Figure FIGREF22). However, other tasksare not affected by the adversarial fine-tuning. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "14\n",
      "What tasks were used to directly assess the semantic cross-lingual properties of mBERT in this paper?\n",
      "\n",
      "The tasks used to directly assess the semantic cross-lingual properties of mBERT in this paper were cross-lingual sentence retrieval, word alignment, and machine translation quality estimation.\n",
      "Question : for the text Multilingual BERT (mBERT; BIBREF0) is gaining popularity as a contextual representation for various multilingual tasks, such as dependency parsing BIBREF1, BIBREF2, cross-lingual natural language inference (XNLI) or named-entity recognition (NER) BIBREF3, BIBREF4, BIBREF5..BIBREF3 present an exploratory paper showing that mBERT can be used cross-lingually for zero-shot transfer in morphological and syntactic tasks, at least for typologically similar languages. They also study an interesting semantic task, sentence-retrieval, with promising initial results. Their work leaves many open questions in terms of how good the cross-lingual mBERT representation is for semantics, motivating our work..In this paper, we directly assess the semantic cross-lingual properties of mBERT. To avoid methodological issues with zero-shot transfer (possible language overfitting, hyper-parameter tuning), we selected tasks that only involve a direct comparison of the representations: cross-lingual sentence retrieval, word alignment, and machine translation quality estimation (MT QE). Additionally, we explore how the language is represented in the embeddings by training language identification classifiers and assessing how the representation similarity corresponds to phylogenetic language families..Our results show that the mBERT representations, even after language-agnostic fine-tuning, are not very language-neutral. However, the identity of the language can be approximated as a constant shift in the representation space. An even higher language-neutrality can still be achieved by a linear projection fitted on a small amount of parallel data..Finally, we present attempts to strengthen the language-neutral component via fine-tuning: first, for multi-lingual syntactic and morphological analysis; second, towards language identity removal via a adversarial classifier. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "15\n",
      "Question 1: What are the probing tasks used to evaluate the language neutrality of the representations?\n",
      "\n",
      "Answer 1: The authors employed five probing tasks to evaluate the language neutrality of the representations.\n",
      "Question : for the text We employ five probing tasks to evaluate the language neutrality of the representations. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "16\n",
      "Question 1: What is the purpose of training a linear classifier on top of a sentence representation for language identification?\n",
      "\n",
      "Answer 1: The purpose is to classify the language of the sentence, based on a representation that captures all phenomena in a language-neutral way. Unlike other tasks, language identification requires fitting a classifier to make this determination.\n",
      "Question : for the text With a representation that captures all phenomena in a language-neutral way, it should be difficult to determine what language the sentence is written in. Unlike other tasks, language identification does require fitting a classifier. We train a linear classifier on top of a sentence representation to try to classify the language of the sentence. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "17\n",
      "Question 1: How do they quantify the observation that similar languages tend to get similar representations on average?\n",
      "\n",
      "Answer 1: They quantify this observation by measuring how languages tend to cluster by the language families using V-measure over hierarchical clustering of the language centroid. (BIBREF3 and BIBREF7)\n",
      "Question : for the text Experiments with POS tagging BIBREF3 suggest that similar languages tend to get similar representations on average. We quantify that observation by measuring how languages tend to cluster by the language families using V-measure over hierarchical clustering of the language centeroid BIBREF7. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "18\n",
      "Question 1: What is the standard evaluation metric for MT QE?\n",
      "Answer 1: The standard evaluation metric for MT QE is the correlation with the Human-targeted Translation Error Rate, which measures the number of edit operations a human translator would need to do to correct the system output.\n",
      "Question : for the text MT QE assesses the quality of an MT system output without having access to a reference translation..The standard evaluation metric is the correlation with the Human-targeted Translation Error Rate which is the number of edit operations a human translator would need to do to correct the system output. This is a more challenging task than the two previous ones because it requires capturing more fine-grained differences in meaning..We evaluate how cosine distance of the representation of the source sentence and of the MT output reflects the translation quality. In addition to plain and centered representations, we also test trained bilingual projection, and a fully supervised regression trained on training data. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "19\n",
      "Question 1: How do they select the sentence with the smallest distance in the multi-parallel corpus?\n",
      "\n",
      "Answer 1: They compute the cosine distance of each sentence's representation with representations of all sentences on the parallel side of the corpus and select the sentence with the smallest distance.\n",
      "Question : for the text For each sentence in a multi-parallel corpus, we compute the cosine distance of its representation with representations of all sentences on the parallel side of the corpus and select the sentence with the smallest distance..Besides the plain and centered [cls] and mean-pooled representations, we evaluate explicit projection into the “English space”. For each language, we fit a linear regression projecting the representations into English representation space using a small set of parallel sentences. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "20\n",
      "Question 1: What is the method used for computing bilingual alignment?\n",
      "\n",
      "Answer 1: The method used for computing bilingual alignment is finding the word alignment as a minimum weighted edge cover of a bipartite graph. The graph connects the tokens of the sentences in the two languages and edges between them are weighted with the cosine distance of the token representation.\n",
      "Question : for the text While sentence retrieval could be done with keyword spotting, computing bilingual alignment requires resolving detailed correspondence on the word level..We find the word alignment as a minimum weighted edge cover of a bipartite graph. The graph connects the tokens of the sentences in the two languages and edges between them are weighted with the cosine distance of the token representation. Tokens that get split into multiple subwords are represented using the average of the embeddings of the subwords. Note that this algorithm is invariant to representation centering which would only change the edge weights by a constant offset..We evaluate the alignment using the F$_1$ score over both sure and possible alignment links in a manually aligned gold standard. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "21\n",
      "What is the observation made by BIBREF3 in their assessment of mBERT on cross-lingual sentence retrieval? \n",
      "\n",
      "Answer 1: BIBREF3 observed that when they subtract the average difference between the embeddings from the target language representation, the retrieval accuracy significantly increases.\n",
      "Question : for the text Since the publication of mBERT BIBREF0, many positive experimental results were published..BIBREF2 reached impressive results in zero-shot dependency parsing. However, the representation used for the parser was a bilingual projection of the contextual embeddings based on word-alignment trained on parallel data..BIBREF3 recently examined the cross-lingual properties of mBERT on zero-shot NER and part-of-speech (POS) tagging but the success of zero-shot transfer strongly depends on how typologically similar the languages are. Similarly, BIBREF4 trained good multilingual models for POS tagging, NER, and XNLI, but struggled to achieve good results in the zero-shot setup..BIBREF3 assessed mBERT on cross-lingual sentence retrieval between three language pairs. They observed that if they subtract the average difference between the embeddings from the target language representation, the retrieval accuracy significantly increases. We systematically study this idea in the later sections..Many experiments show BIBREF4, BIBREF5, BIBREF1 that downstream task models can extract relevant features from the multilingual representations. But these results do not directly show language-neutrality, i.e., to what extent are similar phenomena are represented similarly across languages. The models can obtain the task-specific information based on the knowledge of the language, which (as we show later) can be easily identified. Our choice of evaluation tasks eliminates this risk by directly comparing the representations. Limited success in zero-shot setups and the need for explicit bilingual projection in order to work well BIBREF3, BIBREF4, BIBREF6 also shows limited language neutrality of mBERT. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "22\n",
      "Question 1: What is the effect of centering sentence representations on language identification accuracy?\n",
      "\n",
      "Answer 1: Centering sentence representations decreases language identification accuracy, particularly with mean-pooled embeddings, suggesting that the proposed centering procedure effectively removes language-specific information.\n",
      "Question : for the text Table TABREF7 shows that centering the sentence representations considerably decreases the accuracy of language identification, especially in the case of mean-pooled embeddings. This indicates that the proposed centering procedure does indeed remove the language-specific information to a great extent. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "23\n",
      "Question 1: What does Figure FIGREF9 show?\n",
      "\n",
      "Answer 1: Figure FIGREF9 is a tSNE plot BIBREF16 of the language centroids, demonstrating that the similarity of the centroids typically corresponds to the similarity of the languages.\n",
      "Question : for the text Figure FIGREF9 is a tSNE plot BIBREF16 of the language centroids, showing that the similarity of the centroids tends to correspond to the similarity of the languages. Table TABREF10 confirms that the hierarchical clustering of the language centroids mostly corresponds to the language families. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "24\n",
      "Question 1: How does the linear projection between representations perform in relation to sentence complexity in MT QE?\n",
      "\n",
      "Answer 1: The linear projection between representations only captures a rough semantic correspondence, which is not sufficient for MT QE. Sentence complexity appears to be the most indicative feature in MT QE.\n",
      "Question : for the text Qualitative results of MT QE are tabulated in Table TABREF18. Unlike sentence retrieval, QE is more sensitive to subtle differences between sentences. Measuring the distance of the non-centered sentence vectors does not correlate with translation quality at all. Centering or explicit projection only leads to a mild correlation, much lower than a supervisedly trained regression;and even better performance is possible BIBREF15. The results show that the linear projection between the representations only captures a rough semantic correspondence, which does not seem to be sufficient for QE, where the most indicative feature appears to be sentence complexity. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "25\n",
      "Question 1: What type of projection resulted in a close-to-perfect accuracy improvement in retrieval?\n",
      "\n",
      "Answer 1: An explicitly learned projection of the representations resulted in a close-to-perfect accuracy improvement in retrieval.\n",
      "Question : for the text Results in Table TABREF12 reveal that the representation centering dramatically improves the retrieval accuracy, showing that it makes the representations more language-neutral. However, an explicitly learned projection of the representations leads to a much greater improvement, reaching a close-to-perfect accuracy, even though the projection was fitted on relatively small parallel data. The accuracy is higher for mean-pooled states than for the [cls] embedding and varies according to the layer of mBERT used (see Figure FIGREF13). generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "26\n",
      "Question 1: How does the performance of word-alignment based on mBERT representations compare to that of the standard FastAlign tool?\n",
      "\n",
      "Answer 1: Based on the results presented in Table TABREF15, word-alignment based on mBERT representations outperforms the output of the standard FastAlign tool, even when a large parallel corpus is provided. This suggests that mBERT contextual embeddings are effective in capturing word-level semantics. Additionally, the learning of an explicit projection had a minimal impact on performance in this task.\n",
      "Question : for the text Table TABREF15 shows that word-alignment based on mBERT representations surpasses the outputs of the standard FastAlign tool even if it was provided large parallel corpus. This suggests that word-level semantics are well captured by mBERT contextual embeddings. For this task, learning an explicit projection had a negligible effect on the performance. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "27\n",
      "Question 1: How did you mitigate the issue of CAiRE's lack of ethical values? \n",
      "\n",
      "Answer 1: We first customized CAiRE's persona with ethical values such as wanting to help humans and being a good friend to them. Then we implemented active learning based on user-revised responses to further reduce unethical responses. As CAiRE gathers more data, we plan to improve its performance through negative training and continued active learning.\n",
      "Question : for the text CAiRE was first presented in ACL 2019 keynote talk “Loquentes Machinea: Technology, Applications, and Ethics of Conversational Systems\", and after that, we have released the chatbot to the public. In one week, we received traffic from more than 500 users, along with several reports of unethical dialogues. According to such feedback, CAiRE does not have any sense of ethical value due to the lack of training data informing of inappropriate behavior. Thus, when users raise some ethically concerning questions, CAiRE may respond without considering ethical implications. For example, a user might ask “Would you kill a human?\", and CAiRE could respond “yes, I want!\". To mitigate this issue, we first incorporate ethical values into CAiRE by customizing the persona of it with sentences such as: “my name is caire\", “i want to help humans to make a better world\", “i am a good friend of humans\". Then we perform active learning based on the collected user-revised responses. We observe that this approach can greatly reduce unethical responses. As CAiRE gathers more unethical dialogues and their revisions, its performance can be further improved by negative training BIBREF5 and active learning. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "28\n",
      "Question 1: Can CAiRE understand the user's emotions accurately?\n",
      "Answer 1: Yes, CAiRE is designed to understand the user's emotions and respond appropriately based on their feelings. With further improvements and user feedback, we aim to make CAiRE even more empathetic in the future.\n",
      "Question : for the text We presented CAiRE, an end-to-end generative empathetic chatbot that can understand the user's feeling and reply appropriately. We built a web interface for our model and have made it accessible to multiple users via a web-link. By further collecting user feedback and improving our model, we can make CAiRE more empathetic in the future, which can be a forward step for end-to-end dialogue models.  generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "29\n",
      "Question 1: What is the purpose of integrating persona into CAiRE and pre-training the model on PersonaChat?\n",
      "\n",
      "Answer 1: Integrating persona into CAiRE and pre-training the model on PersonaChat allows the model to have a more consistent persona, which improves the engagement and consistency of the model. This pre-training procedure also expands the topics the model can talk about beyond just the EmpatheticDialogue dataset.\n",
      "Question : for the text We apply the Generative Pre-trained Transformer (GPT) BIBREF2 as our pre-trained language model. GPT is a multi-layer Transformer decoder with a causal self-attention which is pre-trained, unsupervised, on the BooksCorpus dataset. BooksCorpus dataset contains over 7,000 unique unpublished books from a variety of genres. Pre-training on such large contiguous text corpus enables the model to capture long-range dialogue context information. Furthermore, as existing EmpatheticDialogue dataset BIBREF4 is relatively small, fine-tuning only on such dataset will limit the chitchat topic of the model. Hence, we first integrate persona into CAiRE, and pre-train the model on PersonaChat BIBREF3 , following a previous transfer-learning strategy BIBREF1 . This pre-training procedure allows CAiRE to have a more consistent persona, thus improving the engagement and consistency of the model. We refer interested readers to the code repository recently released by HuggingFace. Finally, in order to optimize empathy in CAiRE, we fine-tune this pre-trained model using EmpatheticDialogue dataset to help CAiRE understand users' feeling. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "30\n",
      "Question 1: Why is incorporating empathy important in chatbots?\n",
      "\n",
      "Answer 1: Incorporating empathy is essential in chatbots to achieve better human-robot interaction because humans naturally express and perceive emotions in natural language to increase their sense of social bonding. Empathetic chatbots can understand user emotions and respond appropriately, creating a more personalized and engaging experience for the user.\n",
      "Question : for the text Empathetic chatbots are conversational agents that can understand user emotions and respond appropriately. Incorporating empathy into the dialogue system is essential to achieve better human-robot interaction because naturally, humans express and perceive emotion in natural language to increase their sense of social bonding. In the early development stage of such conversational systems, most of the efforts were put into developing hand-crafted rules of engagement. Recently, a modularized empathetic dialogue system, XiaoIce BIBREF0 achieved an impressive number of conversational turns per session, which was even higher than average conversations between humans. Despite the promising results of XiaoIce, this system is designed using a complex architecture with hundreds of independent components, such as Natural Language Understanding and Response Generation modules, using a tremendous amount of labeled data for training each of them..In contrast to such modularized dialogue system, end-to-end systems learn all components as a single model in a fully data-driven manner, and mitigate the lack of labeled data by sharing representations among different modules. In this paper, we build an end-to-end empathetic chatbot by fine-tuning BIBREF1 the Generative Pre-trained Transformer (GPT) BIBREF2 on the PersonaChat dataset BIBREF3 and the Empathetic-Dialogue dataset BIBREF4 . We establish a web-based user interface which allows multiple users to asynchronously chat with CAiRE online. CAiRE can also collect user feedback and continuously improve its response quality and discard undesirable generation behaviors (e.g. unethical responses) via active learning and negative training. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "31\n",
      "Question 1: How many concurrent requests were we able to support during the stress testing with 8 GPUs?\n",
      "\n",
      "Answer 1: We were able to support more than 50 concurrent requests during the stress testing with 8 GPUs.\n",
      "Question : for the text Due to the high demand for GPU computations during response generation, the computation cost needs to be well distributed across different GPUs to support multiple users. We adopt several approaches to maximize the utility of GPUs without crashing the system. Firstly, we set up two independent processes in each GTX 1080Ti, where we found the highest GPU utilities to be around 90%, with both processes working stably. Secondly, we employ a load-balancing module to distribute the requests to idle processes based on their working loads. During a stress testing, we simulated users sending requests every 2 seconds, and using 8 GPUs, we were able to support more than 50 concurrent requests. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "32\n",
      "Question 1: What interface is used in the chatbot?\n",
      "Answer 1: The interface used in the chatbot is based solely on text inputs. Users can type anything in the input box and get a response immediately from the server.\n",
      "Question : for the text As shown in Figure FIGREF4 , our user interface is based solely on text inputs. Users can type anything in the input box and get a response immediately from the server. A report button is added at the bottom to allow users to report unethical dialogues, which will then be marked and saved in our back-end server separately. To facilitate the need for teaching our chatbot how to respond properly, we add an edit button next to the response. When the user clicks it, a new input box will appear, and the user can type in the appropriate response they think the chatbot should have replied with. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "33\n",
      "What is the source of funding for this project?\n",
      "Answer 1: This project has received funding from the European Research Council (ERC) under the European Union's Horizon 2020 research and innovation programme, grant agreement No. 802774 (iEXTRACT).\n",
      "Question : for the text We thank Yanai Elazar for welcome input on the presentation and organization of the paper. We also thank the reviewers for additional feedback and pointing to relevant literature in HCI and IUI..This project has received funding from the Europoean Research Council (ERC) under the Europoean Union's Horizon 2020 research and innovation programme, grant agreement No. 802774 (iEXTRACT). generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "34\n",
      "What is the proposed approach to evaluating interpretability?\n",
      "The proposed approach is to separate and solely focus on evaluating faithfulness without conflating it with plausibility, and to evaluate faithfulness on a nuanced \"grayscale\" rather than strict binary categorization.\n",
      "Question : for the text The opinion proposed in this paper is two-fold:.First, interpretability evaluation often conflates evaluating faithfulness and plausibility together. We should tease apart the two definitions and focus solely on evaluating faithfulness without any supervision or influence of the convincing power of the interpretation..Second, faithfulness is often evaluated in a binary “faithful or not faithful” manner, and we believe strictly faithful interpretation is a “unicorn” which will likely never be found. We should instead evaluate faithfulness on a more nuanced “grayscale” that allows interpretations to be useful even if they are not globally and definitively faithful. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "35\n",
      "in except\n",
      "35\n",
      "Question 1: What is one possible approach to developing a formal definition and evaluation for faithfulness?\n",
      "\n",
      "Answer 1: One possible approach to developing a formal definition and evaluation for faithfulness is to assess the degree of faithfulness at the level of specific models and tasks or at the level of subspaces of the input space.\n",
      "Question : for the text We argue that a way out of this standstill is in a more practical and nuanced methodology for defining and evaluating faithfulness. We propose the following challenge to the community: We must develop formal definition and evaluation for faithfulness that allows us the freedom to say when a method is sufficiently faithful to be useful in practice..We note two possible approaches to this end:.Across models and tasks: The degree (as grayscale) of faithfulness at the level of specific models and tasks. Perhaps some models or tasks allow sufficiently faithful interpretation, even if the same is not true for others..For example, the method may not be faithful for some question-answering task, but faithful for movie review sentiment, perhaps based on various syntactic and semantic attributes of those tasks..Across input space: The degree of faithfulness at the level of subspaces of the input space, such as neighborhoods of similar inputs, or singular inputs themselves. If we are able to say with some degree of confidence whether a specific decision's explanation is faithful to the model, even if the interpretation method is not considered universally faithful, it can be used with respect to those specific areas or instances only. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "1\n",
      "What is the difference between attention visualization and attention saliency in NLI tasks?\n",
      "\n",
      "The attention visualization in NLI tasks only shows how the model aligns the premise with the hypothesis and does not reveal how such alignment impacts the decision. On the other hand, the attention saliency helps pinpoint which part of the alignments contribute most critically to the final prediction.\n",
      "Question : for the text Attention has been widely used in many NLP tasks BIBREF12 , BIBREF13 , BIBREF14 and is probably one of the most critical parts that affects the inference decisions. Several pieces of prior work in NLI have attempted to visualize the attention layer to provide some understanding of their models BIBREF5 , BIBREF15 . Such visualizations generate a heatmap representing the similarity between the hidden states of the premise and the hypothesis (Eq. 19 of Appendix). Unfortunately the similarities are often the same regardless of the decision..Let us consider the following example, where the same premise “A kid is playing in the garden”, is paired with three different hypotheses:.A kid is taking a nap in the garden.A kid is having fun in the garden with her family.A kid is having fun in the garden. Note that the ground truth relationships are Contradiction, Neutral, and Entailment, respectively..The first row of Fig. 1 shows the visualization of normalized attention for the three cases produced by ESIM-50, which makes correct predictions for all of them. As we can see from the figure, the three attention maps are fairly similar despite the completely different decisions. The key issue is that the attention visualization only allows us to see how the model aligns the premise with the hypothesis, but does not show how such alignment impacts the decision. This prompts us to consider the saliency of attention..The concept of saliency was first introduced in vision for visualizing the spatial support on an image for a particular object class BIBREF16 . In NLP, saliency has been used to study the importance of words toward a final decision BIBREF0 ..We propose to examine the saliency of attention. Specifically, given a premise-hypothesis pair and the model's decision $y$ , we consider the similarity between a pair of premise and hypothesis hidden states $e_{ij}$ as a variable. The score of the decision $S(y)$ is thus a function of $e_{ij}$ for all $i$ and $j$ . The saliency of $e_{ij}$ is then defined to be $|\\frac{\\partial S(y)}{\\partial {e_{ij}}}|$ ..The second row of Fig. 1 presents the attention saliency map for the three examples acquired by the same ESIM-50 model. Interestingly, the saliencies are clearly different across the examples, each highlighting different parts of the alignment. Specifically, for h1, we see the alignment between “is playing” and “taking a nap” and the alignment of “in a garden” to have the most prominent saliency toward the decision of Contradiction. For h2, the alignment of “kid” and “her family” seems to be the most salient for the decision of Neutral. Finally, for h3, the alignment between “is having fun” and “kid is playing” have the strongest impact toward the decision of Entailment..From this example, we can see that by inspecting the attention saliency, we effectively pinpoint which part of the alignments contribute most critically to the final prediction whereas simply visualizing the attention itself reveals little information..In the previous examples, we study the behavior of the same model on different inputs. Now we use the attention saliency to compare the two different ESIM models: ESIM-50 and ESIM-300..Consider two examples with a shared hypothesis of “A man ordered a book” and premise:.John ordered a book from amazon.Mary ordered a book from amazon. Here ESIM-50 fails to capture the gender connections of the two different names and predicts Neutral for both inputs, whereas ESIM-300 correctly predicts Entailment for the first case and Contradiction for the second..In the first two columns of Fig. 2 (column a and b) we visualize the attention of the two examples for ESIM-50 (left) and ESIM-300 (right) respectively. Although the two models make different predictions, their attention maps appear qualitatively similar..In contrast, columns 3-4 of Fig. 2 (column c and d) present the attention saliency for the two examples by ESIM-50 and ESIM-300 respectively. We see that for both examples, ESIM-50 primarily focused on the alignment of “ordered”, whereas ESIM-300 focused more on the alignment of “John” and “Mary” with “man”. It is interesting to note that ESIM-300 does not appear to learn significantly different similarity values compared to ESIM-50 for the two critical pairs of words (“John”, “man”) and (“Mary”, “man”) based on the attention map. The saliency map, however, reveals that the two models use these values quite differently, with only ESIM-300 correctly focusing on them. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "2\n",
      "Question 1: What is the ESIM model and what are its three main parts?\n",
      "Answer 1: The ESIM model is a neural model for natural language inference (NLI) consisting of three main parts: input encoding using bidirectional LSTM, attention using soft alignment method, and inference using another BiLSTM and a multilayer perceptron (MLP) classifier.\n",
      "Question : for the text We propose new visualization and interpretation strategies for neural models to understand how and why they work. We demonstrate the effectiveness of the proposed strategies on a complex task (NLI). Our strategies are able to provide interesting insights not achievable by previous explanation techniques. Our future work will extend our study to consider other NLP tasks and models with the goal of producing useful insights for further improving these models. Model In this section we describe the ESIM model. We divide ESIM to three main parts: 1) input encoding, 2) attention, and 3) inference. Figure 4 demonstrates a high-level view of the ESIM framework. Let $u=[u_1, \\cdots , u_n]$ and $v=[v_1, \\cdots , v_m]$ be the given premise with length $n$ and hypothesis with length $m$ respectively, where $u_i, v_j \\in \\mathbb {R}^r$ are word embeddings of $r$ -dimensional vector. The goal is to predict a label $y$ that indicates the logical relationship between premise $u$ and hypothesis $v$ . Below we briefly explain the aforementioned parts. Input Encoding It utilizes a bidirectional LSTM (BiLSTM) for encoding the given premise and hypothesis using Equations 16 and 17 respectively. .$$\\hat{u} \\in \\mathbb {R}^{n \\times 2d}$$   (Eq. ) .$$\\hat{v} \\in \\mathbb {R}^{m \\times 2d}$$   (Eq. ) where $u$ and $v=[v_1, \\cdots , v_m]$0 are the reading sequences of $v=[v_1, \\cdots , v_m]$1 and $v=[v_1, \\cdots , v_m]$2 respectively. Attention It employs a soft alignment method to associate the relevant sub-components between the given premise and hypothesis. Equation 19 (energy function) computes the unnormalized attention weights as the similarity of hidden states of the premise and hypothesis. .$$u$$   (Eq. ) where $v=[v_1, \\cdots , v_m]$3 and $v=[v_1, \\cdots , v_m]$4 are the hidden representations of $v=[v_1, \\cdots , v_m]$5 and $v=[v_1, \\cdots , v_m]$6 respectively which are computed earlier in Equations 16 and 17 . Next, for each word in either premise or hypothesis, the relevant semantics in the other sentence is extracted and composed according to $v=[v_1, \\cdots , v_m]$7 . Equations 20 and 21 provide formal and specific details of this procedure. .$$\\tilde{v}_j$$   (Eq. ) .$$\\hat{u}$$   (Eq. ) where $v=[v_1, \\cdots , v_m]$8 represents the extracted relevant information of $v=[v_1, \\cdots , v_m]$9 by attending to $n$0 while $n$1 represents the extracted relevant information of $n$2 by attending to $n$3 . Next, it passes the enriched information through a projector layer which produce the final output of attention stage. Equations 22 and 23 formally represent this process. .$$p$$   (Eq. ) .$$q$$   (Eq. ) Here $n$4 stands for element-wise product while $n$5 and $n$6 are the trainable weights and biases of the projector layer respectively. $n$7 and $n$8 indicate the output of attention devision for premise and hypothesis respectively. Inference During this phase, it uses another BiLSTM to aggregate the two sequences of computed matching vectors, $n$9 and $m$0 from the attention stage (Equations 27 and 28 ). .$$\\emph {softmax}$$   (Eq. ) .$$\\hat{u} = \\textit {BiLSTM}(u)$$   (Eq. 16) where $m$1 and $m$2 are the reading sequences of $m$3 and $m$4 respectively. Finally the concatenation max and average pooling of $m$5 and $m$6 are pass through a multilayer perceptron (MLP) classifier that includes a hidden layer with $m$7 activation and $m$8 output layer. The model is trained in an end-to-end manner. Attention Study Here we provide more examples on the NLI task which intend to examine specific behavior in this model. Such examples indicate interesting observation that we can analyze them in the future works. Table 1 shows the list of all example. LSTM Gating Signal Finally, Figure 11 depicts the backward LSTM gating signals study.  generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "3\n",
      "What is the main contribution of this paper?\n",
      "\n",
      "The main contribution of this paper is to introduce new strategies for interpreting the behavior of deep models in their intermediate layers, specifically examining the saliency of attention and the gating signals, and providing an extensive analysis of the state-of-the-art model for the NLI task, which reveals interesting insights not available from traditional methods of inspecting attention and word saliency.\n",
      "Question : for the text Deep learning has achieved tremendous success for many NLP tasks. However, unlike traditional methods that provide optimized weights for human understandable features, the behavior of deep learning models is much harder to interpret. Due to the high dimensionality of word embeddings, and the complex, typically recurrent architectures used for textual data, it is often unclear how and why a deep learning model reaches its decisions..There are a few attempts toward explaining/interpreting deep learning-based models, mostly by visualizing the representation of words and/or hidden states, and their importances (via saliency or erasure) on shallow tasks like sentiment analysis and POS tagging BIBREF0 , BIBREF1 , BIBREF2 , BIBREF3 . In contrast, we focus on interpreting the gating and attention signals of the intermediate layers of deep models in the challenging task of Natural Language Inference. A key concept in explaining deep models is saliency, which determines what is critical for the final decision of a deep model. So far, saliency has only been used to illustrate the impact of word embeddings. In this paper, we extend this concept to the intermediate layer of deep models to examine the saliency of attention as well as the LSTM gating signals to understand the behavior of these components and their impact on the final decision..We make two main contributions. First, we introduce new strategies for interpreting the behavior of deep models in their intermediate layers, specifically, by examining the saliency of the attention and the gating signals. Second, we provide an extensive analysis of the state-of-the-art model for the NLI task and show that our methods reveal interesting insights not available from traditional methods of inspecting attention and word saliency..In this paper, our focus was on NLI, which is a fundamental NLP task that requires both understanding and reasoning. Furthermore, the state-of-the-art NLI models employ complex neural architectures involving key mechanisms, such as attention and repeated reading, widely seen in successful models for other NLP tasks. As such, we expect our methods to be potentially useful for other natural understanding tasks as well. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "4\n",
      "What do LSTM gating signals indicate and why are they rarely analyzed?\n",
      "\n",
      "LSTM gating signals determine the flow of information within a model and indicate how LSTM reads word sequences and combines information from different parts. They are rarely analyzed, possibly due to their high dimensionality and complexity.\n",
      "Question : for the text LSTM gating signals determine the flow of information. In other words, they indicate how LSTM reads the word sequences and how the information from different parts is captured and combined. LSTM gating signals are rarely analyzed, possibly due to their high dimensionality and complexity. In this work, we consider both the gating signals and their saliency, which is computed as the partial derivative of the score of the final decision with respect to each gating signal..Instead of considering individual dimensions of the gating signals, we aggregate them to consider their norm, both for the signal and for its saliency. Note that ESIM models have two LSTM layers, the first (input) LSTM performs the input encoding and the second (inference) LSTM generates the representation for inference..In Fig. 3 we plot the normalized signal and saliency norms for different gates (input, forget, output) of the Forward input (bottom three rows) and inference (top three rows) LSTMs. These results are produced by the ESIM-50 model for the three examples of Section 3.1, one for each column..From the figure, we first note that the saliency tends to be somewhat consistent across different gates within the same LSTM, suggesting that we can interpret them jointly to identify parts of the sentence important for the model's prediction..Comparing across examples, we see that the saliency curves show pronounced differences across the examples. For instance, the saliency pattern of the Neutral example is significantly different from the other two examples, and heavily concentrated toward the end of the sentence (“with her family”). Note that without this part of the sentence, the relationship would have been Entailment. The focus (evidenced by its strong saliency and strong gating signal) on this particular part, which presents information not available from the premise, explains the model's decision of Neutral..Comparing the behavior of the input LSTM and the inference LSTM, we observe interesting shifts of focus. In particular, we see that the inference LSTM tends to see much more concentrated saliency over key parts of the sentence, whereas the input LSTM sees more spread of saliency. For example, for the Contradiction example, the input LSTM sees high saliency for both “taking” and “in”, whereas the inference LSTM primarily focuses on “nap”, which is the key word suggesting a Contradiction. Note that ESIM uses attention between the input and inference LSTM layers to align/contrast the sentences, hence it makes sense that the inference LSTM is more focused on the critical differences between the sentences. This is also observed for the Neutral example as well..It is worth noting that, while revealing similar general trends, the backward LSTM can sometimes focus on different parts of the sentence (e.g., see Fig. 11 of Appendix), suggesting the forward and backward readings provide complementary understanding of the sentence. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "5\n",
      "What is ESIM? \n",
      "ESIM (Enhanced Sequential Inference Model) is a type of NLI model that reads two given sentences independently using LSTM, applies attention to align or contrast the sentences, and produces final representations, which are compared to make the prediction of the logical relationship between the sentences.\n",
      "Question : for the text In NLI BIBREF4 , we are given two sentences, a premise and a hypothesis, the goal is to decide the logical relationship (Entailment, Neutral, or Contradiction) between them..Many of the top performing NLI models BIBREF5 , BIBREF6 , BIBREF7 , BIBREF8 , BIBREF9 , BIBREF10 , BIBREF11 , are variants of the ESIM model BIBREF11 , which we choose to analyze in this paper. ESIM reads the sentences independently using LSTM at first, and then applies attention to align/contrast the sentences. Another round of LSTM reading then produces the final representations, which are compared to make the prediction. Detailed description of ESIM can be found in the Appendix..Using the SNLI BIBREF4 data, we train two variants of ESIM, with dimensionality 50 and 300 respectively, referred to as ESIM-50 and ESIM-300 in the remainder of the paper. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "6\n",
      "Question 1: What aspects of the NLI model are the authors primarily interested in studying?\n",
      "Answer 1: The authors are primarily interested in studying the internal workings of the NLI model, specifically the attention and gating signals of LSTM readers and how they relate to the model's decisions.\n",
      "Question : for the text In this work, we are primarily interested in the internal workings of the NLI model. In particular, we focus on the attention and the gating signals of LSTM readers, and how they contribute to the decisions of the model. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "7\n",
      "What does this paper propose to improve question answering and machine reading?\n",
      "The paper proposes to introduce syntactic information to help encode questions in neural networks and model different types of questions and information shared among them as an adaptation task, leading to proposed adaptation models that can attain better results over a competitive baseline on the Stanford Question Answering Dataset (SQuAD).\n",
      "Question : for the text Closely modelling questions could be of importance for question answering and machine reading. In this paper, we introduce syntactic information to help encode questions in neural networks. We view and model different types of questions and the information shared among them as an adaptation task and proposed adaptation models for them. On the Stanford Question Answering Dataset (SQuAD), we show that these approaches can help attain better results over a competitive baseline. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "8\n",
      "What is the Stanford Question Answering Dataset (SQuAD)?\n",
      "\n",
      "Answer 1: The Stanford Question Answering Dataset (SQuAD) is a dataset used to test and benchmark models for machine comprehension and question answering.\n",
      "Question : for the text Enabling computers to understand given documents and answer questions about their content has recently attracted intensive interest, including but not limited to the efforts as in BIBREF0 , BIBREF1 , BIBREF2 , BIBREF3 , BIBREF4 , BIBREF5 . Many specific problems such as machine comprehension and question answering often involve modeling such question-document pairs..The recent availability of relatively large training datasets (see Section \"Related Work\" for more details) has made it more feasible to train and estimate rather complex models in an end-to-end fashion for these problems, in which a whole model is fit directly with given question-answer tuples and the resulting model has shown to be rather effective..In this paper, we take a closer look at modeling questions in such an end-to-end neural network framework, since we regard question understanding is of importance for such problems. We first introduced syntactic information to help encode questions. We then viewed and modelled different types of questions and the information shared among them as an adaptation problem and proposed adaptation models for them. On the Stanford Question Answering Dataset (SQuAD), we show that these approaches can help attain better results on our competitive baselines. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "9\n",
      "Question 1: How is syntactic information incorporated in question representation?\n",
      "Answer 1: Syntactic information is incorporated in question representation using TreeLSTM, which performs semantic composition over given syntactic structures. A TreeLSTM captures long-distance interaction on a tree and its nodes are configured with four gates to consider each child's cell vector. The parse tree information is obtained using Stanford CoreNLP, and the root node of TreeLSTM is used as the representation for the whole question. Question type and discriminative block are also used for question adaptation and modeling different types of questions.\n",
      "Question : for the text The interplay of syntax and semantics of natural language questions is of interest for question representation. We attempt to incorporate syntactic information in questions representation with TreeLSTM BIBREF13 , BIBREF14 . In general a TreeLSTM could perform semantic composition over given syntactic structures..Unlike the chain-structured LSTM BIBREF17 , the TreeLSTM captures long-distance interaction on a tree. The update of a TreeLSTM node is described at a high level with Equation ( 31 ), and the detailed computation is described in (–). Specifically, the input of a TreeLSTM node is used to configure four gates: the input gate $\\mathbf {i}_t$ , output gate $\\mathbf {o}_t$ , and the two forget gates $\\mathbf {f}_t^L$ for the left child input and $\\mathbf {f}_t^R$ for the right. The memory cell $\\mathbf {c}_t$ considers each child's cell vector, $\\mathbf {c}_{t-1}^L$ and $\\mathbf {c}_{t-1}^R$ , which are gated by the left forget gate $\\mathbf {f}_t^L$ and right forget gate $\\mathbf {f}_t^R$ , respectively..$$\\mathbf {h}_t &= \\text{TreeLSTM}(\\mathbf {x}_t, \\mathbf {h}_{t-1}^L, \\mathbf {h}_{t-1}^R), \\\\\n",
      "\n",
      "\\mathbf {h}_t &= \\mathbf {o}_t \\circ \\tanh (\\mathbf {c}_{t}),\\\\\n",
      "\\mathbf {o}_t &= \\sigma (\\mathbf {W}_o \\mathbf {x}_t + \\mathbf {U}_o^L \\mathbf {h}_{t-1}^L + \\mathbf {U}_o^R \\mathbf {h}_{t-1}^R), \\\\\\mathbf {c}_t &= \\mathbf {f}_t^L \\circ \\mathbf {c}_{t-1}^L + \\mathbf {f}_t^R \\circ \\mathbf {c}_{t-1}^R + \\mathbf {i}_t \\circ \\mathbf {u}_t, \\\\\\mathbf {f}_t^L &= \\sigma (\\mathbf {W}_f \\mathbf {x}_t + \\mathbf {U}_f^{LL} \\mathbf {h}_{t-1}^L + \\mathbf {U}_f^{LR} \\mathbf {h}_{t-1}^R),\\\\\n",
      "\\mathbf {f}_t^R &= \\sigma (\\mathbf {W}_f \\mathbf {x}_t + \\mathbf {U}_f^{RL} \\mathbf {h}_{t-1}^L + \\mathbf {U}_f^{RR} \\mathbf {h}_{t-1}^R), \\\\\\mathbf {i}_t &= \\sigma (\\mathbf {W}_i \\mathbf {x}_t + \\mathbf {U}_i^L \\mathbf {h}_{t-1}^L + \\mathbf {U}_i^R \\mathbf {h}_{t-1}^R), \\\\\\mathbf {u}_t &= \\tanh (\\mathbf {W}_c \\mathbf {x}_t + \\mathbf {U}_c^L \\mathbf {h}_{t-1}^L + \\mathbf {U}_c^R \\mathbf {h}_{t-1}^R),$$   (Eq. 31) .where $\\sigma $ is the sigmoid function, $\\circ $ is the element-wise multiplication of two vectors, and all $\\mathbf {W}$ , $\\mathbf {U}$ are trainable matrices..To obtain the parse tree information, we use Stanford CoreNLP (PCFG Parser) BIBREF23 , BIBREF24 to produce a binarized constituency parse for each question and build the TreeLSTM based on the parse tree. The root node of TreeLSTM is used as the representation for the whole question. More specifically, we use it as TreeLSTM Q-code $\\mathbf {Q}^{TL}\\in \\mathbb {R} ^{d_c}$ , by not only simply concatenating it to the alignment layer output but also using it as a question filter, just as we discussed in the question-based filtering section: .$$\\mathbf {Q}^{TL}=\\text{TreeLSTM}(\\mathbf {Q}^e) \\in \\mathbb {R} ^{d_c}$$   (Eq. 32) .$$\\mathbf {b}^{TL}=norm(\\mathbf {Q}^{TL} \\cdot \\mathbf {D}^{c\\mathrm {T}}) \\in \\mathbb {R} ^{M}$$   (Eq. 33) .where $\\mathbf {I}_{new}$ is the new output of alignment layer, and function $repmat$ copies $\\mathbf {Q}^{TL}$ for M times to fit with $\\mathbf {I}$ ..Questions by nature are often composed to fulfill different types of information needs. For example, a \"when\" question seeks for different types of information (i.e., temporal information) than those for a \"why\" question. Different types of questions and the corresponding answers could potentially have different distributional regularity..The previous models are often trained for all questions without explicitly discriminating different question types; however, for a target question, both the common features shared by all questions and the specific features for a specific type of question are further considered in this paper, as they could potentially obey different distributions. In this paper we further explicitly model different types of questions in the end-to-end training. We start from a simple way to first analyze the word frequency of all questions, and obtain top-10 most frequent question types: what, how, who, when, which, where, why, be, whose, and whom, in which be stands for the questions beginning with different forms of the word be such as is, am, and are. We explicitly encode question-type information to be an 11-dimensional one-hot vector (the top-10 question types and \"other\" question type). Each question type is with a trainable embedding vector. We call this explicit question type code, $\\mathbf {ET}\\in \\mathbb {R} ^{d_{ET}}$ . Then the vector for each question type is tuned during training, and is added to the system with the following equation: .$$\\mathbf {I}_{new}=[\\mathbf {I}, repmat(\\mathbf {ET})]$$   (Eq. 38) .As discussed, different types of questions and their answers may share common regularity and have separate property at the same time. We also view this as an adaptation problem in order to let different types of questions share a basic model but still discriminate them when needed. Specifically, we borrow ideas from speaker adaptation BIBREF18 in speech recognition, where neural-network-based adaptation is performed among different groups of speakers..Conceptually we regard a type of questions as a group of acoustically similar speakers. Specifically we propose a question discriminative block or simply called a discriminative block (Figure 3 ) below to perform question adaptation. The main idea is described below: .$$\\mathbf {x^\\prime } = f([\\mathbf {x}, \\mathbf {\\bar{x}}^c, \\mathbf {\\delta _x}])$$   (Eq. 40) .For each input question $\\mathbf {x}$ , we can decompose it to two parts: the cluster it belong(i.e., question type) and the diverse in the cluster. The information of the cluster is encoded in a vector $\\mathbf {\\bar{x}}^c$ . In order to keep calculation differentiable, we compute the weight of all the clusters based on the distances of $\\mathbf {x}$ and each cluster center vector, in stead of just choosing the closest cluster. Then the discriminative vector $\\mathbf {\\delta _x}$ with regard to these most relevant clusters are computed. All this information is combined to obtain the discriminative information. In order to keep the full information of input, we also copy the input question $\\mathbf {x}$ , together with the acquired discriminative information, to a feed-forward layer to obtain a new representation $\\mathbf {x^\\prime }$ for the question..More specifically, the adaptation algorithm contains two steps: adapting and updating, which is detailed as follows:.Adapting In the adapting step, we first compute the similarity score between an input question vector $\\mathbf {x}\\in \\mathbb {R} ^{h}$ and each centroid vector of $K$ clusters $~\\mathbf {\\bar{x}}\\in \\mathbb {R} ^{K \\times h}$ . Each cluster here models a question type. Unlike the explicit question type modeling discussed above, here we do not specify what question types we are modeling but let the system to learn. Specifically, we only need to pre-specific how many clusters, $K$ , we are modeling. The similarity between an input question and cluster centroid can be used to compute similarity weight $\\mathbf {w}^a$ : .$$w_k^a = softmax(cos\\_sim(\\mathbf {x}, \\mathbf {\\bar{x}}_k), \\alpha ), \\forall k \\in [1, \\dots , K]$$   (Eq. 43) .$$cos\\_sim(\\mathbf {u}, \\mathbf {v}) = \\frac{<\\mathbf {u},\\mathbf {v}>}{||\\mathbf {u}|| \\cdot ||\\mathbf {v}||}$$   (Eq. 44) .We set $\\alpha $ equals 50 to make sure only closest class will have a high weight while maintain differentiable. Then we acquire a soft class-center vector $\\mathbf {\\bar{x}}^c$ : .$$\\mathbf {\\bar{x}}^c = \\sum _k w^a_k \\mathbf {\\bar{x}}_k \\in \\mathbb {R} ^{h}$$   (Eq. 46) .We then compute a discriminative vector $\\mathbf {\\delta _x}$ between the input question with regard to the soft class-center vector: .$$\\mathbf {\\delta _x} = \\mathbf {x} - \\mathbf {\\bar{x}}^c$$   (Eq. 47) .Note that $\\bar{\\mathbf {x}}^c$ here models the cluster information and $\\mathbf {\\delta _x}$ represents the discriminative information in the cluster. By feeding $\\mathbf {x}$ , $\\bar{\\mathbf {x}}^c$ and $\\mathbf {\\delta _x}$ into feedforward layer with Relu, we obtain $\\mathbf {x^{\\prime }}\\in \\mathbb {R} ^{K}$ : .$$\\mathbf {x^{\\prime }} = Relu(\\mathbf {W} \\cdot [\\mathbf {x},\\bar{\\mathbf {x}}^c,\\mathbf {\\delta _x}])$$   (Eq. 48) .With $\\mathbf {x^{\\prime }}$ ready, we can apply Discriminative Block to any question code and obtain its adaptation Q-code. In this paper, we use TreeLSTM Q-code as the input vector $\\mathbf {x}$ , and obtain TreeLSTM adaptation Q-code $\\mathbf {Q}^{TLa}\\in \\mathbb {R} ^{d_c}$ . Similar to TreeLSTM Q-code $\\mathbf {Q}^{TL}$ , we concatenate $\\mathbf {Q}^{TLa}$ to alignment output $\\mathbf {I}$ and also use it as a question filter: .$$\\mathbf {Q}^{TLa} = Relu(\\mathbf {W} \\cdot [\\mathbf {Q}^{TL},\\overline{\\mathbf {Q}^{TL}}^c,\\mathbf {\\delta _{\\mathbf {Q}^{TL}}}])$$   (Eq. 49) .$$\\mathbf {b}^{TLa}=norm(\\mathbf {Q}^{TLa} \\cdot \\mathbf {D}^{c\\mathrm {T}}) \\in \\mathbb {R} ^{M}$$   (Eq. 50) .Updating The updating stage attempts to modify the center vectors of the $K$ clusters in order to fit each cluster to model different types of questions. The updating is performed according to the following formula: .$$\\mathbf {\\bar{x}^{\\prime }}_k = (1-\\beta \\text{w}_k^a)\\mathbf {\\bar{x}}_k+\\beta \\text{w}_k^a\\mathbf {x}, \\forall k \\in [1, \\dots , K]$$   (Eq. 54) .In the equation, $\\beta $ is an updating rate used to control the amount of each updating, and we set it to 0.01. When $\\mathbf {x}$ is far away from $K$ -th cluster center $\\mathbf {\\bar{x}}_k$ , $\\text{w}_k^a$ is close to be value 0 and the $k$ -th cluster center $\\mathbf {\\bar{x}}_k$ tends not to be updated. If $\\mathbf {x}$ is instead close to the $j$ -th cluster center $\\mathbf {\\bar{x}}_j$ , $\\mathbf {x}$0 is close to the value 1 and the centroid of the $\\mathbf {x}$1 -th cluster $\\mathbf {x}$2 will be updated more aggressively using $\\mathbf {x}$3 . generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "10\n",
      "Question 1: What type of data does the Children's Book Test use to test reading comprehension? \n",
      "\n",
      "Answer 1: The Children's Book Test (CBT) leverages named entities, common nouns, verbs, and prepositions to test reading comprehension.\n",
      "Question : for the text Recent advance on reading comprehension and question answering has been closely associated with the availability of various datasets. BIBREF0 released the MCTest data consisting of 500 short, fictional open-domain stories and 2000 questions. The CNN/Daily Mail dataset BIBREF1 contains news articles for close style machine comprehension, in which only entities are removed and tested for comprehension. Children's Book Test (CBT) BIBREF2 leverages named entities, common nouns, verbs, and prepositions to test reading comprehension. The Stanford Question Answering Dataset (SQuAD) BIBREF3 is more recently released dataset, which consists of more than 100,000 questions for documents taken from Wikipedia across a wide range of topics. The question-answer pairs are annotated through crowdsourcing. Answers are spans of text marked in the original documents. In this paper, we use SQuAD to evaluate our models..Many neural network models have been studied on the SQuAD task. BIBREF6 proposed match LSTM to associate documents and questions and adapted the so-called pointer Network BIBREF7 to determine the positions of the answer text spans. BIBREF8 proposed a dynamic chunk reader to extract and rank a set of answer candidates. BIBREF9 focused on word representation and presented a fine-grained gating mechanism to dynamically combine word-level and character-level representations based on the properties of words. BIBREF10 proposed a multi-perspective context matching (MPCM) model, which matched an encoded document and question from multiple perspectives. BIBREF11 proposed a dynamic decoder and so-called highway maxout network to improve the effectiveness of the decoder. The bi-directional attention flow (BIDAF) BIBREF12 used the bi-directional attention to obtain a question-aware context representation..In this paper, we introduce syntactic information to encode questions with a specific form of recursive neural networks BIBREF13 , BIBREF14 , BIBREF15 , BIBREF16 . More specifically, we explore a tree-structured LSTM BIBREF13 , BIBREF14 which extends the linear-chain long short-term memory (LSTM) BIBREF17 to a recursive structure, which has the potential to capture long-distance interactions over the structures..Different types of questions are often used to seek for different types of information. For example, a \"what\" question could have very different property from that of a \"why\" question, while they may share information and need to be trained together instead of separately. We view this as a \"adaptation\" problem to let different types of questions share a basic model but still discriminate them when needed. Specifically, we are motivated by the ideas \"i-vector\" BIBREF18 in speech recognition, where neural network based adaptation is performed among different (groups) of speakers and we focused instead on different types of questions here. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "11\n",
      "Question 1: What was the EM score achieved by the model on the SQuAD test set?\n",
      "\n",
      "Answer 1: The model achieved a 68.73% EM score on the SQuAD test set.\n",
      "Question : for the text Table 1 shows the official leaderboard on SQuAD test set when we submitted our system. Our model achieves a 68.73% EM score and 77.39% F1 score, which is ranked among the state of the art single models (without model ensembling)..Table 2 shows the ablation performances of various Q-code on the development set. Note that since the testset is hidden from us, we can only perform such an analysis on the development set. Our baseline model using no Q-code achieved a 68.00% and 77.36% EM and F1 scores, respectively. When we added the explicit question type T-code into the baseline model, the performance was improved slightly to 68.16%(EM) and 77.58%(F1). We then used TreeLSTM introduce syntactic parses for question representation and understanding (replacing simple question type as question understanding Q-code), which consistently shows further improvement. We further incorporated the soft adaptation. When letting the number of hidden question types ( $K$ ) to be 20, the performance improves to 68.73%/77.74% on EM and F1, respectively, which corresponds to the results of our model reported in Table 1 . Furthermore, after submitted our result, we have experimented with a large value of $K$ and found that when $K=100$ , we can achieve a better performance of 69.10%/78.38% on the development set..Figure UID61 shows the EM/F1 scores of different question types while Figure UID62 is the question type amount distribution on the development set. In Figure UID61 we can see that the average EM/F1 of the \"when\" question is highest and those of the \"why\" question is the lowest. From Figure UID62 we can see the \"what\" question is the major class..Figure 5 shows the composition of F1 score. Take our best model as an example, we observed a 78.38% F1 score on the whole development set, which can be separated into two parts: one is where F1 score equals to 100%, which means an exact match. This part accounts for 69.10% of the entire development set. And the other part accounts for 30.90%, of which the average F1 score is 30.03%. For the latter, we can further divide it into two sub-parts: one is where the F1 score equals to 0%, which means that predict answer is totally wrong. This part occupies 14.89% of the total development set. The other part accounts for 16.01% of the development set, of which average F1 score is 57.96%. From this analysis we can see that reducing the zero F1 score (14.89%) is potentially an important direction to further improve the system. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "12\n",
      "Question 1: What is the SQuAD dataset? \n",
      "\n",
      "Answer 1: The SQuAD dataset consists of more than 100,000 questions annotated by crowdsourcing workers on a selected set of Wikipedia articles, and the answer to each question is a span of text in the Wikipedia articles.\n",
      "Question : for the text We test our models on Stanford Question Answering Dataset (SQuAD) BIBREF3 . The SQuAD dataset consists of more than 100,000 questions annotated by crowdsourcing workers on a selected set of Wikipedia articles, and the answer to each question is a span of text in the Wikipedia articles. Training data includes 87,599 instances and validation set has 10,570 instances. The test data is hidden and kept by the organizer. The evaluation of SQuAD is Exact Match (EM) and F1 score..We use pre-trained 300-D Glove 840B vectors BIBREF20 to initialize our word embeddings. Out-of-vocabulary (OOV) words are initialized randomly with Gaussian samples. CharCNN filter length is 1,3,5, each is 50 dimensions. All vectors including word embedding are updated during training. The cluster number K in discriminative block is 100. The Adam method BIBREF25 is used for optimization. And the first momentum is set to be 0.9 and the second 0.999. The initial learning rate is 0.0004 and the batch size is 32. We will half learning rate when meet a bad iteration, and the patience is 7. Our early stop evaluation is the EM and F1 score of validation set. All hidden states of GRUs, and TreeLSTMs are 500 dimensions, while word-level embedding $d_w$ is 300 dimensions. We set max length of document to 500, and drop the question-document pairs beyond this on training set. Explicit question-type dimension $d_{ET}$ is 50. We apply dropout to the Encoder layer and aggregation layer with a dropout rate of 0.5. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "13\n",
      "Question 1: What components does the baseline model consist of?\n",
      "\n",
      "Answer 1: The baseline model consists of word embedding, input encoder, alignment, aggregation, and prediction components.\n",
      "Question : for the text Our baseline model is composed of the following typical components: word embedding, input encoder, alignment, aggregation, and prediction. Below we discuss these components in more details..We concatenate embedding at two levels to represent a word: the character composition and word-level embedding. The character composition feeds all characters of a word into a convolutional neural network (CNN) BIBREF19 to obtain a representation for the word. And we use the pre-trained 300-D GloVe vectors BIBREF20 (see the experiment section for details) to initialize our word-level embedding. Each word is therefore represented as the concatenation of the character-composition vector and word-level embedding. This is performed on both questions and documents, resulting in two matrices: the $\\mathbf {Q}^e \\in \\mathbb {R} ^{N\\times d_w}$ for a question and the $\\mathbf {D}^e \\in \\mathbb {R} ^{M\\times d_w}$ for a document, where $N$ is the question length (number of word tokens), $M$ is the document length, and $d_w$ is the embedding dimensionality..The above word representation focuses on representing individual words, and an input encoder here employs recurrent neural networks to obtain the representation of a word under its context. We use bi-directional GRU (BiGRU) BIBREF21 for both documents and questions..$${\\mathbf {Q}^c_i}&=\\text{BiGRU}(\\mathbf {Q}^e_i,i),\\forall i \\in [1, \\dots , N] \\\\\n",
      "{\\mathbf {D}^c_j}&=\\text{BiGRU}(\\mathbf {D}^e_j,j),\\forall j \\in [1, \\dots , M]$$   (Eq. 5) .A BiGRU runs a forward and backward GRU on a sequence starting from the left and the right end, respectively. By concatenating the hidden states of these two GRUs for each word, we obtain the a representation for a question or document: $\\mathbf {Q}^c \\in \\mathbb {R} ^{N\\times d_c}$ for a question and $\\mathbf {D}^c \\in \\mathbb {R} ^{M\\times d_c}$ for a document..Questions and documents interact closely. As in most previous work, our framework use both soft attention over questions and that over documents to capture the interaction between them. More specifically, in this soft-alignment layer, we first feed the contextual representation matrix $\\mathbf {Q}^c$ and $\\mathbf {D}^c$ to obtain alignment matrix $\\mathbf {U} \\in \\mathbb {R} ^{N\\times M}$ : .$$\\mathbf {U}_{ij} =\\mathbf {Q}_i^c \\cdot \\mathbf {D}_j^{c\\mathrm {T}}, \\forall i \\in [1, \\dots , N], \\forall j \\in [1, \\dots , M]$$   (Eq. 7) .Each $\\mathbf {U}_{ij}$ represents the similarity between a question word $\\mathbf {Q}_i^c$ and a document word $\\mathbf {D}_j^c$ ..Word-level Q-code Similar as in BIBREF12 , we obtain a word-level Q-code. Specifically, for each document word $w_j$ , we find which words in the question are relevant to it. To this end, $\\mathbf {a}_j\\in \\mathbb {R} ^{N}$ is computed with the following equation and used as a soft attention weight: .$$\\mathbf {a}_j = softmax(\\mathbf {U}_{:j}), \\forall j \\in [1, \\dots , M]$$   (Eq. 8) .With the attention weights computed, we obtain the encoding of the question for each document word $w_j$ as follows, which we call word-level Q-code in this paper: .$$\\mathbf {Q}^w=\\mathbf {a}^{\\mathrm {T}} \\cdot \\mathbf {Q}^{c} \\in \\mathbb {R} ^{M\\times d_c}$$   (Eq. 9) .Question-based filtering To better explore question understanding, we design this question-based filtering layer. As detailed later, different question representation can be easily incorporated to this layer in addition to being used as a filter to find key information in the document based on the question. This layer is expandable with more complicated question modeling..In the basic form of question-based filtering, for each question word $w_i$ , we find which words in the document are associated. Similar to $\\mathbf {a}_j$ discussed above, we can obtain the attention weights on document words for each question word $w_i$ : .$$\\mathbf {b}_i=softmax(\\mathbf {U}_{i:})\\in \\mathbb {R} ^{M}, \\forall i \\in [1, \\dots , N]$$   (Eq. 10) .By pooling $\\mathbf {b}\\in \\mathbb {R} ^{N\\times M}$ , we can obtain a question-based filtering weight $\\mathbf {b}^f$ : .$$\\mathbf {b}^f=norm(pooling(\\mathbf {b})) \\in \\mathbb {R} ^{M}$$   (Eq. 11) .$$norm(\\mathbf {x})=\\frac{\\mathbf {x}}{\\sum _i x_i}$$   (Eq. 12) .where the specific pooling function we used include max-pooling and mean-pooling. Then the document softly filtered based on the corresponding question $\\mathbf {D}^f$ can be calculated by: .$$\\mathbf {D}_j^{f_{max}}=b^{f_{max}}_j \\mathbf {D}_j^{c}, \\forall j \\in [1, \\dots , M]$$   (Eq. 13) .$$\\mathbf {D}_j^{f_{mean}}=b^{f_{mean}}_j \\mathbf {D}_j^{c}, \\forall j \\in [1, \\dots , M]$$   (Eq. 14) .Through concatenating the document representation $\\mathbf {D}^c$ , word-level Q-code $\\mathbf {Q}^w$ and question-filtered document $\\mathbf {D}^f$ , we can finally obtain the alignment layer representation: .$$\\mathbf {I}=[\\mathbf {D}^c, \\mathbf {Q}^w,\\mathbf {D}^c \\circ \\mathbf {Q}^w,\\mathbf {D}^c - \\mathbf {Q}^w, \\mathbf {D}^f, \\mathbf {b}^{f_{max}}, \\mathbf {b}^{f_{mean}}] \\in \\mathbb {R} ^{M \\times (6d_c+2)}$$   (Eq. 16) .where \" $\\circ $ \" stands for element-wise multiplication and \" $-$ \" is simply the vector subtraction..After acquiring the local alignment representation, key information in document and question has been collected, and the aggregation layer is then performed to find answers. We use three BiGRU layers to model the process that aggregates local information to make the global decision to find the answer spans. We found a residual architecture BIBREF22 as described in Figure 2 is very effective in this aggregation process: .$$\\mathbf {I}^1_i=\\text{BiGRU}(\\mathbf {I}_i)$$   (Eq. 18) .$$\\mathbf {I}^2_i=\\mathbf {I}^1_i + \\text{BiGRU}(\\mathbf {I}^1_i)$$   (Eq. 19) .The SQuAD QA task requires a span of text to answer a question. We use a pointer network BIBREF7 to predict the starting and end position of answers as in BIBREF6 . Different from their methods, we use a two-directional prediction to obtain the positions. For one direction, we first predict the starting position of the answer span followed by predicting the end position, which is implemented with the following equations: .$$P(s+)=softmax(W_{s+}\\cdot I^3)$$   (Eq. 23) .$$P(e+)=softmax(W_{e+} \\cdot I^3 + W_{h+} \\cdot h_{s+})$$   (Eq. 24) .where $\\mathbf {I}^3$ is inference layer output, $\\mathbf {h}_{s+}$ is the hidden state of the first step, and all $\\mathbf {W}$ are trainable matrices. We also perform this by predicting the end position first and then the starting position: .$$P(e-)=softmax(W_{e-}\\cdot I^3)$$   (Eq. 25) .$$P(s-)=softmax(W_{s-} \\cdot I^3 + W_{h-} \\cdot h_{e-})$$   (Eq. 26) .We finally identify the span of an answer with the following equation: .$$P(s)=pooling([P(s+), P(s-)])$$   (Eq. 27) .$$P(e)=pooling([P(e+), P(e-)])$$   (Eq. 28) .We use the mean-pooling here as it is more effective on the development set than the alternatives such as the max-pooling. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "14\n",
      "Question 1: What organizations provided support for this work? \n",
      "\n",
      "Answer 1: The Research Center of the Athens University of Economics and Business, and the French National Research Agency supported this work.\n",
      "Question : for the text We would like to thank the anonymous reviewers for their helpful feedback on this work. The work has been partly supported by the Research Center of the Athens University of Economics and Business, and by the French National Research Agency under project ANR-16-CE33-0013. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "15\n",
      "Question 1: What datasets were used in the experiments for the Sum-QE model? \n",
      "\n",
      "Answer 1: The datasets used in the experiments for the Sum-QE model came from the NIST DUC shared tasks, which comprise newswire articles.\n",
      "Question : for the text We propose a novel Quality Estimation model for summarization which does not require human references to estimate the quality of automatically produced summaries. Sum-QE successfully predicts qualitative aspects of summaries that recall-oriented evaluation metrics fail to approximate. Leveraging powerful BERT representations, it achieves high correlations with human scores for most linguistic qualities rated, on three different datasets. Future work involves extending the Sum-QE model to capture content-related aspects, either in combination with existing evaluation metrics (like Pyramid and ROUGE) or, preferably, by identifying important information in the original text and modelling its preservation in the proposed summaries. This would preserve Sum-QE's independence from human references, a property of central importance in real-life usage scenarios and system development settings..The datasets used in our experiments come from the NIST DUC shared tasks which comprise newswire articles. We believe that Sum-QE could be easily applied to other domains. A small amount of annotated data would be needed for fine-tuning – especially in domains with specialized vocabulary (e.g., biomedical) – but the model could also be used out of the box. A concrete estimation of performance in this setting will be part of future work. Also, the model could serve to estimate linguistic qualities other than the ones in the DUC dataset with mininum effort..Finally, Sum-QE could serve to assess the quality of other types of texts, not only summaries. It could thus be applied to other text generation tasks, such as natural language generation and sentence compression. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "16\n",
      "Question 1: What datasets were used in the experiments?\n",
      "\n",
      "Answer 1: The datasets used in the experiments were from the NIST DUC-05, DUC-06, and DUC-07 shared tasks, which included a total of 3,790 summaries. The summaries were manually evaluated based on content preservation using the Pyramid score and five linguistic quality criteria, and were scored on a five-point scale. The experiments focused on analyzing these datasets from 2005 onwards.\n",
      "Question : for the text We use datasets from the NIST DUC-05, DUC-06 and DUC-07 shared tasks BIBREF7, BIBREF19, BIBREF20. Given a question and a cluster of newswire documents, the contestants were asked to generate a 250-word summary answering the question. DUC-05 contains 1,600 summaries (50 questions x 32 systems); in DUC-06, 1,750 summaries are included (50 questions x 35 systems); and DUC-07 has 1,440 summaries (45 questions x 32 systems)..The submitted summaries were manually evaluated in terms of content preservation using the Pyramid score, and according to five linguistic quality criteria ($\\mathcal {Q}1, \\dots , \\mathcal {Q}5$), described in Figure FIGREF2, that do not involve comparison with a model summary. Annotators assigned scores on a five-point scale, with 1 and 5 indicating that the summary is bad or good with respect to a specific $\\mathcal {Q}$. The overall score for a contestant with respect to a specific $\\mathcal {Q}$ is the average of the manual scores assigned to the summaries generated by the contestant. Note that the DUC-04 shared task involved seven $\\mathcal {Q}$s, but some of them were found to be highly overlapping and were grouped into five in subsequent years BIBREF20. We address these five criteria and use DUC data from 2005 onwards in our experiments. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "17\n",
      "Question 1: How is the correlation between predicted scores and manual scores measured in the evaluation process?\n",
      "\n",
      "Answer 1: The correlation between predicted scores and manual scores is measured using Spearman's $\\rho$, Kendall's $\\tau$, and Pearson's $r$ across all contestants. The evaluation also involves calculating the average predicted and manual scores for each contestant's summaries, and training and testing the Sum-QE and BiGRU-ATT versions using a 3-fold procedure with held-out subsets for tuning hyper-parameters.\n",
      "Question : for the text To evaluate our methods for a particular $\\mathcal {Q}$, we calculate the average of the predicted scores for the summaries of each particular contestant, and the average of the corresponding manual scores assigned to the contestant's summaries. We measure the correlation between the two (predicted vs. manual) across all contestants using Spearman's $\\rho $, Kendall's $\\tau $ and Pearson's $r$..We train and test the Sum-QE and BiGRU-ATT versions using a 3-fold procedure. In each fold, we train on two datasets (e.g., DUC-05, DUC-06) and test on the third (e.g., DUC-07). We follow the same procedure with the three BiGRU-based models. Hyper-perameters are tuned on a held out subset from the training set of each fold. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "18\n",
      "What is the purpose of Sum-QE?\n",
      "Sum-QE is a proposed model used in machine translation to predict linguistic qualities of summaries that traditional evaluation metrics fail to capture. Its purpose is to inform users of the quality of automatically produced summaries and other types of generated text, and to select the best among summaries output by multiple systems.\n",
      "Question : for the text Quality Estimation (QE) is a term used in machine translation (MT) to refer to methods that measure the quality of automatically translated text without relying on human references BIBREF0, BIBREF1. In this study, we address QE for summarization. Our proposed model, Sum-QE, successfully predicts linguistic qualities of summaries that traditional evaluation metrics fail to capture BIBREF2, BIBREF3, BIBREF4, BIBREF5. Sum-QE predictions can be used for system development, to inform users of the quality of automatically produced summaries and other types of generated text, and to select the best among summaries output by multiple systems..Sum-QE relies on the BERT language representation model BIBREF6. We use a pre-trained BERT model adding just a task-specific layer, and fine-tune the entire model on the task of predicting linguistic quality scores manually assigned to summaries. The five criteria addressed are given in Figure FIGREF2. We provide a thorough evaluation on three publicly available summarization datasets from NIST shared tasks, and compare the performance of our model to a wide variety of baseline methods capturing different aspects of linguistic quality. Sum-QE achieves very high correlations with human ratings, showing the ability of BERT to model linguistic qualities that relate to both text content and form. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "19\n",
      "Question 1: How is the stack of BiGRU s with self-attention used in the Sum-QE model?\n",
      "\n",
      "Answer 1: The stack of BiGRU s with self-attention is used to replace the BERT instance in the Sum-QE model. The resulting context-aware token embeddings are weighted by their self-attention scores, and the final summary representation is the sum of these weighted embeddings. There are three flavors of the model: one single-task (BiGRU-ATT-S-1) and two multi-task (BiGRU-ATT-M-1 and BiGRU-ATT-M-5).\n",
      "Question : for the text This is very similar to Sum-QE but now $\\mathcal {E}$ is a stack of BiGRU s with self-attention BIBREF21, instead of a BERT instance. The final summary representation ($h$) is the sum of the resulting context-aware token embeddings ($h = \\sum _i a_i h_i$) weighted by their self-attention scores ($a_i$). We again have three flavors: one single-task (BiGRU-ATT-S-1) and two multi-task (BiGRU-ATT-M-1 and BiGRU-ATT-M-5). generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "20\n",
      "Question 1: What is used to estimate the grammaticality of a peer summary?\n",
      "Answer 1: A reasonable estimate of grammaticality in a peer summary is the perplexity returned by a pre-trained language model, such as GPT-2 or BERT-FR-LM. The perplexity is computed by considering only the lowest probability tokens in the summary, which can adversely affect the grammaticality of the entire summary.\n",
      "Question : for the text For a peer summary, a reasonable estimate of $\\mathcal {Q}1$ (Grammaticality) is the perplexity returned by a pre-trained language model. We experiment with the pre-trained GPT-2 model BIBREF22, and with the probability estimates that BERT can produce for each token when the token is treated as masked (BERT-FR-LM). Given that the grammaticality of a summary can be corrupted by just a few bad tokens, we compute the perplexity by considering only the $k$ worst (lowest LM probability) tokens of the peer summary, where $k$ is a tuned hyper-parameter. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "21\n",
      "Question 1: What are the two tasks involved in BERT training?\n",
      "\n",
      "Answer 1: The two tasks involved in BERT training are predicting masked tokens and next sentence prediction.\n",
      "Question : for the text BERT training relies on two tasks: predicting masked tokens and next sentence prediction. The latter seems to be aligned with the definitions of $\\mathcal {Q}3$ (Referential Clarity), $\\mathcal {Q}4$ (Focus) and $\\mathcal {Q}5$ (Structure & Coherence). Intuitively, when a sentence follows another with high probability, it should involve clear referential expressions and preserve the focus and local coherence of the text. We, therefore, use a pre-trained BERT model (BERT-FR-NS) to calculate the sentence-level perplexity of each summary:.where $p(s_i|s_{i-1})$ is the probability that BERT assigns to the sequence of sentences $\\left< s_{i-1}, s \\right>$, and $n$ is the number of sentences in the peer summary. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "22\n",
      "Question 1: What does ROUGE focus on?\n",
      "\n",
      "Answer 1: ROUGE focuses on surface similarities between peer and reference summaries.\n",
      "Question : for the text This baseline is the ROUGE version that performs best on each dataset, among the versions considered by BIBREF13. Although ROUGE focuses on surface similarities between peer and reference summaries, we would expect properties like grammaticality, referential clarity and coherence to be captured to some extent by ROUGE versions based on long $n$-grams or longest common subsequences. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "23\n",
      "Question 1: What is the main encoder used in Sum-QE and how is it fine-tuned?\n",
      "\n",
      "Answer 1: The main encoder used in Sum-QE is BERT, which is fine-tuned in three ways to produce three different versions of Sum-QE. The encoder converts each peer summary into a sequence of token embeddings, which are then used to produce a summary representation through an encoder. A regressor then predicts the quality score as an affine transformation of this summary representation.\n",
      "Question : for the text In Sum-QE, each peer summary is converted into a sequence of token embeddings, consumed by an encoder $\\mathcal {E}$ to produce a (dense vector) summary representation $h$. Then, a regressor $\\mathcal {R}$ predicts a quality score $S_{\\mathcal {Q}}$ as an affine transformation of $h$:.Non-linear regression could also be used, but a linear (affine) $\\mathcal {R}$ already performs well. We use BERT as our main encoder and fine-tune it in three ways, which leads to three versions of Sum-QE. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "24\n",
      "Question 1: How does the third version of Sum-QE differ from BERT-FT-M-1?\n",
      "\n",
      "Answer 1: The third version of Sum-QE is similar to BERT-FT-M-1, but it uses five different linear regressors (one per quality score) whereas BERT-FT-M-1 uses only one. Although mathematically equivalent, these two versions produce different results due to implementation details related to how the losses of the regressors are combined.\n",
      "Question : for the text The third version of Sum-QE is similar to BERT-FT-M-1, but we now use five different linear (affine) regressors, one per quality score:.Although BERT-FT-M-5 is mathematically equivalent to BERT-FT-M-1, in practice these two versions of Sum-QE produce different results because of implementation details related to how the losses of the regressors (five or one) are combined. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "25\n",
      "Question 1: How does the second version of Sum-QE predict all five quality scores?\n",
      "\n",
      "Answer 1: The second version of Sum-QE uses one estimator, $\\mathcal {R}$, to predict all five quality scores at once from a single encoding, $h$, produced by a single BERT instance. $\\mathcal {E}$ learns to create richer representations for $h$ so that $\\mathcal {R}(h)[i]$ can predict the $i$-th element of the vector returned by $\\mathcal {R}$.\n",
      "Question : for the text The second version of Sum-QE uses one estimator to predict all five quality scores at once, from a single encoding $h$ of the summary, produced by a single BERT instance. The intuition is that $\\mathcal {E}$ will learn to create richer representations so that $\\mathcal {R}$ (an affine transformation of $h$ with 5 outputs) will be able to predict all quality scores:.where $\\mathcal {R}(h)[i]$ is the $i$-th element of the vector returned by $\\mathcal {R}$. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "26\n",
      "Question 1: How many estimators does the first version of Sum-QE use and what is the purpose of each estimator?\n",
      "Answer 1: The first version of Sum-QE uses five separate estimators, one per quality score. Each estimator has its own encoder $\\mathcal {E}_i$ and regressor $\\mathcal {R}_i$, which are BERT instances generating $h_i$ and a separate linear regression layer, respectively. The purpose of these estimators is to generate quality scores for summaries.\n",
      "Question : for the text The first version of Sum-QE uses five separate estimators, one per quality score, each having its own encoder $\\mathcal {E}_i$ (a separate BERT instance generating $h_i$) and regressor $\\mathcal {R}_i$ (a separate linear regression layer on top of the corresponding BERT instance): generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "27\n",
      "Question 1: What are some widely used evaluation metrics for text summarization?\n",
      "\n",
      "Answer 1: Some widely used evaluation metrics for text summarization include Pyramid, ROUGE, and Quality Estimation. ROUGE is the most commonly used evaluation metric, while Pyramid requires substantial human effort. Quality Estimation is well established in Machine Translation and provides a quality indicator for summarization output at run-time without relying on human references.\n",
      "Question : for the text Summarization evaluation metrics like Pyramid BIBREF5 and ROUGE BIBREF3, BIBREF2 are recall-oriented; they basically measure the content from a model (reference) summary that is preserved in peer (system generated) summaries. Pyramid requires substantial human effort, even in its more recent versions that involve the use of word embeddings BIBREF8 and a lightweight crowdsourcing scheme BIBREF9. ROUGE is the most commonly used evaluation metric BIBREF10, BIBREF11, BIBREF12. Inspired by BLEU BIBREF4, it relies on common $n$-grams or subsequences between peer and model summaries. Many ROUGE versions are available, but it remains hard to decide which one to use BIBREF13. Being recall-based, ROUGE correlates well with Pyramid but poorly with linguistic qualities of summaries. BIBREF14 proposed a regression model for measuring summary quality without references. The scores of their model correlate well with Pyramid and Responsiveness, but text quality is only addressed indirectly..Quality Estimation is well established in MT BIBREF15, BIBREF0, BIBREF1, BIBREF16, BIBREF17. QE methods provide a quality indicator for translation output at run-time without relying on human references, typically needed by MT evaluation metrics BIBREF4, BIBREF18. QE models for MT make use of large post-edited datasets, and apply machine learning methods to predict post-editing effort scores and quality (good/bad) labels..We apply QE to summarization, focusing on linguistic qualities that reflect the readability and fluency of the generated texts. Since no post-edited datasets – like the ones used in MT – are available for summarization, we use instead the ratings assigned by human annotators with respect to a set of linguistic quality criteria. Our proposed models achieve high correlation with human judgments, showing that it is possible to estimate summary quality without human references. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "28\n",
      "What is the reason for the higher relative performance of BERT-based models compared to BiGRU-based models?\n",
      "\n",
      "The possible reason for the higher relative performance of the BERT-based models, which achieve a moderate positive correlation, is that BiGRU captures long-distance relations less effectively than BERT, which utilizes Transformers and has a larger receptive field.\n",
      "Question : for the text Table TABREF23 shows Spearman's $\\rho $, Kendall's $\\tau $ and Pearson's $r$ for all datasets and models. The three fine-tuned BERT versions clearly outperform all other methods. Multi-task versions seem to perform better than single-task ones in most cases. Especially for $\\mathcal {Q}4$ and $\\mathcal {Q}5$, which are highly correlated, the multi-task BERT versions achieve the best overall results. BiGRU-ATT also benefits from multi-task learning..The correlation of Sum-QE with human judgments is high or very high BIBREF23 for all $\\mathcal {Q}$s in all datasets, apart from $\\mathcal {Q}2$ in DUC-05 where it is only moderate. Manual scores for $\\mathcal {Q}2$ in DUC-05 are the highest among all $\\mathcal {Q}$s and years (between 4 and 5) and with the smallest standard deviation, as shown in Table TABREF24. Differences among systems are thus small in this respect, and although Sum-QE predicts scores in this range, it struggles to put them in the correct order, as illustrated in Figure FIGREF26..BEST-ROUGE has a negative correlation with the ground-truth scores for $\\mathcal {Q}$2 since it does not account for repetitions. The BiGRU-based models also reach their lowest performance on $\\mathcal {Q}$2 in DUC-05. A possible reason for the higher relative performance of the BERT-based models, which achieve a moderate positive correlation, is that BiGRU captures long-distance relations less effectively than BERT, which utilizes Transformers BIBREF24 and has a larger receptive field. A possible improvement would be a stacked BiGRU, since the states of higher stack layers have a larger receptive field as well..The BERT multi-task versions perform better with highly correlated qualities like $\\mathcal {Q}4$ and $\\mathcal {Q}5$ (as illustrated in Figures 2 to 4 in the supplementary material). However, there is not a clear winner among them. Mathematical equivalence does not lead to deterministic results, especially when random initialization and stochastic learning algorithms are involved. An in-depth exploration of this point would involve further investigation, which will be part of future work. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "29\n",
      "Question 1: Can the HAKE model infer the (anti)symmetry, inversion, and composition relation patterns?\n",
      "\n",
      "Answer 1: Yes, the HAKE model can infer these patterns. Proposition 1 shows that it can infer the (anti)symmetry pattern, Proposition 2 shows it can infer the inversion pattern, and Proposition 3 shows it can infer the composition pattern.\n",
      "Question : for the text In this section, we prove that our HAKE model can infer the (anti)symmetry, inversion and composition relation patterns. Detailed propositions and their proofs are as follows..Proposition 1 HAKE can infer the (anti)symmetry pattern..If $r(x, y)$ and $r(y, x)$ hold, we have.Then we have.Otherwise, if $r(x, y)$ and $\\lnot r(y, x)$ hold, we have.Proposition 2 HAKE can infer the inversion pattern..If $r_1(x, y)$ and $r_2(y, x)$ hold, we have.Then, we have..Proposition 3 HAKE can infer the composition pattern..If $r_1(x, z)$, $r_2(x, y)$ and $r_3(y, z)$ hold, we have.Then we have generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "30\n",
      "Question 1: What will the appendix provide analysis on?\n",
      "Answer 1: The appendix will provide analysis on relation patterns, negative entity embeddings, and moduli of entity embeddings, as well as more visualization results on semantic hierarchies.\n",
      "Question : for the text In this appendix, we will provide analysis on relation patterns, negative entity embeddings, and moduli of entity embeddings. Then, we will give more visualization results on semantic hierarchies. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "31\n",
      "Question 1: What is the difference between linked and unlinked entity pairs?\n",
      "\n",
      "Answer 1: Linked entity pairs are those that are connected by some relation, whereas unlinked entity pairs are those that are not present in the train/valid/test dataset. It is important to note that unlinked pairs may contain valid triples as the knowledge graph is incomplete.\n",
      "Question : for the text We denote the linked entity pairs as the set of entity pairs linked by some relation, and denote the unlinked entity pairs as the set of entity pairs that no triple contains in the train/valid/test dataset. It is worth noting that the unlinked paris may contain valid triples, as the knowledge graph is incomplete. For both the linked and the unlinked entity pairs, we count the embedding entries of two entities that have different signs. Figure FIGREF34 shows the result..For the linked entity pairs, as we expected, most of the entries have the same sign. Due to the large amount of unlinked entity pairs, we randomly sample a part of them for plotting. For the unlinked entity pairs, around half of the entries have different signs, which is consistent with the random initialization. The results support our hypothesis that the negative signs of entity embeddings can help our model to distinguish positive and negative triples. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "32\n",
      "Question 1: How does RotatE encourage the modulus of embeddings to be the same?\n",
      "\n",
      "Answer 1: RotatE models relations as rotations in a complex space, therefore it encourages the modulus of embeddings to be the same.\n",
      "Question : for the text Figure FIGREF37 shows the modulus of entity embeddings. We can observe that RotatE encourages the modulus of embeddings to be the same, as the relations are modeled as rotations in a complex space. Compared with RotatE, the modulus of entity embeddings in HAKE are more dispersed, making it to have more potential to model the semantic hierarchies. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "33\n",
      "Question 1: What is HAKE and how does it perform on the link prediction task compared to existing methods?\n",
      "\n",
      "Answer 1: HAKE is a hierarchy-aware knowledge graph embedding model that maps entities into the polar coordinate system to model semantic hierarchies. Experimental results show that HAKE outperforms several existing state-of-the-art methods on benchmark datasets for the link prediction task.\n",
      "Question : for the text To model the semantic hierarchies in knowledge graphs, we propose a novel hierarchy-aware knowledge graph embedding model—HAKE—which maps entities into the polar coordinate system. Experiments show that our proposed HAKE significantly outperforms several existing state-of-the-art methods on benchmark datasets for the link prediction task. A further investigation shows that HAKE is capable of modeling entities at both different levels and the same levels in the semantic hierarchies. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "34\n",
      "Question 1: How do the visualization results of the head and tail entities in WN18RR suggest that HAKE is better than RotatE in modeling entities in different and same hierarchies?\n",
      "\n",
      "Answer 1: The visualization results of the head and tail entities in WN18RR in Figure FIGREF41 suggest that compared with RotatE, HAKE can better model the entities both in different hierarchies and in the same hierarchy.\n",
      "Question : for the text In this part, we visualize more triples from WN18RR. We plot the head and tail entities on 2D planes using the same method as that in the main text. The visualization results are in Figure FIGREF41, where the subcaptions demonstrate the corresponding triples. The figures show that, compared with RotatE, our HAKE model can better model the entities both in different hierarchies and in the same hierarchy.. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "35\n",
      "Question 1: Where can the code of HAKE be found?\n",
      "Answer 1: The code of HAKE is available on GitHub at https://github.com/MIRALab-USTC/KGE-HAKE.\n",
      "Question : for the text This section is organized as follows. First, we introduce the experimental settings in detail. Then, we show the effectiveness of our proposed model on three benchmark datasets. Finally, we analyze the embeddings generated by HAKE, and show the results of ablation studies. The code of HAKE is available on GitHub at https://github.com/MIRALab-USTC/KGE-HAKE. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "36\n",
      "Q: What is the conclusion of the ablation studies conducted on HAKE? \n",
      "\n",
      "A: The conclusion of the ablation studies conducted on HAKE is that the bias item can significantly improve the performance of HAKE on nearly all metrics, while the modulus part of HAKE does not perform well on all datasets. Combining the two parts, the modulus part and the phase part, is important for modeling semantic hierarchies in knowledge graphs.\n",
      "Question : for the text In this part, we conduct ablation studies on the modulus part and the phase part of HAKE, as well as the mixture bias item. Table TABREF26 shows the results on three benchmark datasets..We can see that the bias can improve the performance of HAKE on nearly all metrics. Specifically, the bias improves the H@1 score of $4.7\\%$ on YAGO3-10 dataset, which illustrates the effectiveness of the bias..We also observe that the modulus part of HAKE does not perform well on all datasets, due to its inability to distinguish the entities at the same level of the hierarchy. When only using the phase part, HAKE degenerates to the pRotatE model BIBREF7. It performs better than the modulus part, because it can well model entities at the same level of the hierarchy. However, our HAKE model significantly outperforms the modulus part and the phase part on all datasets, which demonstrates the importance to combine the two parts for modeling semantic hierarchies in knowledge graphs. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "37\n",
      "Question 1: What is the benefit of using a logarithmic scale in visualizing entity embeddings?\n",
      "\n",
      "Answer 1: The benefit of using a logarithmic scale in visualizing entity embeddings is that it better displays the differences between entity embeddings, especially since all the moduli have values less than one. After applying the logarithm operation, the larger radii in the figures will actually represent smaller modulus.\n",
      "Question : for the text In this part, to further show that HAKE can capture the semantic hierarchies between entities, we visualize the embeddings of several entity pairs..We plot the entity embeddings of two models: the previous state-of-the-art RotatE and our proposed HAKE. RotatE regards each entity as a group of complex numbers. As a complex number can be seen as a point on a 2D plane, we can plot the entity embeddings on a 2D plane. As for HAKE, we have mentioned that it maps entities into the polar coordinate system. Therefore, we can also plot the entity embeddings generated by HAKE on a 2D plane based on their polar coordinates. For a fair comparison, we set $k=500$. That is, each plot contains 500 points, and the actual dimension of entity embeddings is 1000. Note that we use the logarithmic scale to better display the differences between entity embeddings. As all the moduli have values less than one, after applying the logarithm operation, the larger radii in the figures will actually represent smaller modulus..Figure FIGREF29 shows the visualization results of three triples from the WN18RR dataset. Compared with the tail entities, the head entities in Figures FIGREF29a, FIGREF29b, and FIGREF29c are at lower levels, similar levels, higher levels in the semantic hierarchy, respectively. We can see that there exist clear concentric circles in the visualization results of HAKE, which demonstrates that HAKE can effectively model the semantic hierarchies. However, in RotatE, the entity embeddings in all three subfigures are mixed, making it hard to distinguish entities at different levels in the hierarchy. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "38\n",
      "What does the distribution of relation embeddings moduli plotted in Figure FIGREF20 show?\n",
      "\n",
      "The distribution histograms of relation embeddings moduli plotted in Figure FIGREF20 show that most entries of relations in group (A) have values around one, indicating that head and tail entities have approximately the same moduli, while relations in groups (B) and (C) have moduli values less than one and more than one, respectively, showing that head and tail entities have different moduli. This indicates that HAKE can effectively model the hierarchy structures in knowledge graphs.\n",
      "Question : for the text In this part, we first show that HAKE can effectively model the hierarchy structures by analyzing the moduli of relation embeddings. Then, we show that the phase part of HAKE can help us to distinguish entities at the same level of the hierarchy by analyzing the phases of relation embeddings..In Figure FIGREF20, we plot the distribution histograms of moduli of six relations. These relations are drawn from WN18RR, FB15k-237, and YAGO3-10. Specifically, the relations in Figures FIGREF20a, FIGREF20c, FIGREF20e and FIGREF20f are drawn from WN18RR. The relation in Figure FIGREF20d is drawn from FB15k-237. The relation in Figure FIGREF20b is drawn from YAGO3-10. We divide the relations in Figure FIGREF20 into three groups..Relations in Figures FIGREF20c and FIGREF20d connect the entities at the same level of the semantic hierarchy;.Relations in Figures FIGREF20a and FIGREF20b represent that tail entities are at higher levels than head entities of the hierarchy;.Relations in Figures FIGREF20e and FIGREF20f represent that tail entities are at lower levels than head entities of the hierarchy..As described in the model description section, we expect entities at higher levels of the hierarchy to have small moduli. The experiments validate our expectation. For both ModE and HAKE, most entries of the relations in the group (A) take values around one, which leads to that the head entities and tail entities have approximately the same moduli. In the group (B), most entries of the relations take values less than one, which results in that the head entities have smaller moduli than the tail entities. The cases in the group (C) are contrary to that in the group (B). These results show that our model can capture the semantic hierarchies in knowledge graphs. Moreover, compared with ModE, the relation embeddings' moduli of HAKE have lower variances, which shows that HAKE can model hierarchies more clearly..As mentioned above, relations in the group (A) reflect the same semantic hierarchy, and are expected to have the moduli of about one. Obviously, it is hard to distinguish entities linked by these relations only using the modulus part. In Figure FIGREF22, we plot the phases of the relations in the group (A). The results show that the entities at the same level of the hierarchy can be distinguished by their phases, as many phases have the values of $\\pi $. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "39\n",
      "What is the best H@10 score obtained by the WHE+STC version of TKRL on the FB15k dataset compared to HAKE's score?\n",
      "\n",
      "Answer: The best H@10 score obtained by the WHE+STC version of TKRL on the FB15k dataset is .734, while HAKE's score is .884, indicating that HAKE significantly outperforms TKRL.\n",
      "Question : for the text We compare our models with TKRL models BIBREF12, which also aim to model the hierarchy structures. For the difference between HAKE and TKRL, please refer to the Related Work section. Table TABREF27 shows the H@10 scores of HAKE and TKRLs on FB15k dataset. The best performance of TKRL is .734 obtained by the WHE+STC version, while the H@10 score of our HAKE model is .884. The results show that HAKE significantly outperforms TKRL, though it does not require additional information. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "40\n",
      "What are the three datasets used for evaluation?\n",
      "Answer 1: The three datasets used for evaluation are WN18RR, FB15k-237, and YAGO3-10.\n",
      "\n",
      "What is the test set leakage problem in WN18 and FB15k?\n",
      "Answer 2: As pointed out by some references, WN18 and FB15k suffer from the test set leakage problem. One can attain state-of-the-art results even using simple rule-based models.\n",
      "\n",
      "What are the evaluation metrics used?\n",
      "Answer 3: The evaluation metrics used are Mean Reciprocal Rank (MRR) and Hits at N (H@N). Higher MRR or H@N indicate better performance.\n",
      "\n",
      "What is the optimizer used for training?\n",
      "Answer 4: Adam is used as the optimizer for training.\n",
      "\n",
      "What is the proposed model that uses only the modulus part?\n",
      "Answer 5: The proposed model that uses only the modulus part is called ModE.\n",
      "Question : for the text We evaluate our proposed models on three commonly used knowledge graph datasets—WN18RR BIBREF26, FB15k-237 BIBREF18, and YAGO3-10 BIBREF27. Details of these datasets are summarized in Table TABREF18..WN18RR, FB15k-237, and YAGO3-10 are subsets of WN18 BIBREF8, FB15k BIBREF8, and YAGO3 BIBREF27, respectively. As pointed out by BIBREF26 and BIBREF18, WN18 and FB15k suffer from the test set leakage problem. One can attain the state-of-the-art results even using a simple rule based model. Therefore, we use WN18RR and FB15k-237 as the benchmark datasets..Evaluation Protocol Following BIBREF8, for each triple $(h,r,t)$ in the test dataset, we replace either the head entity $h$ or the tail entity $t$ with each candidate entity to create a set of candidate triples. We then rank the candidate triples in descending order by their scores. It is worth noting that we use the “Filtered” setting as in BIBREF8, which does not take any existing valid triples into accounts at ranking. We choose Mean Reciprocal Rank (MRR) and Hits at N (H@N) as the evaluation metrics. Higher MRR or H@N indicate better performance..Training Protocol We use Adam BIBREF28 as the optimizer, and use grid search to find the best hyperparameters based on the performance on the validation datasets. To make the model easier to train, we add an additional coefficient to the distance function, i.e., $d_{r}(\\textbf {h},\\textbf {t})=\\lambda _1d_{r,m}(\\textbf {h}_m,\\textbf {t}_m)+\\lambda _2 d_{r,p}(\\textbf {h}_p,\\textbf {t}_p)$, where $\\lambda _1,\\lambda _2\\in \\mathbb {R}$..Baseline Model One may argue that the phase part is unnecessary, as we can distinguish entities in the category (b) by allowing $[\\textbf {r}]_i$ to be negative. We propose a model—ModE—that uses only the modulus part but allow $[\\textbf {r}]_i<0$. Specifically, the distance function of ModE is generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "41\n",
      "What are the state-of-the-art methods that HAKE and ModE were compared against? \n",
      "\n",
      "The state-of-the-art methods that HAKE and ModE were compared against are TransE, DistMult, ComplEx, ConvE, and RotatE.\n",
      "Question : for the text In this part, we show the performance of our proposed models—HAKE and ModE—against existing state-of-the-art methods, including TransE BIBREF8, DistMult BIBREF9, ComplEx BIBREF17, ConvE BIBREF18, and RotatE BIBREF7..Table TABREF19 shows the performance of HAKE, ModE, and several previous models. Our baseline model ModE shares similar simplicity with TransE, but significantly outperforms it on all datasets. Surprisingly, ModE even outperforms more complex models such as DistMult, ConvE and Complex on all datasets, and beats the state-of-the-art model—RotatE—on FB15k-237 and YAGO3-10 datasets, which demonstrates the great power of modulus information. Table TABREF19 also shows that our HAKE significantly outperforms existing state-of-the-art methods on all datasets..WN18RR dataset consists of two kinds of relations: the symmetric relations such as $\\_similar\\_to$, which link entities in the category (b); other relations such as $\\_hypernym$ and $\\_member\\_meronym$, which link entities in the category (a). Actually, RotatE can model entities in the category (b) very well BIBREF7. However, HAKE gains a 0.021 higher MRR, a 2.4% higher H@1, and a 2.4% higher H@3 against RotatE, respectively. The superior performance of HAKE compared with RotatE implies that our proposed model can better model different levels in the hierarchy..FB15k-237 dataset has more complex relation types and fewer entities, compared with WN18RR and YAGO3-10. Although there are relations that reflect hierarchy in FB15k-237, there are also lots of relations, such as “/location/location/time_zones” and “/film/film/prequel”, that do not lead to hierarchy. The characteristic of this dataset accounts for why our proposed models doesn't outperform the previous state-of-the-art as much as that of WN18RR and YAGO3-10 datasets. However, the results also show that our models can gain better performance so long as there exists semantic hierarchies in knowledge graphs. As almost all knowledge graphs have such hierarchy structures, our model is widely applicable..YAGO3-10 datasets contains entities with high relation-specific indegree BIBREF18. For example, the link prediction task $(?, hasGender, male)$ has over 1000 true answers, which makes the task challenging. Fortunately, we can regard “male” as an entity at higher level of the hierarchy and the predicted head entities as entities at lower level. In this way, YAGO3-10 is a dataset that clearly has semantic hierarchy property, and we can expect that our proposed models is capable of working well on this dataset. Table TABREF19 validates our expectation. Both ModE and HAKE significantly outperform the previous state-of-the-art. Notably, HAKE gains a 0.050 higher MRR, 6.0% higher H@1 and 4.6% higher H@3 than RotatE, respectively. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "42\n",
      "What is the main problem that knowledge graph completion aims to solve?\n",
      "Knowledge graph completion, also known as link prediction, aims to automatically predict missing links between entities in a knowledge graph based on the known links, as commonly used knowledge graphs still suffer from the incompleteness problem where a lot of valid triples are missing.\n",
      "Question : for the text Knowledge graphs are usually collections of factual triples—(head entity, relation, tail entity), which represent human knowledge in a structured way. In the past few years, we have witnessed the great achievement of knowledge graphs in many areas, such as natural language processing BIBREF0, question answering BIBREF1, and recommendation systems BIBREF2..Although commonly used knowledge graphs contain billions of triples, they still suffer from the incompleteness problem that a lot of valid triples are missing, as it is impractical to find all valid triples manually. Therefore, knowledge graph completion, also known as link prediction in knowledge graphs, has attracted much attention recently. Link prediction aims to automatically predict missing links between entities based on known links. It is a challenging task as we not only need to predict whether there is a relation between two entities, but also need to determine which relation it is..Inspired by word embeddings BIBREF3 that can well capture semantic meaning of words, researchers turn to distributed representations of knowledge graphs (aka, knowledge graph embeddings) to deal with the link prediction problem. Knowledge graph embeddings regard entities and relations as low dimensional vectors (or matrices, tensors), which can be stored and computed efficiently. Moreover, like in the case of word embeddings, knowledge graph embeddings can preserve the semantics and inherent structures of entities and relations. Therefore, other than the link prediction task, knowledge graph embeddings can also be used in various downstream tasks, such as triple classification BIBREF4, relation inference BIBREF5, and search personalization BIBREF6..The success of existing knowledge graph embedding models heavily relies on their ability to model connectivity patterns of the relations, such as symmetry/antisymmetry, inversion, and composition BIBREF7. For example, TransE BIBREF8, which represent relations as translations, can model the inversion and composition patterns. DistMult BIBREF9, which models the three-way interactions between head entities, relations, and tail entities, can model the symmetry pattern. RotatE BIBREF7, which represents entities as points in a complex space and relations as rotations, can model relation patterns including symmetry/antisymmetry, inversion, and composition. However, many existing models fail to model semantic hierarchies in knowledge graphs..Semantic hierarchy is a ubiquitous property in knowledge graphs. For instance, WordNet BIBREF10 contains the triple [arbor/cassia/palm, hypernym, tree], where “tree” is at a higher level than “arbor/cassia/palm” in the hierarchy. Freebase BIBREF11 contains the triple [England, /location/location/contains, Pontefract/Lancaster], where “Pontefract/Lancaster” is at a lower level than “England” in the hierarchy. Although there exists some work that takes the hierarchy structures into account BIBREF12, BIBREF13, they usually require additional data or process to obtain the hierarchy information. Therefore, it is still challenging to find an approach that is capable of modeling the semantic hierarchy automatically and effectively..In this paper, we propose a novel knowledge graph embedding model—namely, Hierarchy-Aware Knowledge Graph Embedding (HAKE). To model the semantic hierarchies, HAKE is expected to distinguish entities in two categories: (a) at different levels of the hierarchy; (b) at the same level of the hierarchy. Inspired by the fact that entities that have the hierarchical properties can be viewed as a tree, we can use the depth of a node (entity) to model different levels of the hierarchy. Thus, we use modulus information to model entities in the category (a), as the size of moduli can reflect the depth. Under the above settings, entities in the category (b) will have roughly the same modulus, which is hard to distinguish. Inspired by the fact that the points on the same circle can have different phases, we use phase information to model entities in the category (b). Combining the modulus and phase information, HAKE maps entities into the polar coordinate system, where the radial coordinate corresponds to the modulus information and the angular coordinate corresponds to the phase information. Experiments show that our proposed HAKE model can not only clearly distinguish the semantic hierarchies of entities, but also significantly and consistently outperform several state-of-the-art methods on the benchmark datasets...Notations Throughout this paper, we use lower-case letters $h$, $r$, and $t$ to represent head entities, relations, and tail entities, respectively. The triplet $(h,r,t)$ denotes a fact in knowledge graphs. The corresponding boldface lower-case letters $\\textbf {h}$, $\\textbf {r}$ and $\\textbf {t}$ denote the embeddings (vectors) of head entities, relations, and tail entities. The $i$-th entry of a vector $\\textbf {h}$ is denoted as $[\\textbf {h}]_i$. Let $k$ denote the embedding dimension..Let $\\circ :\\mathbb {R}^n\\times \\mathbb {R}^n\\rightarrow \\mathbb {R}^n$ denote the Hadamard product between two vectors, that is,.and $\\Vert \\cdot \\Vert _1$, $\\Vert \\cdot \\Vert _2$ denote the $\\ell _1$ and $\\ell _2$ norm, respectively. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "43\n",
      "Question 1: What are the two aspects in which the related work and our work differ?\n",
      "Answer 1: The two aspects in which the related work and our work differ are the model category and the way to model hierarchy structures in knowledge graphs.\n",
      "Question : for the text In this section, we will describe the related work and the key differences between them and our work in two aspects—the model category and the way to model hierarchy structures in knowledge graphs. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "44\n",
      "What are the three categories of knowledge graph embedding models? \n",
      "\n",
      "The three categories are translational distance models, bilinear models, and neural network based models.\n",
      "Question : for the text Roughly speaking, we can divide knowledge graph embedding models into three categories—translational distance models, bilinear models, and neural network based models. Table TABREF2 exhibits several popular models..Translational distance models describe relations as translations from source entities to target entities. TransE BIBREF8 supposes that entities and relations satisfy $\\textbf {h}+\\textbf {r}\\approx \\textbf {t}$, where $\\textbf {h}, \\textbf {r}, \\textbf {t} \\in \\mathbb {R}^n$, and defines the corresponding score function as $f_r(\\textbf {h},\\textbf {t})=-\\Vert \\textbf {h}+\\textbf {r}-\\textbf {t}\\Vert _{1/2}$. However, TransE does not perform well on 1-N, N-1 and N-N relations BIBREF14. TransH BIBREF14 overcomes the many-to-many relation problem by allowing entities to have distinct representations given different relations. The score function is defined as $f_r(\\textbf {h},\\textbf {t})=-\\Vert \\textbf {h}_{\\perp }+\\textbf {r}-\\textbf {t}_{\\perp }\\Vert _2$, where $\\textbf {h}_{\\perp }$ and $\\textbf {t}_{\\perp }$ are the projections of entities onto relation-specific hyperplanes. ManifoldE BIBREF15 deals with many-to-many problems by relaxing the hypothesis $\\textbf {h}+\\textbf {r}\\approx \\textbf {t}$ to $\\Vert \\textbf {h}+\\textbf {r}-\\textbf {t}\\Vert _2^2\\approx \\theta _r^2$ for each valid triple. In this way, the candidate entities can lie on a manifold instead of exact point. The corresponding score function is defined as $f_r(\\textbf {h},\\textbf {t})=-(\\Vert \\textbf {h}+\\textbf {r}-\\textbf {t}\\Vert _2^2-\\theta _r^2)^2$. More recently, to better model symmetric and antisymmetric relations, RotatE BIBREF7 defines each relation as a rotation from source entities to target entities in a complex vector space. The score function is defined as $f_r(\\textbf {h},\\textbf {t})=-\\Vert \\textbf {h}\\circ \\textbf {r}-\\textbf {t}\\Vert _1$, where $\\textbf {h},\\textbf {r},\\textbf {t}\\in \\mathbb {C}^k$ and $|[\\textbf {r}]_i|=1$..Bilinear models product-based score functions to match latent semantics of entities and relations embodied in their vector space representations. RESCAL BIBREF16 represents each relation as a full rank matrix, and defines the score function as $f_r(\\textbf {h},\\textbf {t})=\\textbf {h}^\\top \\textbf {M}_r \\textbf {t}$, which can also be seen as a bilinear function. As full rank matrices are prone to overfitting, recent works turn to make additional assumptions on $\\textbf {M}_r$. For example, DistMult BIBREF9 assumes $\\textbf {M}_r$ to be a diagonal matrix, and ANALOGY BIBREF19 supposes that $\\textbf {M}_r$ is normal. However, these simplified models are usually less expressive and not powerful enough for general knowledge graphs. Differently, ComplEx BIBREF17 extends DistMult by introducing complex-valued embeddings to better model asymmetric and inverse relations. HolE BIBREF20 combines the expressive power of RESCAL with the efficiency and simplicity of DistMult by using the circular correlation operation..Neural network based models have received greater attention in recent years. For example, MLP BIBREF21 and NTN BIBREF22 use a fully connected neural network to determine the scores of given triples. ConvE BIBREF18 and ConvKB BIBREF23 employ convolutional neural networks to define score functions. Recently, graph convolutional networks are also introduced, as knowledge graphs obviously have graph structures BIBREF24..Our proposed model HAKE belongs to the translational distance models. More specifically, HAKE shares similarities with RotatE BIBREF7, in which the authors claim that they use both modulus and phase information. However, there exist two major differences between RotatE and HAKE. Detailed differences are as follows..The aims are different. RotatE aims to model the relation patterns including symmetry/antisymmetry, inversion, and composition. HAKE aims to model the semantic hierarchy, while it can also model all the relation patterns mentioned above..The ways to use modulus information are different. RotatE models relations as rotations in the complex space, which encourages two linked entities to have the same modulus, no matter what the relation is. The different moduli in RotatE come from the inaccuracy in training. Instead, HAKE explicitly models the modulus information, which significantly outperforms RotatE in distinguishing entities at different levels of the hierarchy. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "45\n",
      "Question 1: How is the approach proposed in this work different from previous work on modeling hierarchy structures in knowledge graphs?\n",
      "\n",
      "Answer 1: This work considers the link prediction task, while previous work focuses on tasks such as concept categorization and hierarchical classification. This approach can automatically learn the semantic hierarchy in knowledge graphs without using clustering algorithms and does not require any additional information other than the triples in the knowledge graphs.\n",
      "Question : for the text Another related problem is how to model hierarchy structures in knowledge graphs. Some recent work considers the problem in different ways. BIBREF25 embed entities and categories jointly into a semantic space and designs models for the concept categorization and dataless hierarchical classification tasks. BIBREF13 use clustering algorithms to model the hierarchical relation structures. BIBREF12 proposed TKRL, which embeds the type information into knowledge graph embeddings. That is, TKRL requires additional hierarchical type information for entities..Different from the previous work, our work.considers the link prediction task, which is a more common task for knowledge graph embeddings;.can automatically learn the semantic hierarchy in knowledge graphs without using clustering algorithms;.does not require any additional information other than the triples in knowledge graphs. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "46\n",
      "Question 1: What is the proposed model introduced in this section?\n",
      "\n",
      "Answer 1: The proposed model introduced in this section is HAKE.\n",
      "Question : for the text In this section, we introduce our proposed model HAKE. We first introduce two categories of entities that reflect the semantic hierarchies in knowledge graphs. Afterwards, we introduce our proposed HAKE that can model entities in both of the categories. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "47\n",
      "Question 1: What is the purpose of the modulus part in HAKE?\n",
      "\n",
      "Answer 1: The modulus part in HAKE aims to model entities at different levels of the hierarchy. It uses moduli to reflect the depth of a node in a hierarchical tree and allows for the distinction between entities in category (a). The entities at higher levels of the hierarchy are expected to have a smaller modulus. Moreover, if HAKE were to only use the modulus part to embed knowledge graphs, the entities in category (b) would have the same modulus, and embeddings for these entities would consequently be the same.\n",
      "Question : for the text To model both of the above categories, we propose a hierarchy-aware knowledge graph embedding model—HAKE. HAKE consists of two parts—the modulus part and the phase part—which aim to model entities in the two different categories, respectively. Figure FIGREF13 gives an illustration of the proposed model..To distinguish embeddings in the different parts, we use $\\textbf {e}_m$ ($\\textbf {e}$ can be $\\textbf {h}$ or $\\textbf {t}$) and $\\textbf {r}_m$ to denote the entity embedding and relation embedding in the modulus part, and use $\\textbf {e}_p$ ($\\textbf {e}$ can be $\\textbf {h}$ or $\\textbf {t}$) and $\\textbf {r}_p$ to denote the entity embedding and relation embedding in the phase part..The modulus part aims to model the entities at different levels of the hierarchy. Inspired by the fact that entities that have hierarchical property can be viewed as a tree, we can use the depth of a node (entity) to model different levels of the hierarchy. Therefore, we use modulus information to model entities in the category (a), as moduli can reflect the depth in a tree. Specifically, we regard each entry of $\\textbf {h}_m$ and $\\textbf {t}_m$, that is, $[\\textbf {h}_m]_i$ and $[\\textbf {t}_m]_i$, as a modulus, and regard each entry of $\\textbf {r}_m$, that is, $[\\textbf {r}]_i$, as a scaling transformation between two moduli. We can formulate the modulus part as follows:.The corresponding distance function is:.Note that we allow the entries of entity embeddings to be negative but restrict the entries of relation embeddings to be positive. This is because that the signs of entity embeddings can help us to predict whether there exists a relation between two entities. For example, if there exists a relation $r$ between $h$ and $t_1$, and no relation between $h$ and $t_2$, then $(h, r, t_1)$ is a positive sample and $(h, r, t_2)$ is a negative sample. Our goal is to minimize $d_r(\\textbf {h}_m, \\textbf {t}_{1,m})$ and maximize $d_r(\\textbf {h}_m, \\textbf {t}_{2,m})$, so as to make a clear distinction between positive and negative samples. For the positive sample, $[\\textbf {h}]_i$ and $[\\textbf {t}_1]_i$ tend to share the same sign, as $[\\textbf {r}_m]_i>0$. For the negative sample, the signs of $[\\textbf {h}_m]_i$ and $[\\textbf {t}_{2,m}]_i$ can be different if we initialize their signs randomly. In this way, $d_r(\\textbf {h}_m, \\textbf {t}_{2,m})$ is more likely to be larger than $d_r(\\textbf {h}_m, \\textbf {t}_{1,m})$, which is exactly what we desire. We will validate this argument by experiments in Section 4 of the supplementary material..Further, we can expect the entities at higher levels of the hierarchy to have smaller modulus, as these entities are more close to the root of the tree..If we use only the modulus part to embed knowledge graphs, then the entities in the category (b) will have the same modulus. Moreover, suppose that $r$ is a relation that reflects the same semantic hierarchy, then $[\\textbf {r}]_i$ will tend to be one, as $h\\circ r\\circ r=h$ holds for all $h$. Hence, embeddings of the entities in the category (b) tend to be the same, which makes it hard to distinguish these entities. Therefore, a new module is required to model the entities in the category (b)..The phase part aims to model the entities at the same level of the semantic hierarchy. Inspired by the fact that points on the same circle (that is, have the same modulus) can have different phases, we use phase information to distinguish entities in the category (b). Specifically, we regard each entry of $\\textbf {h}_p$ and $\\textbf {t}_p$, that is, $[\\textbf {h}_p]_i$ and $[\\textbf {t}_p]_i$ as a phase, and regard each entry of $\\textbf {r}_p$, that is, $[\\textbf {r}_p]_i$, as a phase transformation. We can formulate the phase part as follows:.The corresponding distance function is:.where $\\sin (\\cdot )$ is an operation that applies the sine function to each element of the input. Note that we use a sine function to measure the distance between phases instead of using $\\Vert \\textbf {h}_p+\\textbf {r}_p-\\textbf {t}_p\\Vert _1$, as phases have periodic characteristic. This distance function shares the same formulation with that of pRotatE BIBREF7..Combining the modulus part and the phase part, HAKE maps entities into the polar coordinate system, where the radial coordinate and the angular coordinates correspond to the modulus part and the phase part, respectively. That is, HAKE maps an entity $h$ to $[\\textbf {h}_m;\\textbf {h}_p]$, where $\\textbf {h}_m$ and $\\textbf {h}_p$ are generated by the modulus part and the phase part, respectively, and $[\\,\\cdot \\,; \\,\\cdot \\,]$ denotes the concatenation of two vectors. Obviously, $([\\textbf {h}_m]_i,[\\textbf {h}_p]_i)$ is a 2D point in the polar coordinate system. Specifically, we formulate HAKE as follows:.The distance function of HAKE is:.where $\\lambda \\in \\mathbb {R}$ is a parameter that learned by the model. The corresponding score function is.When two entities have the same moduli, then the modulus part $d_{r,m}(\\textbf {h}_m,\\textbf {t}_m)=0$. However, the phase part $d_{r,p}(\\textbf {h}_p,\\textbf {t}_p)$ can be very different. By combining the modulus part and the phase part, HAKE can model the entities in both the category (a) and the category (b). Therefore, HAKE can model semantic hierarchies of knowledge graphs..When evaluating the models, we find that adding a mixture bias to $d_{r,m}(\\textbf {h},\\textbf {t})$ can help to improve the performance of HAKE. The modified $d_{r,m}(\\textbf {h},\\textbf {t})$ is given by:.where $0<\\textbf {r}^{\\prime }_m<1$ is a vector that have the same dimension with $\\textbf {r}_m$. Indeed, the above distance function is equivalent to.where $/$ denotes the element-wise division operation. If we let $\\textbf {r}_m\\leftarrow (1-\\textbf {r}_m^{\\prime })/(\\textbf {r}_m+\\textbf {r}_m^{\\prime })$, then the modified distance function is exactly the same as the original one when compare the distances of different entity pairs. For notation convenience, we still use $d_{r,m}(\\textbf {h},\\textbf {t})=\\Vert \\textbf {h}_m\\circ \\textbf {r}_m-\\textbf {t}_m\\Vert _2$ to represent the modulus part. We will conduct ablation studies on the bias in the experiment section. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "48\n",
      "Question 1: What loss functions are used in training the model?\n",
      "\n",
      "Answer 1: The negative sampling loss functions with self-adversarial training are used in training the model.\n",
      "Question : for the text To train the model, we use the negative sampling loss functions with self-adversarial training BIBREF7:.where $\\gamma $ is a fixed margin, $\\sigma $ is the sigmoid function, and $(h^{\\prime }_i,r,t^{\\prime }_i)$ is the $i$th negative triple. Moreover,.is the probability distribution of sampling negative triples, where $\\alpha $ is the temperature of sampling. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "49\n",
      "Question 1: What are the two categories of entities that a knowledge graph embedding model needs to distinguish?\n",
      "Answer 1: The two categories of entities that a knowledge graph embedding model needs to distinguish are entities at different levels of the hierarchy, such as \"mammal\" and \"dog\", and entities at the same level of the hierarchy, such as \"rose\" and \"peony\".\n",
      "Question : for the text To model the semantic hierarchies of knowledge graphs, a knowledge graph embedding model must be capable of distinguishing entities in the following two categories..Entities at different levels of the hierarchy. For example, “mammal” and “dog”, “run” and ”move”..Entities at the same level of the hierarchy. For example, “rose” and “peony”, “truck” and ”lorry”. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "50\n",
      "Question 1: Who partially supported the researchers to join the Future Technology Conference - 2019?\n",
      "\n",
      "Answer 1: The Future Technology Conference - 2019 committee partially supported the researchers to join the conference and one of their colleagues - Faheem Abrar, Software Developer also supported them by providing fund.\n",
      "Question : for the text We would like to thank Dr. Khandaker Tabin Hasan, Head of the Depertment of Computer Science, American International University-Bangladesh for his inspiration and encouragement in all of our research works. Also, thanks to Future Technology Conference - 2019 committee for partially supporting us to join the conference and one of our colleague - Faheem Abrar, Software Developer for his thorough review and comments on this research work and supporting us by providing fund. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "1\n",
      "Question 1: What is the aim of the paper on text-to-code framework?\n",
      "Answer 1: The aim of the paper is to make the text-to-code framework work for general purpose programming language, primarily Python, and to explore the possibility of a unified programming interface in the future using machine learning.\n",
      "Question : for the text The main advantage of translating to a programming language is - it has a concrete and strict lexical and grammatical structure which human languages lack. The aim of this paper was to make the text-to-code framework work for general purpose programming language, primarily Python. In later phase, phrase-based word embedding can be incorporated for improved vocabulary mapping. To get more accurate target code for each line, Abstract Syntax Tree(AST) can be beneficial..The contribution of this research is a machine learning model which can turn the human expression into coding expressions. This paper also discusses available methods which convert natural language to programming language successfully in fixed or tightly bounded linguistic paradigm. Approaching this problem using machine learning will give us the opportunity to explore the possibility of unified programming interface as well in the future. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "2\n",
      "Question 1: What is the main challenge in creating tools to generate code through natural language?\n",
      "\n",
      "Answer 1: The main challenge is that programming languages are diverse and individuals express logical statements differently, making Natural Language Processing (NLP) of programming statements challenging. Additionally, both human and programming language evolve over time.\n",
      "Question : for the text Removing computer-human language barrier is an inevitable advancement researchers are thriving to achieve for decades. One of the stages of this advancement will be coding through natural human language instead of traditional programming language. On naturalness of computer programming D. Knuth said, “Let us change our traditional attitude to the construction of programs: Instead of imagining that our main task is to instruct a computer what to do, let us concentrate rather on explaining to human beings what we want a computer to do.”BIBREF0. Unfortunately, learning programming language is still necessary to instruct it. Researchers and developers are working to overcome this human-machine language barrier. Multiple branches exists to solve this challenge (i.e. inter-conversion of different programming language to have universally connected programming languages). Automatic code generation through natural language is not a new concept in computer science studies. However, it is difficult to create such tool due to these following three reasons–.Programming languages are diverse.An individual person expresses logical statements differently than other.Natural Language Processing (NLP) of programming statements is challenging since both human and programming language evolve over time.In this paper, a neural approach to translate pseudo-code or algorithm like human language expression into programming language code is proposed. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "3\n",
      "Question 1: What are the restricting factors of developing text-to-code conversion methods and what problems need to be solved?\n",
      "\n",
      "Answer 1: The restricting factors of developing text-to-code conversion methods are the complexity of natural language and the non-deterministic nature of programming languages. Additionally, the lack of standardized programming structures and the large variation in code syntax also pose challenges for such conversion methods. The problems that need to be solved include improving the accuracy of the conversion, handling ambiguous and incomplete natural language input, and identifying the best programming language and structure to use for a given input.\n",
      "Question : for the text Code repositories (i.e. Git, SVN) flourished in the last decade producing big data of code allowing data scientists to perform machine learning on these data. In 2017, Allamanis M et al. published a survey in which they presented the state-of-the-art of the research areas where machine learning is changing the way programmers code during software engineering and development process BIBREF1. This paper discusses what are the restricting factors of developing such text-to-code conversion method and what problems need to be solved– generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "4\n",
      "What is the challenge in processing human language in programming?\n",
      "\n",
      "The challenge in processing human language in programming is to properly understand it due to the semantic brittleness of code and the wide range of syntax in high-level languages, which leads to programmers using different linguistic expressions to explain their code. Small changes, such as swapping function arguments, can significantly change the meaning of the code.\n",
      "Question : for the text One of the motivations behind this paper is - as long as it is about programming, there is a finite and small set of expression which is used in human vocabulary. For instance, programmers express a for-loop in a very few specific ways BIBREF3. Variable declaration and value assignment expressions are also limited in nature. Although all codes are executable, human representation through text may not due to the semantic brittleness of code. Since high-level languages have a wide range of syntax, programmers use different linguistic expressions to explain those. For instance, small changes like swapping function arguments can significantly change the meaning of the code. Hence the challenge remains in processing human language to understand it properly which brings us to the next problem- generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "5\n",
      "What is the role of semantic analysis in programming?\n",
      "\n",
      "Semantic analysis plays an important role in extracting information accurately from the statements of the code. It helps in determining the initial value, step value, and termination of loops, among other things.\n",
      "Question : for the text Although there is a finite set of expressions for each programming statements, it is a challenge to extract information from the statements of the code accurately. Semantic analysis of linguistic expression plays an important role in this information extraction. For instance, in case of a loop, what is the initial value? What is the step value? When will the loop terminate?.Mihalcea R. et al. has achieved a variable success rate of 70-80% in producing code just from the problem statement expressed in human natural language BIBREF3. They focused solely on the detection of step and loops in their research. Another research group from MIT, Lei et al. use a semantic learning model for text to detect the inputs. The model produces a parser in C++ which can successfully parse more than 70% of the textual description of input BIBREF4. The test dataset and model was initially tested and targeted against ACM-ICPC participantsínputs which contains diverse and sometimes complex input instructions..A recent survey from Allamanis M. et al. presented the state-of-the-art on the area of naturalness of programming BIBREF1. A number of research works have been conducted on text-to-code or code-to-text area in recent years. In 2015, Oda et al. proposed a way to translate each line of Python code into natural language pseudocode using Statistical Machine Learning Technique (SMT) framework BIBREF5 was used. This translation framework was able to - it can successfully translate the code to natural language pseudo coded text in both English and Japanese. In the same year, Chris Q. et al. mapped natural language with simple if-this-then-that logical rules BIBREF6. Tihomir G. and Viktor K. developed an Integrated Development Environment (IDE) integrated code assistant tool anyCode for Java which can search, import and call function just by typing desired functionality through text BIBREF7. They have used model and mapping framework between function signatures and utilized resources like WordNet, Java Corpus, relational mapping to process text online and offline..Recently in 2017, P. Yin and G. Neubig proposed a semantic parser which generates code through its neural model BIBREF8. They formulated a grammatical model which works as a skeleton for neural network training. The grammatical rules are defined based on the various generalized structure of the statements in the programming language. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "6\n",
      "How many actively maintained programming languages are there according to the sources mentioned in the text?\n",
      "\n",
      "According to the sources mentioned in the text, there are more than a thousand actively maintained programming languages.\n",
      "Question : for the text According to the sources, there are more than a thousand actively maintained programming languages, which signifies the diversity of these language . These languages were created to achieve different purpose and use different syntaxes. Low-level languages such as assembly languages are easier to express in human language because of the low or no abstraction at all whereas high-level, or Object-Oriented Programing (OOP) languages are more diversified in syntax and expression, which is challenging to bring into a unified human language structure. Nonetheless, portability and transparency between different programming languages also remains a challenge and an open research area. George D. et al. tried to overcome this problem through XML mapping BIBREF2. They tried to convert codes from C++ to Java using XML mapping as an intermediate language. However, the authors encountered challenges to support different features of both languages. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "7\n",
      "Question 1: What is the success rate of machine learning techniques in converting human text to executable code?\n",
      "\n",
      "Answer 1: Machine learning techniques, such as SMT, have proven to be at most 75% successful in converting human text to executable code. (BIBREF9)\n",
      "Question : for the text The use of machine learning techniques such as SMT proved to be at most 75% successful in converting human text to executable code. BIBREF9. A programming language is just like a language with less vocabulary compared to a typical human language. For instance, the code vocabulary of the training dataset was 8814 (including variable, function, class names), whereas the English vocabulary to express the same code was 13659 in total. Here, programming language is considered just like another human language and widely used SMT techniques have been applied. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "8\n",
      "Question 1: What is the role of SMT in Natural Language Processing (NLP)?\n",
      "\n",
      "Answer 1: SMT techniques are widely used in Natural Language Processing (NLP) for translation from one language to another, particularly in lexical and grammatical rule extraction. In SMT, bilingual grammatical structures are automatically formed by statistical approaches without explicitly providing a grammatical model, reducing the need for significant collaboration between bi-lingual linguists.\n",
      "Question : for the text SMT techniques are widely used in Natural Language Processing (NLP). SMT plays a significant role in translation from one language to another, especially in lexical and grammatical rule extraction. In SMT, bilingual grammatical structures are automatically formed by statistical approaches instead of explicitly providing a grammatical model. This reduces months and years of work which requires significant collaboration between bi-lingual linguistics. Here, a neural network based machine translation model is used to translate regular text into programming code. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "9\n",
      "Question 1: What is the requirement for SMT techniques?\n",
      "\n",
      "Answer 1: SMT techniques require a parallel corpus in the source and target languages, which is used in training. The parallel corpus used in training has 18805 aligned data in it, where the expression of each line code is written in the English language in the source data, and in the Python programming language in the target data.\n",
      "Question : for the text SMT techniques require a parallel corpus in thr source and thr target language. A text-code parallel corpus similar to Fig. FIGREF12 is used in training. This parallel corpus has 18805 aligned data in it . In source data, the expression of each line code is written in the English language. In target data, the code is written in Python programming language. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "10\n",
      "Question 1: What coding framework is used in the translation model?\n",
      "\n",
      "Answer 1: PyTorch is used as Neural Network coding framework in the translation model.\n",
      "Question : for the text In order to train the translation model between text-to-code an open source Neural Machine Translation (NMT) - OpenNMT implementation is utilized BIBREF11. PyTorch is used as Neural Network coding framework. For training, three types of Recurrent Neural Network (RNN) layers are used – an encoder layer, a decoder layer and an output layer. These layers together form a LSTM model. LSTM is typically used in seq2seq translation..In Fig. FIGREF13, the neural model architecture is demonstrated. The diagram shows how it takes the source and target text as input and uses it for training. Vector representation of tokenized source and target text are fed into the model. Each token of the source text is passed into an encoder cell. Target text tokens are passed into a decoder cell. Encoder cells are part of the encoder RNN layer and decoder cells are part of the decoder RNN layer. End of the input sequence is marked by a $<$eos$>$ token. Upon getting the $<$eos$>$ token, the final cell state of encoder layer initiate the output layer sequence. At each target cell state, attention is applied with the encoder RNN state and combined with the current hidden state to produce the prediction of next target token. This predictions are then fed back to the target RNN. Attention mechanism helps us to overcome the fixed length restriction of encoder-decoder sequence and allows us to process variable length between input and output sequence. Attention uses encoder state and pass it to the decoder cell to give particular attention to the start of an output layer sequence. The encoder uses an initial state to tell the decoder what it is supposed to generate. Effectively, the decoder learns to generate target tokens, conditioned on the input sequence. Sigmoidal optimization is used to optimize the prediction. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "11\n",
      "Question 1: What is the purpose of creating separate vocabulary files for source texts and codes?\n",
      "\n",
      "Answer 1: The purpose of creating separate vocabulary files for source texts and codes is to convert them into a computational entity for training the neural model. Tokenization of words is done and the popular word2vec method is used to put the words into their contextual vector space, making them computational.\n",
      "Question : for the text To train the neural model, the texts should be converted to a computational entity. To do that, two separate vocabulary files are created - one for the source texts and another for the code. Vocabulary generation is done by tokenization of words. Afterwards, the words are put into their contextual vector space using the popular word2vec BIBREF10 method to make the words computational. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "12\n",
      "Question 1: Why is the generated code often incoherent and predicts wrong code token?\n",
      "\n",
      "Answer 1: The generated code is often incoherent and predicts wrong code token due to the limited amount of training data. LSTM generally requires a more extensive set of data (100k+ in such a scenario) to build a more accurate model.\n",
      "Question : for the text Training parallel corpus had 18805 lines of annotated code in it. The training model is executed several times with different training parameters. During the final training process, 500 validation data is used to generate the recurrent neural model, which is 3% of the training data. We run the training with epoch value of 10 with a batch size of 64. After finishing the training, the accuracy of the generated model using validation data from the source corpus was 74.40% (Fig. FIGREF17)..Although the generated code is incoherent and often predict wrong code token, this is expected because of the limited amount of training data. LSTM generally requires a more extensive set of data (100k+ in such scenario) to build a more accurate model. The incoherence can be resolved by incorporating coding syntax tree model in future. For instance–.\"define the method tzname with 2 arguments: self and dt.\".is translated into–.def __init__ ( self , regex ) :..The translator is successfully generating the whole codeline automatically but missing the noun part (parameter and function name) part of the syntax. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "13\n",
      "Question 1: What are some key methods used to generate visually-rich and photo-realistic images from text-based natural language?\n",
      "\n",
      "Answer 1: Some key methods used to generate visually-rich and photo-realistic images from text-based natural language include generative adversarial networks (GANs), deep convolutional decoder networks, and multimodal learning methods. The survey paper also proposed a taxonomy to organize GAN-based text-to-image synthesis frameworks into four major groups: semantic enhancement GANs, resolution enhancement GANs, diversity enhancement GANs, and motion enhancement GANs. Some advanced GAN frameworks include StackGAN, StackGAN++, AttnGAN, DC-GAN, AC-GAN, TAC-GAN, HDGAN, Text-SeGAn, and StoryGAN.\n",
      "Question : for the text The recent advancement in text-to-image synthesis research opens the door to several compelling methods and architectures. The main objective of text-to-image synthesis initially was to create images from simple labels, and this objective later scaled to natural languages. In this paper, we reviewed novel methods that generate, in our opinion, the most visually-rich and photo-realistic images, from text-based natural language. These generated images often rely on generative adversarial networks (GANs), deep convolutional decoder networks, and multimodal learning methods..blackIn the paper, we first proposed a taxonomy to organize GAN based text-to-image synthesis frameworks into four major groups: semantic enhancement GANs, resolution enhancement GANs, diversity enhancement GANs, and motion enhancement GANs. The taxonomy provides a clear roadmap to show the motivations, architectures, and difference of different methods, and also outlines their evolution timeline and relationships. Following the proposed taxonomy, we reviewed important features of each method and their architectures. We indicated the model definition and key contributions from some advanced GAN framworks, including StackGAN, StackGAN++, AttnGAN, DC-GAN, AC-GAN, TAC-GAN, HDGAN, Text-SeGAn, StoryGAN etc. Many of the solutions surveyed in this paper tackled the highly complex challenge of generating photo-realistic images beyond swatch size samples. In other words, beyond the work of BIBREF8 in which images were generated from text in 64$\\times $64 tiny swatches. Lastly, all methods were evaluated on datasets that included birds, flowers, humans, and other miscellaneous elements. We were also able to allocate some important papers that were as impressive as the papers we finally surveyed. Though, these notable papers have yet to contribute directly or indirectly to the expansion of the vast computer vision AI field. Looking into the future, an excellent extension from the works surveyed in this paper would be to give more independence to the several learning methods (e.g. less human intervention) involved in the studies as well as increasing the size of the output images. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "14\n",
      "Question 1: Which models were missing data for certain datasets in the evaluation?\n",
      "\n",
      "Answer 1: AttnGAN and HDGAN were missing data for certain datasets in the evaluation.\n",
      "Question : for the text While we gathered all the data we could find on scores for each model on the CUB, Oxford, and COCO datasets using IS, FID, FCN, and human classifiers, we unfortunately were unable to find certain data for AttnGAN and HDGAN (missing in Table TABREF48). The best evaluation we can give for those with missing data is our own opinions by looking at examples of generated images provided in their papers. In this regard, we observed that HDGAN produced relatively better visual results on the CUB and Oxford datasets while AttnGAN produced far more impressive results than the rest on the more complex COCO dataset. This is evidence that the attentional model and DAMSM introduced by AttnGAN are very effective in producing high-quality images. Examples of the best results of birds and plates of vegetables generated by each model are presented in Figures FIGREF50 and FIGREF51, respectively..blackIn terms of inception score (IS), which is the metric that was applied to majority models except DC-GAN, the results in Table TABREF48 show that StackGAN++ only showed slight improvement over its predecessor, StackGAN, for text-to-image synthesis. However, StackGAN++ did introduce a very worthy enhancement for unconditional image generation by organizing the generators and discriminators in a “tree-like” structure. This indicates that revising the structures of the discriminators and/or generators can bring a moderate level of improvement in text-to-image synthesis..blackIn addition, the results in Table TABREF48 also show that DM-GAN BIBREF53 has the best performance, followed by Obj-GAN BIBREF81. Notice that both DM-GAN and Obj-GAN are most recently developed methods in the field (both published in 2019), indicating that research in text to image synthesis is continuously improving the results for better visual perception and interception. Technical wise, DM-GAN BIBREF53 is a model using dynamic memory to refine fuzzy image contents initially generated from the GAN networks. A memory writing gate is used for DM-GAN to select important text information and generate images based on he selected text accordingly. On the other hand, Obj-GAN BIBREF81 focuses on object centered text-to-image synthesis. The proposed framework of Obj-GAN consists of a layout generation, including a bounding box generator and a shape generator, and an object-driven attentive image generator. The designs and advancement of DM-GAN and Obj-GAN indicate that research in text-to-image synthesis is advancing to put more emphasis on the image details and text semantics for better understanding and perception. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "15\n",
      "Question 1: What other applications of GANs in image synthesis were mentioned in the text besides text-to-image synthesis?\n",
      "\n",
      "Answer 1: The text mentions other applications of GANs in image synthesis such as generating images of faces based on facial attributes, generating text descriptions from images with great accuracy, and a color-regularization term that resulted in significantly better results for unconditional image generation.\n",
      "Question : for the text It is worth noting that although this survey mainly focuses on text-to-image synthesis, there have been other applications of GANs in broader image synthesis field that we found fascinating and worth dedicating a small section to. For example, BIBREF72 used Sem-Latent GANs to generate images of faces based on facial attributes, producing impressive results that, at a glance, could be mistaken for real faces. BIBREF82, BIBREF70, and BIBREF83 demonstrated great success in generating text descriptions from images (image captioning) with great accuracy, with BIBREF82 using an attention-based model that automatically learns to focus on salient objects and BIBREF83 using deep visual-semantic alignments. Finally, there is a contribution made by StackGAN++ that was not mentioned in the dedicated section due to its relation to unconditional image generation as opposed to conditional, namely a color-regularization term BIBREF47. This additional term aims to keep the samples generated from the same input at different stages more consistent in color, which resulted in significantly better results for the unconditional model. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "16\n",
      "What is text-to-image synthesis and what are its potential applications in industries?\n",
      "Text-to-image synthesis is an application of computer vision AI that uses deep convolutional networks and GANs to translate sequential text descriptions into images. Its potential applications include the medical, government, military, entertainment, and online social media fields, among others.\n",
      "Question : for the text Computer vision applications have strong potential for industries including but not limited to the medical, government, military, entertainment, and online social media fields BIBREF7, BIBREF66, BIBREF67, BIBREF68, BIBREF69, BIBREF70. Text-to-image synthesis is one such application in computer vision AI that has become the main focus in recent years due to its potential for providing beneficial properties and opportunities for a wide range of applicable areas..Text-to-image synthesis is an application byproduct of deep convolutional decoder networks in combination with GANs BIBREF7, BIBREF8, BIBREF10. Deep convolutional networks have contributed to several breakthroughs in image, video, speech, and audio processing. This learning method intends, among other possibilities, to help translate sequential text descriptions to images supplemented by one or many additional methods. Algorithms and methods developed in the computer vision field have allowed researchers in recent years to create realistic images from plain sentences. Advances in the computer vision, deep convolutional nets, and semantic units have shined light and redirected focus to this research area of text-to-image synthesis, having as its prime directive: to aid in the generation of compelling images with as much fidelity to text descriptions as possible..To date, models for generating synthetic images from textual natural language in research laboratories at universities and private companies have yielded compelling images of flowers and birds BIBREF8. Though flowers and birds are the most common objects studied thus far, research has been applied to other classes as well. For example, there have been studies focused solely on human faces BIBREF7, BIBREF8, BIBREF71, BIBREF72..It’s a fascinating time for computer vision AI and deep learning researchers and enthusiasts. The consistent advancement in hardware, software, and contemporaneous development of computer vision AI research disrupts multiple industries. These advances in technology allow for the extraction of several data types from a variety of sources. For example, image data captured from a variety of photo-ready devices, such as smart-phones, and online social media services opened the door to the analysis of large amounts of media datasets BIBREF70. The availability of large media datasets allow new frameworks and algorithms to be proposed and tested on real-world data. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "17\n",
      "Question 1: What are the main datasets commonly used for evaluation of proposed GAN models for text-to-image synthesis?\n",
      "\n",
      "Answer 1: The main datasets commonly used for evaluation of proposed GAN models for text-to-image synthesis are CUB, Oxford, COCO, and CIFAR-10. CUB contains birds with matching text descriptions, Oxford contains flowers with matching text descriptions, COCO is much more complex and contains 91 different object types in 328k images, and CIFAR-10 consists of 10 classes of 60000 32$times$32 colour images.\n",
      "Question : for the text A summary of some reviewed methods and benchmark datasets used for validation is reported in Table TABREF43. In addition, the performance of different GANs with respect to the benchmark datasets and performance metrics is reported in Table TABREF48..In order to synthesize images from text descriptions, many frameworks have taken a minimalistic approach by creating small and background-less images BIBREF73. In most cases, the experiments were conducted on simple datasets, initially containing images of birds and flowers. BIBREF8 contributed to these data sets by adding corresponding natural language text descriptions to subsets of the CUB, MSCOCO, and Oxford-102 datasets, which facilitated the work on text-to-image synthesis for several papers released more recently..While most deep learning algorithms use MNIST BIBREF74 dataset as the benchmark, there are three main datasets that are commonly used for evaluation of proposed GAN models for text-to-image synthesis: CUB BIBREF75, Oxford BIBREF76, COCO BIBREF77, and CIFAR-10 BIBREF78. CUB BIBREF75 contains 200 birds with matching text descriptions and Oxford BIBREF76 contains 102 categories of flowers with 40-258 images each and matching text descriptions. These datasets contain individual objects, with the text description corresponding to that object, making them relatively simple. COCO BIBREF77 is much more complex, containing 328k images with 91 different object types. CIFAI-10 BIBREF78 dataset consists of 60000 32$times$32 colour images in 10 classes, with 6000 images per class. In contrast to CUB and Oxford, whose images each contain an individual object, COCO’s images may contain multiple objects, each with a label, so there are many labels per image. The total number of labels over the 328k images is 2.5 million BIBREF77. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "18\n",
      "Question 1: What does a high Frechet Inception Distance (FID) score indicate in image generation with text-to-image GANs?\n",
      "Answer 1: A high FID score indicates that there is little relationship between the statistics of the synthetic and real images, and lower FIDs are considered better in text-to-image GANs.\n",
      "Question : for the text Several evaluation metrics are used for judging the images produced by text-to-image GANs. Proposed by BIBREF25, Inception Scores (IS) calculates the entropy (randomness) of the conditional distribution, obtained by applying the Inception Model introduced in BIBREF79, and marginal distribution of a large set of generated images, which should be low and high, respectively, for meaningful images. Low entropy of conditional distribution means that the evaluator is confident that the images came from the data distribution, and high entropy of the marginal distribution means that the set of generated images is diverse, which are both desired features. The IS score is then computed as the KL-divergence between the two entropies. FCN-scores BIBREF2 are computed in a similar manner, relying on the intuition that realistic images generated by a GAN should be able to be classified correctly by a classifier trained on real images of the same distribution. Therefore, if the FCN classifier classifies a set of synthetic images accurately, the image is probably realistic, and the corresponding GAN gets a high FCN score. Frechet Inception Distance (FID) BIBREF80 is the other commonly used evaluation metric, and takes a different approach, actually comparing the generated images to real images in the distribution. A high FID means there is little relationship between statistics of the synthetic and real images and vice versa, so lower FIDs are better..black The performance of different GANs with respect to the benchmark datasets and performance metrics is reported in Table TABREF48. In addition, Figure FIGREF49 further lists the performance of 14 GANs with respect to their Inception Scores (IS). generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "19\n",
      "Question 1: What are some potential applications of automatic text-to-image synthesis technology?\n",
      "\n",
      "Answer 1: This technology could be used in various areas such as computer-aided design, image editing, game engines for video game development, and even pictorial art generation.\n",
      "Question : for the text “ (GANs), and the variations that are now being proposed is the most interesting idea in the last 10 years in ML, in my opinion.” (2016).– Yann LeCun.A picture is worth a thousand words! While written text provide efficient, effective, and concise ways for communication, visual content, such as images, is a more comprehensive, accurate, and intelligible method of information sharing and understanding. Generation of images from text descriptions, i.e. text-to-image synthesis, is a complex computer vision and machine learning problem that has seen great progress over recent years. Automatic image generation from natural language may allow users to describe visual elements through visually-rich text descriptions. The ability to do so effectively is highly desirable as it could be used in artificial intelligence applications such as computer-aided design, image editing BIBREF0, BIBREF1, game engines for the development of the next generation of video gamesBIBREF2, and pictorial art generation BIBREF3. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "20\n",
      "Question 1: What are the main drivers used in generative model based text-to-image synthesis?\n",
      "\n",
      "Answer 1: The main drivers used in generative model based text-to-image synthesis are generative adversarial networks (GANs) and deep convolutional decoder networks. GANs consist of two neural networks paired with a discriminator and a generator that compete with each other to produce synthetic/fake samples that can fool the discriminator. The process can be further customized using text descriptions to specify the types of images to generate.\n",
      "Question : for the text Although generative model based text-to-image synthesis provides much more realistic image synthesis results, the image generation is still conditioned by the limited attributes. In recent years, several papers have been published on the subject of text-to-image synthesis. Most of the contributions from these papers rely on multimodal learning approaches that include generative adversarial networks and deep convolutional decoder networks as their main drivers to generate entrancing images from text BIBREF7, BIBREF8, BIBREF9, BIBREF10, BIBREF11..First introduced by Ian Goodfellow et al. BIBREF9, generative adversarial networks (GANs) consist of two neural networks paired with a discriminator and a generator. These two models compete with one another, with the generator attempting to produce synthetic/fake samples that will fool the discriminator and the discriminator attempting to differentiate between real (genuine) and synthetic samples. Because GANs' adversarial training aims to cause generators to produce images similar to the real (training) images, GANs can naturally be used to generate synthetic images (image synthesis), and this process can even be customized further by using text descriptions to specify the types of images to generate, as shown in Figure FIGREF6..Much like text-to-speech and speech-to-text conversion, there exists a wide variety of problems that text-to-image synthesis could solve in the computer vision field specifically BIBREF8, BIBREF12. Nowadays, researchers are attempting to solve a plethora of computer vision problems with the aid of deep convolutional networks, generative adversarial networks, and a combination of multiple methods, often called multimodal learning methods BIBREF8. For simplicity, multiple learning methods will be referred to as multimodal learning hereafter BIBREF13. Researchers often describe multimodal learning as a method that incorporates characteristics from several methods, algorithms, and ideas. This can include ideas from two or more learning approaches in order to create a robust implementation to solve an uncommon problem or improve a solution BIBREF8, BIBREF14, BIBREF15, BIBREF16, BIBREF17..black In this survey, we focus primarily on reviewing recent works that aim to solve the challenge of text-to-image synthesis using generative adversarial networks (GANs). In order to provide a clear roadmap, we propose a taxonomy to summarize reviewed GANs into four major categories. Our review will elaborate the motivations of methods in each category, analyze typical models, their network architectures, and possible drawbacks for further improvement. The visual abstract of the survey and the list of reviewed GAN frameworks is shown in Figure FIGREF8..black The remainder of the survey is organized as follows. Section 2 presents a brief summary of existing works on subjects similar to that of this paper and highlights the key distinctions making ours unique. Section 3 gives a short introduction to GANs and some preliminary concepts related to image generation, as they are the engines that make text-to-image synthesis possible and are essential building blocks to achieve photo-realistic images from text descriptions. Section 4 proposes a taxonomy to summarize GAN based text-to-image synthesis, discusses models and architectures of novel works focused solely on text-to-image synthesis. This section will also draw key contributions from these works in relation to their applications. Section 5 reviews GAN based text-to-image synthesis benchmarks, performance metrics, and comparisons, including a simple review of GANs for other applications. In section 6, we conclude with a brief summary and outline ideas for future interesting developments in the field of text-to-image synthesis. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "21\n",
      "Question 1: What is the major limitation of traditional learning based text-to-image synthesis approaches?\n",
      "\n",
      "Answer 1: The major limitation of traditional learning based text-to-image synthesis approaches is that they lack the ability to generate new image content; they can only change the characteristics of the given/training images.\n",
      "Question : for the text In the early stages of research, text-to-image synthesis was mainly carried out through a search and supervised learning combined process BIBREF4, as shown in Figure FIGREF4. In order to connect text descriptions to images, one could use correlation between keywords (or keyphrase) & images that identifies informative and “picturable” text units; then, these units would search for the most likely image parts conditioned on the text, eventually optimizing the picture layout conditioned on both the text and the image parts. Such methods often integrated multiple artificial intelligence key components, including natural language processing, computer vision, computer graphics, and machine learning..The major limitation of the traditional learning based text-to-image synthesis approaches is that they lack the ability to generate new image content; they can only change the characteristics of the given/training images. Alternatively, research in generative models has advanced significantly and delivers solutions to learn from training images and produce new visual content. For example, Attribute2Image BIBREF5 models each image as a composite of foreground and background. In addition, a layered generative model with disentangled latent variables is learned, using a variational auto-encoder, to generate visual content. Because the learning is customized/conditioned by given attributes, the generative models of Attribute2Image can generate images with respect to different attributes, such as gender, hair color, age, etc., as shown in Figure FIGREF5. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "22\n",
      "Question 1: What is a cGAN and how is it related to GANs and text-to-image synthesis?\n",
      "\n",
      "Answer 1: A cGAN, or conditional GAN, is a variant of the popular Generative Adversarial Network (GAN) model that allows for the generation of images with specific characteristics based on input conditions. In the context of text-to-image synthesis, cGANs serve as a building block for many models that aim to generate images from textual descriptions.\n",
      "Question : for the text In this section, we first introduce preliminary knowledge of GANs and one of its commonly used variants, conditional GAN (i.e. cGAN), which is the building block for many GAN based text-to-image synthesis models. After that, we briefly separate GAN based text-to-image synthesis into two types, Simple GAN frameworks vs. Advanced GAN frameworks, and discuss why advanced GAN architecture for image synthesis..black Notice that the simple vs. advanced GAN framework separation is rather too brief, our taxonomy in the next section will propose a taxonomy to summarize advanced GAN frameworks into four categories, based on their objective and designs. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "23\n",
      "Question 1: What is the recent publication that proposes using discriminator to measure semantic relevance between image and text?\n",
      "\n",
      "Answer 1: The recent publication that proposes using discriminator to measure semantic relevance between image and text is cited as BIBREF38.\n",
      "Question : for the text Motivated by the GAN and conditional GAN (cGAN) design, many GAN based frameworks have been proposed to generate images, with different designs and architectures, such as using multiple discriminators, using progressively trained discriminators, or using hierarchical discriminators. Figure FIGREF17 outlines several advanced GAN frameworks in the literature. In addition to these frameworks, many news designs are being proposed to advance the field with rather sophisticated designs. For example, a recent work BIBREF37 proposes to use a pyramid generator and three independent discriminators, blackeach focusing on a different aspect of the images, to lead the generator towards creating images that are photo-realistic on multiple levels. Another recent publication BIBREF38 proposes to use discriminator to measure semantic relevance between image and text instead of class prediction (like most discriminator in GANs does), resulting a new GAN structure outperforming text conditioned auxiliary classifier (TAC-GAN) BIBREF16 and generating diverse, realistic, and relevant to the input text regardless of class..black In the following section, we will first propose a taxonomy that summarizes advanced GAN frameworks for text-to-image synthesis, and review most recent proposed solutions to the challenge of generating photo-realistic images conditioned on natural language text descriptions using GANs. The solutions we discuss are selected based on relevance and quality of contributions. Many publications exist on the subject of image-generation using GANs, but in this paper we focus specifically on models for text-to-image synthesis, with the review emphasizing on the “model” and “contributions” for text-to-image synthesis. At the end of this section, we also briefly review methods using GANs for other image-synthesis applications..black generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "24\n",
      "What is the goal of the discriminator in GANs?\n",
      "\n",
      "The goal of the discriminator in GANs is to distinguish between samples generated by the generator and samples from the true data distribution by calculating the probability of the sample coming from either source. It is learned to differentiate between a genuine/true image (labeled as 1) and a fake image (labeled as 0).\n",
      "Question : for the text Before moving on to a discussion and analysis of works applying GANs for text-to-image synthesis, there are some preliminary concepts, enhancements of GANs, datasets, and evaluation metrics that are present in some of the works described in the next section and are thus worth introducing..As stated previously, GANs were introduced by Ian Goodfellow et al. BIBREF9 in 2014, and consist of two deep neural networks, a generator and a discriminator, which are trained independently with conflicting goals: The generator aims to generate samples closely related to the original data distribution and fool the discriminator, while the discriminator aims to distinguish between samples from the generator model and samples from the true data distribution by calculating the probability of the sample coming from either source. A conceptual view of the generative adversarial network (GAN) architecture is shown in Figure FIGREF11..The training of GANs is an iterative process that, with each iteration, updates the generator and the discriminator with the goal of each defeating the other. leading each model to become increasingly adept at its specific task until a threshold is reached. This is analogous to a min-max game between the two models, according to the following equation:.In Eq. (DISPLAY_FORM10), $x$ denotes a multi-dimensional sample, e.g., an image, and $z$ denotes a multi-dimensional latent space vector, e.g., a multidimensional data point following a predefined distribution function such as that of normal distributions. $D_{\\theta _d}()$ denotes a discriminator function, controlled by parameters $\\theta _d$, which aims to classify a sample into a binary space. $G_{\\theta _g}()$ denotes a generator function, controlled by parameters $\\theta _g$, which aims to generate a sample from some latent space vector. For example, $G_{\\theta _g}(z)$ means using a latent vector $z$ to generate a synthetic/fake image, and $D_{\\theta _d}(x)$ means to classify an image $x$ as binary output (i.e. true/false or 1/0). In the GAN setting, the discriminator $D_{\\theta _d}()$ is learned to distinguish a genuine/true image (labeled as 1) from fake images (labeled as 0). Therefore, given a true image $x$, the ideal output from the discriminator $D_{\\theta _d}(x)$ would be 1. Given a fake image generated from the generator $G_{\\theta _g}(z)$, the ideal prediction from the discriminator $D_{\\theta _d}(G_{\\theta _g}(z))$ would be 0, indicating the sample is a fake image..Following the above definition, the $\\min \\max $ objective function in Eq. (DISPLAY_FORM10) aims to learn parameters for the discriminator ($\\theta _d$) and generator ($\\theta _g$) to reach an optimization goal: The discriminator intends to differentiate true vs. fake images with maximum capability $\\max _{\\theta _d}$ whereas the generator intends to minimize the difference between a fake image vs. a true image $\\min _{\\theta _g}$. In other words, the discriminator sets the characteristics and the generator produces elements, often images, iteratively until it meets the attributes set forth by the discriminator. GANs are often used with images and other visual elements and are notoriously efficient in generating compelling and convincing photorealistic images. Most recently, GANs were used to generate an original painting in an unsupervised fashion BIBREF24. The following sections go into further detail regarding how the generator and discriminator are trained in GANs..Generator - In image synthesis, the generator network can be thought of as a mapping from one representation space (latent space) to another (actual data) BIBREF21. When it comes to image synthesis, all of the images in the data space fall into some distribution in a very complex and high-dimensional feature space. Sampling from such a complex space is very difficult, so GANs instead train a generator to create synthetic images from a much more simple feature space (usually random noise) called the latent space. The generator network performs up-sampling of the latent space and is usually a deep neural network consisting of several convolutional and/or fully connected layers BIBREF21. The generator is trained using gradient descent to update the weights of the generator network with the aim of producing data (in our case, images) that the discriminator classifies as real..Discriminator - The discriminator network can be thought of as a mapping from image data to the probability of the image coming from the real data space, and is also generally a deep neural network consisting of several convolution and/or fully connected layers. However, the discriminator performs down-sampling as opposed to up-sampling. Like the generator, it is trained using gradient descent but its goal is to update the weights so that it is more likely to correctly classify images as real or fake..In GANs, the ideal outcome is for both the generator's and discriminator's cost functions to converge so that the generator produces photo-realistic images that are indistinguishable from real data, and the discriminator at the same time becomes an expert at differentiating between real and synthetic data. This, however, is not possible since a reduction in cost of one model generally leads to an increase in cost of the other. This phenomenon makes training GANs very difficult, and training them simultaneously (both models performing gradient descent in parallel) often leads to a stable orbit where neither model is able to converge. To combat this, the generator and discriminator are often trained independently. In this case, the GAN remains the same, but there are different training stages. In one stage, the weights of the generator are kept constant and gradient descent updates the weights of the discriminator, and in the other stage the weights of the discriminator are kept constant while gradient descent updates the weights of the generator. This is repeated for some number of epochs until a desired low cost for each model is reached BIBREF25. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "25\n",
      "Question 1: What is the disadvantage of using cGAN for text-to-image synthesis?\n",
      "Answer 1: The disadvantage of using cGAN for text-to-image synthesis is that it cannot handle complicated textual descriptions for image generation, because cGAN uses labels as conditions to restrict the GAN inputs. If the text inputs have multiple keywords (or long text descriptions) they cannot be used simultaneously to restrict the input.\n",
      "Question : for the text In order to generate images from text, one simple solution is to employ the conditional GAN (cGAN) designs and add conditions to the training samples, such that the GAN is trained with respect to the underlying conditions. Several pioneer works have followed similar designs for text-to-image synthesis..black An essential disadvantage of using cGAN for text-to-image synthesis is that that it cannot handle complicated textual descriptions for image generation, because cGAN uses labels as conditions to restrict the GAN inputs. If the text inputs have multiple keywords (or long text descriptions) they cannot be used simultaneously to restrict the input. Instead of using text as conditions, another two approaches BIBREF8, BIBREF16 use text as input features, and concatenate such features with other features to train discriminator and generator, as shown in Figure FIGREF15(b) and (c). To ensure text being used as GAN input, a feature embedding or feature representation learning BIBREF29, BIBREF30 function $\\varphi ()$ is often introduced to convert input text as numeric features, which are further concatenated with other features to train GANs..black generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "26\n",
      "What is the main technical innovation of cGAN?\n",
      "\n",
      "Answer 1: The main technical innovation of cGAN is that it introduces an additional input or inputs to the original GAN model, allowing the model to be trained on information such as class labels or other conditioning variables as well as the samples themselves, concurrently. This enables directing the model to generate more tailored outputs than the original GAN model.\n",
      "Question : for the text Conditional Generative Adversarial Networks (cGAN) are an enhancement of GANs proposed by BIBREF26 shortly after the introduction of GANs by BIBREF9. The objective function of the cGAN is defined in Eq. (DISPLAY_FORM13) which is very similar to the GAN objective function in Eq. (DISPLAY_FORM10) except that the inputs to both discriminator and generator are conditioned by a class label $y$..The main technical innovation of cGAN is that it introduces an additional input or inputs to the original GAN model, allowing the model to be trained on information such as class labels or other conditioning variables as well as the samples themselves, concurrently. Whereas the original GAN was trained only with samples from the data distribution, resulting in the generated sample reflecting the general data distribution, cGAN enables directing the model to generate more tailored outputs..In Figure FIGREF14, the condition vector is the class label (text string) \"Red bird\", which is fed to both the generator and discriminator. It is important, however, that the condition vector is related to the real data. If the model in Figure FIGREF14 was trained with the same set of real data (red birds) but the condition text was \"Yellow fish\", the generator would learn to create images of red birds when conditioned with the text \"Yellow fish\"..Note that the condition vector in cGAN can come in many forms, such as texts, not just limited to the class label. Such a unique design provides a direct solution to generate images conditioned by predefined specifications. As a result, cGAN has been used in text-to-image synthesis since the very first day of its invention although modern approaches can deliver much better text-to-image synthesis results..black generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "27\n",
      "What is the focus of this survey paper and how does it differ from previous surveys on GANs?\n",
      "\n",
      "The focus of this survey paper is specifically on text-to-image synthesis using GANs. It differs from previous surveys on GANs, such as BIBREF6 and BIBREF7, by going into more detail regarding the contributions of state-of-the-art models and outlining their models and contributions. Previous surveys have had a broader scope, covering a range of applications for image synthesis using GANs.\n",
      "Question : for the text With the growth and success of GANs, deep convolutional decoder networks, and multimodal learning methods, these techniques were some of the first procedures which aimed to solve the challenge of image synthesis. Many engineers and scientists in computer vision and AI have contributed through extensive studies and experiments, with numerous proposals and publications detailing their contributions. Because GANs, introduced by BIBREF9, are emerging research topics, their practical applications to image synthesis are still in their infancy. Recently, many new GAN architectures and designs have been proposed to use GANs for different applications, e.g. using GANs to generate sentimental texts BIBREF18, or using GANs to transform natural images into cartoons BIBREF19..Although GANs are becoming increasingly popular, very few survey papers currently exist to summarize and outline contemporaneous technical innovations and contributions of different GAN architectures BIBREF20, BIBREF21. Survey papers specifically attuned to analyzing different contributions to text-to-image synthesis using GANs are even more scarce. We have thus found two surveys BIBREF6, BIBREF7 on image synthesis using GANs, which are the two most closely related publications to our survey objective. In the following paragraphs, we briefly summarize each of these surveys and point out how our objectives differ from theirs..In BIBREF6, the authors provide an overview of image synthesis using GANs. In this survey, the authors discuss the motivations for research on image synthesis and introduce some background information on the history of GANs, including a section dedicated to core concepts of GANs, namely generators, discriminators, and the min-max game analogy, and some enhancements to the original GAN model, such as conditional GANs, addition of variational auto-encoders, etc.. In this survey, we will carry out a similar review of the background knowledge because the understanding of these preliminary concepts is paramount for the rest of the paper. Three types of approaches for image generation are reviewed, including direct methods (single generator and discriminator), hierarchical methods (two or more generator-discriminator pairs, each with a different goal), and iterative methods (each generator-discriminator pair generates a gradually higher-resolution image). Following the introduction, BIBREF6 discusses methods for text-to-image and image-to-image synthesis, respectively, and also describes several evaluation metrics for synthetic images, including inception scores and Frechet Inception Distance (FID), and explains the significance of the discriminators acting as learned loss functions as opposed to fixed loss functions..Different from the above survey, which has a relatively broad scope in GANs, our objective is heavily focused on text-to-image synthesis. Although this topic, text-to-image synthesis, has indeed been covered in BIBREF6, they did so in a much less detailed fashion, mostly listing the many different works in a time-sequential order. In comparison, we will review several representative methods in the field and outline their models and contributions in detail..Similarly to BIBREF6, the second survey paper BIBREF7 begins with a standard introduction addressing the motivation of image synthesis and the challenges it presents followed by a section dedicated to core concepts of GANs and enhancements to the original GAN model. In addition, the paper covers the review of two types of applications: (1) unconstrained applications of image synthesis such as super-resolution, image inpainting, etc., and (2) constrained image synthesis applications, namely image-to-image, text-to-image, and sketch-to image, and also discusses image and video editing using GANs. Again, the scope of this paper is intrinsically comprehensive, while we focus specifically on text-to-image and go into more detail regarding the contributions of novel state-of-the-art models..Other surveys have been published on related matters, mainly related to the advancements and applications of GANs BIBREF22, BIBREF23, but we have not found any prior works which focus specifically on text-to-image synthesis using GANs. To our knowledge, this is the first paper to do so..black generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "28\n",
      "Question 1: What are the four categories in the proposed taxonomy for advanced GAN based text-to-image synthesis frameworks?\n",
      "\n",
      "Answer 1: The four categories in the proposed taxonomy for advanced GAN based text-to-image synthesis frameworks are Semantic Enhancement GANs, Resolution Enhancement GANs, Diversity Enhancement GANs, and Motion Enhancement GANs.\n",
      "Question : for the text In this section, we propose a taxonomy to summarize advanced GAN based text-to-image synthesis frameworks, as shown in Figure FIGREF24. The taxonomy organizes GAN frameworks into four categories, including Semantic Enhancement GANs, Resolution Enhancement GANs, Diversity Enhancement GANs, and Motion Enhancement GAGs. Following the proposed taxonomy, each subsection will introduce several typical frameworks and address their techniques of using GANS to solve certain aspects of the text-to-mage synthesis challenges..black generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "29\n",
      "Question 1: What is the main goal of text-to-image synthesis methods discussed in this subsection?\n",
      "\n",
      "Answer 1: The main goal of text-to-image synthesis methods discussed in this subsection is to maximize the diversity of the output images based on the text descriptions.\n",
      "Question : for the text In this subsection, we introduce text-to-image synthesis methods which try to maximize the diversity of the output images, based on the text descriptions..black generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "30\n",
      "Question 1: What is the difference between cGAN and AC-GAN's approach to addressing the diversity problem in traditional GANs?\n",
      "\n",
      "Answer 1: While both cGAN and AC-GAN use additional information to generate images with respect to text descriptions, AC-GAN stands out from cGAN by using an auxiliary classifier layer to predict the class of the image. This allows AC-GAN to ensure that the output consists of images from different classes, resulting in more diversified synthesis images.\n",
      "Question : for the text Two issues arise in the traditional GANs BIBREF58 for image synthesis: (1) scalabilirty problem: traditional GANs cannot predict a large number of image categories; and (2) diversity problem: images are often subject to one-to-many mapping, so one image could be labeled as different tags or being described using different texts. To address these problems, GAN conditioned on additional information, e.g. cGAN, is an alternative solution. However, although cGAN and many previously introduced approaches are able to generate images with respect to the text descriptions, they often output images with similar types and visual appearance..black Slightly different from the cGAN, auxiliary classifier GANs (AC-GAN) BIBREF27 proposes to improve the diversity of output images by using an auxiliary classifier to control output images. The overall structure of AC-GAN is shown in Fig. FIGREF15(c). In AC-GAN, every generated image is associated with a class label, in addition to the true/fake label which are commonly used in GAN or cGAN. The discriminator of AC-GAN not only outputs a probability distribution over sources (i.e. whether the image is true or fake), it also output a probability distribution over the class label (i.e. predict which class the image belong to)..black By using an auxiliary classifier layer to predict the class of the image, AC-GAN is able to use the predicted class labels of the images to ensure that the output consists of images from different classes, resulting in diversified synthesis images. The results show that AC-GAN can generate images with high diversity..black generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "31\n",
      "Question 1: How does MirrorGAN ensure semantic consistency and diversity between text and image?\n",
      "\n",
      "Answer 1: MirrorGAN employs a mirror structure that includes a semantic text embedding module, a global-local collaborative attentive module for cascaded image generation, and a semantic text regeneration and alignment module. It uses a back to back Text-to-Image (T2I) and Image-to-Text (I2T) process to ensure generated images are consistent with the input texts. Additionally, to enhance diversity, Scene Graph GAN uses visual scene graphs to describe the layout of the objects, allowing specific relationships between objects in the images.\n",
      "Question : for the text Due to the inherent complexity of the visual images, and the diversity of text descriptions (i.e. same words could imply different meanings), it is difficulty to precisely match the texts to the visual images at the semantic levels. For most methods we have discussed so far, they employ a direct text to image generation process, but there is no validation about how generated images comply with the text in a reverse fashion..black To ensure the semantic consistency and diversity, MirrorGAN BIBREF60 employs a mirror structure, which reversely learns from generated images to output texts (an image-to-text process) to further validate whether generated are indeed consistent to the input texts. MirrowGAN includes three modules: a semantic text embedding module (STEM), a global-local collaborative attentive module for cascaded image generation (GLAM), and a semantic text regeneration and alignment module (STREAM). The back to back Text-to-Image (T2I) and Image-to-Text (I2T) are combined to progressively enhance the diversity and semantic consistency of the generated images..black In order to enhance the diversity of the output image, Scene Graph GAN BIBREF61 proposes to use visual scene graphs to describe the layout of the objects, allowing users to precisely specific the relationships between objects in the images. In order to convert the visual scene graph as input for GAN to generate images, this method uses graph convolution to process input graphs. It computes a scene layout by predicting bounding boxes and segmentation masks for objects. After that, it converts the computed layout to an image with a cascaded reﬁnement network..black generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "32\n",
      "Question 1: What is the major difference between TAC-GAN and AC-GAN?\n",
      "\n",
      "Answer 1: The major difference between TAC-GAN and AC-GAN is that TAC-GAN conditions the generated images on textual descriptions instead of class labels.\n",
      "Question : for the text Building on the AC-GAN, TAC-GAN BIBREF59 is proposed to replace the class information with textual descriptions as the input to perform the task of text to image synthesis. The architecture of TAC-GAN is shown in Fig. FIGREF15(d), which is similar to AC-GAN. Overall, the major difference between TAC-GAN and AC-GAN is that TAC-GAN conditions the generated images on text descriptions instead of on a class label. This design makes TAC-GAN more generic for image synthesis..black For TAC-GAN, it imposes restrictions on generated images in both texts and class labels. The input vector of TAC-GAN's generative network is built based on a noise vector and embedded vector representation of textual descriptions. The discriminator of TAC-GAN is similar to that of the AC-GAN, which not only predicts whether the image is fake or not, but also predicts the label of the images. A minor difference of TAC-GAN's discriminator, compared to that of the AC-GAN, is that it also receives text information as input before performing its classification..black The experiments and validations, on the Oxford-102 flowers dataset, show that the results produced by TAC-GAN are “slightly better” that other approaches, including GAN-INT-CLS and StackGAN..black generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "33\n",
      "Question 1: How does Text-SeGAN differ from AC-GAN and TAC-GAN in predicting image labels?\n",
      "\n",
      "Answer 1: Text-SeGAN adds a regression layer to estimate semantic relevance between the image and text, instead of predicting class labels like AC-GAN and TAC-GAN, which can be inherently restrictive in describing image semantics. The estimated relevance is a fractional value ranging between 0 and 1, with a higher value reflecting better semantic relevance between the image and text.\n",
      "Question : for the text In order to improve the diversity of the output images, both AC-GAN and TAC-GAN's discriminators predict class labels of the synthesised images. This process likely enforces the semantic diversity of the images, but class labels are inherently restrictive in describing image semantics, and images described by text can be matched to multiple labels. Therefore, instead of predicting images' class labels, an alternative solution is to directly quantify their semantic relevance..black The architecture of Text-SeGAN is shown in Fig. FIGREF15(e). In order to directly quantify semantic relevance, Text-SeGAN BIBREF28 adds a regression layer to estimate the semantic relevance between the image and text instead of a classifier layer of predicting labels. The estimated semantic reference is a fractional value ranging between 0 and 1, with a higher value reflecting better semantic relevance between the image and text. Due to this unique design, an inherent advantage of Text-SeGAN is that the generated images are not limited to certain classes and are semantically matching to the text input..black Experiments and validations, on Oxford-102 flower dataset, show that Text-SeGAN can generate diverse images that are semantically relevant to the input text. In addition, the results of Text-SeGAN show improved inception score compared to other approaches, including GAN-INT-CLS, StackGAN, TAC-GAN, and HDGAN..black generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "34\n",
      "Question 1: What is the main focus of Diversity Enhancement GANs in text-to-image synthesis?\n",
      "\n",
      "Answer 1: Diversity Enhancement GANs aim to generate output images that are not only semantically related to the text input but also have different types and visual appearance, thus diversifying the output. This is achieved through an additional component that estimates the semantic relevance between the generated images and texts to maximize output diversity.\n",
      "Question : for the text Although the ultimate goal of Text-to-Image synthesis is to generate images closely related to the textual descriptions, the relevance of the images to the texts are often validated from different perspectives, due to the inherent diversity of human perceptions. For example, when generating images matching to the description “rose flowers”, some users many know the exact type of flowers they like and intend to generate rose flowers with similar colors. Other users, may seek to generate high quality rose flowers with a nice background (e.g. garden). The third group of users may be more interested in generating flowers similar to rose but with different colors and visual appearance, e.g. roses, begonia, and peony. The fourth group of users may want to not only generate flower images, but also use them to form a meaningful action, e.g. a video clip showing flower growth, performing a magic show using those flowers, or telling a love story using the flowers..blackFrom the text-to-Image synthesis point of view, the first group of users intend to precisely control the semantic of the generated images, and their goal is to match the texts and images at the semantic level. The second group of users are more focused on the resolutions and the qualify of the images, in addition to the requirement that the images and texts are semantically related. For the third group of users, their goal is to diversify the output images, such that their images carry diversified visual appearances and are also semantically related. The fourth user group adds a new dimension in image synthesis, and aims to generate sequences of images which are coherent in temporal order, i.e. capture the motion information..black Based on the above descriptions, we categorize GAN based Text-to-Image Synthesis into a taxonomy with four major categories, as shown in Fig. FIGREF24..Semantic Enhancement GANs: Semantic enhancement GANs represent pioneer works of GAN frameworks for text-to-image synthesis. The main focus of the GAN frameworks is to ensure that the generated images are semantically related to the input texts. This objective is mainly achieved by using a neural network to encode texts as dense features, which are further fed to a second network to generate images matching to the texts..Resolution Enhancement GANs: Resolution enhancement GANs mainly focus on generating high qualify images which are semantically matched to the texts. This is mainly achieved through a multi-stage GAN framework, where the outputs from earlier stage GANs are fed to the second (or later) stage GAN to generate better qualify images..Diversity Enhancement GANs: Diversity enhancement GANs intend to diversify the output images, such that the generated images are not only semantically related but also have different types and visual appearance. This objective is mainly achieved through an additional component to estimate semantic relevance between generated images and texts, in order to maximize the output diversity..Motion Enhancement GANs: Motion enhancement GANs intend to add a temporal dimension to the output images, such that they can form meaningful actions with respect to the text descriptions. This goal mainly achieved though a two-step process which first generates images matching to the “actions” of the texts, followed by a mapping or alignment procedure to ensure that images are coherent in the temporal order..black In the following, we will introduce how these GAN frameworks evolve for text-to-image synthesis, and will also review some typical methods of each category..black generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "35\n",
      "Question 1: What is the goal of generating videos from texts in text-to-image synthesis research?\n",
      "\n",
      "Answer 1: The goal of generating videos from texts is to create useful resources for automated assistance or story telling in addition to images. This approach focuses on creating sequences of images that can be used to convey information and narratives more effectively.\n",
      "Question : for the text Instead of focusing on generating static images, another line of text-to-image synthesis research focuses on generating videos (i.e. sequences of images) from texts. In this context, the synthesised videos are often useful resources for automated assistance or story telling..black generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "36\n",
      "Question 1: What is the purpose of ObamaNet?\n",
      "Answer 1: The purpose of ObamaNet is to generate spoofed speech and lip-sync videos of Barack Obama based on text input.\n",
      "Question : for the text One early/interesting work of motion enhancement GANs is to generate spoofed speech and lip-sync videos (or talking face) of Barack Obama (i.e. ObamaNet) based on text input BIBREF62. This framework is consisted of three parts, i.e. text to speech using “Char2Wav”, mouth shape representation synced to the audio using a time-delayed LSTM and “video generation” conditioned on the mouth shape using “U-Net” architecture. Although the results seem promising, ObamaNet only models the mouth region and the videos are not generated from noise which can be regarded as video prediction other than video generation..black Another meaningful trial of using synthesised videos for automated assistance is to translate spoken language (e.g. text) into sign language video sequences (i.e. T2S) BIBREF63. This is often achieved through a two step process: converting texts as meaningful units to generate images, followed by a learning component to arrange images into sequential order for best representation. More specifically, using RNN based machine translation methods, texts are translated into sign language gloss sequences. Then, glosses are mapped to skeletal pose sequences using a lookup-table. To generate videos, a conditional DCGAN with the input of concatenation of latent representation of the image for a base pose and skeletal pose information is built..black generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "37\n",
      "Question 1: What are the main components of StoryGAN?\n",
      "\n",
      "Answer 1: The main components of StoryGAN are the story encoder, context encoder, and two discriminators (image and story discriminators).\n",
      "Question : for the text Different from T2V which generates videos from a single text, StoryGAN aims to produce dynamic scenes consistent of specified texts (i.e. story written in a multi-sentence paragraph) using a sequential GAN model BIBREF65. Story encoder, context encoder, and discriminators are the main components of this model. By using stochastic sampling, the story encoder intends to learn an low-dimensional embedding vector for the whole story to keep the continuity of the story. The context encoder is proposed to capture contextual information during sequential image generation based on a deep RNN. Two discriminators of StoryGAN are image discriminator which evaluates the generated images and story discriminator which ensures the global consistency..black The experiments and comparisons, on CLEVR dataset and Pororo cartoon dataset which are originally used for visual question answering, show that StoryGAN improves the generated video qualify in terms of Structural Similarity Index (SSIM), visual qualify, consistence, and relevance (the last three measure are based on human evaluation). generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "38\n",
      "Question 1: What are the two types of features used by the T2V model to generate videos from text?\n",
      "\n",
      "Answer 1: The T2V model relies on two types of features, static features called \"gist\" and dynamic features. Static features are used to sketch text-conditioned background color and object layout structure, while dynamic features are considered by transforming input text into an image filter which eventually forms the video generator.\n",
      "Question : for the text In BIBREF64, a text-to-video model (T2V) is proposed based on the cGAN in which the input is the isometric Gaussian noise with the text-gist vector served as the generator. A key component of generating videos from text is to train a conditional generative model to extract both static and dynamic information from text, followed by a hybrid framework combining a Variational Autoencoder (VAE) and a Generative Adversarial Network (GAN)..black More specifically, T2V relies on two types of features, static features and dynamic features, to generate videos. Static features, called “gist” are used to sketch text-conditioned background color and object layout structure. Dynamic features, on the other hand, are considered by transforming input text into an image filter which eventually forms the video generator which consists of three entangled neural networks. The text-gist vector is generated by a gist generator which maintains static information (e.g. background) and a text2filter which captures the dynamic information (i.e. actions) in the text to generate videos..black As demonstrated in the paper BIBREF64, the generated videos are semantically related to the texts, but have a rather low quality (e.g. only $64 \\times 64$ resolution)..black generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "39\n",
      "Question 1: What is the difference between StackGAN and StackGAN++?\n",
      "\n",
      "Answer 1: StackGAN++ uses multi-stage GANs to generate multi-scale images, while StackGAN only has two stages and focuses on refining low-resolution images in the second stage. Additionally, StackGAN++ incorporates a color-consistency regularization term to maintain image consistency across different scales.\n",
      "Question : for the text Due to the fact that training GANs will be much difficult when generating high-resolution images, a two stage GAN (i.e. stackGAN) is proposed in which rough images(i.e. low-resolution images) are generated in stage-I and refined in stage-II. To further improve the quality of generated images, the second version of StackGAN (i.e. Stack++) is proposed to use multi-stage GANs to generate multi-scale images. A color-consistency regularization term is also added into the loss to keep the consistency of images in different scales..black While stackGAN and StackGAN++ are both built on the global sentence vector, AttnGAN is proposed to use attention mechanism (i.e. Deep Attentional Multimodal Similarity Model (DAMSM)) to model the multi-level information (i.e. word level and sentence level) into GANs. In the following, StackGAN, StackGAN++ and AttnGAN will be explained in detail..black Recently, Dynamic Memory Generative Adversarial Network (i.e. DM-GAN)BIBREF53 which uses a dynamic memory component is proposed to focus on refiningthe initial generated image which is the key to the success of generating high quality images. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "40\n",
      "Question 1: What are the two major contributions introduced in AttnGAN?\n",
      "\n",
      "Answer 1: The two major contributions introduced in AttnGAN are the attentional generative network and the Deep Attentional Multimodal Similarity Model (DAMSM). The attentional generative network matches specific regions of each stage's output image to conditioning variables from the word-level text embedding, allowing each consecutive stage to focus on specific regions of the image independently. DAMSM is used after the result of the final stage to calculate the similarity between the generated image and the text embedding at both the sentence level and the more fine-grained word level.\n",
      "Question : for the text Attentional Generative Adversarial Network (AttnGAN) BIBREF10 is very similar, in terms of its structure, to StackGAN++ BIBREF47, discussed in the previous section, but some novel components are added. Like previous works BIBREF56, BIBREF8, BIBREF33, BIBREF47, a text encoder generates a text embedding with conditioning variables based on the overall sentence. Additionally, the text encoder generates a separate text embedding with conditioning variables based on individual words. This process is optimized to produce meaningful variables using a bidirectional recurrent neural network (BRNN), more specifically bidirectional Long Short Term Memory (LSTM) BIBREF57, which, for each word in the description, generates conditions based on the previous word as well as the next word (bidirectional). The first stage of AttnGAN generates a low-resolution image based on the sentence-level text embedding and random noise vector. The output is fed along with the word-level text embedding to an “attention model”, which matches the word-level conditioning variables to regions of the stage I image, producing a word-context matrix. This is then fed to the next stage of the model along with the raw previous stage output. Each consecutive stage works in the same manner, but produces gradually higher-resolution images conditioned on the previous stage..Two major contributions were introduced in AttnGAN: the attentional generative network and the Deep Attentional Multimodal Similarity Model (DAMSM) BIBREF47. The attentional generative network matches specific regions of each stage's output image to conditioning variables from the word-level text embedding. This is a very worthy contribution, allowing each consecutive stage to focus on specific regions of the image independently, adding “attentional” details region by region as opposed to the whole image. The DAMSM is also a key feature introduced by AttnGAN, which is used after the result of the final stage to calculate the similarity between the generated image and the text embedding at both the sentence level and the more fine-grained word level. Table TABREF48 shows scores from different metrics for StackGAN, StackGAN++, AttnGAN, and HDGAN on the CUB, Oxford, and COCO datasets. The table shows that AttnGAN outperforms the other models in terms of IS on the CUB dataset by a small amount and greatly outperforms them on the COCO dataset. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "41\n",
      "What is the main objective of the Hierarchically-nested adversarial network (HDGAN) method?\n",
      "\n",
      "The main objective of the HDGAN method is to tackle the difficult problem of dealing with photographic images from semantic text descriptions. This method introduces adversarial objectives nested inside hierarchically oriented networks and introduces a visual-semantic similarity measure to aid in evaluating the consistency of generated images. The end product is images that are semantically mapped from text-based natural language descriptions to each area on the picture.\n",
      "Question : for the text Hierarchically-nested adversarial network (HDGAN) is a method proposed by BIBREF36, and its main objective is to tackle the difficult problem of dealing with photographic images from semantic text descriptions. These semantic text descriptions are applied on images from diverse datasets. This method introduces adversarial objectives nested inside hierarchically oriented networks BIBREF36. Hierarchical networks helps regularize mid-level manifestations. In addition to regularize mid-level manifestations, it assists the training of the generator in order to capture highly complex still media elements. These elements are captured in statistical order to train the generator based on settings extracted directly from the image. The latter is an ideal scenario. However, this paper aims to incorporate a single-stream architecture. This single-stream architecture functions as the generator that will form an optimum adaptability towards the jointed discriminators. Once jointed discriminators are setup in an optimum manner, the single-stream architecture will then advance generated images to achieve a much higher resolution BIBREF36..The main contributions of the HDGANs include the introduction of a visual-semantic similarity measure BIBREF36. This feature will aid in the evaluation of the consistency of generated images. In addition to checking the consistency of generated images, one of the key objectives of this step is to test the logical consistency of the end product BIBREF36. The end product in this case would be images that are semantically mapped from text-based natural language descriptions to each area on the picture e.g. a wing on a bird or petal on a flower. Deep learning has created a multitude of opportunities and challenges for researchers in the computer vision AI field. Coupled with GAN and multimodal learning architectures, this field has seen tremendous growth BIBREF8, BIBREF39, BIBREF40, BIBREF41, BIBREF42, BIBREF22, BIBREF26. Based on these advancements, HDGANs attempt to further extend some desirable and less common features when generating images from textual natural language BIBREF36. In other words, it takes sentences and treats them as a hierarchical structure. This has some positive and negative implications in most cases. For starters, it makes it more complex to generate compelling images. However, one of the key benefits of this elaborate process is the realism obtained once all processes are completed. In addition, one common feature added to this process is the ability to identify parts of sentences with bounding boxes. If a sentence includes common characteristics of a bird, it will surround the attributes of such bird with bounding boxes. In practice, this should happen if the desired image have other elements such as human faces (e.g. eyes, hair, etc), flowers (e.g. petal size, color, etc), or any other inanimate object (e.g. a table, a mug, etc). Finally, HDGANs evaluated some of its claims on common ideal text-to-image datasets such as CUB, COCO, and Oxford-102 BIBREF8, BIBREF36, BIBREF39, BIBREF40, BIBREF41, BIBREF42, BIBREF22, BIBREF26. These datasets were first utilized on earlier works BIBREF8, and most of them sport modified features such image annotations, labels, or descriptions. The qualitative and quantitative results reported by researchers in this study were far superior of earlier works in this same field of computer vision AI..black generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "42\n",
      "Question 1: What is the major contribution of StackGAN in text-to-image synthesis?\n",
      "\n",
      "Answer 1: One major contribution of StackGAN is the use of cascaded GANs for text-to-image synthesis through a sketch-refinement process. Another significant contribution is Conditioning Augmentation, which introduces small variations to the original text embedding for a particular training image, resulting in more diverse images in the same distribution when compared to a fixed text embedding.\n",
      "Question : for the text In 2017, Zhang et al. proposed a model for generating photo-realistic images from text descriptions called StackGAN (Stacked Generative Adversarial Network) BIBREF33. In their work, they define a two-stage model that uses two cascaded GANs, each corresponding to one of the stages. The stage I GAN takes a text description as input, converts the text description to a text embedding containing several conditioning variables, and generates a low-quality 64x64 image with rough shapes and colors based on the computed conditioning variables. The stage II GAN then takes this low-quality stage I image as well as the same text embedding and uses the conditioning variables to correct and add more detail to the stage I result. The output of stage II is a photorealistic 256$times$256 image that resembles the text description with compelling accuracy..One major contribution of StackGAN is the use of cascaded GANs for text-to-image synthesis through a sketch-refinement process. By conditioning the stage II GAN on the image produced by the stage I GAN and text description, the stage II GAN is able to correct defects in the stage I output, resulting in high-quality 256x256 images. Prior works have utilized “stacked” GANs to separate the image generation process into structure and style BIBREF42, multiple stages each generating lower-level representations from higher-level representations of the previous stage BIBREF35, and multiple stages combined with a laplacian pyramid approach BIBREF54, which was introduced for image compression by P. Burt and E. Adelson in 1983 and uses the differences between consecutive down-samples of an original image to reconstruct the original image from its down-sampled version BIBREF55. However, these works did not use text descriptions to condition their generator models..Conditioning Augmentation is the other major contribution of StackGAN. Prior works transformed the natural language text description into a fixed text embedding containing static conditioning variables which were fed to the generator BIBREF8. StackGAN does this and then creates a Gaussian distribution from the text embedding and randomly selects variables from the Gaussian distribution to add to the set of conditioning variables during training. This encourages robustness by introducing small variations to the original text embedding for a particular training image while keeping the training image that the generated output is compared to the same. The result is that the trained model produces more diverse images in the same distribution when using Conditioning Augmentation than the same model using a fixed text embedding BIBREF33. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "43\n",
      "What is the final resolution of the image produced by StackGAN++?\n",
      "\n",
      "The final stage of StackGAN++ produces a high-quality image with a resolution of 256$\\times$256.\n",
      "Question : for the text Proposed by the same users as StackGAN, StackGAN++ is also a stacked GAN model, but organizes the generators and discriminators in a “tree-like” structure BIBREF47 with multiple stages. The first stage combines a noise vector and conditioning variables (with Conditional Augmentation introduced in BIBREF33) for input to the first generator, which generates a low-resolution image, 64$\\times $64 by default (this can be changed depending on the desired number of stages). Each following stage uses the result from the previous stage and the conditioning variables to produce gradually higher-resolution images. These stages do not use the noise vector again, as the creators assume that the randomness it introduces is already preserved in the output of the first stage. The final stage produces a 256$\\times $256 high-quality image..StackGAN++ introduces the joint conditional and unconditional approximation in their designs BIBREF47. The discriminators are trained to calculate the loss between the image produced by the generator and the conditioning variables (measuring how accurately the image represents the description) as well as the loss between the image and real images (probability of the image being real or fake). The generators then aim to minimize the sum of these losses, improving the final result. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "44\n",
      "What is one of the most important criteria for text-to-image synthesis?\n",
      "One of the most important criteria for text-to-image synthesis is semantic relevance.\n",
      "Question : for the text Semantic relevance is one the of most important criteria of the text-to-image synthesis. For most GNAs discussed in this survey, they are required to generate images semantically related to the text descriptions. However, the semantic relevance is a rather subjective measure, and images are inherently rich in terms of its semantics and interpretations. Therefore, many GANs are further proposed to enhance the text-to-image synthesis from different perspectives. In this subsection, we will review several classical approaches which are commonly served as text-to-image synthesis baseline..black generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "45\n",
      "Question 1: What is the main goal of DC-GAN?\n",
      "\n",
      "Answer 1: The main goal of DC-GAN is to train a deep convolutional generative adversarial network on text features in order to generate realistic images automatically from natural language text.\n",
      "Question : for the text Deep convolution generative adversarial network (DC-GAN) BIBREF8 represents the pioneer work for text-to-image synthesis using GANs. Its main goal is to train a deep convolutional generative adversarial network (DC-GAN) on text features. During this process these text features are encoded by another neural network. This neural network is a hybrid convolutional recurrent network at the character level. Concurrently, both neural networks have also feed-forward inference in the way they condition text features. Generating realistic images automatically from natural language text is the motivation of several of the works proposed in this computer vision field. However, actual artificial intelligence (AI) systems are far from achieving this task BIBREF8, BIBREF39, BIBREF40, BIBREF41, BIBREF42, BIBREF22, BIBREF26. Lately, recurrent neural networks led the way to develop frameworks that learn discriminatively on text features. At the same time, generative adversarial networks (GANs) began recently to show some promise on generating compelling images of a whole host of elements including but not limited to faces, birds, flowers, and non-common images such as room interiorsBIBREF8. DC-GAN is a multimodal learning model that attempts to bridge together both of the above mentioned unsupervised machine learning algorithms, the recurrent neural networks (RNN) and generative adversarial networks (GANs), with the sole purpose of speeding the generation of text-to-image synthesis..black Deep learning shed some light to some of the most sophisticated advances in natural language representation, image synthesis BIBREF7, BIBREF8, BIBREF43, BIBREF35, and classification of generic data BIBREF44. However, a bulk of the latest breakthroughs in deep learning and computer vision were related to supervised learning BIBREF8. Even though natural language and image synthesis were part of several contributions on the supervised side of deep learning, unsupervised learning saw recently a tremendous rise in input from the research community specially on two subproblems: text-based natural language and image synthesis BIBREF45, BIBREF14, BIBREF8, BIBREF46, BIBREF47. These subproblems are typically subdivided as focused research areas. DC-GAN's contributions are mainly driven by these two research areas. In order to generate plausible images from natural language, DC-GAN contributions revolve around developing a straightforward yet effective GAN architecture and training strategy that allows natural text to image synthesis. These contributions are primarily tested on the Caltech-UCSD Birds and Oxford-102 Flowers datasets. Each image in these datasets carry five text descriptions. These text descriptions were created by the research team when setting up the evaluation environment. The DC-GANs model is subsequently trained on several subcategories. Subcategories in this research represent the training and testing sub datasets. The performance shown by these experiments display a promising yet effective way to generate images from textual natural language descriptions BIBREF8..black generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "46\n",
      "What are some of the revised network structures proposed to improve the semantic relevance of images to text in DC-GAN framework?\n",
      "GAN-CLS with image-text matching discriminator, GAN-INT learned with text manifold interpolation and GAN-INT-CLS which combines both are some of the revised network structures proposed to improve the semantic relevance of images to text in DC-GAN framework.\n",
      "Question : for the text Following the pioneer DC-GAN framework BIBREF8, many researches propose revised network structures (e.g. different discriminaotrs) in order to improve images with better semantic relevance to the texts. Based on the deep convolutional adversarial network (DC-GAN) network architecture, GAN-CLS with image-text matching discriminator, GAN-INT learned with text manifold interpolation and GAN-INT-CLS which combines both are proposed to find semantic match between text and image. Similar to the DC-GAN architecture, an adaptive loss function (i.e. Perceptual Loss BIBREF48) is proposed for semantic image synthesis which can synthesize a realistic image that not only matches the target text description but also keep the irrelavant features(e.g. background) from source images BIBREF49. Regarding to the Perceptual Losses, three loss functions (i.e. Pixel reconstruction loss, Activation reconstruction loss and Texture reconstruction loss) are proposed in BIBREF50 in which they construct the network architectures based on the DC-GAN, i.e. GAN-INT-CLS-Pixel, GAN-INT-CLS-VGG and GAN-INT-CLS-Gram with respect to three losses. In BIBREF49, a residual transformation unit is added in the network to retain similar structure of the source image..black Following the BIBREF49 and considering the features in early layers address background while foreground is obtained in latter layers in CNN, a pair of discriminators with different architectures (i.e. Paired-D GAN) is proposed to synthesize background and foreground from a source image seperately BIBREF51. Meanwhile, the skip-connection in the generator is employed to more precisely retain background information in the source image..black generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "47\n",
      "What is the unique feature of MC-GAN in synthesizing images?\n",
      "The unique feature of MC-GAN is the synthesis block which extracts the background feature from the given image without non-linear function and uses the feature map from the previous layer as the foreground feature, allowing proper modeling of both foreground and background components in generating new images.\n",
      "Question : for the text When synthesising images, most text-to-image synthesis methods consider each output image as one single unit to characterize its semantic relevance to the texts. This is likely problematic because most images naturally consist of two crucial components: foreground and background. Without properly separating these two components, it's hard to characterize the semantics of an image if the whole image is treated as a single unit without proper separation..black In order to enhance the semantic relevance of the images, a multi-conditional GAN (MC-GAN) BIBREF52 is proposed to synthesize a target image by combining the background of a source image and a text-described foreground object which does not exist in the source image. A unique feature of MC-GAN is that it proposes a synthesis block in which the background feature is extracted from the given image without non-linear function (i.e. only using convolution and batch normalization) and the foreground feature is the feature map from the previous layer..black Because MC-GAN is able to properly model the background and foreground of the generated images, a unique strength of MC-GAN is that users are able to provide a base image and MC-GAN is able to preserve the background information of the base image to generate new images. black generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "48\n",
      "Q: Do the authors have any conflicts of interest regarding the publication of this article?\n",
      "A: No, the authors declare that there is no conflict of interest regarding the publication of this article.\n",
      "Question : for the text The authors declare that there is no conflict of interest regarding the publication of this article. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "49\n",
      "Question 1: Who provided support for the research by donating one of the GPUs used?\n",
      "\n",
      "Answer 1: The NVIDIA Corporation provided support for the research by donating one of the GPUs used.\n",
      "Question : for the text Thanks to Edison Marrese-Taylor and Pablo Loyola for their feedback on early versions of this manuscript. We also gratefully acknowledge the support of the NVIDIA Corporation with the donation of one of the GPUs used for this research. Jorge A. Balazs is partially supported by the Japanese Government MEXT Scholarship. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "50\n",
      "Question 1: What is the focus of the study mentioned in the text?\n",
      "\n",
      "Answer 1: The study is focused on analyzing the impact of combining word representations obtained from a word embedding lookup table and those generated from a function over the characters that compose the word on the quality of the final word representation.\n",
      "Question : for the text We are interested in studying different ways of combining word representations, obtained from different hierarchies, into a single word representation. Specifically, we want to study how combining word representations (1) taken directly from a word embedding lookup table, and (2) obtained from a function over the characters composing them, affects the quality of the final word representations..Let INLINEFORM0 be a set, or vocabulary, of words with INLINEFORM1 elements, and INLINEFORM2 a vocabulary of characters with INLINEFORM3 elements. Further, let INLINEFORM4 be a sequence of words, and INLINEFORM5 be the sequence of characters composing INLINEFORM6 . Each token INLINEFORM7 can be represented as a vector INLINEFORM8 extracted directly from an embedding lookup table INLINEFORM9 , pre-trained or otherwise, and as a vector INLINEFORM10 built from the characters that compose it; in other words, INLINEFORM11 , where INLINEFORM12 is a function that maps a sequence of characters to a vector..The methods for combining word and character-level representations we study, are of the form INLINEFORM0 where INLINEFORM1 is the final word representation. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "1\n",
      "What are the two baselines compared in the study?\n",
      "\n",
      "The two baselines compared in the study are using pre-trained word vectors only and using character-only features for representing words.\n",
      "Question : for the text We tested three different methods for combining INLINEFORM0 with INLINEFORM1 : simple concatenation, a learned scalar gate BIBREF11 , and a learned vector gate (also referred to as feature-wise sigmoidal gate). Additionally, we compared these methods to two baselines: using pre-trained word vectors only, and using character-only features for representing words. See fig:methods for a visual description of the proposed methods..word-only (w) considers only INLINEFORM0 and ignores INLINEFORM1 : DISPLAYFORM0 .char-only (c) considers only INLINEFORM0 and ignores INLINEFORM1 : DISPLAYFORM0 .concat (cat) concatenates both word and character-level representations: DISPLAYFORM0 .scalar gate (sg) implements the scalar gating mechanism described by BIBREF11 : DISPLAYFORM0 .where INLINEFORM0 and INLINEFORM1 are trainable parameters, INLINEFORM2 , and INLINEFORM3 is the sigmoid function..vector gate (vg): DISPLAYFORM0 .where INLINEFORM0 and INLINEFORM1 are trainable parameters, INLINEFORM2 , INLINEFORM3 is the element-wise sigmoid function, INLINEFORM4 is the element-wise product for vectors, and INLINEFORM5 is a vector of ones..The vector gate is inspired by BIBREF11 and BIBREF12 , but is different to the former in that the gating mechanism acts upon each dimension of the word and character-level vectors, and different to the latter in that it does not rely on external sources of information for calculating the gating mechanism..Finally, note that word only and char only are special cases of both gating mechanisms: INLINEFORM0 (scalar gate) and INLINEFORM1 (vector gate) correspond to word only; INLINEFORM2 and INLINEFORM3 correspond to char only. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "2\n",
      "Question 1: According to the empirical study presented, which combination method performed consistently better in word similarity and relatedness tasks? \n",
      "\n",
      "Answer 1: The vector gate method consistently performed better in a variety of word similarity and relatedness tasks compared to other combination methods.\n",
      "Question : for the text We presented an empirical study showing the effect that different ways of combining character and word representations has in word-level and sentence-level evaluation tasks..We showed that a vector gate performed consistently better across a variety of word similarity and relatedness tasks. Additionally, despite showing inconsistent results in sentence evaluation tasks, it performed significantly better than the other methods in semantic similarity tasks..We further showed through this mechanism, that learning character-level representations is always beneficial, and becomes increasingly so with less common words..In the future it would be interesting to study how the choice of mechanism for combining subword and word representations affects the more recent language-model-based pretraining methods such as ELMo BIBREF49 , GPT BIBREF50 , BIBREF51 and BERT BIBREF52 . generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "3\n",
      "What is the most widely-used evaluation method for evaluating word representations?\n",
      "\n",
      "The most widely-used evaluation method for evaluating word representations is the semantic similarity task.\n",
      "Question : for the text Word-level Semantic Similarity A desirable property of vector representations of words is that semantically similar words should have similar vector representations. Assessing whether a set of word representations possesses this quality is referred to as the semantic similarity task. This is the most widely-used evaluation method for evaluating word representations, despite its shortcomings BIBREF20 ..This task consists of comparing the similarity between word vectors measured by a distance metric (usually cosine distance), with a similarity score obtained from human judgements. High correlation between these similarities is an indicator of good performance..A problem with this formulation though, is that the definition of “similarity” often confounds the meaning of both similarity and relatedness. For example, cup and tea are related but dissimilar words, and this type of distinction is not always clear BIBREF21 , BIBREF22 ..To face the previous problem, we tested our methods in a wide variety of datasets, including some that explicitly model relatedness (WS353R), some that explicitly consider similarity (WS353S, SimLex999, SimVerb3500), and some where the distinction is not clear (MEN, MTurk287, MTurk771, RG, WS353). We also included the RareWords (RW) dataset for evaluating the quality of rare word representations. See appendix:datasets for a more complete description of the datasets we used..Sentence-level Evaluation Tasks Unlike word-level representations, there is no consensus on the desirable properties sentence representations should have. In response to this, BIBREF13 created SentEval, a sentence representation evaluation benchmark designed for assessing how well sentence representations perform in various downstream tasks BIBREF23 ..Some of the datasets included in SentEval correspond to sentiment classification (CR, MPQA, MR, SST2, and SST5), subjectivity classification (SUBJ), question-type classification (TREC), recognizing textual entailment (SICK E), estimating semantic relatedness (SICK R), and measuring textual semantic similarity (STS16, STSB). The datasets are described by BIBREF13 , and we provide pointers to their original sources in the appendix table:sentence-eval-datasets..To evaluate these sentence representations SentEval trained a linear model on top of them, and evaluated their performance in the validation sets accompanying each dataset. The only exception was the STS16 task, in which our representations were evaluated directly. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "4\n",
      "Question 1: What was the purpose of evaluating the quality of each trained model's word representations in 10 word similarity tasks?\n",
      "\n",
      "Answer 1: The purpose was to evaluate the quality of each trained model's word representations in 10 word similarity tasks using the system created by BIBREF17.\n",
      "Question : for the text We trained our models for solving the Natural Language Inference (NLI) task in two datasets, SNLI BIBREF15 and MultiNLI BIBREF16 , and validated them in each corresponding development set (including the matched and mismatched development sets of MultiNLI)..For each dataset-method combination we trained 7 models initialized with different random seeds, and saved each when it reached its best validation accuracy. We then evaluated the quality of each trained model's word representations INLINEFORM0 in 10 word similarity tasks, using the system created by BIBREF17 ..Finally, we fed these obtained word vectors to a BiLSTM with max-pooling and evaluated the final sentence representations in 11 downstream transfer tasks BIBREF13 , BIBREF18 . generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "5\n",
      "What did the two recent works show about the importance of character-level representations in rare words and word-level representations in common words?\n",
      "\n",
      "Answer 1: The two recent works showed that the gating mechanisms assigned greater importance to character-level representations in rare words and to word-level representations in common ones, reaffirming the previous findings that subword structures in general, and characters in particular, are beneficial for modeling uncommon words.\n",
      "Question : for the text To the best of our knowledge, there are only two recent works that specifically study how to combine word and subword-level vector representations.. BIBREF11 propose to use a trainable scalar gating mechanism capable of learning a weighting scheme for combining character-level and word-level representations. They compared their proposed method to manually weighting both levels; using characters only; words only; or their concatenation. They found that in some datasets a specific manual weighting scheme performed better, while in others the learned scalar gate did.. BIBREF12 further expand the gating concept by making the mechanism work at a finer-grained level, learning how to weight each vector's dimensions independently, conditioned on external word-level features such as part-of-speech and named-entity tags. Similarly, they compared their proposed mechanism to using words only, characters only, and a concatenation of both, with and without external features. They found that their vector gate performed better than the other methods in all the reported tasks, and beat the state of the art in two reading comprehension tasks..Both works showed that the gating mechanisms assigned greater importance to character-level representations in rare words, and to word-level representations in common ones, reaffirming the previous findings that subword structures in general, and characters in particular, are beneficial for modeling uncommon words. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "6\n",
      "Question 1: What is the impact of scaling and biasing in feature-wise transformation methods?\n",
      "\n",
      "Answer 1: The review suggests that in general, scaling has a greater impact than biasing in feature-wise transformation methods, and that limiting the scaling parameter to a certain range can hurt performance in some settings. These insights can inform future decisions involving the design of mechanisms for combining character and word-level representations.\n",
      "Question : for the text  BIBREF47 provide a review on feature-wise transformation methods, of which the mechanisms presented in this paper form a part of. In a few words, the INLINEFORM0 parameter, in both scalar gate and vector gate mechanisms, can be understood as a scaling parameter limited to the INLINEFORM1 range and conditioned on word representations, whereas adding the scaled INLINEFORM2 and INLINEFORM3 representations can be seen as biasing word representations conditioned on character representations..The previous review extends the work by BIBREF48 , which describes the Feature-wise Linear Modulation (FiLM) framework as a generalization of Conditional Normalization methods, and apply it in visual reasoning tasks. Some of the reported findings are that, in general, scaling has greater impact than biasing, and that in a setting similar to the scalar gate, limiting the scaling parameter to INLINEFORM0 hurt performance. Future decisions involving the design of mechanisms for combining character and word-level representations should be informed by these insights. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "7\n",
      "Question 1: How were words with only one appearance handled in the training and development datasets?\n",
      "\n",
      "Answer 1: Words that appeared only once were considered UNK, and only words that appeared at least twice were used for training and development.\n",
      "Question : for the text We only considered words that appear at least twice, for each dataset. Those that appeared only once were considered UNK. We used the Treebank Word Tokenizer as implemented in NLTK for tokenizing the training and development datasets..In the same fashion as conneau2017supervised, we used a batch size of 64, an SGD optmizer with an initial learning rate of INLINEFORM0 , and at each epoch divided the learning rate by 5 if the validation accuracy decreased. We also used gradient clipping when gradients where INLINEFORM1 ..We defined character vector representations as 50-dimensional vectors randomly initialized by sampling from the uniform distribution in the INLINEFORM0 range..The output dimension of the character-level BiLSTM was 300 per direction, and remained of such size after combining forward and backward representations as depicted in eq. EQREF9 ..Word vector representations where initialized from the 300-dimensional GloVe vectors BIBREF14 , trained in 840B tokens from the Common Crawl, and finetuned during training. Words not present in the GloVe vocabulary where randomly initialized by sampling from the uniform distribution in the INLINEFORM0 range..The input size of the word-level LSTM was 300 for every method except concat in which it was 600, and its output was always 2048 per direction, resulting in a 4096-dimensional sentence representation. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "8\n",
      "What is the reason for the improvement in word representations when incorporating sub-word structures?\n",
      "The improvement in word representations is related to sub-word structures containing information that is usually ignored by standard word-level models, and these structures include morphemes, substrings, and characters. By incorporating these structures, the quality of the word representations is significantly increased as reflected by intrinsic metrics and performance in a wide range of downstream tasks.\n",
      "Question : for the text Incorporating sub-word structures like substrings, morphemes and characters to the creation of word representations significantly increases their quality as reflected both by intrinsic metrics and performance in a wide range of downstream tasks BIBREF0 , BIBREF1 , BIBREF2 , BIBREF3 ..The reason for this improvement is related to sub-word structures containing information that is usually ignored by standard word-level models. Indeed, when representing words as vectors extracted from a lookup table, semantically related words resulting from inflectional processes such as surf, surfing, and surfed, are treated as being independent from one another. Further, word-level embeddings do not account for derivational processes resulting in syntactically-similar words with different meanings such as break, breakable, and unbreakable. This causes derived words, which are usually less frequent, to have lower-quality (or no) vector representations..Previous works have successfully combined character-level and word-level word representations, obtaining overall better results than using only word-level representations. For example BIBREF1 achieved state-of-the-art results in a machine translation task by representing unknown words as a composition of their characters. BIBREF4 created word representations by adding the vector representations of the words' surface forms and their morphemes ( INLINEFORM0 ), obtaining significant improvements on intrinsic evaluation tasks, word similarity and machine translation. BIBREF5 concatenated character-level and word-level representations for creating word representations, and then used them as input to their models for obtaining state-of-the-art results in Named Entity Recognition on several languages..What these works have in common is that the models they describe first learn how to represent subword information, at character BIBREF1 , morpheme BIBREF4 , or substring BIBREF0 levels, and then combine these learned representations at the word level. The incorporation of information at a finer-grained hierarchy results in higher-quality modeling of rare words, morphological processes, and semantics BIBREF6 ..There is no consensus, however, on which combination method works better in which case, or how the choice of a combination method affects downstream performance, either measured intrinsically at the word level, or extrinsically at the sentence level..In this paper we aim to provide some intuitions about how the choice of mechanism for combining character-level with word-level representations influences the quality of the final word representations, and the subsequent effect these have in the performance of downstream tasks. Our contributions are as follows: generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "9\n",
      "Question 1: What is the purpose of the context function in the function INLINEFORM0?\n",
      "\n",
      "Answer 1: The context function takes the input INLINEFORM0 and returns a context-enriched matrix representation INLINEFORM1, which contains a measure of information about its context and interactions with its neighbors. This is done by feeding the input to a BiLSTM, which reads the input both from left to right and right to left, returning the forward and backward LSTM outputs. The aggregation function then uses the forward and backward last hidden states to compute a single vector, which represents the character-level representation of the word.\n",
      "Question : for the text The function INLINEFORM0 is composed of an embedding layer, an optional context function, and an aggregation function..The embedding layer transforms each character INLINEFORM0 into a vector INLINEFORM1 of dimension INLINEFORM2 , by directly taking it from a trainable embedding lookup table INLINEFORM3 . We define the matrix representation of word INLINEFORM4 as INLINEFORM5 ..The context function takes INLINEFORM0 as input and returns a context-enriched matrix representation INLINEFORM1 , in which each INLINEFORM2 contains a measure of information about its context, and interactions with its neighbors. In particular, we chose to do this by feeding INLINEFORM3 to a BiLSTM BIBREF7 , BIBREF8 ..Informally, we can think of LSTM BIBREF10 as a function INLINEFORM0 that takes a matrix INLINEFORM1 as input and returns a context-enriched matrix representation INLINEFORM2 , where each INLINEFORM3 encodes information about the previous elements INLINEFORM4 ..A BiLSTM is simply composed of 2 LSTM, one that reads the input from left to right (forward), and another that does so from right to left (backward). The output of the forward and backward LSTM are INLINEFORM0 and INLINEFORM1 respectively. In the backward case the LSTM reads INLINEFORM2 first and INLINEFORM3 last, therefore INLINEFORM4 will encode the context from INLINEFORM5 ..The aggregation function takes the context-enriched matrix representation of word INLINEFORM0 for both directions, INLINEFORM1 and INLINEFORM2 , and returns a single vector INLINEFORM3 . To do so we followed BIBREF11 , and defined the character-level representation INLINEFORM4 of word INLINEFORM5 as the linear combination of the forward and backward last hidden states returned by the context function: DISPLAYFORM0 .where INLINEFORM0 and INLINEFORM1 are trainable parameters, and INLINEFORM2 represents the concatenation operation between two vectors. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "10\n",
      "Question 1: What method was used to obtain a sentence representation from the word vectors?\n",
      "\n",
      "Answer 1: A BiLSTM with max pooling was used to obtain a sentence representation from the word vectors.\n",
      "Question : for the text To enable sentence-level classification we need to obtain a sentence representation from the word vectors INLINEFORM0 . We achieved this by using a BiLSTM with max pooling, which was shown to be a good universal sentence encoding mechanism BIBREF13 ..Let INLINEFORM0 , be an input sentence and INLINEFORM1 its matrix representation, where each INLINEFORM2 was obtained by one of the methods described in subsec:methods. INLINEFORM3 is the context-enriched matrix representation of INLINEFORM4 obtained by feeding INLINEFORM5 to a BiLSTM of output dimension INLINEFORM6 . Lastly, INLINEFORM11 is the final sentence representation of INLINEFORM12 obtained by max-pooling INLINEFORM13 along the sequence dimension..Finally, we initialized the word representations INLINEFORM0 using GloVe embeddings BIBREF14 , and fine-tuned them during training. Refer to app:hyperparams for details on the other hyperparameters we used. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "11\n",
      "Question 1: Is there a clear correlation between word similarity tasks and sentence-level evaluation tasks?\n",
      "\n",
      "Answer 1: No, there is no clear correlation between word similarity tasks and sentence-level evaluation tasks, as shown by the lack of correlation between performance in the STSBenchmark and any word-similarity dataset.\n",
      "Question : for the text It is clear that the better performance the vector gate had in word similarity tasks did not translate into overall better performance in downstream tasks. This confirms previous findings indicating that intrinsic word evaluation metrics are not good predictors of downstream performance BIBREF29 , BIBREF30 , BIBREF20 , BIBREF31 ..subfig:mnli-correlations shows that the word representations created by the vector gate trained in MultiNLI had positively-correlated results within several word-similarity tasks. This hints at the generality of the word representations created by this method when modeling similarity and relatedness..However, the same cannot be said about sentence-level evaluation performance; there is no clear correlation between word similarity tasks and sentence-evaluation tasks. This is clearly illustrated by performance in the STSBenchmark, the only in which the vector gate was significantly superior, not being correlated with performance in any word-similarity dataset. This can be interpreted simply as word-level representations capturing word-similarity not being a sufficient condition for good performance in sentence-level tasks..In general, fig:correlations shows that there are no general correlation effects spanning both training datasets and combination mechanisms. For example, subfig:snli-correlations shows that, for both word-only and concat models trained in SNLI, performance in word similarity tasks correlates positively with performance in most sentence evaluation tasks, however, this does not happen as clearly for the same models trained in MultiNLI (subfig:mnli-correlations). generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "12\n",
      "Question 1: What does the provided URLs in the sentence-eval-datasets list correspond to?\n",
      "\n",
      "Answer 1: The provided URLs in the sentence-eval-datasets list correspond to the original sources for the evaluation datasets used in the paper, and not necessarily to where SentEval obtained the data from.\n",
      "Question : for the text table:sentence-eval-datasets lists the sentence-level evaluation datasets used in this paper. The provided URLs correspond to the original sources, and not necessarily to the URLs where SentEval got the data from..The version of the CR, MPQA, MR, and SUBJ datasets used in this paper were the ones preprocessed by BIBREF75 . Both SST2 and SST5 correspond to preprocessed versions of the SST dataset by BIBREF74 . SST2 corresponds to a subset of SST used by BIBREF54 containing flat representations of sentences annotated with binary sentiment labels, and SST5 to another subset annotated with more fine-grained sentiment labels (very negative, negative, neutral, positive, very positive). generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "13\n",
      "Question 1: What is the best sentence encoding architecture for producing highly transferable sentence representations?\n",
      "\n",
      "Answer 1: According to BIBREF13, a BiLSTM with max-pooling was found to be the best at producing highly transferable sentence representations.\n",
      "Question : for the text The problem of representing sentences as fixed-length vectors has been widely studied.. BIBREF32 suggested a self-adaptive hierarchical model that gradually composes words into intermediate phrase representations, and adaptively selects specific hierarchical levels for specific tasks. BIBREF33 proposed an encoder-decoder model trained by attempting to reconstruct the surrounding sentences of an encoded passage, in a fashion similar to Skip-gram BIBREF34 . BIBREF35 overcame the previous model's need for ordered training sentences by using autoencoders for creating the sentence representations. BIBREF36 implemented a model simpler and faster to train than the previous two, while having competitive performance. Similar to BIBREF33 , BIBREF37 suggested predicting future sentences with a hierarchical CNN-LSTM encoder.. BIBREF13 trained several sentence encoding architectures on a combination of the SNLI and MultiNLI datasets, and showed that a BiLSTM with max-pooling was the best at producing highly transferable sentence representations. More recently, BIBREF18 empirically showed that sentence representations created in a multi-task setting BIBREF38 , performed increasingly better the more tasks they were trained in. BIBREF39 proposed using an autoencoder that relies on multi-head self-attention over the concatenation of the max and mean pooled encoder outputs for producing sentence representations. Finally, BIBREF40 show that modern sentence embedding methods are not vastly superior to random methods..The works mentioned so far usually evaluate the quality of the produced sentence representations in sentence-level downstream tasks. Common benchmarks grouping these kind of tasks include SentEval BIBREF23 , and GLUE BIBREF41 . Another trend, however, is to probe sentence representations to understand what linguistic phenomena they encode BIBREF42 , BIBREF43 , BIBREF44 , BIBREF45 , BIBREF46 . generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "14\n",
      "Question 1: Did any method significantly outperform the word only baseline in classification tasks?\n",
      "\n",
      "Answer 1: No, there was no method that significantly outperformed the word only baseline in classification tasks.\n",
      "Question : for the text table:sentlevelresults shows the impact that different methods for combining character and word-level word representations have in the quality of the sentence representations produced by our models..We can observe the same trend mentioned in subsec:word-similarity-eval, and highlighted by the difference between bold values, that models trained in MultiNLI performed better than those trained in SNLI at a statistically significant level, confirming the findings of BIBREF13 . In other words, training sentence encoders on MultiNLI yields more general sentence representations than doing so on SNLI..The two exceptions to the previous trend, SICKE and SICKR, benefited more from models trained on SNLI. We hypothesize this is again due to both SNLI and SICK BIBREF26 having similar data distributions..Additionally, there was no method that significantly outperformed the word only baseline in classification tasks. This means that the added expressivity offered by explicitly modeling characters, be it through concatenation or gating, was not significantly better than simply fine-tuning the pre-trained GloVe embeddings for this type of task. We hypothesize this is due to the conflation of two effects. First, the fact that morphological processes might not encode important information for solving these tasks; and second, that SNLI and MultiNLI belong to domains that are too dissimilar to the domains in which the sentence representations are being tested..On the other hand, the vector gate significantly outperformed every other method in the STSB task when trained in both datasets, and in the STS16 task when trained in SNLI. This again hints at this method being capable of modeling phenomena at the word level, resulting in improved semantic representations at the sentence level. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "15\n",
      "What trend is observed in both figures in regards to gating values?\n",
      "\n",
      "Answer 1: Gating values tend to be low on average, ranging from INLINEFORM0 to INLINEFORM1, and this result is supported by the findings of previous studies.\n",
      "Question : for the text fig:gatingviz shows that for more common words the vector gate mechanism tends to favor only a few dimensions while keeping a low average gating value across dimensions. On the other hand, values are greater and more homogeneous across dimensions in rarer words. Further, fig:freqvsgatevalue shows this mechanism assigns, on average, a greater gating value to less frequent words, confirming the findings by BIBREF11 , and BIBREF12 ..In other words, the less frequent the word, the more this mechanism allows the character-level representation to influence the final word representation, as shown by eq:vg. A possible interpretation of this result is that exploiting character information becomes increasingly necessary as word-level representations' quality decrease..Another observable trend in both figures is that gating values tend to be low on average. Indeed, it is possible to see in fig:freqvsgatevalue that the average gating values range from INLINEFORM0 to INLINEFORM1 . This result corroborates the findings by BIBREF11 , stating that setting INLINEFORM2 in eq:scalar-gate, was better than setting it to higher values..In summary, the gating mechanisms learn how to compensate the lack of expressivity of underrepresented words by selectively combining their representations with those of characters. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "16\n",
      "Question 1: Which model performed the best in terms of capturing a notion of word similarity and relatedness that is closer to that of humans? \n",
      "Answer 1: The vector gates model outperformed the other methods regardless of training dataset, indicating that learning how to combine character and word-level representations at the dimension level produces word vector representations that capture a notion of word similarity and relatedness that is closer to that of humans.\n",
      "Question : for the text table:wordlevelresults shows the quality of word representations in terms of the correlation between word similarity scores obtained by the proposed models and word similarity scores defined by humans..First, we can see that for each task, character only models had significantly worse performance than every other model trained on the same dataset. The most likely explanation for this is that these models are the only ones that need to learn word representations from scratch, since they have no access to the global semantic knowledge encoded by the GloVe embeddings..Further, bold results show the overall trend that vector gates outperformed the other methods regardless of training dataset. This implies that learning how to combine character and word-level representations at the dimension level produces word vector representations that capture a notion of word similarity and relatedness that is closer to that of humans..Additionally, results from the MNLI row in general, and underlined results in particular, show that training on MultiNLI produces word representations better at capturing word similarity. This is probably due to MultiNLI data being richer than that of SNLI. Indeed, MultiNLI data was gathered from various sources (novels, reports, letters, and telephone conversations, among others), rather than the single image captions dataset from which SNLI was created..Exceptions to the previous rule are models evaluated in MEN and RW. The former case can be explained by the MEN dataset containing only words that appear as image labels in the ESP-Game and MIRFLICKR-1M image datasets BIBREF24 , and therefore having data that is more closely distributed to SNLI than to MultiNLI..More notably, in the RareWords dataset BIBREF25 , the word only, concat, and scalar gate methods performed equally, despite having been trained in different datasets ( INLINEFORM0 ), and the char only method performed significantly worse when trained in MultiNLI. The vector gate, however, performed significantly better than its counterpart trained in SNLI. These facts provide evidence that this method is capable of capturing linguistic phenomena that the other methods are unable to model..table:word-similarity-dataset lists the word-similarity datasets and their corresponding reference. As mentioned in subsec:datasets, all the word-similarity datasets contain pairs of words annotated with similarity or relatedness scores, although this difference is not always explicit. Below we provide some details for each..MEN contains 3000 annotated word pairs with integer scores ranging from 0 to 50. Words correspond to image labels appearing in the ESP-Game and MIRFLICKR-1M image datasets..MTurk287 contains 287 annotated pairs with scores ranging from 1.0 to 5.0. It was created from words appearing in both DBpedia and in news articles from The New York Times..MTurk771 contains 771 annotated pairs with scores ranging from 1.0 to 5.0, with words having synonymy, holonymy or meronymy relationships sampled from WordNet BIBREF56 ..RG contains 65 annotated pairs with scores ranging from 0.0 to 4.0 representing “similarity of meaning”..RW contains 2034 pairs of words annotated with similarity scores in a scale from 0 to 10. The words included in this dataset were obtained from Wikipedia based on their frequency, and later filtered depending on their WordNet synsets, including synonymy, hyperonymy, hyponymy, holonymy and meronymy. This dataset was created with the purpose of testing how well models can represent rare and complex words..SimLex999 contains 999 word pairs annotated with similarity scores ranging from 0 to 10. In this case the authors explicitly considered similarity and not relatedness, addressing the shortcomings of datasets that do not, such as MEN and WS353. Words include nouns, adjectives and verbs..SimVerb3500 contains 3500 verb pairs annotated with similarity scores ranging from 0 to 10. Verbs were obtained from the USF free association database BIBREF66 , and VerbNet BIBREF63 . This dataset was created to address the lack of representativity of verbs in SimLex999, and the fact that, at the time of creation, the best performing models had already surpassed inter-annotator agreement in verb similarity evaluation resources. Like SimLex999, this dataset also explicitly considers similarity as opposed to relatedness..WS353 contains 353 word pairs annotated with similarity scores from 0 to 10..WS353R is a subset of WS353 containing 252 word pairs annotated with relatedness scores. This dataset was created by asking humans to classify each WS353 word pair into one of the following classes: synonyms, antonyms, identical, hyperonym-hyponym, hyponym-hyperonym, holonym-meronym, meronym-holonym, and none-of-the-above. These annotations were later used to group the pairs into: similar pairs (synonyms, antonyms, identical, hyperonym-hyponym, and hyponym-hyperonym), related pairs (holonym-meronym, meronym-holonym, and none-of-the-above with a human similarity score greater than 5), and unrelated pairs (classified as none-of-the-above with a similarity score less than or equal to 5). This dataset is composed by the union of related and unrelated pairs..WS353S is another subset of WS353 containing 203 word pairs annotated with similarity scores. This dataset is composed by the union of similar and unrelated pairs, as described previously. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "17\n",
      "Question 1: Who would the authors like to extend their gratitude to for their feedback on the paper?\n",
      "Answer 1: The authors would like to thank the anonymous reviewers for their valuable and constructive comments on this paper.\n",
      "Question : for the text We would like to thank the anonymous reviewers for their valuable and constructive comments on this paper. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "18\n",
      "Question 1: What is the highest F1 score achieved by the WordDecoding model with a single attention mechanism?\n",
      "\n",
      "Answer 1: The highest F1 score achieved by the WordDecoding model with a single attention mechanism is achieved on both datasets.\n",
      "Question : for the text We include the performance of different attention mechanisms with our WordDecoding model, effects of our masking-based copy mechanism, and ablation results of three variants of the single attention mechanism with our PtrNetDecoding model in Table TABREF17. WordDecoding with single attention achieves the highest F1 score on both datasets. We also see that our copy mechanism improves F1 scores by around 4–7% in each attention mechanism with both datasets. PtrNetDecoding achieves the highest F1 scores when we combine the two attention mechanisms with respect to the previous hidden vector of the decoder LSTM ($\\mathbf {h}_{t-1}^D$) and representation of all previously extracted tuples ($\\mathbf {y}_{prev}$). generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "19\n",
      "What are the possible reasons for erroneous relation tuples extracted by a joint model?\n",
      "\n",
      "The possible reasons for erroneous relation tuples extracted by a joint model are: (i) extracted entities are wrong; (ii) extracted relations are wrong; (iii) pairings of entities with relations are wrong.\n",
      "Question : for the text The relation tuples extracted by a joint model can be erroneous for multiple reasons such as: (i) extracted entities are wrong; (ii) extracted relations are wrong; (iii) pairings of entities with relations are wrong. To see the effects of the first two reasons, we analyze the performance of HRL and our models on entity generation and relation generation separately. For entity generation, we only consider those entities which are part of some tuple. For relation generation, we only consider the relations of the tuples. We include the performance of our two models and HRL on entity generation and relation generation in Table TABREF20. Our proposed models perform better than HRL on both tasks. Comparing our two models, PtrNetDecoding performs better than WordDecoding on both tasks, although WordDecoding achieves higher F1 scores in tuple extraction. This suggests that PtrNetDecoding makes more errors while pairing the entities with relations. We further analyze the outputs of our models and HRL to determine the errors due to ordering of entities (Order), mismatch of the first entity (Ent1), and mismatch of the second entity (Ent2) in Table TABREF21. WordDecoding generates fewer errors than the other two models in all the categories and thus achieves the highest F1 scores on both datasets. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "20\n",
      "What is the reason for the difference in F1 scores between NYT24 and NYT29 datasets?\n",
      "\n",
      "The reason for the difference in F1 scores between NYT24 and NYT29 datasets is that NYT24 has a higher percentage of overlapping tuples between the training and test data, making it easier for deep neural networks to achieve higher F1 scores due to their memorization power. In contrast, NYT29 has a lower percentage of overlapping tuples, making it more challenging for the models.\n",
      "Question : for the text From Table TABREF15, we see that CopyR, HRL, and our models achieve significantly higher F1 scores on the NYT24 dataset than the NYT29 dataset. Both datasets have a similar set of relations and similar texts (NYT). So task-wise both datasets should pose a similar challenge. However, the F1 scores suggest that the NYT24 dataset is easier than NYT29. The reason is that NYT24 has around 72.0% of overlapping tuples between the training and test data (% of test tuples that appear in the training data with different source sentences). In contrast, NYT29 has only 41.7% of overlapping tuples. Due to the memorization power of deep neural networks, it can achieve much higher F1 score on NYT24. The difference between the F1 scores of WordDecoding and PtrNetDecoding on NYT24 is marginally higher than NYT29, since WordDecoding has more trainable parameters (about 27 million) than PtrNetDecoding (about 24.5 million) and NYT24 has very high tuple overlap. However, their ensemble versions achieve closer F1 scores on both datasets..Despite achieving marginally lower F1 scores, the pointer network-based model can be considered more intuitive and suitable for this task. WordDecoding may not extract the special tokens and relation tokens at the right time steps, which is critical for finding the tuples from the generated sequence of words. PtrNetDecoding always extracts two entities of varying length and a relation for every tuple. We also observe that PtrNetDecoding is more than two times faster and takes one-third of the GPU memory of WordDecoding during training and inference. This speedup and smaller memory consumption are achieved due to the fewer number of decoding steps of PtrNetDecoding compared to WordDecoding. PtrNetDecoding extracts an entire tuple at each time step, whereas WordDecoding extracts just one word at each time step and so requires eight time steps on average to extract a tuple (assuming that the average length of an entity is two). The softmax operation at the projection layer of WordDecoding is applied across the entire vocabulary and the vocabulary size can be large (more than 40,000 for our datasets). In case of PtrNetDecoding, the softmax operation is applied across the sentence length (maximum of 100 in our experiments) and across the relation set (24 and 29 for our datasets). The costly softmax operation and the higher number of decoding time steps significantly increase the training and inference time for WordDecoding. The encoder-decoder model proposed by BIBREF9 (BIBREF9) faces a similar softmax-related problem as their target vocabulary contains the entire Wikidata entity IDs and relation IDs which is in the millions. HRL, which uses a deep reinforcement learning algorithm, takes around 8x more time to train than PtrNetDecoding with a similar GPU configuration. The speedup and smaller memory consumption will be useful when we move from sentence-level extraction to document-level extraction, since document length is much higher than sentence length and a document contains a higher number of tuples. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "21\n",
      "Question 1: What makes the task of extracting relation tuples from sentences challenging?\n",
      "\n",
      "Answer 1: The different length of entities, the presence of multiple tuples, and overlapping of entities among tuples make the task of extracting relation tuples from sentences challenging.\n",
      "Question : for the text Extracting relation tuples from sentences is a challenging task due to different length of entities, the presence of multiple tuples, and overlapping of entities among tuples. In this paper, we propose two novel approaches using encoder-decoder architecture to address this task. Experiments on the New York Times (NYT) corpus show that our proposed models achieve significantly improved new state-of-the-art F1 scores. As future work, we would like to explore our proposed models for a document-level tuple extraction task. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "22\n",
      "Q: What is the representation scheme used for the relation tuples in the first approach?\n",
      "\n",
      "A: The representation scheme used for the relation tuples in the first approach is entity1; entity2; relation, separated by `;' and for multiple tuples, `|' token is used as a separator.\n",
      "Question : for the text In this task, input to the system is a sequence of words, and output is a set of relation tuples. In our first approach, we represent each tuple as entity1 ; entity2 ; relation. We use `;' as a separator token to separate the tuple components. Multiple tuples are separated using the `$\\vert $' token. We have included one example of such representation in Table TABREF1. Multiple relation tuples with overlapping entities and different lengths of entities can be represented in a simple way using these special tokens (; and $\\vert $). During inference, after the end of sequence generation, relation tuples can be extracted easily using these special tokens. Due to this uniform representation scheme, where entity tokens, relation tokens, and special tokens are treated similarly, we use a shared vocabulary between the encoder and decoder which includes all of these tokens. The input sentence contains clue words for every relation which can help generate the relation tokens. We use two special tokens so that the model can distinguish between the beginning of a relation tuple and the beginning of a tuple component. To extract the relation tuples from a sentence using the encoder-decoder model, the model has to generate the entity tokens, find relation clue words and map them to the relation tokens, and generate the special tokens at appropriate time. Our experiments show that the encoder-decoder models can achieve this quite effectively. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "23\n",
      "What are the three different attention mechanisms used for the word-level decoding model? \n",
      "\n",
      "The three different attention mechanisms used for the word-level decoding model are: (1) Avg., where the context vector is obtained by averaging the hidden vectors of the encoder; (2) N-gram, where the context vector is obtained by the N-gram attention mechanism; and (3) Single, where the context vector is obtained by the attention mechanism proposed by BIBREF10, which gives the best performance with the word-level decoding model.\n",
      "Question : for the text We experimented with three different attention mechanisms for our word-level decoding model to obtain the source context vector $\\mathbf {e}_t$:.(1) Avg.: The context vector is obtained by averaging the hidden vectors of the encoder: $\\mathbf {e}_t=\\frac{1}{n}\\sum _{i=1}^n \\mathbf {h}_i^E$.(2) N-gram: The context vector is obtained by the N-gram attention mechanism of BIBREF9 (BIBREF9) with N=3..$\\textnormal {a}_i^g=(\\mathbf {h}_n^{E})^T \\mathbf {V}^g \\mathbf {w}_i^g$, $\\alpha ^g = \\mathrm {softmax}(\\mathbf {a}^g)$.$\\mathbf {e}_t=[\\mathbf {h}_n^E \\Vert \\sum _{g=1}^N \\mathbf {W}^g (\\sum _{i=1}^{\\vert G^g \\vert } \\alpha _i^g \\mathbf {w}_i^g)$].Here, $\\mathbf {h}_n^E$ is the last hidden state of the encoder, $g \\in \\lbrace 1, 2, 3\\rbrace $ refers to the word gram combination, $G^g$ is the sequence of g-gram word representations for the input sentence, $\\mathbf {w}_i^g$ is the $i$th g-gram vector (2-gram and 3-gram representations are obtained by average pooling), $\\alpha _i^g$ is the normalized attention score for the $i$th g-gram vector, $\\mathbf {W} \\in \\mathbb {R}^{d_h \\times d_h}$ and $\\mathbf {V} \\in \\mathbb {R}^{d_h \\times d_h}$ are trainable parameters..(3) Single: The context vector is obtained by the attention mechanism proposed by BIBREF10 (BIBREF10). This attention mechanism gives the best performance with the word-level decoding model..$\\mathbf {u}_t^i = \\mathbf {W}_{u} \\mathbf {h}_i^E, \\quad \\mathbf {q}_t^i = \\mathbf {W}_{q} \\mathbf {h}_{t-1}^D + \\mathbf {b}_{q}$,.$\\textnormal {a}_t^i = \\mathbf {v}_a \\tanh (\\mathbf {q}_t^i + \\mathbf {u}_t^i), \\quad \\alpha _t = \\mathrm {softmax}(\\mathbf {a}_t)$,.$\\mathbf {e}_t = \\sum _{i=1}^n \\alpha _t^i \\mathbf {h}_i^E$.where $\\mathbf {W}_u \\in \\mathbb {R}^{d_h \\times d_h}$, $\\mathbf {W}_q \\in \\mathbb {R}^{d_h \\times d_h}$, and $\\mathbf {v}_a \\in \\mathbb {R}^{d_h}$ are all trainable attention parameters and $\\mathbf {b}_q \\in \\mathbb {R}^{d_h}$ is a bias vector. $\\alpha _t^i$ is the normalized attention score of the $i$th source word at the decoding time step $t$..For our pointer network-based decoding model, we use three variants of the single attention model. First, we use $\\mathbf {h}_{t-1}^D$ to calculate $\\mathbf {q}_t^i$ in the attention mechanism. Next, we use $\\mathbf {y}_{prev}$ to calculate $\\mathbf {q}_t^i$, where $\\mathbf {W}_q \\in \\mathbb {R}^{(8d_p + d_r) \\times d_h}$. In the final variant, we obtain the attentive context vector by concatenating the two attentive vectors obtained using $\\mathbf {h}_{t-1}^D$ and $\\mathbf {y}_{prev}$. This gives the best performance with the pointer network-based decoding model. These variants are referred to as $\\mathrm {dec_{hid}}$, $\\mathrm {tup_{prev}}$, and $\\mathrm {combo}$ in Table TABREF17. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "24\n",
      "Question 1: What is the purpose of creating a single vocabulary V?\n",
      "\n",
      "Answer 1: The purpose of creating a single vocabulary V is to consist of the source sentence tokens, relation names from relation set R, special separator tokens (`;', `$\\vert$'), start-of-target-sequence token (SOS), end-of-target-sequence token (EOS), and unknown word token (UNK) to form word-level embeddings for the input tokens.\n",
      "Question : for the text We create a single vocabulary $V$ consisting of the source sentence tokens, relation names from relation set $R$, special separator tokens (`;', `$\\vert $'), start-of-target-sequence token (SOS), end-of-target-sequence token (EOS), and unknown word token (UNK). Word-level embeddings are formed by two components: (1) pre-trained word vectors (2) character embedding-based feature vectors. We use a word embedding layer $\\mathbf {E}_w \\in \\mathbb {R}^{\\vert V \\vert \\times d_w}$ and a character embedding layer $\\mathbf {E}_c \\in \\mathbb {R}^{\\vert A \\vert \\times d_c}$, where $d_w$ is the dimension of word vectors, $A$ is the character alphabet of input sentence tokens, and $d_c$ is the dimension of character embedding vectors. Following BIBREF7 (BIBREF7), we use a convolutional neural network with max-pooling to extract a feature vector of size $d_f$ for every word. Word embeddings and character embedding-based feature vectors are concatenated ($\\Vert $) to obtain the representation of the input tokens..A source sentence $\\mathbf {S}$ is represented by vectors of its tokens $\\mathbf {x}_1, \\mathbf {x}_2,....,\\mathbf {x}_n$, where $\\mathbf {x}_i \\in \\mathbb {R}^{(d_w+d_f)}$ is the vector representation of the $i$th word and $n$ is the length of $\\mathbf {S}$. These vectors $\\mathbf {x}_i$ are passed to a bi-directional LSTM BIBREF8 (Bi-LSTM) to obtain the hidden representation $\\mathbf {h}_i^E$. We set the hidden dimension of the forward and backward LSTM of the Bi-LSTM to be $d_h/2$ to obtain $\\mathbf {h}_i^E \\in \\mathbb {R}^{d_h}$, where $d_h$ is the hidden dimension of the sequence generator LSTM of the decoder described below. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "25\n",
      "Question 1: What is the purpose of minimizing $\\mathcal {L}_{word}$ and $\\mathcal {L}_{ptr}$?\n",
      "\n",
      "Answer 1: The purpose of minimizing $\\mathcal {L}_{word}$ is to reduce the negative log-likelihood loss of the generated words for word-level decoding. The purpose of minimizing $\\mathcal {L}_{ptr}$ is to reduce the sum of negative log-likelihood loss of relation classification and the four pointer locations for pointer network-based decoding.\n",
      "Question : for the text We minimize the negative log-likelihood loss of the generated words for word-level decoding ($\\mathcal {L}_{word}$) and minimize the sum of negative log-likelihood loss of relation classification and the four pointer locations for pointer network-based decoding ($\\mathcal {L}_{ptr}$)..$v_t^b$ is the softmax score of the target word at time step $t$ for the word-level decoding model. $r$, $s$, and $e$ are the softmax score of the corresponding true relation label, true start and end pointer location of an entity. $b$, $t$, and $c$ refer to the $b$th training instance, $t$th time step of decoding, and the two entities of a tuple respectively. $B$ and $T$ are the batch size and maximum time step of the decoder respectively. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "26\n",
      "Question 1: What is the additional relation embedding matrix used in the decoder side of the model? \n",
      "\n",
      "Answer 1: The additional relation embedding matrix used in the decoder side of the model is $\\mathbf {E}_r \\in \\mathbb {R}^{\\vert R \\vert \\times d_r}$, where $R$ is the set of relations and $d_r$ is the dimension of relation vectors.\n",
      "Question : for the text In the second approach, we identify the entities in the sentence using their start and end locations. We remove the special tokens and relation names from the word vocabulary and word embeddings are used only at the encoder side along with character embeddings. We use an additional relation embedding matrix $\\mathbf {E}_r \\in \\mathbb {R}^{\\vert R \\vert \\times d_r}$ at the decoder side of our model, where $R$ is the set of relations and $d_r$ is the dimension of relation vectors. The relation set $R$ includes a special relation token EOS which indicates the end of the sequence. Relation tuples are represented as a sequence $T=y_0, y_1,....,y_m$, where $y_t$ is a tuple consisting of four indexes in the source sentence indicating the start and end location of the two entities and a relation between them (see Table TABREF1). $y_0$ is a dummy tuple that represents the start tuple of the sequence and $y_m$ functions as the end tuple of the sequence which has EOS as the relation (entities are ignored for this tuple). The decoder consists of an LSTM with hidden dimension $d_h$ to generate the sequence of tuples, two pointer networks to find the two entities, and a classification network to find the relation of a tuple. At time step $t$, the decoder takes the source sentence encoding ($\\mathbf {e}_t \\in \\mathbb {R}^{d_h}$) and the representation of all previously generated tuples ($\\mathbf {y}_{prev}=\\sum _{j=0}^{t-1}\\mathbf {y}_{j}$) as the input and generates the hidden representation of the current tuple, $\\mathbf {h}_t^D \\in \\mathbb {R}^{d_h}$. The sentence encoding vector $\\mathbf {e}_t$ is obtained using an attention mechanism as explained later. Relation tuples are a set and to prevent the decoder from generating the same tuple again, we pass the information about all previously generated tuples at each time step of decoding. $\\mathbf {y}_j$ is the vector representation of the tuple predicted at time step $j < t$ and we use the zero vector ($\\mathbf {y}_0=\\overrightarrow{0}$) to represent the dummy tuple $y_0$. $\\mathbf {h}_{t-1}^D$ is the hidden state of the LSTM at time step $t-1$. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "27\n",
      "What is the purpose of the first pointer network in the model?\n",
      "Answer 1: The first pointer network is used to identify the start and end location of the first entity in the predicted tuple, based on the probabilities of each source sentence token being the start and end location.\n",
      "Question : for the text After obtaining the hidden representation of the current tuple $\\mathbf {h}_t^D$, we first find the start and end pointers of the two entities in the source sentence. We concatenate the vector $\\mathbf {h}_t^D$ with the hidden vectors $\\mathbf {h}_i^E$ of the encoder and pass them to a Bi-LSTM layer with hidden dimension $d_p$ for forward and backward LSTM. The hidden vectors of this Bi-LSTM layer $\\mathbf {h}_i^k \\in \\mathbb {R}^{2d_p}$ are passed to two feed-forward networks (FFN) with softmax to convert each hidden vector into two scalar values between 0 and 1. Softmax operation is applied across all the words in the input sentence. These two scalar values represent the probability of the corresponding source sentence token to be the start and end location of the first entity. This Bi-LSTM layer with the two feed-forward layers is the first pointer network which identifies the first entity of the current relation tuple..where $\\mathbf {W}_s^1 \\in \\mathbb {R}^{1 \\times 2d_p}$, $\\mathbf {W}_e^1 \\in \\mathbb {R}^{1 \\times 2d_p}$, ${b}_s^1$, and ${b}_e^1$ are the weights and bias parameters of the feed-forward layers. ${s}_i^1$, ${e}_i^1$ represent the normalized probabilities of the $i$th source word being the start and end token of the first entity of the predicted tuple. We use another pointer network to extract the second entity of the tuple. We concatenate the hidden vectors $\\mathbf {h}_i^k$ with $\\mathbf {h}_t^D$ and $\\mathbf {h}_i^E$ and pass them to the second pointer network to obtain ${s}_i^2$ and ${e}_i^2$, which represent the normalized probabilities of the $i$th source word being the start and end of the second entity. These normalized probabilities are used to find the vector representation of the two entities, $\\mathbf {a}_t^1$ and $\\mathbf {a}_t^2$..We concatenate the entity vector representations $\\mathbf {a}_t^1$ and $\\mathbf {a}_t^2$ with $\\mathbf {h}_t^D$ and pass it to a feed-forward network (FFN) with softmax to find the relation. This feed-forward layer has a weight matrix $\\mathbf {W}_r \\in \\mathbb {R}^{\\vert R \\vert \\times (8d_p + d_h)}$ and a bias vector $\\mathbf {b}_r \\in \\mathbb {R}^{\\vert R \\vert }$..$\\mathbf {r}_t$ represents the normalized probabilities of the relation at time step $t$. The relation embedding vector $\\mathbf {z}_t$ is obtained using $\\mathrm {argmax}$ of $\\mathbf {r}_t$ and $\\mathbf {E}_r$. $\\mathbf {y}_t \\in \\mathbb {R}^{(8d_p + d_r)}$ is the vector representation of the tuple predicted at time step $t$. During training, we pass the embedding vector of the gold label relation in place of the predicted relation. So the $\\mathrm {argmax}$ function does not affect the back-propagation during training. The decoder stops the sequence generation process when the predicted relation is EOS. This is the classification network of the decoder..During inference, we select the start and end location of the two entities such that the product of the four pointer probabilities is maximized keeping the constraints that the two entities do not overlap with each other and $1 \\le b \\le e \\le n$ where $b$ and $e$ are the start and end location of the corresponding entities. We first choose the start and end location of entity 1 based on the maximum product of the corresponding start and end pointer probabilities. Then we find entity 2 in a similar way excluding the span of entity 1 to avoid overlap. The same procedure is repeated but this time we first find entity 2 followed by entity 1. We choose that pair of entities which gives the higher product of four pointer probabilities between these two choices. This model is referred to as PtrNetDecoding (PNDec) henceforth. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "28\n",
      "Question 1: What technique is used to prevent the decoder from predicting tokens not present in the current sentence, set of relations or special tokens during inference?\n",
      "\n",
      "Answer 1: A masking technique is used while applying the softmax operation at the projection layer. All words of the vocabulary except the current source sentence tokens, relation tokens, separator tokens (';', '|'), UNK and EOS tokens are masked (excluded) in the softmax operation. The corresponding value in the output scores is set to '-∞' and the softmax score is set to zero to ensure the copying of entities from the source sentence only. The UNK token is included in the softmax operation to generate new entities during inference.\n",
      "Question : for the text A target sequence $\\mathbf {T}$ is represented by only word embedding vectors of its tokens $\\mathbf {y}_0, \\mathbf {y}_1,....,\\mathbf {y}_m$ where $\\mathbf {y}_i \\in \\mathbb {R}^{d_w}$ is the embedding vector of the $i$th token and $m$ is the length of the target sequence. $\\mathbf {y}_0$ and $\\mathbf {y}_m$ represent the embedding vector of the SOS and EOS token respectively. The decoder generates one token at a time and stops when EOS is generated. We use an LSTM as the decoder and at time step $t$, the decoder takes the source sentence encoding ($\\mathbf {e}_t \\in \\mathbb {R}^{d_h}$) and the previous target word embedding ($\\mathbf {y}_{t-1}$) as the input and generates the hidden representation of the current token ($\\mathbf {h}_t^D \\in \\mathbb {R}^{d_h}$). The sentence encoding vector $\\mathbf {e}_t$ can be obtained using attention mechanism. $\\mathbf {h}_t^D$ is projected to the vocabulary $V$ using a linear layer with weight matrix $\\mathbf {W}_v \\in \\mathbb {R}^{\\vert V \\vert \\times d_h}$ and bias vector $\\mathbf {b}_v \\in \\mathbb {R}^{\\vert V \\vert }$ (projection layer)..$\\mathbf {o}_t$ represents the normalized scores of all the words in the embedding vocabulary at time step $t$. $\\mathbf {h}_{t-1}^D$ is the previous hidden state of the LSTM..The projection layer of the decoder maps the decoder output to the entire vocabulary. During training, we use the gold label target tokens directly. However, during inference, the decoder may predict a token from the vocabulary which is not present in the current sentence or the set of relations or the special tokens. To prevent this, we use a masking technique while applying the softmax operation at the projection layer. We mask (exclude) all words of the vocabulary except the current source sentence tokens, relation tokens, separator tokens (`;', `$\\vert $'), UNK, and EOS tokens in the softmax operation. To mask (exclude) some word from softmax, we set the corresponding value in $\\hat{\\mathbf {o}}_t$ at $-\\infty $ and the corresponding softmax score will be zero. This ensures the copying of entities from the source sentence only. We include the UNK token in the softmax operation to make sure that the model generates new entities during inference. If the decoder predicts an UNK token, we replace it with the corresponding source word which has the highest attention score. During inference, after decoding is finished, we extract all tuples based on the special tokens, remove duplicate tuples and tuples in which both entities are the same or tuples where the relation token is not from the relation set. This model is referred to as WordDecoding (WDec) henceforth. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "29\n",
      "What is the N-gram attention mechanism used in the model?\n",
      "\n",
      "The N-gram attention mechanism is used in the model for knowledge-base completion using distantly supervised data. The encoder uses the source tokens as its vocabulary and the decoder uses the entire Wikidata entity IDs and relation IDs as its vocabulary. The encoder takes the source sentence as input and the decoder outputs the two entity IDs and relation ID for every tuple. During training, it uses the mapping of entity names and their Wikidata IDs of the entire Wikidata for proper alignment.\n",
      "Question : for the text We compare our model with the following state-of-the-art joint entity and relation extraction models:.(1) SPTree BIBREF4: This is an end-to-end neural entity and relation extraction model using sequence LSTM and Tree LSTM. Sequence LSTM is used to identify all the entities first and then Tree LSTM is used to find the relation between all pairs of entities..(2) Tagging BIBREF5: This is a neural sequence tagging model which jointly extracts the entities and relations using an LSTM encoder and an LSTM decoder. They used a Cartesian product of entity tags and relation tags to encode the entity and relation information together. This model does not work when tuples have overlapping entities..(3) CopyR BIBREF6: This model uses an encoder-decoder approach for joint extraction of entities and relations. It copies only the last token of an entity from the source sentence. Their best performing multi-decoder model is trained with a fixed number of decoders where each decoder extracts one tuple..(4) HRL BIBREF11: This model uses a reinforcement learning (RL) algorithm with two levels of hierarchy for tuple extraction. A high-level RL finds the relation and a low-level RL identifies the two entities using a sequence tagging approach. This sequence tagging approach cannot always ensure extraction of exactly two entities..(5) GraphR BIBREF14: This model considers each token in a sentence as a node in a graph, and edges connecting the nodes as relations between them. They use graph convolution network (GCN) to predict the relations of every edge and then filter out some of the relations..(6) N-gram Attention BIBREF9: This model uses an encoder-decoder approach with N-gram attention mechanism for knowledge-base completion using distantly supervised data. The encoder uses the source tokens as its vocabulary and the decoder uses the entire Wikidata BIBREF15 entity IDs and relation IDs as its vocabulary. The encoder takes the source sentence as input and the decoder outputs the two entity IDs and relation ID for every tuple. During training, it uses the mapping of entity names and their Wikidata IDs of the entire Wikidata for proper alignment. Our task of extracting relation tuples with the raw entity names from a sentence is more challenging since entity names are not of fixed length. Our more generic approach is also helpful for extracting new entities which are not present in the existing knowledge bases such as Wikidata. We use their N-gram attention mechanism in our model to compare its performance with other attention models (Table TABREF17)..We use the same evaluation method used by BIBREF11 (BIBREF11) in their experiments. We consider the extracted tuples as a set and remove the duplicate tuples. An extracted tuple is considered as correct if the corresponding full entity names are correct and the relation is also correct. We report precision, recall, and F1 score for comparison. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "30\n",
      "What corpus is used for the experiments?\n",
      "The New York Times (NYT) corpus is used for the experiments.\n",
      "Question : for the text We focus on the task of extracting multiple tuples with overlapping entities from sentences. We choose the New York Times (NYT) corpus for our experiments. This corpus has multiple versions, and we choose the following two versions as their test dataset has significantly larger number of instances of multiple relation tuples with overlapping entities. (i) The first version is used by BIBREF6 (BIBREF6) (mentioned as NYT in their paper) and has 24 relations. We name this version as NYT24. (ii) The second version is used by BIBREF11 (BIBREF11) (mentioned as NYT10 in their paper) and has 29 relations. We name this version as NYT29. We select 10% of the original training data and use it as the validation dataset. The remaining 90% is used for training. We include statistics of the training and test datasets in Table TABREF11. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "31\n",
      "Question 1: What is the advantage of the ensemble mechanism used in the experiments?\n",
      "\n",
      "Answer 1: The ensemble mechanism used in the experiments increases precision significantly on both datasets with a small improvement in recall. In the ensemble scenario, compared to HRL, WDec and PNDec achieve higher F1 scores.\n",
      "Question : for the text Among the baselines, HRL achieves significantly higher F1 scores on the two datasets. We run their model and our models five times and report the median results in Table TABREF15. Scores of other baselines in Table TABREF15 are taken from previous published papers BIBREF6, BIBREF11, BIBREF14. Our WordDecoding (WDec) model achieves F1 scores that are $3.9\\%$ and $4.1\\%$ higher than HRL on the NYT29 and NYT24 datasets respectively. Similarly, our PtrNetDecoding (PNDec) model achieves F1 scores that are $3.0\\%$ and $1.3\\%$ higher than HRL on the NYT29 and NYT24 datasets respectively. We perform a statistical significance test (t-test) under a bootstrap pairing between HRL and our models and see that the higher F1 scores achieved by our models are statistically significant ($p < 0.001$). Next, we combine the outputs of five runs of our models and five runs of HRL to build ensemble models. For a test instance, we include those tuples which are extracted in the majority ($\\ge 3$) of the five runs. This ensemble mechanism increases the precision significantly on both datasets with a small improvement in recall as well. In the ensemble scenario, compared to HRL, WDec achieves $4.2\\%$ and $3.5\\%$ higher F1 scores and PNDec achieves $4.2\\%$ and $2.9\\%$ higher F1 scores on the NYT29 and NYT24 datasets respectively. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "32\n",
      "Question 1: What is the word embedding dimension used in the model?\n",
      "\n",
      "Answer 1: The word embedding dimension used in the model is $d_w=300$.\n",
      "Question : for the text We run the Word2Vec BIBREF12 tool on the NYT corpus to initialize the word embeddings. The character embeddings and relation embeddings are initialized randomly. All embeddings are updated during training. We set the word embedding dimension $d_w=300$, relation embedding dimension $d_r=300$, character embedding dimension $d_c=50$, and character-based word feature dimension $d_f=50$. To extract the character-based word feature vector, we set the CNN filter width at 3 and the maximum length of a word at 10. The hidden dimension $d_h$ of the decoder LSTM cell is set at 300 and the hidden dimension of the forward and the backward LSTM of the encoder is set at 150. The hidden dimension of the forward and backward LSTM of the pointer networks is set at $d_p=300$. The model is trained with mini-batch size of 32 and the network parameters are optimized using Adam BIBREF13. Dropout layers with a dropout rate fixed at $0.3$ are used in our network to avoid overfitting. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "33\n",
      "What are the three major challenges in relation tuple extraction using encoder-decoder models and how does the proposed approach address them?\n",
      "\n",
      "The three major challenges in relation tuple extraction using encoder-decoder models are: (i) extracting entities and relations together, (ii) handling multiple tuples with overlapping entities, and (iii) extracting exactly two entities of a tuple with their full names. The proposed approach addresses these challenges by proposing a new representation scheme for relation tuples, employing an encoder-decoder model with a pointer network-based decoding framework, and using a masking-based copy mechanism to extract the entities from the source sentence only. These approaches outperform all previous state-of-the-art models significantly and set a new benchmark on the New York Times datasets.\n",
      "Question : for the text Distantly-supervised information extraction systems extract relation tuples with a set of pre-defined relations from text. Traditionally, researchers BIBREF0, BIBREF1, BIBREF2 use pipeline approaches where a named entity recognition (NER) system is used to identify the entities in a sentence and then a classifier is used to find the relation (or no relation) between them. However, due to the complete separation of entity detection and relation classification, these models miss the interaction between multiple relation tuples present in a sentence..Recently, several neural network-based models BIBREF3, BIBREF4 were proposed to jointly extract entities and relations from a sentence. These models used a parameter-sharing mechanism to extract the entities and relations in the same network. But they still find the relations after identifying all the entities and do not fully capture the interaction among multiple tuples. BIBREF5 (BIBREF5) proposed a joint extraction model based on neural sequence tagging scheme. But their model could not extract tuples with overlapping entities in a sentence as it could not assign more than one tag to a word. BIBREF6 (BIBREF6) proposed a neural encoder-decoder model for extracting relation tuples with overlapping entities. However, they used a copy mechanism to copy only the last token of the entities, thus this model could not extract the full entity names. Also, their best performing model used a separate decoder to extract each tuple which limited the power of their model. This model was trained with a fixed number of decoders and could not extract tuples beyond that number during inference. Encoder-decoder models are powerful models and they are successful in many NLP tasks such as machine translation, sentence generation from structured data, and open information extraction..In this paper, we explore how encoder-decoder models can be used effectively for extracting relation tuples from sentences. There are three major challenges in this task: (i) The model should be able to extract entities and relations together. (ii) It should be able to extract multiple tuples with overlapping entities. (iii) It should be able to extract exactly two entities of a tuple with their full names. To address these challenges, we propose two novel approaches using encoder-decoder architecture. We first propose a new representation scheme for relation tuples (Table TABREF1) such that it can represent multiple tuples with overlapping entities and different lengths of entities in a simple way. We employ an encoder-decoder model where the decoder extracts one word at a time like machine translation models. At the end of sequence generation, due to the unique representation of the tuples, we can extract the tuples from the sequence of words. Although this model performs quite well, generating one word at a time is somewhat unnatural for this task. Each tuple has exactly two entities and one relation, and each entity appears as a continuous text span in a sentence. The most effective way to identify them is to find their start and end location in the sentence. Each relation tuple can then be represented using five items: start and end location of the two entities and the relation between them (see Table TABREF1). Keeping this in mind, we propose a pointer network-based decoding framework. This decoder consists of two pointer networks which find the start and end location of the two entities in a sentence, and a classification network which identifies the relation between them. At every time step of the decoding, this decoder extracts an entire relation tuple, not just a word. Experiments on the New York Times (NYT) datasets show that our approaches work effectively for this task and achieve state-of-the-art performance. To summarize, the contributions of this paper are as follows:.(1) We propose a new representation scheme for relation tuples such that an encoder-decoder model, which extracts one word at each time step, can still find multiple tuples with overlapping entities and tuples with multi-token entities from sentences. We also propose a masking-based copy mechanism to extract the entities from the source sentence only..(2) We propose a modification in the decoding framework with pointer networks to make the encoder-decoder model more suitable for this task. At every time step, this decoder extracts an entire relation tuple, not just a word. This new decoding framework helps in speeding up the training process and uses less resources (GPU memory). This will be an important factor when we move from sentence-level tuple extraction to document-level extraction..(3) Experiments on the NYT datasets show that our approaches outperform all the previous state-of-the-art models significantly and set a new benchmark on these datasets. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "34\n",
      "What is the use of encoder-decoder models in Natural Language Processing (NLP)?\n",
      "\n",
      "Answer 1: Encoder-decoder models have been used for various NLP applications such as neural machine translation, sentence generation from structured data, and open information extraction. In this context, researchers have also implemented pointer networks with encoder-decoder models to extract relation tuples from sentences.\n",
      "Question : for the text Traditionally, researchers BIBREF0, BIBREF1, BIBREF2, BIBREF16, BIBREF17, BIBREF18, BIBREF19, BIBREF20, BIBREF21, BIBREF22, BIBREF23, BIBREF24, BIBREF25 used a pipeline approach for relation tuple extraction where relations were identified using a classification network after all entities were detected. BIBREF26 (BIBREF26) used an encoder-decoder model to extract multiple relations present between two given entities..Recently, some researchers BIBREF3, BIBREF4, BIBREF27, BIBREF28 tried to bring these two tasks closer together by sharing their parameters and optimizing them together. BIBREF5 (BIBREF5) used a sequence tagging scheme to jointly extract the entities and relations. BIBREF6 (BIBREF6) proposed an encoder-decoder model with copy mechanism to extract relation tuples with overlapping entities. BIBREF11 (BIBREF11) proposed a joint extraction model based on reinforcement learning (RL). BIBREF14 (BIBREF14) used a graph convolution network (GCN) where they treated each token in a sentence as a node in a graph and edges were considered as relations. BIBREF9 (BIBREF9) used an N-gram attention mechanism with an encoder-decoder model for completion of knowledge bases using distant supervised data..Encoder-decoder models have been used for many NLP applications such as neural machine translation BIBREF29, BIBREF10, BIBREF30, sentence generation from structured data BIBREF31, BIBREF32, and open information extraction BIBREF33, BIBREF34. Pointer networks BIBREF35 have been used to extract a text span from text for tasks such as question answering BIBREF36, BIBREF37. For the first time, we use pointer networks with an encoder-decoder model to extract relation tuples from sentences. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "35\n",
      "Question 1: How are sentences divided into different classes based on the entities and relations present in them?\n",
      "\n",
      "Answer 1: Sentences are divided into three classes based on the overlap of entities in relation tuples. These classes are No Entity Overlap (NEO), Entity Pair Overlap (EPO), and Single Entity Overlap (SEO). The class of a sentence is determined by whether the tuples present in it share no entities, both entities, or exactly one entity.\n",
      "Question : for the text A relation tuple consists of two entities and a relation. Such tuples can be found in sentences where an entity is a text span in a sentence and a relation comes from a pre-defined set $R$. These tuples may share one or both entities among them. Based on this, we divide the sentences into three classes: (i) No Entity Overlap (NEO): A sentence in this class has one or more tuples, but they do not share any entities. (ii) Entity Pair Overlap (EPO): A sentence in this class has more than one tuple, and at least two tuples share both the entities in the same or reverse order. (iii) Single Entity Overlap (SEO): A sentence in this class has more than one tuple and at least two tuples share exactly one entity. It should be noted that a sentence can belong to both EPO and SEO classes. Our task is to extract all relation tuples present in a sentence. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "36\n",
      "Question 1: What institution supported this work with a grant number?\n",
      "Answer 1: The work was supported by the National Institutes of Health with the grant number 1R01GM095476.\n",
      "Question : for the text We would like to thank all of the authors who took the time to answer our citation ranking survey. This work is supported by National Institutes of Health with the grant number 1R01GM095476. The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "37\n",
      "Question 1: What are the four types of comparisons performed with the Google Scholar data?\n",
      "\n",
      "Answer 1: The four types of comparisons performed with the Google Scholar data are: (1) predicting Google Scholar's ranking using the gold standard model, (2) comparing Google Scholar's rankings with their own using the same feature set, (3) predicting the gold standard using a model trained on Google Scholar's rankings, and (4) comparing a model trained on the gold standard to predict the gold standard.\n",
      "Question : for the text We compare our system to a variety of baselines. (1) Rank by the number of times a citation is mentioned in the document. (2) Rank by the number of times the citation is cited in the literature (citation impact). (3) Rank using Google Scholar Related Articles. (4) Rank by the TF*IDF weighted cosine similarity. (5) Rank using a learning-to-rank model trained on text similarity rankings. The first two baseline systems are models where the values are ordered from highest to lowest to generate the ranking. The idea behind them is that the number of times a citation is mentioned in an article, or the citation impact may already be good indicators of their closeness. The text similarity model is trained using the same features and methods used by the annotation model, but trained using text similarity rankings instead of the author's judgments..We also compare our rankings to those found on the popular scientific article search engine Google Scholar. Google Scholar is a “black box” IR system: they do not release details about which features they are using and how they judge relevance of documents. Google Scholar provides a “Related Articles” feature for each document in its index that shows the top 100 related documents for each article. To compare our rankings, we search through these related documents and record the ranking at which each of the citations we selected appeared. We scale these rankings such that the lowest ranked article from Google Scholar has the highest relevance ranking in our set. If the cited document does not appear in the set, we set its relevance-ranking equal to one below the lowest relevance ranking found..Four comparisons are performed with the Google Scholar data. (1) We first train a model using our gold standard and see if we can predict Google Scholar's ranking. (2) We compare to a baseline of using Google Scholar's rankings to train and compare with their own rankings using our feature set. (3) Then we train a model using Google Scholar's rankings and try to predict our gold standard. (4) We compare it to the model trained on our gold standard to predict our gold standard. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "38\n",
      "Question 1: Why did the authors choose PLoS journals for collecting recent publications for their benchmark dataset? \n",
      "\n",
      "Answer 1: The authors chose PLoS journals because they cover a wide array of topics and their full text articles are available in XML format, which made it easier to extract relevant information. In addition, previous work in crowd-sourcing annotation showed that authors' willingness to participate in an unpaid annotation task declines with the age of publication, which made recent publications a better choice.\n",
      "Question : for the text In order to develop and evaluate ranking algorithms we need a benchmark dataset. However, to the best of our knowledge, we know of no openly available benchmark dataset for bibliographic query-by-document systems. We therefore created such a benchmark dataset..The creation of any benchmark dataset is a daunting labor-intensive task, and in particular, challenging in the scientific domain because one must master the technical jargon of a scientific article, and such experts are not easy to find when using traditional crowd-sourcing technologies (e.g., AMT). For our task, the ideal annotator for each of our articles are the authors themselves. The authors of a publication typically have a clear knowledge of the references they cite and their scientific importance to their publication, and therefore may be excellent judges for ranking the reference articles..Given the full text of a scientific publication, we want to rank its citations according to the author's judgments. We collected recent publications from the open-access PLoS journals and asked the authors to rank by closeness five citations we selected from their paper. PLoS articles were selected because its journals cover a wide array of topics and the full text articles are available in XML format. We selected the most recent publications as previous work in crowd-sourcing annotation shows that authors' willingness to participate in an unpaid annotation task declines with the age of publication ( BIBREF23 ). We then extracted the abstract, citations, full text, authors, and corresponding author email address from each document. The titles and abstracts of the citations were retrieved from PubMed, and the cosine similarity between the PLoS abstract and the citation's abstract was calculated. We selected the top five most similar abstracts using TF*IDF weighted cosine similarity, shuffled their order, and emailed them to the corresponding author for annotation. We believe that ranking five articles (rather than the entire collection of the references) is a more manageable task for an author compared to asking them to rank all references. Because the documents to be annotated were selected based on text similarity, they also represent a challenging baseline for models based on text-similarity features. In total 416 authors were contacted, and 92 responded (22% response rate). Two responses were removed from the dataset for incomplete annotation..We asked authors to rank documents by how “close to your work” they were. The definition of closeness was left to the discretion of the author. The dataset is composed of 90 annotated documents with 5 citations each ranked 1 to 5, where 1 is least relevant and 5 is most relevant for a total of 450 annotated citations. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "39\n",
      "What are some important features that are often overlooked in article citation recommendation?\n",
      "\n",
      "Answer 1: The number of times a document is referenced in the text and the literature, as well as recency and citation impact, are important features that are often overlooked in article citation recommendation. The more often a citation is mentioned in the text, the more likely it is to be important. Recency is also important, as newer citations are more likely to be directly important than older, more foundational citations. However, citation impact and age of the citation are both negatively correlated with rank, likely because they can be indicators of recency.\n",
      "Question : for the text We found that authors rank the references they cite substantially differently from rankings based on text-similarity. Our results show that decomposing a document into a set of features that is able to capture that difference is key. While text similarity is indeed important (as evidenced by the Similarity(a,a) feature in Table TABREF15 ), we also found that the number of times a document is referenced in the text and the number of times a document is referenced in the literature are also both important features (via feature selection). The more often a citation is mentioned in the text, the more likely it is to be important. This feature is often overlooked in article citation recommendation. We also found that recency is important: the age of the citation is negatively correlated with the rank. Newer citations are more likely to be directly important than older, more foundational citations. Additionally, the number of times a document is cited in the literature is negatively correlated with rank. This is likely due to highly cited documents being more foundational works; they may be older papers that are important to the field but not directly influential to the new work..The model trained using the author's judgments does significantly better than the model trained using the text-similarity-based judgments. An error analysis was performed to find out why some of the rankings disagreed with the author's annotations. We found that in some cases our features were unable to capture the relationship: for example a biomedical document applying a model developed in another field to the dataset may use very different language to describe the model than the citation. Previous work adopting topic models to query document search may prove useful for such cases..A small subset of features ended up performing as well as the full list of features. The number of times a citation was mentioned and the citation impact score in the literature ended up being two of the most important features. Indeed, without the citation-based features, the model performs as though it were trained with the text-similarity rankings. Feature engineering is a part of any learning-to-rank system, especially in domain-specific contexts. Citations are an integral feature of our dataset. For learning-to-rank to be applied to other datasets feature engineering must also occur to exploit the unique properties of those datasets. However, we show that combining the domain-specific features with more traditional text-based features does improve the model's scores over simply using the domain-specific features themselves..Interestingly, citation impact and age of the citation are both negatively correlated with rank. We hypothesize that this is because both measures can be indicators of recency: a new publication is more likely to be directly influenced by more recent work. Many other related search tools, however, treat the citation impact as a positive feature of relatedness: documents with a higher citation impact appear higher on the list of related articles than those with lower citation impacts. This may be the opposite of what the user actually desires..We also found that rankings from our text-similarity based IR system or Google Scholar's IR system were unable to rank documents by the authors' annotations as well as our system. In one sense, this is reasonable: the rankings coming from these systems were from a different system than the author annotations. However, in domain-specific IR, domain experts are the best judges. We built a system that exploits these expert judgments. The text similarity and Google Scholar models were able to do this to some extent, performing above the random baseline, but not on the level of our model..Additionally, we observe that NDCG may not be the most appropriate measure for comparing short ranked lists where all of the documents are relevant to some degree. NDCG gives a lot of credit to relevant documents that occur in the highest ranks. However, all of the documents here are relevant, just to varying degrees. Thus, NDCG does not seem to be the most appropriate measure, as is evident in our scores. The correlation coefficients from Kendall's INLINEFORM0 and INLINEFORM1 seem to be far more appropriate for this case, as they are not concerned with relevance, only ranking..One limitation of our work is that we selected a small set of references based on their similarities to the article that cites them. Ideally, we would have had authors rank all of their citations for us, but this would have been a daunting task for authors to perform. We chose to use the Google Scholar dataset in order to attempt to mitigate this: we obtain a ranking for the set of references from a system that is also ranking many other documents. The five citations selected by TF*IDF weighted cosine similarity represent a “hard” gold standard: we are attempting to rank documents that are known to all be relevant by their nature, and have high similarity with the text. Additionally, there are plethora of other, more expensive features we could explore to improve the model. Citation network features, phrasal concepts, and topic models could all be used to help improve our results, at the cost of computational complexity..We have developed a model for fast related-document ranking based on crowd-sourced data. The model, data, and data collection software are all publicly available and can easily be used in future applications as an automatic search to help users find the most important citations given a particular document. The experimental setup is portable to other datasets with some feature engineering. We were able to identify that several domain-specific features were crucial to our model, and that we were able to improve on the results of simply using those features alone by adding more traditional features..Query-by-document is a complicated and challenging task. We provide an approach with an easily obtained dataset and a computationally inexpensive model. By working with biomedical researchers we were able to build a system that ranks documents in a quantitatively different way than previous systems, and to provide a tool that helps researchers find related documents. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "40\n",
      "What is NDCG and how is it calculated?\n",
      "\n",
      "NDCG is a measure used to compare a list of estimated document relevance judgments with a list of known judgments. It is calculated by first calculating a ranking's Discounted Cumulative Gain (DCG) and then normalizing it using the Ideal DCG (IDCG). The NDCG value ranges from 0 to 1, with 0 meaning that no relevant documents were retrieved and 1 meaning that the relevant documents were retrieved and in the correct order of their relevance judgments.\n",
      "Question : for the text Normalized Discounted Cumulative Gain (NDCG) is a common measure for comparing a list of estimated document relevance judgments with a list of known judgments ( BIBREF28 ). To calculate NDCG we first calculate a ranking's Discounted Cumulative Gain (DCG) as: DISPLAYFORM0 .where rel INLINEFORM0 is the relevance judgment at position INLINEFORM1 . Intuitively, DCG penalizes retrieval of documents that are not relevant (rel INLINEFORM2 ). However, DCG is an unbounded value. In order to compare the DCG between two models, we must normalize it. To do this, we use the ideal DCG (IDCG), i.e., the maximum possible DCG given the relevance judgments. The maximum possible DCG occurs when the relevance judgments are in the correct order. DISPLAYFORM0 .The NDCG value is in the range of 0 to 1, where 0 means that no relevant documents were retrieved, and 1 means that the relevant documents were retrieved and in the correct order of their relevance judgments..Kendall's INLINEFORM0 is a measure of the correlation between two ranked lists. It compares the number of concordant pairs with the number of discordant pairs between each list. A concordant pair is defined over two observations INLINEFORM1 and INLINEFORM2 . If INLINEFORM3 and INLINEFORM4 , then the pair at indices INLINEFORM5 is concordant, that is, the ranking at INLINEFORM6 in both ranking sets INLINEFORM7 and INLINEFORM8 agree with each other. Similarly, a pair INLINEFORM9 is discordant if INLINEFORM10 and INLINEFORM11 or INLINEFORM12 and INLINEFORM13 . Kendall's INLINEFORM14 is then defined as: DISPLAYFORM0 .where C is the number of concordant pairs, D is the number of discordant pairs, and the denominator represents the total number of possible pairs. Thus, Kendall's INLINEFORM0 falls in the range of INLINEFORM1 , where -1 means that the ranked lists are perfectly negatively correlated, 0 means that they are not significantly correlated, and 1 means that the ranked lists are perfectly correlated. One downside of this measure is that it does not take into account where in the ranked list an error occurs. Information retrieval, in general, cares more about errors near the top of the list rather than errors near the bottom of the list..Average-Precision INLINEFORM0 ( BIBREF29 ) (or INLINEFORM1 ) extends on Kendall's INLINEFORM2 by incorporating the position of errors. If an error occurs near the top of the list, then that is penalized heavier than an error occurring at the bottom of the list. To achieve this, INLINEFORM3 incorporates ideas from the popular Average Precision measure, were we calculate the precision at each index of the list and then average them together. INLINEFORM4 is defined as: DISPLAYFORM0 .Intuitively, if an error occurs at the top of the list, then that error is propagated into each iteration of the summation, meaning that it's penalty is added multiple times. INLINEFORM0 's range is between -1 and 1, where -1 means the lists are perfectly negatively correlated, 0 means that they are not significantly correlated, and 1 means that they are perfectly correlated. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "41\n",
      "Question 1: What are the four types of features used to represent the relationship between a published article and its citation?\n",
      "\n",
      "Answer 1: The four types of features used are text similarity, citation count and location, age of the citation, and citation impact (the number of times the citation has appeared in the literature).\n",
      "Question : for the text Each citation is turned into a feature vector representing the relationship between the published article and the citation. Four types of features are used: text similarity, citation count and location, age of the citation, and the number of times the citation has appeared in the literature (citation impact). Text similarity features measure the similarity of the words used in different parts of the document. In this work, we calculate the similarity between a document INLINEFORM0 and a document it cites INLINEFORM1 by transforming the their text into term vectors. For example, to calculate the similarity of the abstracts between INLINEFORM2 and INLINEFORM3 we transform the abstracts into two term vectors, INLINEFORM4 and INLINEFORM5 . The length of each of the term vectors is INLINEFORM6 . We then weight each word by its Term-frequency * Inverse-document frequency (TF*IDF) weight. TF*IDF is a technique to give higher weight to words that appear frequently in a document but infrequently in the corpus. Term frequency is simply the number of times that a word INLINEFORM7 appears in a document. Inverse-document frequency is the logarithmically-scaled fraction of documents in the corpus in which the word INLINEFORM8 appears. Or, more specifically: INLINEFORM9 .where INLINEFORM0 is the total number of documents in the corpus, and the denominator is the number of documents in which a term INLINEFORM1 appears in the corpus INLINEFORM2 . Then, TF*IDF is defined as: INLINEFORM3 .where INLINEFORM0 is a term, INLINEFORM1 is the document, and INLINEFORM2 is the corpus. For example, the word “the” may appear often in a document, but because it also appears in almost every document in the corpus it is not useful for calculating similarity, thus it receives a very low weight. However, a word such as “neurogenesis” may appear often in a document, but does not appear frequently in the corpus, and so it receives a high weight. The similarity between term vectors is then calculated using cosine similarity: INLINEFORM3 .where INLINEFORM0 and INLINEFORM1 are two term vectors. The cosine similarity is a measure of the angle between the two vectors. The smaller the angle between the two vectors, i.e., the more similar they are, then the closer the value is to 1. Conversely, the more dissimilar the vectors, the closer the cosine similarity is to 0..We calculate the text similarity between several different sections of the document INLINEFORM0 and the document it cites INLINEFORM1 . From the citing article INLINEFORM2 , we use the title, full text, abstract, the combined discussion/conclusion sections, and the 10 words on either side of the place in the document where the actual citation occurs. From the document it cites INLINEFORM3 we only use the title and the abstract due to limited availability of the full text. In this work we combine the discussion and conclusion sections of each document because some documents have only a conclusion section, others have only a discussion, and some have both. The similarity between each of these sections from the two documents is calculated and used as features in the model..The age of the citation may be relevant to its importance. As a citation ages, we hypothesize that it is more likely to become a “foundational” citation rather than one that directly influenced the development of the article. Therefore more recent citations may be more likely relevant to the article. Similarly, “citation impact”, that is, the number of times a citation has appeared in the literature (as measured by Google Scholar) may be an indicator of whether or not an article is foundational rather than directly related. We hypothesize that the fewer times an article is cited in the literature, the more impact it had on the article at hand..We also keep track of the number of times a citation is mentioned in both the full text and discussion/conclusion sections. We hypothesize that if a citation is mentioned multiple times, it is more important than citations that are mentioned only once. Further, citations that appear in the discussion/conclusion sections are more likely to be crucial to understanding the results. We normalize the counts of the citations by the total number of citations in that section. In total we select 15 features, shown in Table TABREF15 . The features are normalized within each document so that each of citation features is on a scale from 0 to 1, and are evenly distributed within that range. This is done because some of the features (such as years since citation) are unbounded. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "42\n",
      "Question 1: What is the purpose of Forward feature selection? \n",
      "\n",
      "Answer 1: The purpose of Forward feature selection is to iteratively test and select the most powerful features in a model, allowing for exploration of combinations and optimal number of features.\n",
      "Question : for the text Forward feature selection was performed by iteratively testing each feature one at a time. The highest performing feature is kept in the model, and another sweep is done over the remaining features. This continues until all features have been selected. This approach allows us to explore the effect of combinations of features and the effect of having too many or too few features. It also allows us to evaluate which features and combinations of features are the most powerful. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "43\n",
      "Question 1: What is the proposed approach for determining relatedness between research articles in bibliographic retrieval?\n",
      "\n",
      "Answer 1: The proposed approach is to build a model that infers relatedness from the authors' judgments rather than measuring relatedness by text-similarity measures. The authors were asked to rank documents by \"closeness\" to their work, and this subjective ranking was used to determine relatedness in the model.\n",
      "Question : for the text The number of biomedical research papers published has increased dramatically in recent years. As of October, 2016, PubMed houses over 26 million citations, with almost 1 million from the first 3 quarters of 2016 alone . It has become impossible for any one person to actually read all of the work being published. We require tools to help us determine which research articles would be most informative and related to a particular question or document. For example, a common task when reading articles is to find articles that are most related to another. Major research search engines offer such a “related articles” feature. However, we propose that instead of measuring relatedness by text-similarity measures, we build a model that is able to infer relatedness from the authors' judgments.. BIBREF0 consider two kinds of queries important to bibliographic information retrieval: the first is a search query written by the user and the second is a request for documents most similar to a document already judged relevant by the user. Such a query-by-document (or query-by-example) system has been implemented in the de facto scientific search engine PubMed—called Related Citation Search. BIBREF1 show that 19% of all PubMed searches performed by users have at least one click on a related article. Google Scholar provides a similar Related Articles system. Outside of bibliographic retrieval, query-by-document systems are commonly used for patent retrieval, Internet search, and plagiarism detection, amongst others. Most work in the area of query-by-document uses text-based similarity measures ( BIBREF2 , BIBREF3 , BIBREF4 ). However, scientific research is hypothesis driven and therefore we question whether text-based similarity alone is the best model for bibliographic retrieval. In this study we asked authors to rank documents by “closeness” to their work. The definition of “closeness” was left for the authors to interpret, as the goal is to model which documents the authors subjectively feel are closest to their own. Throughout the paper we will use “closeness” and “relatedness” interchangeably..We found that researchers' ranking by closeness differs significantly from the ranking provided by a traditional IR system. Our contributions are three fold:.The principal ranking algorithms of query-by-document in bibliographic information retrieval rely mainly on text similarity measures ( BIBREF1 , BIBREF0 ). For example, the foundational work of BIBREF0 introduced the concept of a “document neighborhood” in which they pre-compute a text-similarity based distance between each pair of documents. When a user issues a query, first an initial set of related documents is retrieved. Then, the neighbors of each of those documents is retrieved, i.e., documents with the highest text similarity to those in the initial set. In a later work, BIBREF1 develop the PMRA algorithm for PubMed related article search. PMRA is an unsupervised probabilistic topic model that is trained to model “relatedness” between documents. BIBREF5 introduce the competing algorithm Find-Similar for this task, treating the full text of documents as a query and selecting related documents from the results..Outside bibliographic IR, prior work in query-by-document includes patent retrieval ( BIBREF6 , BIBREF3 ), finding related documents given a manuscript ( BIBREF1 , BIBREF7 ), and web page search ( BIBREF8 , BIBREF9 ). Much of the work focuses on generating shorter queries from the lengthy document. For example, noun-phrase extraction has been used for extracting short, descriptive phrases from the original lengthy text ( BIBREF10 ). Topic models have been used to distill a document into a set of topics used to form query ( BIBREF11 ). BIBREF6 generated queries using the top TF*IDF weighted terms in each document. BIBREF4 suggested extracting phrasal concepts from a document, which are then used to generate queries. BIBREF2 combined query extraction and pseudo-relevance feedback for patent retrieval. BIBREF9 employ supervised machine learning model (i.e., Conditional Random Fields) ( BIBREF12 ) for query generation. BIBREF13 explored ontology to identify chemical concepts for queries..There are also many biomedical-document specific search engines available. Many information retrieval systems focus on question answering systems such as those developed for the TREC Genomics Track ( BIBREF14 ) or BioASQ Question-Answer ( BIBREF15 ) competitions. Systems designed for question-answering use a combination of natural language processing techniques to identify biomedical entities, and then information retrieval systems to extract relevant answers to questions. Systems like those detailed in BIBREF16 can provide answers to yes/no biomedical questions with high precision. However what we propose differs from these systems in a fundamental way: given a specific document, suggest the most important documents that are related to it..The body of work most related to ours is that of citation recommendation. The goal of citation recommendation is to suggest a small number of publications that can be used as high quality references for a particular article ( BIBREF17 , BIBREF1 ). Topic models have been used to rank articles based on the similarity of latent topic distribution ( BIBREF11 , BIBREF18 , BIBREF1 ). These models attempt to decompose a document into a few important keywords. Specifically, these models attempt to find a latent vector representation of a document that has a much smaller dimensionality than the document itself and compare the reduced dimension vectors..Citation networks have also been explored for ranking articles by importance, i.e., authority ( BIBREF19 , BIBREF20 ). BIBREF17 introduced heterogeneous network models, called meta-path based models, to incorporate venues (the conference where a paper is published) and content (the term which links two articles, for citation recommendation). Another highly relevant work is BIBREF8 who decomposed a document to represent it with a compact vector, which is then used to measure the similarity with other documents. Note that we exclude the work of context-aware recommendation, which analyze each citation's local context, which is typically short and does not represent a full document..One of the key contributions of our study is an innovative approach for automatically generating a query-by-document gold standard. Crowd-sourcing has generated large databases, including Wikipedia and Freebase. Recently, BIBREF21 concluded that unpaid participants performed better than paid participants for question answering. They attribute this to unpaid participants being more intrinsically motivated than the paid test takers: they performed the task for fun and already had knowledge about the subject being tested. In contrast, another study, BIBREF22 , compared unpaid workers found through Google Adwords (GA) to paid workers found through Amazon Mechanical Turk (AMT). They found that the paid participants from AMT outperform the unpaid ones. This is attributed to the paid workers being more willing to look up information they didn't know. In the bibliographic domain, authors of scientific publications have contributed annotations ( BIBREF23 ). They found that authors are more willing to annotate their own publications ( BIBREF23 ) than to annotate other publications ( BIBREF24 ) even though they are paid. In this work, our annotated dataset was created by the unpaid authors of the articles. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "44\n",
      "Question 1: What is learning-to-rank and how does it re-order the results in a search engine query?\n",
      "\n",
      "Answer 1: Learning-to-rank is a technique that reorders the results returned from a search engine query. It uses algorithms to rank the documents based on their relevance so that the most relevant documents appear at the top of the list. There are three basic types of learning-to-rank algorithms: point-wise, pair-wise, and list-wise. SVMRank is a commonly used point-wise learning-to-rank algorithm that assigns scores to each document, which can be used to rank the documents.\n",
      "Question : for the text Learning-to-rank is a technique for reordering the results returned from a search engine query. Generally, the initial query to a search engine is concerned more with recall than precision: the goal is to obtain a subset of potentially related documents from the corpus. Then, given this set of potentially related documents, learning-to-rank algorithms reorder the documents such that the most relevant documents appear at the top of the list. This process is illustrated in Figure FIGREF6 ..There are three basic types of learning-to-rank algorithms: point-wise, pair-wise, and list-wise. Point-wise algorithms assign a score to each retrieved document and rank them by their scores. Pair-wise algorithms turn learning-to-rank into a binary classification problem, obtaining a ranking by comparing each individual pair of documents. List-wise algorithms try to optimize an evaluation parameter over all queries in the dataset..Support Vector Machine (SVM) ( BIBREF25 ) is a commonly used supervised classification algorithm that has shown good performance over a range of tasks. SVM can be thought of as a binary linear classifier where the goal is to maximize the size of the gap between the class-separating line and the points on either side of the line. This helps avoid over-fitting on the training data. SVMRank is a modification to SVM that assigns scores to each data point and allows the results to be ranked ( BIBREF26 ). We use SVMRank in the experiments below. SVMRank has previously been used in the task of document retrieval in ( BIBREF27 ) for a more traditional short query task and has been shown to be a top-performing system for ranking..SVMRank is a point-wise learning-to-rank algorithm that returns scores for each document. We rank the documents by these scores. It is possible that sometimes two documents will have the same score, resulting in a tie. In this case, we give both documents the same rank, and then leave a gap in the ranking. For example, if documents 2 and 3 are tied, their ranked list will be [5, 3, 3, 2, 1]..Models are trained by randomly splitting the dataset into 70% training data and 30% test data. We apply a random sub-sampling approach where the dataset is randomly split, trained, and tested 100 times due to the relatively small size of the data. A model is learned for each split and a ranking is produced for each annotated document..We test three different supervised models. The first supervised model uses only text similarity features, the second model uses all of the features, and the third model runs forward feature selection to select the best performing combination of features. We also test using two different models trained on two different datasets: one trained using the gold standard annotations, and another trained using the judgments based on text similarity that were used to select the citations to give to the authors..We tested several different learning to rank algorithms for this work. We found in preliminary testing that SVMRank had the best performance, so it will be used in the following experiments. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "45\n",
      "Question: What is the best performing text similarity feature according to the study?\n",
      "\n",
      "Answer: The best performing text similarity feature according to the study is the similarity between the abstract of the annotated document and the abstract of the cited document.\n",
      "Question : for the text We first compare our gold standard to the baselines. A random baseline is provided for reference. Because all of the documents that we rank are relevant, NDCG will be fairly high simply by chance. We find that the number of times a document is mentioned in the annotated document is significantly better than the random baseline or the citation impact. The more times a document is mentioned in a paper, the more likely the author was to annotate it as important. Interestingly, we see a negative correlation with the citation impact. The more times a document is mentioned in the literature, the less likely it is to be important. These results are shown in Table TABREF14 ..Next we rank the raw values of the features and compare them to our gold standard to obtain a baseline (Table TABREF15 ). The best performing text similarity feature is the similarity between the abstract of the annotated document and the abstract of the cited document. However, the number of times that a cited document is mentioned in the text of the annotated document are also high-scoring features, especially in the INLINEFORM0 correlation coefficient. These results indicate that text similarity alone may not be a good measure for judging the rank of a document..Next we test three different feature sets for our supervised learning-to-rank models. The model using only the text similarity features performs poorly: NDCG stays at baseline and the correlation measures are low. Models that incorporate information about the age, number of times a cited document was referenced, and the citation impact of that document in addition to the text similarity features significantly outperformed models that used only text similarity features INLINEFORM0 . Because INLINEFORM1 takes into account the position in the ranking of the errors, this indicates that the All Features model was able to better correctly place highly ranked documents above lower ranked ones. Similarly, because Kendall's INLINEFORM2 is an overall measure of correlation that does not take into account the position of errors, the higher value here means that more rankings were correctly placed. Interestingly, feature selection (which is optimized for NDCG) does not outperform the model using all of the features in terms of our correlation measures. The features chosen during forward feature selection are (1) the citation impact, (2) number of mentions in the full text, (3) text similarity between the annotated document's title and the referenced document's abstract, (4) the text similarity between the annotated document's discussion/conclusion section and the referenced document's title. These results are shown in Table TABREF16 . The models trained on the text similarity judgments perform worse than the models trained on the annotated data. However, in terms of both NDCG and the correlation measures, they perform significantly better than the random baseline..Next we compare our model to Google Scholar's rankings. Using the ranking collected from Google Scholar, we build a training set to try to predict our authors' rankings. We find that Google Scholar performs similarly to the text-only features model. This indicates that the rankings we obtained from the authors are substantially different than the rankings that Google Scholar provides. Results appear in Table TABREF17 . generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "46\n",
      "Question 1: Who supported Amir Hussain and Ahsan Adeel?\n",
      "Answer 1: The UK Engineering and Physical Sciences Research Council (EPSRC) supported Amir Hussain and Ahsan Adeel via grant No. EP/M026981/1.\n",
      "Question : for the text Amir Hussain and Ahsan Adeel were supported by the UK Engineering and Physical Sciences Research Council (EPSRC) grant No.EP/M026981/1.. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "47\n",
      "Question 1: What is the focus of the sentiment analysis models developed in this work?\n",
      "\n",
      "Answer 1: The focus of the sentiment analysis models developed in this work is on the Persian language, specifically for Persian movie reviews. Two deep learning models (deep autoencoders and deep CNNs) were developed and compared with the state-of-the-art shallow MLP based machine learning model.\n",
      "Question : for the text Sentiment analysis has been used extensively for a wide of range of real-world applications, ranging from product reviews, surveys feedback, to business intelligence, and operational improvements. However, the majority of research efforts are devoted to English-language only, where information of great importance is also available in other languages. In this work, we focus on developing sentiment analysis models for Persian language, specifically for Persian movie reviews. Two deep learning models (deep autoencoders and deep CNNs) are developed and compared with the the state-of-the-art shallow MLP based machine learning model. Simulations results revealed the outperformance of our proposed CNN model over autoencoders and MLP. In future, we intend to exploit more advanced deep learning models such as Long Short-Term Memory (LSTM) and LSTM-CNNs to further evaluate the performance of our developed novel Persian dataset. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "48\n",
      "What is the aim of sentiment analysis and how can it be performed?\n",
      "\n",
      "The aim of sentiment analysis is to automatically process large amounts of data and classify text into positive or negative sentiments. It can be performed at two levels: at the document level or at the sentence level. At the document level, it is used to classify the sentiment expressed in the document (positive or negative), whereas at the sentence level, it is used to identify the sentiments expressed only in the sentence under analysis.\n",
      "Question : for the text In recent years, social media, forums, blogs and other forms of online communication tools have radically affected everyday life, especially how people express their opinions and comments. The extraction of useful information (such as people's opinion about companies brand) from the huge amount of unstructured data is vital for most companies and organizations BIBREF0 . The product reviews are important for business owners as they can take business decision accordingly to automatically classify user’s opinions towards products and services. The application of sentiment analysis is not limited to product or movie reviews but can be applied to different fields such as news, politics, sport etc. For example, in online political debates, the sentiment analysis can be used to identify people's opinions on a certain election candidate or political parties BIBREF1 BIBREF2 BIBREF3 . In this context, sentiment analysis has been widely used in different languages by using traditional and advanced machine learning techniques. However, limited research has been conducted to develop models for the Persian language..The sentiment analysis is a method to automatically process large amounts of data and classify text into positive or negative sentiments) BIBREF4 BIBREF5 . Sentiment analysis can be performed at two levels: at the document level or at the sentence level. At document level it is used to classify the sentiment expressed in the document (positive or negative), whereas, at sentence level is used to identify the sentiments expressed only in the sentence under analysis BIBREF6 BIBREF7 ..In the literature, deep learning based automated feature extraction has been shown to outperform state-of-the-art manual feature engineering based classifiers such as Support Vector Machine (SVM), Naive Bayes (NB) or Multilayer Perceptron (MLP) etc. One of the important techniques in deep learning is the autoencoder that generally involves reducing the number of feature dimensions under consideration. The aim of dimensionality reduction is to obtain a set of principal variables to improve the performance of the approach. Similarly, CNNs have been proven to be very effective in sentiment analysis. However, little work has been carried out to exploit deep learning based feature representation for Persian sentiment analysis BIBREF8 BIBREF9 . In this paper, we present two deep learning models (deep autoencoders and CNNs) for Persian sentiment analysis. The obtained deep learning results are compared with MLP..The rest of the paper is organized as follows: Section 2 presents related work. Section 3 presents methodology and experimental results. Finally, section 4 concludes this paper. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "49\n",
      "What techniques were used to pre-process the collected Persian movie reviews dataset?\n",
      "\n",
      "The collected Persian movie reviews dataset was pre-processed using tokenisation, normalisation, and stemming techniques. Tokenisation involves converting sentences into single words or tokens, whereas normalisation is used to convert words containing numbers into their normal forms. Finally, stemming is the process of converting words into their root form. Fasttext was used to convert words into vectors before classification.\n",
      "Question : for the text The novel dataset used in this work was collected manually and includes Persian movie reviews from 2014 to 2016. A subset of dataset was used to train the neural network (60% training dataset) and rest of the data (40%) was used to test and validate the performance of the trained neural network (testing set (30%), validation set (10%)). There are two types of labels in the dataset: positive or negative. The reviews were manually annotated by three native Persian speakers aged between 30 and 50 years old..After data collection, the corpus was pre-processed using tokenisation, normalisation and stemming techniques. The process of converting sentences into single word or token is called tokenisation. For example, \"The movie is great\" is changed to \"The\", \"movie\", \"is\", \"great\" BIBREF21 . There are some words which contain numbers. For example, \"great\" is written as \"gr8\" or \"gooood\" as written as \"good\" . The normalisation is used to convert these words into normal forms BIBREF22 . The process of converting words into their root is called stemming. For example, going was changed to go BIBREF23 . Words were converted into vectors. The fasttext was used to convert each word into 300-dimensions vectors. Fasttext is a library for text classification and representation BIBREF24 BIBREF25 BIBREF9 ..For classification, MLP, autoencoders and CNNs have been used. Fig. 1. depicts the modelled MLP architectures. MLP classifer was trained for 100 iterations BIBREF26 . Fig. 2. depicts the modelled autoencoder architecture. Autoencoder is a feed-forward deep neural network with unsupervised learning and it is used for dimensionality reduction. The autoencoder consists of input, output and hidden layers. Autoencoder is used to compress the input into a latent-space and then the output is reconstructed BIBREF27 BIBREF28 BIBREF29 . The exploited autoencoder model is depcited in Fig. 1. The autoencoder consists of one input layer three hidden layers (1500, 512, 1500) and an output layer. Convolutional Neural Networks contains three layers (input, hidden and output layer). The hidden layer consists of convolutional layers, pooling layers, fully connected layers and normalisation layer. The INLINEFORM0 is denotes the hidden neurons of j, with bias of INLINEFORM1 , is a weight sum over continuous visible nodes v which is given by: DISPLAYFORM0 .The modelled CNN architecture is depicted in Fig. 3 BIBREF29 BIBREF28 . For CNN modelling, each utterance was represented as a concatenation vector of constituent words. The network has total 11 layers: 4 convolution layers, 4 max pooling and 3 fully connected layers. Convolution layers have filters of size 2 and with 15 feature maps. Each convolution layer is followed by a max polling layer with window size 2. The last max pooling layer is followed by fully connected layers of size 5000, 500 and 4. For final layer, softmax activation is used..To evaluate the performance of the proposed approach, precision (1), recall (2), f-Measure (3), and prediction accuracy (4) have been used as a performance matrices. The experimental results are shown in Table 1, where it can be seen that autoencoders outperformed MLP and CNN outperformed autoencoders with the highest achieved accuracy of 82.6%. DISPLAYFORM0 DISPLAYFORM1 .where TP is denotes true positive, TN is true negative, FP is false positive, and FN is false negative. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "50\n",
      "Question 1: What is the advantage of the approach proposed by Zhang et al. in detecting polarity in Japanese movie reviews?\n",
      "\n",
      "Answer 1: The advantage of the approach proposed by Zhang et al. is that it is not depended on any language and could be used for various languages by applying different datasets. Additionally, the approach uses deep learning classifiers to detect polarity, which outperformed traditional SVM and Naive Bayes models in their experimental results.\n",
      "Question : for the text In the literature, extensive research has been carried out to model novel sentiment analysis models using both shallow and deep learning algorithms. For example, the authors in BIBREF10 proposed a novel deep learning approach for polarity detection in product reviews. The authors addressed two major limitations of stacked denoising of autoencoders, high computational cost and the lack of scalability of high dimensional features. Their experimental results showed the effectiveness of proposed autoencoders in achieving accuracy upto 87%. Zhai et al., BIBREF11 proposed a five layers autoencoder for learning the specific representation of textual data. The autoencoders are generalised using loss function and derived discriminative loss function from label information. The experimental results showed that the model outperformed bag of words, denoising autoencoders and other traditional methods, achieving accuracy rate up to 85% . Sun et al., BIBREF12 proposed a novel method to extract contextual information from text using a convolutional autoencoder architecture. The experimental results showed that the proposed model outperformed traditional SVM and Nave Bayes models, reporting accuracy of 83.1 %, 63.9% and 67.8% respectively..Su et al., BIBREF13 proposed an approach for a neural generative autoencoder for learning bilingual word embedding. The experimental results showed the effectiveness of their approach on English-Chinese, English-German, English-French and English-Spanish (75.36% accuracy). Kim et al., BIBREF14 proposed a method to capture the non-linear structure of data using CNN classifier. The experimental results showed the effectiveness of the method on the multi-domain dataset (movie reviews and product reviews). However, the disadvantage is only SVM and Naive Bayes classifiers are used to evaluate the performance of the method and deep learning classifiers are not exploited. Zhang et al., BIBREF15 proposed an approach using deep learning classifiers to detect polarity in Japanese movie reviews. The approach used denoising autoencoder and adapted to other domains such as product reviews. The advantage of the approach is not depended on any language and could be used for various languages by applying different datasets. AP et al., BIBREF16 proposed a CNN based model for cross-language learning of vectorial word representations that is coherent between two languages. The method is evaluated using English and German movie reviews dataset. The experimental results showed CNN (83.45% accuracy) outperformed as compared to SVM (65.25% accuracy)..Zhou et al., BIBREF17 proposed an autoencoder architecture constituting an LSTM-encoder and decoder in order to capture features in the text and reduce dimensionality of data. The LSTM encoder used the interactive scheme to go through the sequence of sentences and LSTM decoder reconstructed the vector of sentences. The model is evaluated using different datasets such as book reviews, DVD reviews, and music reviews, acquiring accuracy up to 81.05%, 81.06%, and 79.40% respectively. Mesnil et al., BIBREF18 proposed an approach using ensemble classification to detect polarity in the movie reviews. The authors combined several machine learning algorithms such as SVM, Naive Bayes and RNN to achieve better results, where autoencoders were used to reduce the dimensionality of features. The experimental results showed the combination of unigram, bigram and trigram features (91.87% accuracy) outperformed unigram (91.56% accuracy) and bigram (88.61% accuracy)..Scheible et al., BIBREF19 trained an approach using semi-supervised recursive autoencoder to detect polarity in movie reviews dataset, consisted of 5000 positive and 5000 negative sentiments. The experimental results demonstrated that the proposed approach successfully detected polarity in movie reviews dataset (83.13% accuracy) and outperformed standard SVM (68.36% accuracy) model. Dai et al., BIBREF20 developed an autoencoder to detect polarity in the text using deep learning classifier. The LSTM was trained on IMDB movie reviews dataset. The experimental results showed the outperformance of their proposed approach over SVM. In table 1 some of the autoencoder approaches are depicted. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "1\n",
      "Question 1: What was found to achieve the best performance in the study on tourist generation models?\n",
      "\n",
      "Answer 1: The model conditioned on a single observation was found to achieve the best performance in the study on tourist generation models.\n",
      "Question : for the text First, we investigate the sensitivity of tourist generation models to the trajectory length, finding that the model conditioned on a single observation (i.e. INLINEFORM0 ) achieves best performance. In the next subsection, we further analyze localization models from human utterances by investigating MASC and no-MASC models with increasing dialogue context. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "2\n",
      "Question 1: Why is it important to communicate both observations and actions in order to achieve high localization accuracy during a random walk?\n",
      "\n",
      "Answer 1: The upperbound for only communicating observations plateaus around 57% for random walks, whereas it exceeds 90% when we also take actions into account. This implies that it is essential to communicate a trajectory, including both observations and actions, in order to achieve high localization accuracy.\n",
      "Question : for the text paragraph4 0.1ex plus0.1ex minus.1ex-1em Task is not too easy The upper-bound on localization performance in Table TABREF32 suggest that communicating a single landmark observation is not sufficient for accurate localization of the tourist ( INLINEFORM0 35% accuracy). This is an important result for the full navigation task because the need for two-way communication disappears if localization is too easy; if the guide knows the exact location of the tourist it suffices to communicate a list of instructions, which is then executed by the tourist. The uncertainty in the tourist's location is what drives the dialogue between the two agents..paragraph4 0.1ex plus0.1ex minus.1ex-1em Importance of actions We observe that the upperbound for only communicating observations plateaus around 57% (even for INLINEFORM0 actions), whereas it exceeds 90% when we also take actions into account. This implies that, at least for random walks, it is essential to communicate a trajectory, including observations and actions, in order to achieve high localization accuracy. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "3\n",
      "Question 1: What is the \"No MASC\" model compared to the proposed MASC model in the experiments?\n",
      "\n",
      "Answer 1: The \"No MASC\" model is compared to the proposed MASC model in the experiments. The \"No MASC\" model uses an ordinary convolutional kernel to convolve the map embedding, while the MASC model predicts a convolution mask from the tourist message. The weights of this convolution are shared at each time step.\n",
      "Question : for the text To better analyze the performance of the models incorporating MASC, we compare against a no-MASC baseline in our experiments, as well as a prediction upper bound..paragraph4 0.1ex plus0.1ex minus.1ex-1em No MASC We compare the proposed MASC model with a model that does not include this mechanism. Whereas MASC predicts a convolution mask from the tourist message, the “No MASC” model uses INLINEFORM0 , the ordinary convolutional kernel to convolve the map embedding INLINEFORM1 to obtain INLINEFORM2 . We also share the weights of this convolution at each time step..paragraph4 0.1ex plus0.1ex minus.1ex-1em Prediction upper-bound Because we have access to the class-conditional likelihood INLINEFORM0 , we are able to compute the Bayes error rate (or irreducible error). No model (no matter how expressive) with any amount of data can ever obtain better localization accuracy as there are multiple locations consistent with the observations and actions. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "4\n",
      "Question 1: What is the Talk The Walk task and dataset?\n",
      "\n",
      "Answer 1: The Talk The Walk task and dataset consists of crowd-sourced dialogues in which two human annotators collaborate to navigate to target locations in the virtual streets of NYC.\n",
      "Question : for the text We introduced the Talk The Walk task and dataset, which consists of crowd-sourced dialogues in which two human annotators collaborate to navigate to target locations in the virtual streets of NYC. For the important localization sub-task, we proposed MASC—a novel grounding mechanism to learn state-transition from the tourist's message—and showed that it improves localization performance for emergent and natural language. We use the localization model to provide baseline numbers on the Talk The Walk task, in order to facilitate future research. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "5\n",
      "Question 1: How was the dataset collection carried out for this study?\n",
      "Answer 1: The dataset collection was crowd-sourced on Amazon Mechanical Turk (MTurk) using the MTurk interface of ParlAI BIBREF6. Workers were given detailed task instructions before starting their task and were paired at random to alternate between the tourist and guide roles.\n",
      "Question : for the text We crowd-sourced the collection of the dataset on Amazon Mechanical Turk (MTurk). We use the MTurk interface of ParlAI BIBREF6 to render 360 images via WebGL and dynamically display neighborhood maps with an HTML5 canvas. Detailed task instructions, which were also given to our workers before they started their task, are shown in Appendix SECREF15 . We paired Turkers at random and let them alternate between the tourist and guide role across different HITs. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "6\n",
      "Question 1: What is on the corner opposite the hotel?\n",
      "\n",
      "Answer 1: The shops are on the corners opposite the hotel.\n",
      "Question : for the text paragraph4 0.1ex plus0.1ex minus.1ex-1em Dataset split We split the full dataset by assigning entire 4x4 grids (independent of the target location) to the train, valid or test set. Specifically, we design the split such that the valid set contains at least one intersection (out of four) is not part of the train set. For the test set, all four intersections are novel. See our source code, available at URL ANONYMIZED, for more details on how this split is realized..paragraph4 0.1ex plus0.1ex minus.1ex-1em Example.Tourist: ACTION:TURNRIGHT ACTION:TURNRIGHT.Guide: Hello, what are you near?.Tourist: ACTION:TURNLEFT ACTION:TURNLEFT ACTION:TURNLEFT.Tourist: Hello, in front of me is a Brooks Brothers.Tourist: ACTION:TURNLEFT ACTION:FORWARD ACTION:TURNLEFT ACTION:TURNLEFT.Guide: Is that a shop or restaurant?.Tourist: ACTION:TURNLEFT.Tourist: It is a clothing shop..Tourist: ACTION:TURNLEFT.Guide: You need to go to the intersection in the northwest corner of the map.Tourist: ACTION:TURNLEFT.Tourist: There appears to be a bank behind me..Tourist: ACTION:TURNLEFT ACTION:TURNLEFT ACTION:TURNRIGHT ACTION:TURNRIGHT.Guide: Ok, turn left then go straight up that road.Tourist: ACTION:TURNLEFT ACTION:TURNLEFT ACTION:TURNLEFT ACTION:FORWARD ACTION:TURNRIGHT. ACTION:FORWARD ACTION:FORWARD ACTION:TURNLEFT ACTION:TURNLEFT ACTION:TURNLEFT.Guide: There should be shops on two of the corners but you. need to go to the corner without a shop..Tourist: ACTION:FORWARD ACTION:FORWARD ACTION:FORWARD ACTION:TURNLEFT ACTION:TURNLEFT.Guide: let me know when you get there..Tourist: on my left is Radio city Music hall.Tourist: ACTION:TURNLEFT ACTION:FORWARD ACTION:TURNLEFT ACTION:TURNRIGHT ACTION:TURNRIGHT.Tourist: I can't go straight any further..Guide: ok. turn so that the theater is on your right..Guide: then go straight.Tourist: That would be going back the way I came.Guide: yeah. I was looking at the wrong bank.Tourist: I'll notify when I am back at the brooks brothers, and the bank..Tourist: ACTION:TURNRIGHT.Guide: make a right when the bank is on your left.Tourist: ACTION:FORWARD ACTION:FORWARD ACTION:TURNRIGHT.Tourist: Making the right at the bank..Tourist: ACTION:FORWARD ACTION:FORWARD.Tourist: I can't go that way..Tourist: ACTION:TURNLEFT.Tourist: Bank is ahead of me on the right.Tourist: ACTION:FORWARD ACTION:FORWARD ACTION:TURNLEFT.Guide: turn around on that intersection.Tourist: I can only go to the left or back the way I just came..Tourist: ACTION:TURNLEFT.Guide: you're in the right place. do you see shops on the corners?.Guide: If you're on the corner with the bank, cross the street.Tourist: I'm back where I started by the shop and the bank..Tourist: ACTION:TURNRIGHT.Guide: on the same side of the street?.Tourist: crossing the street now.Tourist: ACTION:FORWARD ACTION:FORWARD ACTION:TURNLEFT.Tourist: there is an I love new york shop across the street on the left from me now.Tourist: ACTION:TURNRIGHT ACTION:FORWARD.Guide: ok. I'll see if it's right..Guide: EVALUATE_LOCATION.Guide: It's not right..Tourist: What should I be on the look for?.Tourist: ACTION:TURNRIGHT ACTION:TURNRIGHT ACTION:TURNRIGHT.Guide: There should be shops on two corners but you need to be on one of the corners. without the shop..Guide: Try the other corner..Tourist: this intersection has 2 shop corners and a bank corner.Guide: yes. that's what I see on the map..Tourist: should I go to the bank corner? or one of the shop corners?. or the blank corner (perhaps a hotel).Tourist: ACTION:TURNLEFT ACTION:TURNLEFT ACTION:TURNRIGHT ACTION:TURNRIGHT.Guide: Go to the one near the hotel. The map says the hotel is a little. further down but it might be a little off..Tourist: It's a big hotel it's possible..Tourist: ACTION:FORWARD ACTION:TURNLEFT ACTION:FORWARD ACTION:TURNRIGHT.Tourist: I'm on the hotel corner.Guide: EVALUATE_LOCATION generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "7\n",
      "Question 1: How does the Talk The Walk dataset compare to the Visual Dialog and GuessWhat datasets?\n",
      "\n",
      "Answer 1: While the Visual Dialog and GuessWhat datasets are larger, the Talk The Walk dialogs are significantly longer. On average, Turkers needed more than 62 acts to complete the task, whereas Visual Dialog requires 20 acts.\n",
      "Question : for the text The Talk The Walk dataset consists of over 10k successful dialogues—see Table FIGREF66 in the appendix for the dataset statistics split by neighborhood. Turkers successfully completed INLINEFORM0 of all finished tasks (we use this statistic as the human success rate). More than six hundred participants successfully completed at least one Talk The Walk HIT. Although the Visual Dialog BIBREF2 and GuessWhat BIBREF1 datasets are larger, the collected Talk The Walk dialogs are significantly longer. On average, Turkers needed more than 62 acts (i.e utterances and actions) before they successfully completed the task, whereas Visual Dialog requires 20 acts. The majority of acts comprise the tourist's actions, with on average more than 44 actions per dialogue. The guide produces roughly 9 utterances per dialogue, slightly more than the tourist's 8 utterances. Turkers use diverse discourse, with a vocabulary size of more than 10K (calculated over all successful dialogues). An example from the dataset is shown in Appendix SECREF14 . The dataset is available at https://github.com/facebookresearch/talkthewalk. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "8\n",
      "What is the effect of the MASC architecture on performance in tourist localization with emergent language?\n",
      "\n",
      "The MASC architecture significantly improves performance compared to models that do not include this mechanism, with MASC achieving 56.09% on the test set for INLINEFORM0 action and 69.85% for INLINEFORM1, while no-MASC models plateau at 43%.\n",
      "Question : for the text We first report the results for tourist localization with emergent language in Table TABREF32 ..paragraph4 0.1ex plus0.1ex minus.1ex-1em MASC improves performance The MASC architecture significantly improves performance compared to models that do not include this mechanism. For instance, for INLINEFORM0 action, MASC already achieves 56.09 % on the test set and this further increases to 69.85% for INLINEFORM1 . On the other hand, no-MASC models hit a plateau at 43%. In Appendix SECREF11 , we analyze learned MASC values, and show that communicated actions are often mapped to corresponding state-transitions..paragraph4 0.1ex plus0.1ex minus.1ex-1em Continuous vs discrete We observe similar performance for continuous and discrete emergent communication models, implying that a discrete communication channel is not a limiting factor for localization performance. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "9\n",
      "Question 1: What is the name of the algorithm provided for the evaluation of localization models on the full task?\n",
      "\n",
      "Answer 1: The name of the algorithm provided for the evaluation of localization models on the full task is SECREF12.\n",
      "Question : for the text We provide pseudo-code for evaluation of localization models on the full task in Algorithm SECREF12 , as well as results for all emergent communication models in Table TABREF55 .. INLINEFORM0 INLINEFORM1 . INLINEFORM0 take new action INLINEFORM1 INLINEFORM2 .Performance evaluation of location prediction model on full Talk The Walk setup generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "10\n",
      "What is the most important sub-task in the Talk The Walk challenge?\n",
      "The most important sub-task in the Talk The Walk challenge is localization. Without this capability, the guide cannot determine whether the tourist has reached the target location or not.\n",
      "Question : for the text We investigate the difficulty of the proposed task by establishing initial baselines. The final Talk The Walk task is challenging and encompasses several important sub-tasks, ranging from landmark recognition to tourist localization and natural language instruction-giving. Arguably the most important sub-task is localization: without such capabilities the guide can not tell whether the tourist reached the target location. In this work, we establish a minimal baseline for Talk The Walk by utilizing agents trained for localization. Specifically, we let trained tourist models undertake random walks, using the following protocol: at each step, the tourist communicates its observations and actions to the guide, who predicts the tourist's location. If the guide predicts that the tourist is at target, we evaluate its location. If successful, the task ends, otherwise we continue until there have been three wrong evaluations. The protocol is given as pseudo-code in Appendix SECREF12 . generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "11\n",
      "Question 1: What kind of optimizer is used for training the guide in the emergent communication models?\n",
      "\n",
      "Answer 1: The guide is trained with a cross entropy loss using the ADAM optimizer with default hyper-parameters.\n",
      "Question : for the text For the emergent communication models, we use an embedding size INLINEFORM0 . The natural language experiments use 128-dimensional word embeddings and a bidirectional RNN with 256 units. In all experiments, we train the guide with a cross entropy loss using the ADAM optimizer with default hyper-parameters BIBREF33 . We perform early stopping on the validation accuracy, and report the corresponding train, valid and test accuracy. We optimize the localization models with continuous, discrete and natural language communication channels for 200, 200, and 25 epochs, respectively. To facilitate further research on Talk The Walk, we make our code base for reproducing experiments publicly available at https://github.com/facebookresearch/talkthewalk. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "12\n",
      "What is the main goal of the Talk the Walk dataset task?\n",
      "\n",
      "The main goal of the Talk the Walk dataset task is for two agents, a \"guide\" and a \"tourist\" to interact with each other via natural language in order to achieve a common goal: having the tourist navigate towards the correct location. The agents need to work together through communication in order to successfully solve the task.\n",
      "Question : for the text As artificial intelligence plays an ever more prominent role in everyday human lives, it becomes increasingly important to enable machines to communicate via natural language—not only with humans, but also with each other. Learning algorithms for natural language understanding, such as in machine translation and reading comprehension, have progressed at an unprecedented rate in recent years, but still rely on static, large-scale, text-only datasets that lack crucial aspects of how humans understand and produce natural language. Namely, humans develop language capabilities by being embodied in an environment which they can perceive, manipulate and move around in; and by interacting with other humans. Hence, we argue that we should incorporate all three fundamental aspects of human language acquisition—perception, action and interactive communication—and develop a task and dataset to that effect..We introduce the Talk the Walk dataset, where the aim is for two agents, a “guide” and a “tourist”, to interact with each other via natural language in order to achieve a common goal: having the tourist navigate towards the correct location. The guide has access to a map and knows the target location, but does not know where the tourist is; the tourist has a 360-degree view of the world, but knows neither the target location on the map nor the way to it. The agents need to work together through communication in order to successfully solve the task. An example of the task is given in Figure FIGREF3 ..Grounded language learning has (re-)gained traction in the AI community, and much attention is currently devoted to virtual embodiment—the development of multi-agent communication tasks in virtual environments—which has been argued to be a viable strategy for acquiring natural language semantics BIBREF0 . Various related tasks have recently been introduced, but in each case with some limitations. Although visually grounded dialogue tasks BIBREF1 , BIBREF2 comprise perceptual grounding and multi-agent interaction, their agents are passive observers and do not act in the environment. By contrast, instruction-following tasks, such as VNL BIBREF3 , involve action and perception but lack natural language interaction with other agents. Furthermore, some of these works use simulated environments BIBREF4 and/or templated language BIBREF5 , which arguably oversimplifies real perception or natural language, respectively. See Table TABREF15 for a comparison..Talk The Walk is the first task to bring all three aspects together: perception for the tourist observing the world, action for the tourist to navigate through the environment, and interactive dialogue for the tourist and guide to work towards their common goal. To collect grounded dialogues, we constructed a virtual 2D grid environment by manually capturing 360-views of several neighborhoods in New York City (NYC). As the main focus of our task is on interactive dialogue, we limit the difficulty of the control problem by having the tourist navigating a 2D grid via discrete actions (turning left, turning right and moving forward). Our street view environment was integrated into ParlAI BIBREF6 and used to collect a large-scale dataset on Mechanical Turk involving human perception, action and communication..We argue that for artificial agents to solve this challenging problem, some fundamental architecture designs are missing, and our hope is that this task motivates their innovation. To that end, we focus on the task of localization and develop the novel Masked Attention for Spatial Convolutions (MASC) mechanism. To model the interaction between language and action, this architecture repeatedly conditions the spatial dimensions of a convolution on the communicated message sequence..This work makes the following contributions: 1) We present the first large scale dialogue dataset grounded in action and perception; 2) We introduce the MASC architecture for localization and show it yields improvements for both emergent and natural language; 4) Using localization models, we establish initial baselines on the full task; 5) We show that our best model exceeds human performance under the assumption of “perfect perception” and with a learned emergent communication protocol, and sets a non-trivial baseline with natural language. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "13\n",
      "What is the class distribution of landmarks in the Talk The Walk dataset? \n",
      "\n",
      "The class distribution of landmarks in the Talk The Walk dataset is imbalanced, with shops and restaurants as the most frequent landmarks and relatively few play fields and theaters. There are a total of 307 different landmarks divided among nine classes.\n",
      "Question : for the text While the guide has access to the landmark labels, the tourist needs to recognize these landmarks from raw perceptual information. In this section, we study landmark classification as a supervised learning problem to investigate the difficulty of perceptual grounding in Talk The Walk..The Talk The Walk dataset contains a total of 307 different landmarks divided among nine classes, see Figure FIGREF62 for how they are distributed. The class distribution is fairly imbalanced, with shops and restaurants as the most frequent landmarks and relatively few play fields and theaters. We treat landmark recognition as a multi-label classification problem as there can be multiple landmarks on a corner..For the task of landmark classification, we extract the relevant views of the 360 image from which a landmark is visible. Because landmarks are labeled to be on a specific corner of an intersection, we assume that they are visible from one of the orientations facing away from the intersection. For example, for a landmark on the northwest corner of an intersection, we extract views from both the north and west direction. The orientation-specific views are obtained by a planar projection of the full 360-image with a small field of view (60 degrees) to limit distortions. To cover the full field of view, we extract two images per orientation, with their horizontal focus point 30 degrees apart. Hence, we obtain eight images per 360 image with corresponding orientation INLINEFORM0 ..We run the following pre-trained feature extractors over the extracted images:.For the text recognition model, we use a learned look-up table INLINEFORM0 to embed the extracted text features INLINEFORM1 , and fuse all embeddings of four images through a bag of embeddings, i.e., INLINEFORM2 . We use a linear layer followed by a sigmoid to predict the probability for each class, i.e. INLINEFORM3 . We also experiment with replacing the look-up embeddings with pre-trained FastText embeddings BIBREF16 . For the ResNet model, we use a bag of embeddings over the four ResNet features, i.e. INLINEFORM4 , before we pass it through a linear layer to predict the class probabilities: INLINEFORM5 . We also conduct experiments where we first apply PCA to the extracted ResNet and FastText features before we feed them to the model..To account for class imbalance, we train all described models with a binary cross entropy loss weighted by the inverted class frequency. We create a 80-20 class-conditional split of the dataset into a training and validation set. We train for 100 epochs and perform early stopping on the validation loss..The F1 scores for the described methods in Table TABREF65 . We compare to an “all positive” baseline that always predicts that the landmark class is visible and observe that all presented models struggle to outperform this baseline. Although 256-dimensional ResNet features achieve slightly better precision on the validation set, it results in much worse recall and a lower F1 score. Our results indicate that perceptual grounding is a difficult task, which easily merits a paper of its own right, and so we leave further improvements (e.g. better text recognizers) for future work. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "14\n",
      "Question 1: What is the difference between MASC and no-MASC models when trained on varying dialogue context?\n",
      "\n",
      "Answer 1: MASC outperforms the no-MASC models by several accuracy points when trained on the last [1, 3, 5] utterances of the dialogue (including guide utterances) with varying dialogue context. Additionally, the mean predicted INLINEFORM0 (over the test set) increases from 1 to 2 when more dialogue context is included.\n",
      "Question : for the text We conduct an ablation study for MASC on natural language with varying dialogue context. Specifically, we compare localization accuracy of MASC and no-MASC models trained on the last [1, 3, 5] utterances of the dialogue (including guide utterances). We report these results in Table TABREF41 . In all cases, MASC outperforms the no-MASC models by several accuracy points. We also observe that mean predicted INLINEFORM0 (over the test set) increases from 1 to 2 when more dialogue context is included. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "15\n",
      "Question 1: Does the best localization model outperform human performance on the full task?\n",
      "\n",
      "Answer 1: Yes, the best localization model (continuous communication, with MASC, and INLINEFORM0) achieves 88.33% on the test set and exceeds human performance of 76.74% on the full task.\n",
      "Question : for the text Table TABREF36 shows results for the best localization models on the full task, evaluated via the random walk protocol defined in Algorithm SECREF12 ..paragraph4 0.1ex plus0.1ex minus.1ex-1em Comparison with human annotators Interestingly, our best localization model (continuous communication, with MASC, and INLINEFORM0 ) achieves 88.33% on the test set and thus exceed human performance of 76.74% on the full task. While emergent models appear to be stronger localizers, humans might cope with their localization uncertainty through other mechanisms (e.g. better guidance, bias towards taking particular paths, etc). The simplifying assumption of perfect perception also helps..paragraph4 0.1ex plus0.1ex minus.1ex-1em Number of actions Unsurprisingly, humans take fewer steps (roughly 15) than our best random walk model (roughly 34). Our human annotators likely used some form of guidance to navigate faster to the target. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "16\n",
      "Question 1: What is the MASC mechanism used in the guide architecture? \n",
      "Answer 1: The MASC mechanism is used to allow for grounding into the 2D overhead map in order to predict the tourist's location in the guide architecture.\n",
      "Question : for the text This section outlines the tourist and guide architectures. We first describe how the tourist produces messages for the various communication channels across which the messages are sent. We subsequently describe how these messages are processed by the guide, and introduce the novel Masked Attention for Spatial Convolutions (MASC) mechanism that allows for grounding into the 2D overhead map in order to predict the tourist's location. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "17\n",
      "What is the difference in localization accuracy between human utterances and generated tourist utterances?\n",
      "The localization accuracy achieved by human utterances is only INLINEFORM0, whereas the generated tourist utterances using the supervised model and policy gradient model have improved accuracy of more than 10 points but are slightly below the baseline of communicating a single observation. This is due to the generated utterances being grounded in the observed landmarks, which human utterances may not always do.\n",
      "Question : for the text We report the results of tourist localization with natural language in Table TABREF36 . We compare accuracy of the guide model (with MASC) trained on utterances from (i) humans, (ii) a supervised model with various decoding strategies, and (iii) a policy gradient model optimized with respect to the loss of a frozen, pre-trained guide model on human utterances..paragraph4 0.1ex plus0.1ex minus.1ex-1em Human utterances Compared to emergent language, localization from human utterances is much harder, achieving only INLINEFORM0 on the test set. Here, we report localization from a single utterance, but in Appendix SECREF45 we show that including up to five dialogue utterances only improves performance to INLINEFORM1 . We also show that MASC outperform no-MASC models for natural language communication..paragraph4 0.1ex plus0.1ex minus.1ex-1em Generated utterances We also investigate generated tourist utterances from conditional language models. Interestingly, we observe that the supervised model (with greedy and beam-search decoding) as well as the policy gradient model leads to an improvement of more than 10 accuracy points over the human utterances. However, their level of accuracy is slightly below the baseline of communicating a single observation, indicating that these models only learn to ground utterances in a single landmark observation..paragraph4 0.1ex plus0.1ex minus.1ex-1em Better grounding of generated utterances We analyze natural language samples in Table TABREF38 , and confirm that, unlike human utterances, the generated utterances are talking about the observed landmarks. This observation explains why the generated utterances obtain higher localization accuracy. The current language models are most successful when conditioned on a single landmark observation; We show in Appendix UID43 that performance quickly deteriorates when the model is conditioned on more observations, suggesting that it can not produce natural language utterances about multiple time steps. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "18\n",
      "Question 1: What is the importance of grounded language learning in artificial intelligence?\n",
      "\n",
      "Answer 1: Grounded language learning is important in artificial intelligence because it enables systems to learn language in a way that is based on sensorimotor experience of the physical world, which can lead to practical improvements in natural language understanding tasks and the development of robotic systems that can ground meaning in physical experience. It has also been applied to various tasks such as image captioning, visual reasoning, and grounded machine translation, and plays a crucial role in multi-agent communication research.\n",
      "Question : for the text The Talk the Walk task and dataset facilitate future research on various important subfields of artificial intelligence, including grounded language learning, goal-oriented dialogue research and situated navigation. Here, we describe related previous work in these areas..paragraph4 0.1ex plus0.1ex minus.1ex-1em Related tasks There has been a long line of work involving related tasks. Early work on task-oriented dialogue dates back to the early 90s with the introduction of the Map Task BIBREF11 and Maze Game BIBREF25 corpora. Recent efforts have led to larger-scale goal-oriented dialogue datasets, for instance to aid research on visually-grounded dialogue BIBREF2 , BIBREF1 , knowledge-base-grounded discourse BIBREF29 or negotiation tasks BIBREF36 . At the same time, there has been a big push to develop environments for embodied AI, many of which involve agents following natural language instructions with respect to an environment BIBREF13 , BIBREF50 , BIBREF5 , BIBREF39 , BIBREF19 , BIBREF18 , following-up on early work in this area BIBREF38 , BIBREF20 . An early example of navigation using neural networks is BIBREF28 , who propose an online learning approach for robot navigation. Recently, there has been increased interest in using end-to-end trainable neural networks for learning to navigate indoor scenes BIBREF27 , BIBREF26 or large cities BIBREF17 , BIBREF40 , but, unlike our work, without multi-agent communication. Also the task of localization (without multi-agent communication) has recently been studied BIBREF18 , BIBREF48 ..paragraph4 0.1ex plus0.1ex minus.1ex-1em Grounded language learning Grounded language learning is motivated by the observation that humans learn language embodied (grounded) in sensorimotor experience of the physical world BIBREF15 , BIBREF45 . On the one hand, work in multi-modal semantics has shown that grounding can lead to practical improvements on various natural language understanding tasks BIBREF14 , BIBREF31 . In robotics, researchers dissatisfied with purely symbolic accounts of meaning attempted to build robotic systems with the aim of grounding meaning in physical experience of the world BIBREF44 , BIBREF46 . Recently, grounding has also been applied to the learning of sentence representations BIBREF32 , image captioning BIBREF37 , BIBREF49 , visual question answering BIBREF12 , BIBREF22 , visual reasoning BIBREF30 , BIBREF42 , and grounded machine translation BIBREF43 , BIBREF23 . Grounding also plays a crucial role in the emergent research of multi-agent communication, where, agents communicate (in natural language or otherwise) in order to solve a task, with respect to their shared environment BIBREF35 , BIBREF21 , BIBREF41 , BIBREF24 , BIBREF36 , BIBREF47 , BIBREF34 . generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "19\n",
      "Question 1: What was the finding regarding localization accuracy in the Talk The Walk environment?\n",
      "\n",
      "Answer 1: The finding was that for accurate localization, a short random path including actions was necessary.\n",
      "Question : for the text In this section, we describe the findings of various experiments. First, we analyze how much information needs to be communicated for accurate localization in the Talk The Walk environment, and find that a short random path (including actions) is necessary. Next, for emergent language, we show that the MASC architecture can achieve very high localization accuracy, significantly outperforming the baseline that does not include this mechanism. We then turn our attention to the natural language experiments, and find that localization from human utterances is much harder, reaching an accuracy level that is below communicating a single landmark observation. We show that generated utterances from a conditional language model leads to significantly better localization performance, by successfully grounding the utterance on a single landmark observation (but not yet on multiple observations and actions). Finally, we show performance of the localization baseline on the full task, which can be used for future comparisons to this work. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "20\n",
      "Question 1: What types of landmarks are annotated at each corner of the intersection in Talk The Walk?\n",
      "\n",
      "Answer 1: Each corner of the intersection in Talk The Walk is annotated with a set of landmarks, which come from categories such as bar, playfield, bank, hotel, shop, subway, coffee shop, restaurant, and theater.\n",
      "Question : for the text We create a perceptual environment by manually capturing several neighborhoods of New York City (NYC) with a 360 camera. Most parts of the city are grid-like and uniform, which makes it well-suited for obtaining a 2D grid. For Talk The Walk, we capture parts of Hell's Kitchen, East Village, the Financial District, Williamsburg and the Upper East Side—see Figure FIGREF66 in Appendix SECREF14 for their respective locations within NYC. For each neighborhood, we choose an approximately 5x5 grid and capture a 360 view on all four corners of each intersection, leading to a grid-size of roughly 10x10 per neighborhood..The tourist's location is given as a tuple INLINEFORM0 , where INLINEFORM1 are the coordinates and INLINEFORM2 signifies the orientation (north, east, south or west). The tourist can take three actions: turn left, turn right and go forward. For moving forward, we add INLINEFORM3 , INLINEFORM4 , INLINEFORM5 , INLINEFORM6 to the INLINEFORM7 coordinates for the respective orientations. Upon a turning action, the orientation is updated by INLINEFORM8 where INLINEFORM9 for left and INLINEFORM10 for right. If the tourist moves outside the grid, we issue a warning that they cannot go in that direction and do not update the location. Moreover, tourists are shown different types of transitions: a short transition for actions that bring the tourist to a different corner of the same intersection; and a longer transition for actions that bring them to a new intersection..The guide observes a map that corresponds to the tourist's environment. We exploit the fact that urban areas like NYC are full of local businesses, and overlay the map with these landmarks as localization points for our task. Specifically, we manually annotate each corner of the intersection with a set of landmarks INLINEFORM0 , each coming from one of the following categories:. Bar Playfield Bank Hotel Shop Subway Coffee Shop Restaurant Theater .The right-side of Figure FIGREF3 illustrates how the map is presented. Note that within-intersection transitions have a smaller grid distance than transitions to new intersections. To ensure that the localization task is not too easy, we do not include street names in the overhead map and keep the landmark categories coarse. That is, the dialogue is driven by uncertainty in the tourist's current location and the properties of the target location: if the exact location and orientation of the tourist were known, it would suffice to communicate a sequence of actions. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "21\n",
      "Question 1: What is the goal of the two agents in the Talk The Walk task?\n",
      "Answer 1: The shared goal of the two agents is to navigate the tourist to the target location, which is only known to the guide.\n",
      "Question : for the text For the Talk The Walk task, we randomly choose one of the five neighborhoods, and subsample a 4x4 grid (one block with four complete intersections) from the entire grid. We specify the boundaries of the grid by the top-left and bottom-right corners INLINEFORM0 . Next, we construct the overhead map of the environment, i.e. INLINEFORM1 with INLINEFORM2 and INLINEFORM3 . We subsequently sample a start location and orientation INLINEFORM4 and a target location INLINEFORM5 at random..The shared goal of the two agents is to navigate the tourist to the target location INLINEFORM0 , which is only known to the guide. The tourist perceives a “street view” planar projection INLINEFORM1 of the 360 image at location INLINEFORM2 and can simultaneously chat with the guide and navigate through the environment. The guide's role consists of reading the tourist description of the environment, building a “mental map” of their current position and providing instructions for navigating towards the target location. Whenever the guide believes that the tourist has reached the target location, they instruct the system to evaluate the tourist's location. The task ends when the evaluation is successful—i.e., when INLINEFORM3 —or otherwise continues until a total of three failed attempts. The additional attempts are meant to ease the task for humans, as we found that they otherwise often fail at the task but still end up close to the target location, e.g., at the wrong corner of the correct intersection. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "22\n",
      "What is the purpose of the MASC mechanism in predicting the tourist's location on the map?\n",
      "The MASC mechanism is used to ground the tourist's observations and actions on the guide's map in order to predict the tourist's location. It translates landmark embeddings according to state transitions (left, right, up, down) through a differentiable attention-mask over the spatial dimensions of a 3x3 convolution. The MASC operation is repeated for each action step and then aggregated by a sum over positionally-gated embeddings to compute a distribution over the locations of the map.\n",
      "Question : for the text Given a tourist message INLINEFORM0 describing their observations and actions, the objective of the guide is to predict the tourist's location on the map. First, we outline the procedure for extracting observation embedding INLINEFORM1 and action embeddings INLINEFORM2 from the message INLINEFORM3 for each of the types of communication. Next, we discuss the MASC mechanism that takes the observations and actions in order to ground them on the guide's map in order to predict the tourist's location..paragraph4 0.1ex plus0.1ex minus.1ex-1em Continuous For the continuous communication model, we assign the observation message to the observation embedding, i.e. INLINEFORM0 . To extract the action embedding for time step INLINEFORM1 , we apply a linear layer to the action message, i.e. INLINEFORM2 ..paragraph4 0.1ex plus0.1ex minus.1ex-1em Discrete For discrete communication, we obtain observation INLINEFORM0 by applying a linear layer to the observation message, i.e. INLINEFORM1 . Similar to the continuous communication model, we use a linear layer over action message INLINEFORM2 to obtain action embedding INLINEFORM3 for time step INLINEFORM4 ..paragraph4 0.1ex plus0.1ex minus.1ex-1em Natural Language The message INLINEFORM0 contains information about observations and actions, so we use a recurrent neural network with attention mechanism to extract the relevant observation and action embeddings. Specifically, we encode the message INLINEFORM1 , consisting of INLINEFORM2 tokens INLINEFORM3 taken from vocabulary INLINEFORM4 , with a bidirectional LSTM: DISPLAYFORM0 . where INLINEFORM0 is the word embedding look-up table. We obtain observation embedding INLINEFORM1 through an attention mechanism over the hidden states INLINEFORM2 : DISPLAYFORM0 .where INLINEFORM0 is a learned control embedding who is updated through a linear transformation of the previous control and observation embedding: INLINEFORM1 . We use the same mechanism to extract the action embedding INLINEFORM2 from the hidden states. For the observation embedding, we obtain the final representation by summing positionally gated embeddings, i.e., INLINEFORM3 ..We represent the guide's map as INLINEFORM0 , where in this case INLINEFORM1 , where each INLINEFORM2 -dimensional INLINEFORM3 location embedding INLINEFORM4 is computed as the sum of the guide's landmark embeddings for that location..paragraph4 0.1ex plus0.1ex minus.1ex-1em Motivation While the guide's map representation contains only local landmark information, the tourist communicates a trajectory of the map (i.e. actions and observations from multiple locations), implying that directly comparing the tourist's message with the individual landmark embeddings is probably suboptimal. Instead, we want to aggregate landmark information from surrounding locations by imputing trajectories over the map to predict locations. We propose a mechanism for translating landmark embeddings according to state transitions (left, right, up, down), which can be expressed as a 2D convolution over the map embeddings. For simplicity, let us assume that the map embedding INLINEFORM0 is 1-dimensional, then a left action can be realized through application of the following INLINEFORM1 kernel: INLINEFORM2 which effectively shifts all values of INLINEFORM3 one position to the left. We propose to learn such state-transitions from the tourist message through a differentiable attention-mask over the spatial dimensions of a 3x3 convolution..paragraph4 0.1ex plus0.1ex minus.1ex-1em MASC We linearly project each predicted action embedding INLINEFORM0 to a 9-dimensional vector INLINEFORM1 , normalize it by a softmax and subsequently reshape the vector into a 3x3 mask INLINEFORM2 : DISPLAYFORM0 . We learn a 3x3 convolutional kernel INLINEFORM0 , with INLINEFORM1 features, and apply the mask INLINEFORM2 to the spatial dimensions of the convolution by first broadcasting its values along the feature dimensions, i.e. INLINEFORM3 , and subsequently taking the Hadamard product: INLINEFORM4 . For each action step INLINEFORM5 , we then apply a 2D convolution with masked weight INLINEFORM6 to obtain a new map embedding INLINEFORM7 , where we zero-pad the input to maintain identical spatial dimensions..paragraph4 0.1ex plus0.1ex minus.1ex-1em Prediction model We repeat the MASC operation INLINEFORM0 times (i.e. once for each action), and then aggregate the map embeddings by a sum over positionally-gated embeddings: INLINEFORM1 . We score locations by taking the dot-product of the observation embedding INLINEFORM2 , which contains information about the sequence of observed landmarks by the tourist, and the map. We compute a distribution over the locations of the map INLINEFORM3 by taking a softmax over the computed scores: DISPLAYFORM0 .paragraph4 0.1ex plus0.1ex minus.1ex-1em Predicting T While emergent communication models use a fixed length trasjectory INLINEFORM0 , natural language messages may differ in the number of communicated observations and actions. Hence, we predict INLINEFORM1 from the communicated message. Specifically, we use a softmax regression layer over the last hidden state INLINEFORM2 of the RNN, and subsequently sample INLINEFORM3 from the resulting multinomial distribution: DISPLAYFORM0 .We jointly train the INLINEFORM0 -prediction model via REINFORCE, with the guide's loss as reward function and a mean-reward baseline. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "23\n",
      "Question 1: How does the tourist generate an observation message in the continuous vector communication model?\n",
      "\n",
      "Answer 1: In the continuous vector communication model, the tourist generates an observation message by summing the embeddings of the observed landmarks and passing them through a positionally gated embedding, conditioned on time step, to push the embedding information into the appropriate dimensions. The resulting observation message is a concatenated vector with the action message that is sent to the guide.\n",
      "Question : for the text For each of the communication channels, we outline the procedure for generating a message INLINEFORM0 . Given a set of state observations INLINEFORM1 , we represent each observation by summing the INLINEFORM2 -dimensional embeddings of the observed landmarks, i.e. for INLINEFORM3 , INLINEFORM4 , where INLINEFORM5 is the landmark embedding lookup table. In addition, we embed action INLINEFORM6 into a INLINEFORM7 -dimensional embedding INLINEFORM8 via a look-up table INLINEFORM9 . We experiment with three types of communication channel..paragraph4 0.1ex plus0.1ex minus.1ex-1em Continuous vectors The tourist has access to observations of several time steps, whose order is important for accurate localization. Because summing embeddings is order-invariant, we introduce a sum over positionally-gated embeddings, which, conditioned on time step INLINEFORM0 , pushes embedding information into the appropriate dimensions. More specifically, we generate an observation message INLINEFORM1 , where INLINEFORM2 is a learned gating vector for time step INLINEFORM3 . In a similar fashion, we produce action message INLINEFORM4 and send the concatenated vectors INLINEFORM5 as message to the guide. We can interpret continuous vector communication as a single, monolithic model because its architecture is end-to-end differentiable, enabling gradient-based optimization for training..paragraph4 0.1ex plus0.1ex minus.1ex-1em Discrete symbols Like the continuous vector communication model, with discrete communication the tourist also uses separate channels for observations and actions, as well as a sum over positionally gated embeddings to generate observation embedding INLINEFORM0 . We pass this embedding through a sigmoid and generate a message INLINEFORM1 by sampling from the resulting Bernoulli distributions:. INLINEFORM0 .The action message INLINEFORM0 is produced in the same way, and we obtain the final tourist message INLINEFORM1 through concatenating the messages..The communication channel's sampling operation yields the model non-differentiable, so we use policy gradients BIBREF9 , BIBREF10 to train the parameters INLINEFORM0 of the tourist model. That is, we estimate the gradient by INLINEFORM1 . where the reward function INLINEFORM0 is the negative guide's loss (see Section SECREF25 ) and INLINEFORM1 a state-value baseline to reduce variance. We use a linear transformation over the concatenated embeddings as baseline prediction, i.e. INLINEFORM2 , and train it with a mean squared error loss..paragraph4 0.1ex plus0.1ex minus.1ex-1em Natural Language Because observations and actions are of variable-length, we use an LSTM encoder over the sequence of observations embeddings INLINEFORM0 , and extract its last hidden state INLINEFORM1 . We use a separate LSTM encoder for action embeddings INLINEFORM2 , and concatenate both INLINEFORM3 and INLINEFORM4 to the input of the LSTM decoder at each time step: DISPLAYFORM0 . where INLINEFORM0 a look-up table, taking input tokens INLINEFORM1 . We train with teacher-forcing, i.e. we optimize the cross-entropy loss: INLINEFORM2 . At test time, we explore the following decoding strategies: greedy, sampling and a beam-search. We also fine-tune a trained tourist model (starting from a pre-trained model) with policy gradients in order to minimize the guide's prediction loss. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "24\n",
      "Question 1: What is the effect of increasing the beam size during tourist utterance generation?\n",
      "\n",
      "Answer 1: The localization performance decreases from 29.05% to 20.87% accuracy on the test set when the beam size is increased from one to eight.\n",
      "Question : for the text After training the supervised tourist model (conditioned on observations and action from human expert trajectories), there are two ways to train an accompanying guide model. We can optimize a location prediction model on either (i) extracted human trajectories (as in the localization setup from human utterances) or (ii) on all random paths of length INLINEFORM0 (as in the full task evaluation). Here, we investigate the impact of (1) using either human or random trajectories for training the guide model, and (2) the effect of varying the path length INLINEFORM1 during the full-task evaluation. For random trajectories, guide training uses the same path length INLINEFORM2 as is used during evaluation. We use a pre-trained tourist model with greedy decoding for generating the tourist utterances. Table TABREF40 summarizes the results..paragraph4 0.1ex plus0.1ex minus.1ex-1em Human vs random trajectories We only observe small improvements for training on random trajectories. Human trajectories are thus diverse enough to generalize to random trajectories..paragraph4 0.1ex plus0.1ex minus.1ex-1em Effect of path length There is a strong negative correlation between task success and the conditioned trajectory length. We observe that the full task performance quickly deteriorates for both human and random trajectories. This suggests that the tourist generation model can not produce natural language utterances that describe multiple observations and actions. Although it is possible that the guide model can not process such utterances, this is not very likely because the MASC architectures handles such messages successfully for emergent communication..We report localization performance of tourist utterances generated by beam search decoding of varying beam size in Table TABREF40 . We find that performance decreases from 29.05% to 20.87% accuracy on the test set when we increase the beam-size from one to eight. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "25\n",
      "What are the two simplifying assumptions made in the designed navigation protocol?\n",
      "The two simplifying assumptions made in the designed navigation protocol are perfect perception and orientation-agnosticism.\n",
      "Question : for the text The designed navigation protocol relies on a trained localization model that predicts the tourist's location from a communicated message. Before we formalize this localization sub-task in Section UID21 , we further introduce two simplifying assumptions—perfect perception and orientation-agnosticism—so as to overcome some of the difficulties we encountered in preliminary experiments..paragraph4 0.1ex plus0.1ex minus.1ex-1em Perfect Perception Early experiments revealed that perceptual grounding of landmarks is difficult: we set up a landmark classification problem, on which models with extracted CNN BIBREF7 or text recognition features BIBREF8 barely outperform a random baseline—see Appendix SECREF13 for full details. This finding implies that localization models from image input are limited by their ability to recognize landmarks, and, as a result, would not generalize to unseen environments. To ensure that perception is not the limiting factor when investigating the landmark-grounding and action-grounding capabilities of localization models, we assume “perfect perception”: in lieu of the 360 image view, the tourist is given the landmarks at its current location. More formally, each state observation INLINEFORM0 now equals the set of landmarks at the INLINEFORM1 -location, i.e. INLINEFORM2 . If the INLINEFORM3 -location does not have any visible landmarks, we return a single “empty corner” symbol. We stress that our findings—including a novel architecture for grounding actions into an overhead map, see Section UID28 —should carry over to settings without the perfect perception assumption..paragraph4 0.1ex plus0.1ex minus.1ex-1em Orientation-agnosticism We opt to ignore the tourist's orientation, which simplifies the set of actions to [Left, Right, Up, Down], corresponding to adding [(-1, 0), (1, 0), (0, 1), (0, -1)] to the current INLINEFORM0 coordinates, respectively. Note that actions are now coupled to an orientation on the map—e.g. up is equal to going north—and this implicitly assumes that the tourist has access to a compass. This also affects perception, since the tourist now has access to views from all orientations: in conjunction with “perfect perception”, implying that only landmarks at the current corner are given, whereas landmarks from different corners (e.g. across the street) are not visible..Even with these simplifications, the localization-based baseline comes with its own set of challenges. As we show in Section SECREF34 , the task requires communication about a short (random) path—i.e., not only a sequence of observations but also actions—in order to achieve high localization accuracy. This means that the guide needs to decode observations from multiple time steps, as well as understand their 2D spatial arrangement as communicated via the sequence of actions. Thus, in order to get to a good understanding of the task, we thoroughly examine whether the agents can learn a communication protocol that simultaneously grounds observations and actions into the guide's map. In doing so, we thoroughly study the role of the communication channel in the localization task, by investigating increasingly constrained forms of communication: from differentiable continuous vectors to emergent discrete symbols to the full complexity of natural language..The full navigation baseline hinges on a localization model from random trajectories. While we can sample random actions in the emergent communication setup, this is not possible for the natural language setup because the messages are coupled to the trajectories of the human annotators. This leads to slightly different problem setups, as described below..paragraph4 0.1ex plus0.1ex minus.1ex-1em Emergent language A tourist, starting from a random location, takes INLINEFORM0 random actions INLINEFORM1 to reach target location INLINEFORM2 . Every location in the environment has a corresponding set of landmarks INLINEFORM3 for each of the INLINEFORM4 coordinates. As the tourist navigates, the agent perceives INLINEFORM5 state-observations INLINEFORM6 where each observation INLINEFORM7 consists of a set of INLINEFORM8 landmark symbols INLINEFORM9 . Given the observations INLINEFORM10 and actions INLINEFORM11 , the tourist generates a message INLINEFORM12 which is communicated to the other agent. The objective of the guide is to predict the location INLINEFORM13 from the tourist's message INLINEFORM14 ..paragraph4 0.1ex plus0.1ex minus.1ex-1em Natural language In contrast to our emergent communication experiments, we do not take random actions but instead extract actions, observations, and messages from the dataset. Specifically, we consider each tourist utterance (i.e. at any point in the dialogue), obtain the current tourist location as target location INLINEFORM0 , the utterance itself as message INLINEFORM1 , and the sequence of observations and actions that took place between the current and previous tourist utterance as INLINEFORM2 and INLINEFORM3 , respectively. Similar to the emergent language setting, the guide's objective is to predict the target location INLINEFORM4 models from the tourist message INLINEFORM5 . We conduct experiments with INLINEFORM6 taken from the dataset and with INLINEFORM7 generated from the extracted observations INLINEFORM8 and actions INLINEFORM9 . generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "26\n",
      "Q: What do the MASC values in Figure FIGREF46 represent?\n",
      "A: The MASC values in Figure FIGREF46 represent the predicted values for different action sequences taken by a tourist in a learned model with emergent discrete communications and INLINEFORM0 actions.\n",
      "Question : for the text Figure FIGREF46 shows the MASC values for a learned model with emergent discrete communications and INLINEFORM0 actions. Specifically, we look at the predicted MASC values for different action sequences taken by the tourist. We observe that the first action is always mapped to the correct state-transition, but that the second and third MASC values do not always correspond to right state-transitions. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "27\n",
      "What dataset did the researchers use to study embeddings?\n",
      "The researchers used the Quora duplicate question dataset as the best match to represent their problem and studied the embeddings by computing the euclidean distance between semantically similar and dissimilar questions.\n",
      "Question : for the text In order to choose an embedding, we sought a dataset to represent our problem. Although no perfect matches exist, we decided upon the Quora duplicate question dataset BIBREF22 as the best match. To study the embeddings, we computed the euclidean distance between the two questions using various embeddings, to study the distance between semantically similar and dissimilar questions..The graphs in figure 1 show the distances between duplicate and non-duplicate questions using different embedding systems. The X axis shows the euclidean distance between vectors and the Y axis frequency. A perfect result would be a blue peak to the left and an entirely disconnected orange spike to the right, showing that all non-duplicate questions have a greater euclidean distance than the least similar duplicate pair of questions. As can be clearly seen in the figure above, Elmo BIBREF23 and Infersent BIBREF13 show almost no separation and therefore cannot be considered good models for this problem. A much greater disparity is shown by the Google USE models BIBREF14 , and even more for the Google USE Large model. In fact the Google USE Large achieved a F1 score of 0.71 for this task without any specific training, simply by choosing a threshold below which all sentence pairs are considered duplicates..In order to test whether these results generalised to our domain, we devised a test that would make use of what little data we had to evaluate. We had no original data on whether sentences were semantically similar, but we did have a corpus of articles clustered into stories. Working on the assumption that similar claims would be more likely to be in the same story, we developed an equation to judge how well our corpus of sentences was clustered, rewarding clustering which matches the article clustering and the total number of claims clustered. The precise formula is given below, where INLINEFORM0 is the proportion of claims in clusters from one story cluster, INLINEFORM1 is the proportion of claims in the correct claim cluster, where they are from the most common story cluster, and INLINEFORM2 is the number of claims placed in clusters. A,B and C are parameters to tune. INLINEFORM3 . figureFormula to assess the correctness of claim clusters based on article clusters.This method is limited in how well it can represent the problem, but it can give indications as to a good or bad clustering method or embedding, and can act as a check that the findings we obtained from the Quora dataset will generalise to our domain. We ran code which vectorized 2,000 sentences and then used the DBScan clustering method BIBREF24 to cluster using a grid search to find the best INLINEFORM0 value, maximizing this formula. We used DBScan as it mirrored the clustering method used to derive the original article clusters. The results for this experiment can be found in Table TABREF10 . We included TFIDF in the experiment as a baseline to judge other results. It is not suitable for our eventual purposes, but it the basis of the original keyword-based model used to build the clusters . That being said, TFIDF performs very well, with only Google USE Large and Infersent coming close in terms of `accuracy'. In the case of Infersent, this comes with the penalty of a much smaller number of claims included in the clusters. Google USE Large, however, clusters a greater number and for this reason we chose to use Google's USE Large. .Since Google USE Large was the best-performing embedding in both the tests we devised, this was our chosen embedding to use for clustering. However as can be seen from the results shown above, this is not a perfect solution and the inaccuracy here will introduce inaccuracy further down the clustering pipeline. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "28\n",
      "What is the advantage of using DBScan over other clustering methods like K-Means?\n",
      "\n",
      "DBScan does not require the number of clusters to be specified, making it advantageous for dynamic clustering purposes where the number of clusters may change. In contrast, K-Means requires the number of clusters to be predetermined.\n",
      "Question : for the text We decided to follow a methodology upon the DBScan method of clustering BIBREF24 . DBScan considers all distances between pairs of points. If they are under INLINEFORM0 then those two are linked. Once the number of connected points exceeds a minimum size threshold, they are considered a cluster and all other points are considered to be unclustered. This method is advantageous for our purposes because unlike other methods, such as K-Means, it does not require the number of clusters to be specified. To create a system that can build clusters dynamically, adding one point at a time, we set the minimum cluster size to one, meaning that every point is a member of a cluster..A potential disadvantage of this method is that because points require only one connection to a cluster to join it, they may only be related to one point in the cluster, but be considered in the same cluster as all of them. In small examples this is not a problem as all points in the cluster should be very similar. However as the number of points being considered grows, this behaviour raises the prospect of one or several borderline clustering decisions leading to massive clusters made from tenuous connections between genuine clusters. To mitigate this problem we used a method described in the Newslens paper BIBREF25 to solve a similar problem when clustering entire articles. We stored all of our claims in a graph with the connections between them added when the distance between them was determined to be less than INLINEFORM0 . To determine the final clusters we run a Louvain Community Detection BIBREF26 over this graph to split it into defined communities. This improved the compactness of a cluster. When clustering claims one by one, this algorithm can be performed on the connected subgraph featuring the new claim, to reduce the computation required..As this method involves distance calculations between the claim being added and every existing claim, the time taken to add one claim will increase roughly linearly with respect to the number of previous claims. Through much optimization we have brought the computational time down to approximately 300ms per claim, which stays fairly static with respect to the number of previous claims. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "29\n",
      "What is the aim of automated factchecking mentioned in the text?\n",
      "\n",
      "The aim of automated factchecking is to speed up certain aspects of the factchecking process, rather than replace factchecking personnel. This includes monitoring claims made in the news, aiding decisions about which statements are the most important to check, and automatically retrieving existing factchecks that are relevant to a new claim.\n",
      "Question : for the text In recent years, the spread of misinformation has become a growing concern for researchers and the public at large BIBREF1 . Researchers at MIT found that social media users are more likely to share false information than true information BIBREF2 . Due to renewed focus on finding ways to foster healthy political conversation, the profile of factcheckers has been raised..Factcheckers positively influence public debate by publishing good quality information and asking politicians and journalists to retract misleading or false statements. By calling out lies and the blurring of the truth, they make those in positions of power accountable. This is a result of labour intensive work that involves monitoring the news for spurious claims and carrying out rigorous research to judge credibility. So far, it has only been possible to scale their output upwards by hiring more personnel. This is problematic because newsrooms need significant resources to employ factcheckers. Publication budgets have been decreasing, resulting in a steady decline in the size of their workforce BIBREF0 . Factchecking is not a directly profitable activity, which negatively affects the allocation of resources towards it in for-profit organisations. It is often taken on by charities and philanthropists instead..To compensate for this shortfall, our strategy is to harness the latest developments in NLP to make factchecking more efficient and therefore less costly. To this end, the new field of automated factchecking has captured the imagination of both non-profits and start-ups BIBREF3 , BIBREF4 , BIBREF5 . It aims to speed up certain aspects of the factchecking process rather than create AI that can replace factchecking personnel. This includes monitoring claims that are made in the news, aiding decisions about which statements are the most important to check and automatically retrieving existing factchecks that are relevant to a new claim..The claim detection and claim clustering methods that we set out in this paper can be applied to each of these. We sought to devise a system that would automatically detect claims in articles and compare them to previously submitted claims. Storing the results to allow a factchecker's work on one of these claims to be easily transferred to others in the same cluster. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "30\n",
      "Question 1: What approach did the authors take when building their dataset and evaluating their model?\n",
      "\n",
      "Answer 1: The authors emphasized the importance of clear and objective definitions and built upon methodologies that focused on the objective qualities of claims, such as the PolitiTax and Full Fact taxonomies. They annotated sentences from their own database of news articles using a combination of these taxonomies and a definition of a claim as a statement about the world that can be checked. They also used Google USE Large as their feature engineering step and compared their results to TFIDF and Full Fact's results as baselines.\n",
      "Question : for the text It is much easier to build a dataset and reliably evaluate a model if the starting definitions are clear and objective. Questions around what is an interesting or pertinent claim are inherently subjective. For example, it is obvious that a politician will judge their opponents' claims to be more important to factcheck than their own..Therefore, we built on the methodologies that dealt with the objective qualities of claims, which were the PolitiTax and Full Fact taxonomies. We annotated sentences from our own database of news articles based on a combination of these. We also used the Full Fact definition of a claim as a statement about the world that can be checked. Some examples of claims according to this definition are shown in Table TABREF3 . We decided the first statement was a claim since it declares the occurrence of an event, while the second was considered not to be a claim as it is an expression of feeling..Full Fact's approach centred around using sentence embeddings as a feature engineering step, followed by a simple classifier such as logistic regression, which is what we used. They used Facebook's sentence embeddings, InferSent BIBREF13 , which was a recent breakthrough at the time. Such is the speed of new development in the field that since then, several papers describing textual embeddings have been published. Due to the fact that we had already evaluated embeddings for clustering, and therefore knew our system would rely on Google USE Large BIBREF14 , we decided to use this instead. We compared this to TFIDF and Full Fact's results as baselines. The results are displayed in Table TABREF4 ..However, ClaimBuster and Full Fact focused on live factchecking of TV debates. Logically is a news aggregator and we analyse the bodies of published news stories. We found that in our corpus, the majority of sentences are claims and therefore our model needed to be as selective as possible. In practice, we choose to filter out sentences that are predictions since generally the substance of the claim cannot be fully checked until after the event has occurred. Likewise, we try to remove claims based on personal experience or anecdotal evidence as they are difficult to verify. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "31\n",
      "Question 1: Is an embedding a perfect representation of a claim?\n",
      "\n",
      "Answer 1: No, an embedding will always be an imperfect representation of a claim and is therefore an area of improvement.\n",
      "Question : for the text The clustering described above is heavily dependent on the embedding used. The rate of advances in this field has been rapid in recent years, but an embedding will always be an imperfect representation of an claim and therefore always an area of improvement. A domain specific-embedding will likely offer a more accurate representation but creates problems with clustering claims from different domains. They also require a huge amount of data to give a good model and that is not possible in all domains. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "32\n",
      "Question 1: What is the purpose of ClaimBuster and ClaimRank?\n",
      "\n",
      "Answer 1: ClaimBuster and ClaimRank are systems designed to detect politically pertinent statements and sentences that are worthy of factchecking, respectively. They are used to classify the objective qualities that set apart different types of claims and provide a framework for claim detection classifiers.\n",
      "Question : for the text It is important to decide what sentences are claims before attempting to cluster them. The first such claim detection system to have been created is ClaimBuster BIBREF6 , which scores sentences with an SVM to determine how likely they are to be politically pertinent statements. Similarly, ClaimRank BIBREF7 uses real claims checked by factchecking institutions as training data in order to surface sentences that are worthy of factchecking..These methods deal with the question of what is a politically interesting claim. In order to classify the objective qualities of what set apart different types of claims, the ClaimBuster team created PolitiTax BIBREF8 , a taxonomy of claims, and factchecking organisation Full Fact BIBREF9 developed their preferred annotation schema for statements in consultation with their own factcheckers. This research provides a more solid framework within which to construct claim detection classifiers..The above considers whether or not a sentence is a claim, but often claims are subsections of sentences and multiple claims might be found in one sentence. In order to accommodate this, BIBREF10 proposes extracting phrases called Context Dependent Claims (CDC) that are relevant to a certain `Topic'. Along these lines, BIBREF11 proposes new definitions for frames to be incorporated into FrameNet BIBREF12 that are specific to facts, in particular those found in a political context..Traditional text clustering methods, using TFIDF and some clustering algorithm, are poorly suited to the problem of clustering and comparing short texts, as they can be semantically very similar but use different words. This is a manifestation of the the data sparsity problem with Bag-of-Words (BoW) models. BIBREF16 . Dimensionality reduction methods such as Latent Dirichlet Allocation (LDA) can help solve this problem by giving a dense approximation of this sparse representation BIBREF17 . More recently, efforts in this area have used text embedding-based systems in order to capture dense representation of the texts BIBREF18 . Much of this recent work has relied on the increase of focus in word and text embeddings. Text embeddings have been an increasingly popular tool in NLP since the introduction of Word2Vec BIBREF19 , and since then the number of different embeddings has exploded. While many focus on giving a vector representation of a word, an increasing number now exist that will give a vector representation of a entire sentence or text. Following on from this work, we seek to devise a system that can run online, performing text clustering on the embeddings of texts one at a time.Some considerations to bear in mind when deciding on an embedding scheme to use are: the size of the final vector, the complexity of the model itself and, if using a pretrained implementation, the data the model has been trained on and whether it is trained in a supervised or unsupervised manner..The size of the embedding can have numerous results downstream. In our example we will be doing distance calculations on the resultant vectors and therefore any increase in length will increase the complexity of those distance calculations. We would therefore like as short a vector as possible, but we still wish to capture all salient information about the claim; longer vectors have more capacity to store information, both salient and non-salient..A similar effect is seen for the complexity of the model. A more complicated model, with more trainable parameters, may be able to capture finer details about the text, but it will require a larger corpus to achieve this, and will require more computational time to calculate the embeddings. We should therefore attempt to find the simplest embedding system that can accurately solve our problem..When attempting to use pretrained models to help in other areas, it is always important to ensure that the models you are using are trained on similar material, to increase the chance that their findings will generalise to the new problem. Many unsupervised text embeddings are trained on the CommonCrawl dataset of approx. 840 billion tokens. This gives a huge amount of data across many domains, but requires a similarly huge amount of computing power to train on the entire dataset. Supervised datasets are unlikely ever to approach such scale as they require human annotations which can be expensive to assemble. The SNLI entailment dataset is an example of a large open source dataset BIBREF20 . It features pairs of sentences along with labels specifying whether or not one entails the other. Google's Universal Sentence Encoder (USE) BIBREF14 is a sentence embedding created with a hybrid supervised/unsupervised method, leveraging both the vast amounts of unsupervised training data and the extra detail that can be derived from a supervised method. The SNLI dataset and the related MultiNLI dataset are often used for this because textual entailment is seen as a good basis for general Natural Language Understanding (NLU) BIBREF21 . generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "33\n",
      "Question 1: What is path ranking-based approach for knowledge graph completion (KGC)?\n",
      "\n",
      "Answer 1: Path ranking-based approach for knowledge graph completion (KGC) is a technique that samples paths between the subject and object entities in a knowledge base, constructs a feature vector of these paths, and calculates the compatibility between the query triplet and the sampled paths. This approach is used to combat the sparsity of knowledge bases and is a natural solution for the RC-QED$^{\\rm E}$ problem, where the query triplet and the sampled paths correspond to a question and derivation steps, respectively.\n",
      "Question : for the text To highlight the challenges and nature of RC-QED$^{\\rm E}$, we create a simple, transparent, and interpretable baseline model..Recent studies on knowledge graph completion (KGC) explore compositional inferences to combat with the sparsity of knowledge bases BIBREF10, BIBREF11, BIBREF12. Given a query triplet $(h, r, t)$ (e.g. (Macchu Picchu, locatedIn, Peru)), a path ranking-based approach for KGC explicitly samples paths between $h$ and $t$ in a knowledge base (e.g. Macchu Picchu—locatedIn—Andes Mountain—countryOf—Peru), and construct a feature vector of these paths. This feature vector is then used to calculate the compatibility between the query triplet and the sampled paths..RC-QED$^{\\rm E}$ can be naturally solved by path ranking-based KGC (PRKGC), where the query triplet and the sampled paths correspond to a question and derivation steps, respectively. PRKGC meets our purposes because of its glassboxness: we can trace the derivation steps of the model easily. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "34\n",
      "Question 1: What are the nodes and edges in the knowledge graph G(S) constructed from the supporting documents?\n",
      "\n",
      "Answer 1: The nodes in G(S) represent named entities (NEs) in the supporting documents, while the edges represent the textual relations between NEs extracted from the documents.\n",
      "Question : for the text Given supporting documents $S$, we build a knowledge graph. We first apply a coreference resolver to $S$ and then create a directed graph $G(S)$. Therein, each node represents named entities (NEs) in $S$, and each edge represents textual relations between NEs extracted from $S$. Figure FIGREF27 illustrates an example of $G(S)$ constructed from supporting documents in Figure FIGREF1. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "35\n",
      "Question 1: What was the most popular album released by the American band Heart in May 1977?\n",
      "\n",
      "Answer 1: The album was Little Queen, released on Portrait Records.\n",
      "Question : for the text Given a question $Q=(q, r)$ and a candidate entity $c_i$, we estimate the plausibility of $(q, r, c_i)$ as follows:.where $\\sigma $ is a sigmoid function, and $\\mathbf {q, r, c_i}, \\mathbf {\\pi }(q, c_i)$ are vector representations of $q, r, c_i$ and a set $\\pi (q, c_i)$ of shortest paths between $q$ and $c_i$ on $G(S)$. ${\\rm MLP}(\\cdot , \\cdot )$ denotes a multi-layer perceptron. To encode entities into vectors $\\mathbf {q, c_i}$, we use Long-Short Term Memory (LSTM) and take its last hidden state. For example, in Figure FIGREF27, $q =$ Barracuda and $c_i =$ Portrait Records yield $\\pi (q, c_i) = \\lbrace $Barracuda—is the most popular in their album—Little Queen—was released in May 1977 on—Portrait Records, Barracuda—was released from American band Heart—is the second album released by:-1—Little Queen—was released in May 1977 on—Portrait Records$\\rbrace $..To obtain path representations $\\mathbf {\\pi }(q, c_i)$, we attentively aggregate individual path representations: $\\mathbf {\\pi }(q, c_i) = \\sum _j \\alpha _j \\mathbf {\\pi _j}(q, c_i)$, where $\\alpha _j$ is an attention for the $j$-th path. The attention values are calculated as follows: $\\alpha _j = \\exp ({\\rm sc}(q, r, c_i, \\pi _j)) / \\sum _k \\exp ({\\rm sc}(q, r, c_i, \\pi _k))$, where ${\\rm sc}(q, r, c_i, \\pi _j) = {\\rm MLP}(\\mathbf {q}, \\mathbf {r}, \\mathbf {c_i}, \\mathbf {\\pi _j})$. To obtain individual path representations $\\mathbf {\\pi _j}$, we follow toutanova-etal-2015-representing. We use a Bi-LSTM BIBREF13 with mean pooling over timestep in order to encourage similar paths to have similar path representations..For the testing phase, we choose a candidate entity $c_i$ with the maximum probability $P(r|q, c_i)$ as an answer entity, and choose a path $\\pi _j$ with the maximum attention value $\\alpha _j$ as NLDs. To generate NLDs, we simply traverse the path from $q$ to $c_i$ and subsequently concatenate all entities and textual relations as one string. We output Unanswerable when (i) $\\max _{c_i \\in C} P(r|q, c_i) < \\epsilon _k$ or (ii) $G(S)$ has no path between $q$ and all $c_i \\in C$. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "36\n",
      "Question 1: What is the capital city of France?\n",
      "\n",
      "Answer 1: Paris.\n",
      "Question : for the text Let $\\mathcal {K}^+$ be a set of question-answer pairs, where each instance consists of a triplet (a query entity $q_i$, a relation $r_i$, an answer entity $a_i$). Similarly, let $\\mathcal {K}^-$ be a set of question-non-answer pairs. We minimize the following binary cross-entropy loss:.From the NLD point of view, this is unsupervised training. The model is expected to learn the score function ${\\rm sc(\\cdot )}$ to give higher scores to paths (i.e. NLD steps) that are useful for discriminating correct answers from wrong answers by its own. Highly scored NLDs might be useful for answer classification, but these are not guaranteed to be interpretable to humans. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "37\n",
      "To tackle the aforementioned problem, we utilize gold-standard NLDs as a means to guide the path scoring function ${\\rm sc(\\cdot)}$. The dataset $\\mathcal{D}$ consists of question-answer pairs accompanied by binary vectors $\\mathbf{p}_i$, wherein each value $j$ in the vector denotes whether the $j$-th path corresponds to a gold-standard NLD (1) or not (0). We employ the cross-entropy loss for the path attention, and present an example format below:\n",
      "\n",
      "Question 1:\n",
      "Answer 1:\n",
      "Question : for the text To address the above issue, we resort to gold-standard NLDs to guide the path scoring function ${\\rm sc(\\cdot )}$. Let $\\mathcal {D}$ be question-answer pairs coupled with gold-standard NLDs, namely a binary vector $\\mathbf {p}_i$, where the $j$-th value represents whether $j$-th path corresponds to a gold-standard NLD (1) or not (0). We apply the following cross-entropy loss to the path attention: generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "38\n",
      "Question 1: What is RC-QED and what does it require a system to do?\n",
      "\n",
      "Answer 1: RC-QED is a model for reasoning, and it requires a system to output its introspective explanations, as well as answers.\n",
      "Question : for the text Towards RC models that can perform correct reasoning, we have proposed RC-QED that requires a system to output its introspective explanations, as well as answers. Instantiating RC-QED with entity-based multi-hop QA (RC-QED$^{\\rm E}$), we have created a large-scale corpus of NLDs. The developed crowdsourcing annotation framework can be used for annotating other QA datasets with derivations. Our experiments using two simple baseline models have demonstrated that RC-QED$^{\\rm E}$ is a non-trivial task, and that it indeed provides a challenging task of extracting and synthesizing relevant facts from supporting documents. We will make the corpus of reasoning annotations and baseline systems publicly available at https://naoya-i.github.io/rc-qed/..One immediate future work is to expand the annotation to non-entity-based multi-hop QA datasets such as HotpotQA BIBREF2. For modeling, we plan to incorporate a generative mechanism based on recent advances in conditional language modeling. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "39\n",
      "Question 1: How was the large-scale corpus of NLDs acquired?\n",
      "Answer 1: The large-scale corpus of NLDs was acquired using crowdsourcing (CS) as a powerful tool for large-scale dataset creation, with an incentive structure carefully designed for crowdworkers to judge the truth of statements solely based on given articles, not based on their own knowledge.\n",
      "Question : for the text To acquire a large-scale corpus of NLDs, we use crowdsourcing (CS). Although CS is a powerful tool for large-scale dataset creation BIBREF2, BIBREF8, quality control for complex tasks is still challenging. We thus carefully design an incentive structure for crowdworkers, following Yang2018HotpotQA:Answering..Initially, we provide crowdworkers with an instruction with example annotations, where we emphasize that they judge the truth of statements solely based on given articles, not based on their own knowledge. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "40\n",
      "Question 1: How do workers receive bonuses in the judgement task?\n",
      "\n",
      "Answer 1: Workers receive a ¢6 bonus if they select True or Likely in the judgement task. They also receive a probabilistic bonus of ¢14 based on a score they gain for providing shorter summaries (NLDs). Noisy submissions are rejected and workers giving more than 50 submissions with the same answers are excluded.\n",
      "Question : for the text If a worker selects True or Likely in the judgement task, we first ask which sentences in the given articles are justification explanations for a given statement, similarly to HotpotQA BIBREF2. The “summary” text boxes (i.e. NLDs) are then initialized with these selected sentences. We give a ¢6 bonus to those workers who select True or Likely. To encourage an abstraction of selected sentences, we also introduce a gamification scheme to give a bonus to those who provide shorter NLDs. Specifically, we probabilistically give another ¢14 bonus to workers according to a score they gain. The score is always shown on top of the screen, and changes according to the length of NLDs they write in real time. To discourage noisy annotations, we also warn crowdworkers that their work would be rejected for noisy submissions. We periodically run simple filtering to exclude noisy crowdworkers (e.g. workers who give more than 50 submissions with the same answers)..We deployed the task on Amazon Mechanical Turk (AMT). To see how reasoning varies across workers, we hire 3 crowdworkers per one instance. We hire reliable crowdworkers with $\\ge 5,000$ HITs experiences and an approval rate of $\\ge $ 99.0%, and pay ¢20 as a reward per instance..Our data collection pipeline is expected to be applicable to other types of QAs other than entity-based multi-hop QA without any significant extensions, because the interface is not specifically designed for entity-centric reasoning. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "41\n",
      "Question 1: What do workers do when they are unsure about whether a statement can be derived from the articles?\n",
      "\n",
      "Answer 1: If a worker selects Unsure, they are asked to tell us why they are unsure from two choices (“Not stated in the article” or “Other”).\n",
      "Question : for the text Given a statement and articles, workers are asked to judge whether the statement can be derived from the articles at three grades: True, Likely (i.e. Answerable), or Unsure (i.e. Unanswerable). If a worker selects Unsure, we ask workers to tell us why they are unsure from two choices (“Not stated in the article” or “Other”). generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "42\n",
      "Question 1: Where is Macchu Picchu located? \n",
      "\n",
      "Answer 1: Macchu Picchu is located in Peru.\n",
      "Question : for the text Our study uses WikiHop BIBREF0, as it is an entity-based multi-hop QA dataset and has been actively used. We randomly sampled 10,000 instances from 43,738 training instances and 2,000 instances from 5,129 validation instances (i.e. 36,000 annotation tasks were published on AMT). We manually converted structured WikiHop question-answer pairs (e.g. locatedIn(Macchu Picchu, Peru)) into natural language statements (Macchu Picchu is located in Peru) using a simple conversion dictionary..We use supporting documents provided by WikiHop. WikiHop collects supporting documents by finding Wikipedia articles that bridges a question entity $e_i$ and an answer entity $e_j$, where the link between articles is given by a hyperlink. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "43\n",
      "Question 1: What does the abstractiveness of annotated NLDs indicate?\n",
      "\n",
      "Answer 1: The abstractiveness of annotated NLDs indicates the level of summarization in the language used, as it measures the number of tokens in an NLD divided by the number of tokens in its corresponding justification sentences.\n",
      "Question : for the text Table TABREF17 shows the statistics of responses and example annotations. Table TABREF17 also shows the abstractiveness of annotated NLDs ($a$), namely the number of tokens in an NLD divided by the number of tokens in its corresponding justification sentences. This indicates that annotated NLDs are indeed summarized. See Table TABREF53 in Appendix and Supplementary Material for more results. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "44\n",
      "What was the Krippendorff's alpha value obtained for agreement on the number of NLDs?\n",
      "\n",
      "The Krippendorff's alpha value obtained for agreement on the number of NLDs was 0.223 indicating fair agreement.\n",
      "Question : for the text For agreement on the number of NLDs, we obtained a Krippendorff's $\\alpha $ of 0.223, indicating a fair agreement BIBREF9..Our manual inspection of the 10 worst disagreements revealed that majority (7/10) come from Unsure v.s. non-Unsure. It also revealed that crowdworkers who labeled non-Unsure are reliable—6 out 7 non-Unsure annotations can be judged as correct. This partially confirms the effectiveness of our incentive structure. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "45\n",
      "Question 1: How many crowdworkers were asked to evaluate if the NLDs could lead to the statement in the CS task on AMT?\n",
      "\n",
      "Answer 1: Three crowdworkers were asked to evaluate if the NLDs could lead to the statement at four scale levels in the CS task on AMT.\n",
      "Question : for the text To evaluate the quality of annotation results, we publish another CS task on AMT. We randomly sample 300 True and Likely responses in this evaluation. Given NLDs and a statement, 3 crowdworkers are asked if the NLDs can lead to the statement at four scale levels. If the answer is 4 or 3 (“yes” or “likely”), we additionally asked whether each derivation step can be derived from each supporting document; otherwise we asked them the reasons. For a fair evaluation, we encourage crowdworkers to annotate given NLDs with a lower score by stating that we give a bonus if they found a flaw of reasoning on the CS interface..The evaluation results shown in Table TABREF24 indicate that the annotated NLDs are of high quality (Reachability), and each NLD is properly derived from supporting documents (Derivability)..On the other hand, we found the quality of 3-step NLDs is relatively lower than the others. Crowdworkers found that 45.3% of 294 (out of 900) 3-step NLDs has missing steps to derive a statement. Let us consider this example: for annotated NLDs “[1] Kouvola is located in Helsinki. [2] Helsinki is in the region of Uusimaa. [3] Uusimaa borders the regions Southwest Finland, Kymenlaakso and some others.” and for the statement “Kouvola is located in Kymenlaakso”, one worker pointed out the missing step “Uusimaa is in Kymenlaakso.”. We speculate that greater steps of reasoning make it difficult for crowdworkers to check the correctness of derivations during the writing task. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "46\n",
      "Question 1: What are some examples of crowdsourced annotations shown in Table TABREF53?\n",
      "\n",
      "Answer 1: Table TABREF53 provides examples of crowdsourced annotations such as sentiment analysis, image classification, named entity recognition, and text classification.\n",
      "Question : for the text Table TABREF53 shows examples of crowdsourced annotations. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "47\n",
      "Question 1: How effective is supervising path attentions in improving human interpretability of generated NLDs?\n",
      "\n",
      "Answer 1: Supervising path attentions (the PRKGC+NS model) is indeed effective for improving the human interpretability of generated NLDs. It also improves the generalization ability of question answering.\n",
      "Question : for the text As shown in Table TABREF37, the PRKGC models learned to reason over more than simple shortest paths. Yet, the PRKGC model do not give considerably good results, which indicates the non-triviality of RC-QED$^{\\rm E}$. Although the PRKGC model do not receive supervision about human-generated NLDs, paths with the maximum score match human-generated NLDs to some extent..Supervising path attentions (the PRKGC+NS model) is indeed effective for improving the human interpretability of generated NLDs. It also improves the generalization ability of question answering. We speculate that $L_d$ functions as a regularizer, which helps models to learn reasoning that helpful beyond training data. This observation is consistent with previous work where an evidence selection task is learned jointly with a main task BIBREF11, BIBREF2, BIBREF5..As shown in Table TABREF43, as the required derivation step increases, the PRKGC+NS model suffers from predicting answer entities and generating correct NLDs. This indicates that the challenge of RC-QED$^{\\rm E}$ is in how to extract relevant information from supporting documents and synthesize these multiple facts to derive an answer..To obtain further insights, we manually analyzed generated NLDs. Table TABREF44 (a) illustrates a positive example, where the model identifies that altudoceras belongs to pseudogastrioceratinae, and that pseudogastrioceratinae is a subfamily of paragastrioceratidae. Some supporting sentences are already similar to human-generated NLDs, thus simply extracting textual relations works well for some problems..On the other hand, typical derivation error is from non-human readable textual relations. In (b), the model states that bumped has a relationship of “,” with hands up, which is originally extracted from one of supporting sentences It contains the UK Top 60 singles “Bumped”, “Hands Up (4 Lovers)” and .... This provides a useful clue for answer prediction, but is not suitable as a derivation. One may address this issue by incorporating, for example, a relation extractor or a paraphrasing mechanism using recent advances of conditional language models BIBREF20. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "48\n",
      "Question 1: What was compared to the baseline models to check their integrity? \n",
      "\n",
      "Answer 1: Existing neural models tailored for QA under the pure WikiHop setting were compared to the baseline models to check their integrity.\n",
      "Question : for the text To check the integrity of our baseline models, we compare our baseline models with existing neural models tailored for QA under the pure WikiHop setting (i.e. evaluation with only an accuracy of predicted answers). Note that these existing models do not output derivations. We thus cannot make a direct comparison, so it servers as a reference purpose. Because WikiHop has no answerability task, we enforced the PRKGC model to always output answers. As shown in Table TABREF45, the PRKGC models achieve a comparable performance to other sophisticated neural models. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "49\n",
      "Question 1: What does the shortest path model output when the shortest path length is more than 3 or when a query entity is not reachable to any candidate entities on G(S)?\n",
      "\n",
      "Answer 1: The shortest path model outputs the word \"Unanswerable\" in these cases.\n",
      "Question : for the text To check the integrity of the PRKGC model, we created a simple baseline model (shortest path model). It outputs a candidate entity with the shortest path length from a query entity on $G(S)$ as an answer. Similarly to the PRKGC model, it traverses the path to generate NLDs. It outputs Unanswerable if (i) a query entity is not reachable to any candidate entities on $G(S)$ or (ii) the shortest path length is more than 3. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "50\n",
      "Question 1: How were the crowdsourced annotations obtained in Section SECREF3 preprocessed? \n",
      "\n",
      "Answer 1: The NLD annotation was converted to \"Unsure\" if it contained the phrase \"needs to be mentioned\" due to annotator misunderstanding. If at least one annotator marked a statement as \"Unsure,\" the answerability was set to \"Unanswerable\" and all NLD annotations were discarded. Otherwise, all NLD annotations were used as multiple references.\n",
      "Question : for the text We aggregated crowdsourced annotations obtained in Section SECREF3. As a preprocessing, we converted the NLD annotation to Unsure if the derivation contains the phrase needs to be mentioned. This is due to the fact that annotators misunderstand our instruction. When at least one crowdworker state that a statement is Unsure, then we set the answerability to Unanswerable and discard NLD annotations. Otherwise, we employ all NLD annotations from workers as multiple reference NLDs. The statistics is shown in Table TABREF36..Regarding $\\mathcal {K}^+, \\mathcal {K}^-$, we extracted 867,936 instances from the training set of WikiHop BIBREF0. We reserve 10% of these instances as a validation set to find the best model. For $\\mathcal {D}$, we used Answerable questions in the training set. To create supervision of path (i.e. $\\mathbf {p}_i$), we selected the path that is most similar to all NLD annotations in terms of ROUGE-L F1. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "1\n",
      "Question 1: What type of optimizer was used during training and what was the batch size?\n",
      "Answer 1: The Adam optimizer with default parameters was used during training with a batch size of 32.\n",
      "Question : for the text We used 100-dimensional vectors for entities, relations, and textual relation representations. We initialize these representations with 100-dimensional Glove Embeddings BIBREF14 and fine-tuned them during training. We retain only top-100,000 frequent words as a model vocabulary. We used Bi-LSTM with 50 dimensional hidden state as a textual relation encoder, and an LSTM with 100-dimensional hidden state as an entity encoder. We used the Adam optimizer (default parameters) BIBREF15 with a batch size of 32. We set the answerability threshold $\\epsilon _k = 0.5$. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "2\n",
      "Question 1: What is the capital city of France?\n",
      "\n",
      "Answer 1: The capital city of France is Paris. NLD: 1.) Paris is the largest city in France and 2.) France's capital city is Paris.\n",
      "Question : for the text Reading comprehension (RC) has become a key benchmark for natural language understanding (NLU) systems and a large number of datasets are now available BIBREF0, BIBREF1, BIBREF2. However, these datasets suffer from annotation artifacts and other biases, which allow systems to “cheat”: Instead of learning to read texts, systems learn to exploit these biases and find answers via simple heuristics, such as looking for an entity with a matching semantic type BIBREF3, BIBREF4. To give another example, many RC datasets contain a large number of “easy” problems that can be solved by looking at the first few words of the question Sugawara2018. In order to provide a reliable measure of progress, an RC dataset thus needs to be robust to such simple heuristics..Towards this goal, two important directions have been investigated. One direction is to improve the dataset itself, for example, so that it requires an RC system to perform multi-hop inferences BIBREF0 or to generate answers BIBREF1. Another direction is to request a system to output additional information about answers. Yang2018HotpotQA:Answering propose HotpotQA, an “explainable” multi-hop Question Answering (QA) task that requires a system to identify a set of sentences containing supporting evidence for the given answer. We follow the footsteps of Yang2018HotpotQA:Answering and explore an explainable multi-hop QA task..In the community, two important types of explanations have been explored so far BIBREF5: (i) introspective explanation (how a decision is made), and (ii) justification explanation (collections of evidences to support the decision). In this sense, supporting facts in HotpotQA can be categorized as justification explanations. The advantage of using justification explanations as benchmark is that the task can be reduced to a standard classification task, which enables us to adopt standard evaluation metrics (e.g. a classification accuracy). However, this task setting does not evaluate a machine's ability to (i) extract relevant information from justification sentences and (ii) synthesize them to form coherent logical reasoning steps, which are equally important for NLU..To address this issue, we propose RC-QED, an RC task that requires not only the answer to a question, but also an introspective explanation in the form of a natural language derivation (NLD). For example, given the question “Which record company released the song Barracuda?” and supporting documents shown in Figure FIGREF1, a system needs to give the answer “Portrait Records” and to provide the following NLD: 1.) Barracuda is on Little Queen, and 2.) Little Queen was released by Portrait Records..The main difference between our work and HotpotQA is that they identify a set of sentences $\\lbrace s_2,s_4\\rbrace $, while RC-QED requires a system to generate its derivations in a correct order. This generation task enables us to measure a machine's logical reasoning ability mentioned above. Due to its subjective nature of the natural language derivation task, we evaluate the correctness of derivations generated by a system with multiple reference answers. Our contributions can be summarized as follows:.We create a large corpus consisting of 12,000 QA pairs and natural language derivations. The developed crowdsourcing annotation framework can be used for annotating other QA datasets with derivations..Through an experiment using two baseline models, we highlight several challenges of RC-QED..We will make the corpus of reasoning annotations and the baseline system publicly available at https://naoya-i.github.io/rc-qed/. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "3\n",
      "Question 1: What is the motivation behind analyzing the nature of RC datasets?\n",
      "\n",
      "Answer 1: The motivation behind analyzing the nature of RC datasets is to determine the extent to which RC models understand natural language. Several studies have suggested that current RC datasets have unintended bias which allows RC systems to rely on cheap heuristics to answer questions.\n",
      "Question : for the text There is a large body of work on analyzing the nature of RC datasets, motivated by the question to what degree RC models understand natural language BIBREF3, BIBREF4. Several studies suggest that current RC datasets have unintended bias, which enables RC systems to rely on a cheap heuristics to answer questions. For instance, Sugawara2018 show that some of these RC datasets contain a large number of “easy” questions that can be solved by a cheap heuristics (e.g. by looking at a first few tokens of questions). Responding to their findings, we take a step further and explore the new task of RC that requires RC systems to give introspective explanations as well as answers. In addition, recent studies show that current RC models and NLP models are vulnerable to adversarial examples BIBREF29, BIBREF30, BIBREF31. Explicit modeling of NLDs is expected to reguralize RC models, which could prevent RC models' strong dependence on unintended bias in training data (e.g. annotation artifact) BIBREF32, BIBREF8, BIBREF2, BIBREF5, as partially confirmed in Section SECREF46. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "4\n",
      "Question 1: What is the uniqueness of the dataset used in this study?\n",
      "\n",
      "Answer 1: The uniqueness of the dataset used in this study is that it measures a machine's ability to extract relevant information from a set of documents and to build coherent logical reasoning steps. Unlike other datasets annotated with introspective explanations, which offer the classification task of single sentences or sentence pairs, this dataset provides new challenges for NLU by requiring models to output explanations based on multiple documents.\n",
      "Question : for the text There are existing NLP tasks that require models to output explanations (Table TABREF50). FEVER BIBREF25 requires a system to judge the “factness” of a claim as well as to identify justification sentences. As discussed earlier, we take a step further from justification explanations to provide new challenges for NLU..Several datasets are annotated with introspective explanations, ranging from textual entailments BIBREF8 to argumentative texts BIBREF26, BIBREF27, BIBREF33. All these datasets offer the classification task of single sentences or sentence pairs. The uniqueness of our dataset is that it measures a machine's ability to extract relevant information from a set of documents and to build coherent logical reasoning steps. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "5\n",
      "Question 1: What is the most similar work to the authors' dataset of explanations annotated for RC? \n",
      "\n",
      "Answer 1: The most similar work is the Science QA dataset which provides a small set of NLDs annotated for analysis purposes, as stated in the text.\n",
      "Question : for the text There exists few RC datasets annotated with explanations (Table TABREF50). The most similar work to ours is Science QA dataset BIBREF21, BIBREF22, BIBREF23, which provides a small set of NLDs annotated for analysis purposes. By developing the scalable crowdsourcing framework, our work provides one order-of-magnitude larger NLDs which can be used as a benchmark more reliably. In addition, it provides the community with new types of challenges not included in HotpotQA. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "6\n",
      "Question 1: What are the evaluation metrics for RC-QED?\n",
      "Answer 1: The evaluation metrics for RC-QED include answerability (evaluated by Precision/Recall/F1), answer precision (for Answerable predictions only), and derivation precision (evaluated by ROUGE-L and BLEU-4).\n",
      "Question : for the text We formally define RC-QED as follows:.Given: (i) a question $Q$, and (ii) a set $S$ of supporting documents relevant to $Q$;.Find: (i) answerability $s \\in \\lbrace \\textsf {Answerable},$ $\\textsf {Unanswerable} \\rbrace $, (ii) an answer $a$, and (iii) a sequence $R$ of derivation steps..We evaluate each prediction with the following evaluation metrics:.Answerability: Correctness of model's decision on answerability (i.e. binary classification task) evaluated by Precision/Recall/F1..Answer precision: Correctness of predicted answers (for Answerable predictions only). We follow the standard practice of RC community for evaluation (e.g. an accuracy in the case of multiple choice QA)..Derivation precision: Correctness of generated NLDs evaluated by ROUGE-L BIBREF6 (RG-L) and BLEU-4 (BL-4) BIBREF7. We follow the standard practice of evaluation for natural language generation BIBREF1. Derivation steps might be subjective, so we resort to multiple reference answers. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "7\n",
      "Question 1: What is the answer for the question about Barracuda and Little Queen on Portrait Records?\n",
      "\n",
      "Answer 1: The answer is Barracuda.\n",
      "Question : for the text This paper instantiates RC-QED by employing multiple choice, entity-based multi-hop QA BIBREF0 as a testbed (henceforth, RC-QED$^{\\rm E}$). In entity-based multi-hop QA, machines need to combine relational facts between entities to derive an answer. For example, in Figure FIGREF1, understanding the facts about Barracuda, Little Queen, and Portrait Records stated in each article is required. This design choice restricts a problem domain, but it provides interesting challenges as discussed in Section SECREF46. In addition, such entity-based chaining is known to account for the majority of reasoning types required for multi-hop reasoning BIBREF2..More formally, given (i) a question $Q=(r, q)$ represented by a binary relation $r$ and an entity $q$ (question entity), (ii) relevant articles $S$, and (iii) a set $C$ of candidate entities, systems are required to output (i) an answerability $s \\in \\lbrace \\textsf {Answerable}, \\textsf {Unanswerable} \\rbrace $, (ii) an entity $e \\in C$ (answer entity) that $(q, r, e)$ holds, and (iii) a sequence $R$ of derivation steps as to why $e$ is believed to be an answer. We define derivation steps as an $m$ chain of relational facts to derive an answer, i.e. $(q, r_1, e_1), (e_1, r_2, e_2), ..., (e_{m-1}, r_{m-1}, e_m),$ $(e_m, r_m, e_{m+1}))$. Although we restrict the form of knowledge to entity relations, we use a natural language form to represent $r_i$ rather than a closed vocabulary (see Figure FIGREF1 for an example). generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "8\n",
      "Question 1: What datasets did the study explore to compare the opinions of users on microblogs to that of experts?\n",
      "\n",
      "Answer 1: The study explored three datasets: US Presidential Debates 2015-16, Grammy Awards 2013, and Super Bowl 2013.\n",
      "Question : for the text This paper presents a study that compares the opinions of users on microblogs, which is essentially the crowd wisdom, to that of the experts in the field. Specifically, we explore three datasets: US Presidential Debates 2015-16, Grammy Awards 2013, Super Bowl 2013. We determined if the opinions of the crowd and the experts match by using the sentiments of the tweets to predict the outcomes of the debates/Grammys/Super Bowl. We observed that in most of the cases, the predictions were right indicating that crowd wisdom is indeed worth looking at and mining sentiments in microblogs is useful. In some cases where there were disagreements, however, we observed that the opinions of the experts did have some influence on the opinions of the users. We also find that the features that were most useful in our case of multi-label classification was a combination of the document-embedding and topic features. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "9\n",
      "What may have caused a decrease in tweet frequency towards the final debates?\n",
      "\n",
      "According to the analysis, the frequency of tweets decreases consistently towards the final debates. This may be due to the fact that although people still watch the debates, the number of people who actively tweet about it greatly reduces. The tweeting community is mainly youngsters and it is possible that most of them did not watch the later debates.\n",
      "Question : for the text In this section, we analyze the presidential debates data and show some trends..First, we look at the trend of the tweet frequency. Figure FIGREF21 shows the trends of the tweet frequency and the number of TV viewers as the debates progress over time. We observe from Figures FIGREF21 and FIGREF21 that for the first 5 debates considered, the trend of the number of TV viewers matches the trend of the number of tweets. However, we can see that towards the final debates, the frequency of the tweets decreases consistently. This shows an interesting fact that although the people still watch the debates, the number of people who tweet about it are greatly reduced. But the tweeting community are mainly youngsters and this shows that most of the tweeting community, who actively tweet, didn't watch the later debates. Because if they did, then the trends should ideally match..Next we look at how the tweeting activity is on days of the debate: specifically on the day of the debate, the next day and two days later. Figure FIGREF22 shows the trend of the tweet frequency around the day of the 5th republican debate, i.e December 15, 2015. As can be seen, the average number of people tweet more on the day of the debate than a day or two after it. This makes sense intuitively because the debate would be fresh in their heads..Then, we look at how people tweet in the hours of the debate: specifically during the debate, one hour after and then two hours after. Figure FIGREF23 shows the trend of the tweet frequency around the hour of the 5th republican debate, i.e December 15, 2015. We notice that people don't tweet much during the debate but the activity drastically increases after two hours. This might be because people were busy watching the debate and then taking some time to process things, so that they can give their opinion..We have seen the frequency of tweets over time in the previous trends. Now, we will look at how the sentiments of the candidates change over time..First, Figure FIGREF24 shows how the sentiments of the candidates changed across the debates. We find that many of the candidates have had ups and downs towards in the debates. But these trends are interesting in that, it gives some useful information about what went down in the debate that caused the sentiments to change (sometimes drastically). For example, if we look at the graph for Donald Trump, we see that his sentiment was at its lowest point during the debate held on December 15. Looking into the debate, we can easily see why this was the case. At a certain point in the debate, Trump was asked about his ideas for the nuclear triad. It is very important that a presidential candidate knows about this, but Trump had no idea what the nuclear triad was and, in a transparent attempt to cover his tracks, resorted to a “we need to be strong\" speech. It can be due to this embarrassment that his sentiment went down during this debate..Next, we investigate how the sentiments of the users towards the candidates change before and after the debate. In essence, we examine how the debate and the results of the debates given by the experts affects the sentiment of the candidates. Figure FIGREF25 shows the sentiments of the users towards the candidate during the 5th Republican Debate, 15th December 2015. It can be seen that the sentiments of the users towards the candidates does indeed change over the course of two days. One particular example is that of Jeb Bush. It seems that the populace are generally prejudiced towards the candidates, which is reflected in their sentiments of the candidates on the day of the debate. The results of the Washington Post are released in the morning after the debate. One can see the winners suggested by the Washington Post in Table TABREF35. One of the winners in that debate according to them is Jeb Bush. Coincidentally, Figure FIGREF25 suggests that the sentiment of Bush has gone up one day after the debate (essentially, one day after the results given by the experts are out)..There is some influence, for better or worse, of these experts on the minds of the users in the early debates, but towards the final debates the sentiments of the users are mostly unwavering, as can be seen in Figure FIGREF25. Figure FIGREF25 is on the last Republican debate, and suggests that the opinions of the users do not change much with time. Essentially the users have seen enough debates to make up their own minds and their sentiments are not easily wavered. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "10\n",
      "What are hashtags and how are they used in tweets? \n",
      "\n",
      "Answer 1: Hashtags are used by users in tweets to mark topics and increase the visibility of the tweet. They start with the symbol \"#\" and can be used to categorize or follow certain topics or events on Twitter.\n",
      "Question : for the text Twitter is a social networking and microblogging service that allows users to post real-time messages, called tweets. Tweets are very short messages, a maximum of 140 characters in length. Due to such a restriction in length, people tend to use a lot of acronyms, shorten words etc. In essence, the tweets are usually very noisy. There are several aspects to tweets such as: 1) Target: Users use the symbol “@\" in their tweets to refer to other users on the microblog. 2) Hashtag: Hashtags are used by users to mark topics. This is done to increase the visibility of the tweets..We conduct experiments on 3 different datasets, as mentioned earlier: 1) US Presidential Debates, 2) Grammy Awards 2013, 3) Superbowl 2013. To construct our presidential debates dataset, we have used the Twitter Search API to collect the tweets. Since there was no publicly available dataset for this, we had to collect the data manually. The data was collected on 10 different presidential debates: 7 republican and 3 democratic, from October 2015 to March 2016. Different hashtags like “#GOP, #GOPDebate” were used to filter out tweets specific to the debate. This is given in Table TABREF2. We extracted only english tweets for our dataset. We collected a total of 104961 tweets were collected across all the debates. But there were some limitations with the API. Firstly, the server imposes a rate limit and discards tweets when the limit is reached. The second problem is that the API returns many duplicates. Thus, after removing the duplicates and irrelevant tweets, we were left with a total of 17304 tweets. This includes the tweets only on the day of the debate. We also collected tweets on the days following the debate..As for the other two datasets, we collected them from available-online repositories. There were a total of 2580062 tweets for the Grammy Awards 2013, and a total of 2428391 tweets for the Superbowl 2013. The statistics are given in Tables TABREF3 and TABREF3. The tweets for the grammy were before the ceremony and during. However, we only use the tweets before the ceremony (after the nominations were announced), to predict the winners. As for the superbowl, the tweets collected were during the game. But we can predict interesting things like Most Valuable Player etc. from the tweets. The tweets for both of these datasets were annotated and thus did not require any human intervention. However, the tweets for the debates had to be annotated..Since we are using a supervised approach in this paper, we have all the tweets (for debates) in the training set human-annotated. The tweets were already annotated for the Grammys and Super Bowl. Some statistics about our datasets are presented in Tables TABREF3, TABREF3 and TABREF3. The annotations for the debate dataset comprised of 2 labels for each tweet: 1) Candidate: This is the candidate of the debate to whom the tweet refers to, 2) Sentiment: This represents the sentiment of the tweet towards that candidate. This is either positive or negative..The task then becomes a case of multi-label classification. The candidate labels are not so trivial to obtain, because there are tweets that do not directly contain any candidates' name. For example, the tweets, “a business man for president??” and “a doctor might sure bring about a change in America!” are about Donal Trump and Ben Carson respectively. Thus, it is meaningful to have a multi-label task..The annotations for the other two datasets are similar, in that one of the labels was the sentiment and the other was category-dependent in the outcome-prediction task, as discussed in the sections below. For example, if we are trying to predict the \"Album of the Year\" winners for the Grammy dataset, the second label would be the nominees for that category (album of the year). generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "11\n",
      "Question 1: What is the reason behind replacing repeated letters in a word with just three occurrences?\n",
      "\n",
      "Answer 1: The preprocessing filter replaces repeated occurrences of letters (more than two) with just three occurrences to distinguish the exaggerated usage from the regular ones.\n",
      "Question : for the text As noted earlier, tweets are generally noisy and thus require some preprocessing done before using them. Several filters were applied to the tweets such as: (1) Usernames: Since users often include usernames in their tweets to direct their message, we simplify it by replacing the usernames with the token “USER”. For example, @michael will be replaced by USER. (2) URLs: In most of the tweets, users include links that add on to their text message. We convert/replace the link address to the token “URL”. (3) Repeated Letters: Oftentimes, users use repeated letters in a word to emphasize their notion. For example, the word “lol” (which stands for “laugh out loud”) is sometimes written as “looooool” to emphasize the degree of funnyness. We replace such repeated occurrences of letters (more than 2), with just 3 occurrences. We replace with 3 occurrences and not 2, so that we can distinguish the exaggerated usage from the regular ones. (4) Multiple Sentiments: Tweets which contain multiple sentiments are removed, such as \"I hate Donald Trump, but I will vote for him\". This is done so that there is no ambiguity. (5) Retweets: In Twitter, many times tweets of a person are copied and posted by another user. This is known as retweeting and they are commonly abbreviated with “RT”. These are removed and only the original tweets are processed. (6) Repeated Tweets: The Twitter API sometimes returns a tweet multiple times. We remove such duplicates to avoid putting extra weight on any particular tweet. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "12\n",
      "Question 1: What are the two tasks for which different evaluation metrics are defined?\n",
      "Answer 1: The two tasks for which different evaluation metrics are defined are Sentiment Analysis and Outcome Prediction.\n",
      "Question : for the text In this section, we define the different evaluation metrics that we use for different tasks. We have two tasks at hand: 1) Sentiment Analysis, 2) Outcome Prediction. We use different metrics for these two tasks. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "13\n",
      "Question 1: What is the Mean F-measure metric used for in outcome prediction evaluation?\n",
      "\n",
      "Answer 1: The Mean F-measure metric is used to take into account both precision and recall of the relevant and returned results in outcome prediction evaluation.\n",
      "Question : for the text For the case of outcome prediction, we will have a predicted set and an actual set of results. Thus, we can use common information retrieval metrics to evaluate the prediction performance. Those metrics are listed below:.Mean F-measure: F-measure takes into account both the precision and recall of the results. In essence, it takes into account how many of the relevant results are returned and also how many of the returned results are relevant..where $|D|$ is the number of queries (debates/categories for grammy winners etc.), $P_i$ and $R_i$ are the precision and recall for the $i^{th}$ query..Mean Average Precision: As a standard metric used in information retrieval, Mean Average Precision for a set of queries is mean of the average precision scores for each query:.where $|D|$ is the number of queries (e.g., debates), $P_i(k)$ is the precision at $k$ ($P@k$) for the $i^{th}$ query, $rel_i(k)$ is an indicator function, equaling 1 if the document at position $k$ for the $i^th$ query is relevant, else 0, and $|RD_i|$ is the number of relevant documents for the $i^{th}$ query. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "14\n",
      "Question 1: What is Hamming Loss used for in sentiment analysis?\n",
      "\n",
      "Answer 1: Hamming Loss is used to evaluate the performance of different methods in sentiment analysis. It takes into account the prediction error and missing error, normalized over the total number of classes and total number of examples. The performance is considered better when the Hamming Loss is smaller, with 0 being the ideal case.\n",
      "Question : for the text In the study of sentiment analysis, we use “Hamming Loss” to evaluate the performance of the different methods. Hamming Loss, based on Hamming distance, takes into account the prediction error and the missing error, normalized over the total number of classes and total number of examples BIBREF29. The Hamming Loss is given below:.where $|D|$ is the number of examples in the dataset and $|L|$ is the number of labels. $S_i$ and $Y_i$ denote the sets of true and predicted labels for instance $i$ respectively. $\\oplus $ stands for the XOR operation BIBREF30. Intuitively, the performance is better, when the Hamming Loss is smaller. 0 would be the ideal case. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "15\n",
      "Question 1: What is the main focus of the paper's analysis?\n",
      "Answer 1: The main focus of the paper's analysis is on three events: the US Presidential Debates 2015-16, Grammy Awards 2013, and Super Bowl 2013. The paper analyzes the sentiment of tweets related to these events and predicts outcomes based on sentiment analysis. The main focus is on the analysis of the presidential debates, and the paper also performs trend analysis on the sentiments of users toward the candidates over time.\n",
      "Question : for the text Over the past few years, microblogs have become one of the most popular online social networks. Microblogging websites have evolved to become a source of varied kinds of information. This is due to the nature of microblogs: people post real-time messages about their opinions and express sentiment on a variety of topics, discuss current issues, complain, etc. Twitter is one such popular microblogging service where users create status messages (called “tweets\"). With over 400 million tweets per day on Twitter, microblog users generate large amount of data, which cover very rich topics ranging from politics, sports to celebrity gossip. Because the user generated content on microblogs covers rich topics and expresses sentiment/opinions of the mass, mining and analyzing this information can prove to be very beneficial both to the industrial and the academic community. Tweet classification has attracted considerable attention because it has become very important to analyze peoples' sentiments and opinions over social networks..Most of the current work on analysis of tweets is focused on sentiment analysis BIBREF0, BIBREF1, BIBREF2. Not much has been done on predicting outcomes of events based on the tweet sentiments, for example, predicting winners of presidential debates based on the tweets by analyzing the users' sentiments. This is possible intuitively because the sentiment of the users in their tweets towards the candidates is proportional to the performance of the candidates in the debate..In this paper, we analyze three such events: 1) US Presidential Debates 2015-16, 2) Grammy Awards 2013, and 3) Super Bowl 2013. The main focus is on the analysis of the presidential debates. For the Grammys and the Super Bowl, we just perform sentiment analysis and try to predict the outcomes in the process. For the debates, in addition to the analysis done for the Grammys and Super Bowl, we also perform a trend analysis. Our analysis of the tweets for the debates is 3-fold as shown below..Sentiment: Perform a sentiment analysis on the debates. This involves: building a machine learning model which learns the sentiment-candidate pair (candidate is the one to whom the tweet is being directed) from the training data and then using this model to predict the sentiment-candidate pairs of new tweets..Predicting Outcome: Here, after predicting the sentiment-candidate pairs on the new data, we predict the winner of the debates based on the sentiments of the users..Trends: Here, we analyze certain trends of the debates like the change in sentiments of the users towards the candidates over time (hours, days, months) and how the opinion of experts such as Washington Post affect the sentiments of the users..For the sentiment analysis, we look at our problem in a multi-label setting, our two labels being sentiment polarity and the candidate/category in consideration. We test both single-label classifiers and multi-label ones on the problem and as intuition suggests, the multi-label classifier RaKel performs better. A combination of document-embedding features BIBREF3 and topic features (essentially the document-topic probabilities) BIBREF4 is shown to give the best results. These features make sense intuitively because the document-embedding features take context of the text into account, which is important for sentiment polarity classification, and topic features take into account the topic of the tweet (who/what is it about)..The prediction of outcomes of debates is very interesting in our case. Most of the results seem to match with the views of some experts such as the political pundits of the Washington Post. This implies that certain rules that were used to score the candidates in the debates by said-experts were in fact reflected by reading peoples' sentiments expressed over social media. This opens up a wide variety of learning possibilities from users' sentiments on social media, which is sometimes referred to as the wisdom of crowd..We do find out that the public sentiments are not always coincident with the views of the experts. In this case, it is interesting to check whether the views of the experts can affect the public, for example, by spreading through the social media microblogs such as Twitter. Hence, we also conduct experiments to compare the public sentiment before and after the experts' views become public and thus notice the impact of the experts' views on the public sentiment. In our analysis of the debates, we observe that in certain debates, such as the 5th Republican Debate, held on December 15, 2015, the opinions of the users vary from the experts. But we see the effect of the experts on the sentiment of the users by looking at their opinions of the same candidates the next day..Our contributions are mainly: we want to see how predictive the sentiment/opinion of the users are in social media microblogs and how it compares to that of the experts. In essence, we find that the crowd wisdom in the microblog domain matches that of the experts in most cases. There are cases, however, where they don't match but we observe that the crowd's sentiments are actually affected by the experts. This can be seen in our analysis of the presidential debates..The rest of the paper is organized as follows: in section SECREF2, we review some of the literature. In section SECREF3, we discuss the collection and preprocessing of the data. Section SECREF4 details the approach taken, along with the features and the machine learning methods used. Section SECREF7 discusses the results of the experiments conducted and lastly section SECREF8 ends with a conclusion on the results including certain limitations and scopes for improvement to work on in the future. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "16\n",
      "What is the Dictionary of Affect in Language (DAL)?\n",
      "\n",
      "The Dictionary of Affect in Language (DAL) is a dictionary of about 8000 English words that assigns a pleasantness score to each word on a scale of 1-3. It is used to obtain the prior polarity of words in the process of classifying tweets. Words with polarity higher than 0.8 are considered positive and those less than 0.5 are considered negative. If a word is not present in the dictionary, its synonyms are looked up in WordNet and assigned the score of the synonym if it is in the dictionary.\n",
      "Question : for the text In order to classify the tweets, a set of features is extracted from each of the tweets, such as n-gram, part-of-speech etc. The details of these features are given below:.n-gram: This represents the frequency counts of n-grams, specifically that of unigrams and bigrams..punctuation: The number of occurrences of punctuation symbols such as commas, exclamation marks etc..POS (part-of-speech): The frequency of each POS tagger is used as the feature..prior polarity scoring: Here, we obtain the prior polarity of the words BIBREF6 using the Dictionary of Affect in Language (DAL) BIBREF28. This dictionary (DAL) of about 8000 English words assigns a pleasantness score to each word on a scale of 1-3. After normalizing, we can assign the words with polarity higher than $0.8$ as positive and less than $0.5$ as negative. If a word is not present in the dictionary, we lookup its synonyms in WordNet: if this word is there in the dictionary, we assign the original word its synonym's score..Twitter Specific features:.Number of hashtags ($\\#$ symbol).Number of mentioning users ( symbol).Number of hyperlinks.Document embedding features: Here, we use the approach proposed by Mikolov et al. BIBREF3 to embed an entire tweet into a vector of features.Topic features: Here, LDA (Latent Dirichlet Allocation) BIBREF4 is used to extract topic-specific features for a tweet (document). This is basically the topic-document probability that is outputted by the model..In the following experiments, we use 1-$gram$, 2-$gram$ and $(1+2)$-$gram$ to denote unigram, bigram and a combination of unigram and bigram features respectively. We also combine punctuation and the other features as miscellaneous features and use $MISC$ to denote this. We represent the document-embedding features by $DOC$ and topic-specific features by $TOPIC$. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "17\n",
      "Question 1: What are the two categories of algorithms for sentiment classification?\n",
      "Answer 1: The two categories of algorithms for sentiment classification are single-label and multi-label.\n",
      "Question : for the text We compare 4 different models for performing our task of sentiment classification. We then pick the best performing model for the task of outcome prediction. Here, we have two categories of algorithms: single-label and multi-label (We already discussed above why it is meaningful to have a multi-label task earlier), because one can represent $<$candidate, sentiment$>$ as a single class label or have candidate and sentiment as two separate labels. They are listed below: generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "18\n",
      "Question 1: How does RAkEL BIBREF27 handle multi-label classification?\n",
      "\n",
      "Answer 1: RAkEL BIBREF27 creates a single binary classifier for every label combination and uses multiple labeled powerset classifiers, each trained on a random subset of the actual labels, for classification.\n",
      "Question : for the text RAkEL (RAndom k labELsets): RAkEL BIBREF27 is a multi-label classification algorithm that uses labeled powerset (LP) transformation: it basically creates a single binary classifier for every label combination and then uses multiple LP classifiers, each trained on a random subset of the actual labels, for classification. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "19\n",
      "Question 1: What is the Elman Recurrent Neural Network?\n",
      "\n",
      "Answer 1: The Elman Recurrent Neural Network is a type of artificial neural network that was proposed by Jeff Elman in 1990. It is a class of recurrent neural networks where connections between units form a directed cycle, creating an internal state of the network that allows it to exhibit dynamic temporal behavior.\n",
      "Question : for the text Naive Bayes: We use a multinomial Naive Bayes model. A tweet $t$ is assigned a class $c^{*}$ such that.where there are $m$ features and $f_i$ represents the $i^{th}$ feature..Support Vector Machines: Support Vector Machines (SVM) constructs a hyperplane or a set of hyperplanes in a high-dimensional space, which can then be used for classification. In our case, we use SVM with Sequential Minimal Optimization (SMO) BIBREF25, which is an algorithm for solving the quadratic programming (QP) problem that arises during the training of SVMs..Elman Recurrent Neural Network: Recurrent Neural Networks (RNNs) are gaining popularity and are being applied to a wide variety of problems. They are a class of artificial neural networks, where connections between units form a directed cycle. This creates an internal state of the network which allows it to exhibit dynamic temporal behavior. The Elman RNN was proposed by Jeff Elman in the year 1990 BIBREF26. We use this in our task. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "20\n",
      "What is the purpose of trend analysis in analyzing the debates?\n",
      "\n",
      "The purpose of trend analysis in analyzing the debates is to look at the change in sentiments of the users towards the candidates over time, examine the effect of Washington Post on the views of the users, and study the behavior of the users by looking at the correlation of the tweet volume with the number of viewers and the variation of tweet volume over time.\n",
      "Question : for the text Our analysis of the debates is 3-fold including sentiment analysis, outcome prediction, and trend analysis..Sentiment Analysis: To perform a sentiment analysis on the debates, we first extract all the features described below from all the tweets in the training data. We then build the different machine learning models described below on these set of features. After that, we evaluate the output produced by the models on unseen test data. The models essentially predict candidate-sentiment pairs for each tweet..Outcome Prediction: Predict the outcome of the debates. After obtaining the sentiments on the test data for each tweet, we can compute the net normalized sentiment for each presidential candidate in the debate. This is done by looking at the number of positive and negative sentiments for each candidate. We then normalize the sentiment scores of each candidate to be in the same scale (0-1). After that, we rank the candidates based on the sentiment scores and predict the top $k$ as the winners..Trend Analysis: We also analyze some certain trends of the debates. Firstly, we look at the change in sentiments of the users towards the candidates over time (hours, days, months). This is done by computing the sentiment scores for each candidate in each of the debates and seeing how it varies over time, across debates. Secondly, we examine the effect of Washington Post on the views of the users. This is done by looking at the sentiments of the candidates (to predict winners) of a debate before and after the winners are announced by the experts in Washington Post. This way, we can see if Washington Post has had any effect on the sentiments of the users. Besides that, to study the behavior of the users, we also look at the correlation of the tweet volume with the number of viewers as well as the variation of tweet volume over time (hours, days, months) for debates..As for the Grammys and the Super Bowl, we only perform the sentiment analysis and predict the outcomes. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "21\n",
      "Question 1: What is an example of using Twitter data to predict box-office success for movies?\n",
      "\n",
      "Answer 1: Mishne and Glance (BIBREF17) used sentiment analysis to correlate positive sentiments in blog posts with movie box-office scores, but found the correlations to be fairly low and not sufficient for predictive purposes. Another study by BIBREF16 used machine learning to predict box-office success on the release day using Twitter data.\n",
      "Question : for the text Sentiment analysis as a Natural Language Processing task has been handled at many levels of granularity. Specifically on the microblog front, some of the early results on sentiment analysis are by BIBREF0, BIBREF1, BIBREF2, BIBREF5, BIBREF6. Go et al. BIBREF0 applied distant supervision to classify tweet sentiment by using emoticons as noisy labels. Kouloumpis et al. BIBREF7 exploited hashtags in tweets to build training data. Chenhao Tan et al. BIBREF8 determined user-level sentiments on particular topics with the help of the social network graph..There has been some work in event detection and extraction in microblogs as well. In BIBREF9, the authors describe a way to extract major life events of a user based on tweets that either congratulate/offer condolences. BIBREF10 build a key-word graph from the data and then detect communities in this graph (cluster) to find events. This works because words that describe similar events will form clusters. In BIBREF11, the authors use distant supervision to extract events. There has also been some work on event retrieval in microblogs BIBREF12. In BIBREF13, the authors detect time points in the twitter stream when an important event happens and then classify such events based on the sentiments they evoke using only non-textual features to do so. In BIBREF14, the authors study how much of the opinion extracted from Online Social Networks (OSN) user data is reflective of the opinion of the larger population. Researchers have also mined Twitter dataset to analyze public reaction to various events: from election debate performance BIBREF15, where the authors demonstrate visuals and metrics that can be used to detect sentiment pulse, anomalies in that pulse, and indications of controversial topics that can be used to inform the design of visual analytic systems for social media events, to movie box-office predictions on the release day BIBREF16. Mishne and Glance BIBREF17 correlate sentiments in blog posts with movie box-office scores. The correlations they observed for positive sentiments are fairly low and not sufficient to use for predictive purposes. Recently, several approaches involving machine learning and deep learning have also been used in the visual and language domains BIBREF18, BIBREF19, BIBREF20, BIBREF21, BIBREF22, BIBREF23, BIBREF24. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "22\n",
      "Question 1: What is RaKel trained to predict for the unlabeled data?\n",
      "\n",
      "Answer 1: RaKel is trained to predict the sentiment-labels for the unlabeled data.\n",
      "Question : for the text In this section, we show the results for the outcome-prediction of the events. RaKel, as the best performing method, is trained to predict the sentiment-labels for the unlabeled data. The sentiment labels are then used to determine the outcome of the events. In the Tables (TABREF35, TABREF36, TABREF37) of outputs given, we only show as many predictions as there are winners. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "23\n",
      "What are the main categories of Grammy Awards that are most looked forward to?\n",
      "\n",
      "The main categories of Grammy Awards that are most looked forward to are Album of the Year, Record of the Year, Song of the Year, and Best New Artist.\n",
      "Question : for the text A Grammy Award is given to outstanding achievement in the music industry. There are two types of awards: “General Field” awards, which are not restricted by genre, and genre-specific awards. Since, there can be upto 80 categories of awards, we only focus on the main 4: 1) Album of the Year, 2) Record of the Year, 3) Song of the Year, and 4) Best New Artist. These categories are the main in the awards ceremony and most looked forward to. That is also why we choose to predict the outcomes of these categories based on the tweets. We use the tweets before the ceremony (but after the nominations) to predict the outcomes..Basically, we have a list of nominations for each category. We filter the tweets based on these nominations and then predict the winner as with the case of the debates. The outcomes are listed in Table TABREF36. We see that largely, the opinion of the users on the social network, agree with the deciding committee of the awards. The winners agree for all the categories except “Song of the Year”. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "24\n",
      "Question 1: What does Table TABREF35 show for the US presidential debates?\n",
      "\n",
      "Answer 1: Table TABREF35 shows the winners predicted by our system and the winners given out by the Washington Post for the outcome prediction task for the US presidential debates.\n",
      "Question : for the text The results obtained for the outcome prediction task for the US presidential debates is shown in Table TABREF35. Table TABREF35 shows the winners as given in the Washington Post (3rd column) and the winners that are predicted by our system (2nd column). By comparing the set of results obtained from both the sources, we find that the set of candidates predicted match to a large extent with the winners given out by the Washington Post. The result suggests that the opinions of the social media community match with that of the journalists in most cases. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "25\n",
      "Question 1: What is the purpose of determining the Most Valuable Player (MVP) during the Super Bowl game using tweets?\n",
      "\n",
      "Answer 1: The purpose is to determine the candidate with the highest positive sentiment by the end of the game and predict the MVP. This helps in accurately determining the outcomes of the game and evaluating the performance using evaluation metrics.\n",
      "Question : for the text The Super Bowl is the annual championship game of the National Football League. We have collected the data for the year 2013. Here, the match was between the Baltimore Ravens and the San Francisco 49ers. The tweets that we have collected are during the game. From these tweets, one could trivially determine the winner. But an interesting outcome would be to predict the Most Valuable Player (MVP) during the game. To determine this, all the tweets were looked at and we determined the candidate with the highest positive sentiment by the end of the game. The result in Table TABREF37 suggests that we are able to determine the outcomes accurately..Table TABREF43 displays some evaluation metrics for this task. These were computed based on the predicted outcomes and the actual outcomes for each of the different datasets. Since the number of participants varies from debate-to-debate or category-to-category for Grammy etc., we cannot return a fixed number of winners for everything. So, the size of our returned ranked-list is set to half of the number of participants (except for the MVP for Super Bowl; there are so many players and returning half of them when only one of them is relevant is meaningless. So, we just return the top 10 players). As we can see from the metrics, the predicted outcomes match quite well with the actual ones (or the ones given by the experts). generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "26\n",
      "Q: Which algorithm performed best in the sentiment analysis task?\n",
      "A: RaKel performed the best out of all the algorithms tested in the sentiment analysis task.\n",
      "Question : for the text We use 3 different datasets for the problem of sentiment analysis, as already mentioned. We test the four different algorithms mentioned in Section SECREF6, with a different combination of features that are described in Section SECREF10. To evaluate our models, we use the “Hamming Loss” metric as discussed in Section SECREF6. We use this metric because our problem is in the multi-class classification domain. However, the single-label classifiers like SVM, Naive Bayes, Elman RNN cannot be evaluated against this metric directly. To do this, we split the predicted labels into a format that is consistent with that of multi-label classifiers like RaKel. The results of the experiments for each of the datasets are given in Tables TABREF34, TABREF34 and TABREF34. In the table, $f_1$, $f_2$, $f_3$, $f_4$, $f_5$ and $f_6$ denote the features 1-$gram$, 2-$gram$, $(1+2)$-$gram$, $(1+2)$-$gram + MISC$, $DOC$ and $DOC + TOPIC$ respectively. Note that lower values of hamming losses are more desirable. We find that RaKel performs the best out of all the algorithms. RaKel is more suited for the task because our task is a multi-class classification. Among all the single-label classifiers, SVM performs the best. We also observe that as we use more complex feature spaces, the performance increases. This is true for almost all of the algorithms listed..Our best performing features is a combination of paragraph embedding features and topic features from LDA. This makes sense intuitively because paragraph-embedding takes into account the context in the text. Context is very important in determining the sentiment of a short text: having negative words in the text does not always mean that the text contains a negative sentiment. For example, the sentence “never say never is not a bad thing” has many negative words; but the sentence as a whole does not have a negative sentiment. This is why we need some kind of context information to accurately determine the sentiment. Thus, with these embedded features, one would be able to better determine the polarity of the sentence. The other label is the entity (candidate/song etc.) in consideration. Topic features here make sense because this can be considered as the topic of the tweet in some sense. This can be done because that label captures what or whom the tweet is about. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "27\n",
      "Question 1: What is the key idea behind the KANE knowledge graph embedding method?\n",
      "\n",
      "Answer 1: The key idea behind the KANE knowledge graph embedding method is to aggregate all attribute triples with bias and perform embedding propagation based on relation triples when calculating the representations of given entity.\n",
      "Question : for the text Many recent works have demonstrated the benefits of knowledge graph embedding in knowledge graph completion, such as relation extraction. However, We argue that knowledge graph embedding method still have room for improvement. First, TransE and its most extensions only take direct relations between entities into consideration. Second, most existing knowledge graph embedding methods just leverage relation triples of KGs while ignoring a large number of attribute triples. In order to overcome these limitation, inspired by the recent developments of graph convolutional networks, we propose a new knowledge graph embedding methods, named KANE. The key ideal of KANE is to aggregate all attribute triples with bias and perform embedding propagation based on relation triples when calculating the representations of given entity. Empirical results on three datasets show that KANE significantly outperforms seven state-of-arts methods. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "28\n",
      "Question 1: What are the three knowledge graphs evaluated in this study?\n",
      "\n",
      "Answer 1: The three knowledge graphs evaluated in this study are Freebase (FB24K), DBpedia (DBP24K), and a self-construction game knowledge graph (Game30K).\n",
      "Question : for the text In this study, we evaluate our model on three real KG including two typical large-scale knowledge graph: Freebase BIBREF0, DBpedia BIBREF1 and a self-construction game knowledge graph. First, we adapt a dataset extracted from Freebase, i.e., FB24K, which used by BIBREF26. Then, we collect extra entities and relations that from DBpedia which that they should have at least 100 mentions BIBREF7 and they could link to the entities in the FB24K by the sameAs triples. Finally, we build a datasets named as DBP24K. In addition, we build a game datasets from our game knowledge graph, named as Game30K. The statistics of datasets are listed in Table TABREF24. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "29\n",
      "Question 1: What does Figure FIGREF31 show?\n",
      "\n",
      "Answer 1: Figure FIGREF31 shows test accuracy with different embedding size and training data proportions. It indicates that too small embedding size or training data proportions cannot generate sufficient global information.\n",
      "Question : for the text Figure FIGREF30 shows the test accuracy with increasing epoch on DBP24K and Game30K. We can see that test accuracy first rapidly increased in the first ten iterations, but reaches a stable stages when epoch is larger than 40. Figure FIGREF31 shows test accuracy with different embedding size and training data proportions. We can note that too small embedding size or training data proportions can not generate sufficient global information. In order to further analysis the embeddings learned by our method, we use t-SNE tool BIBREF27 to visualize the learned embedding. Figure FIGREF32 shows the visualization of 256 dimensional entity's embedding on Game30K learned by KANE, R-GCN, PransE and TransE. We observe that our method can learn more discriminative entity's embedding than other other methods. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "30\n",
      "Question 1: What evaluation metric was used in entity classification?\n",
      "Answer 1: The accuracy was used as the evaluation metric in entity classification.\n",
      "Question : for the text In entity classification, the aim is to predicate the type of entity. For all baseline models, we first get the entity embedding in different datasets through default parameter settings as in their original papers or implementations.Then, Logistic Regression is used as classifier, which regards the entity's embeddings as feature of classifier. In evaluation, we random selected 10% of training set as validation set and accuracy as evaluation metric. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "31\n",
      "Question 1: Can you explain why KANE works well for knowledge graph embedding?\n",
      "\n",
      "Answer 1: KANE works well for knowledge graph embedding because it can capture high-order structural information of KGs in an efficient, explicit manner and leverages rich information encoded in attribute triples. The variant of KANE that uses LSTM Encoder and Concatenation aggregator outperforms other variants because LSTM encoder can distinguish the word order, and concatenation aggregator combined all embeddings of multi-head attention in a higher level feature space, which can obtain sufficient expressive power.\n",
      "Question : for the text Experimental results of entity classification on the test sets of all the datasets is shown in Table TABREF25. The results is clearly demonstrate that our proposed method significantly outperforms state-of-art results on accuracy for three datasets. For more in-depth performance analysis, we note: (1) Among all baselines, Path-based methods and Attribute-incorporated methods outperform three typical methods. This indicates that incorporating extra information can improve the knowledge graph embedding performance; (2) Four variants of KANE always outperform baseline methods. The main reasons why KANE works well are two fold: 1) KANE can capture high-order structural information of KGs in an efficient, explicit manner and passe these information to their neighboring; 2) KANE leverages rich information encoded in attribute triples. These rich semantic information can further improve the performance of knowledge graph; (3) The variant of KANE that use LSTM Encoder and Concatenation aggregator outperform other variants. The main reasons is that LSTM encoder can distinguish the word order and concatenation aggregator combine all embedding of multi-head attention in a higher leaver feature space, which can obtain sufficient expressive power. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "32\n",
      "What are the three types of models that the method is compared with in evaluation?\n",
      "\n",
      "The three types of models that the method is compared with in evaluation are typical methods, path-based methods, and attribute-incorporated methods.\n",
      "Question : for the text In evaluation, we compare our method with three types of models:.1) Typical Methods. Three typical knowledge graph embedding methods includes TransE, TransR and TransH are selected as baselines. For TransE, the dissimilarity measure is implemented with L1-norm, and relation as well as entity are replaced during negative sampling. For TransR, we directly use the source codes released in BIBREF9. In order for better performance, the replacement of relation in negative sampling is utilized according to the suggestion of author..2) Path-based Methods. We compare our method with two typical path-based model include PTransE, and ALL-PATHS BIBREF18. PTransE is the first method to model relation path in KG embedding task, and ALL-PATHS improve the PTransE through a dynamic programming algorithm which can incorporate all relation paths of bounded length..3) Attribute-incorporated Methods. Several state-of-art attribute-incorporated methods including R-GCN BIBREF24 and KR-EAR BIBREF26 are used to compare with our methods on three real datasets..In addition, four variants of KANE which each of which correspondingly defines its specific way of computing the attribute value embedding and embedding aggregation are used as baseline in evaluation. In this study, we name four three variants as KANE (BOW+Concatenation), KANE (BOW+Average), and KANE (LSTM+Concatenation), KANE (LSTM+Average). Our method is learned with mini-batch SGD. As for hyper-parameters, we select batch size among {16, 32, 64, 128}, learning rate $\\lambda $ for SGD among {0.1, 0.01, 0.001}. For a fair comparison, we also set the vector dimensions of all entity and relation to the same $k \\in ${128, 258, 512, 1024}, the same dissimilarity measure $l_{1}$ or $l_{2}$ distance in loss function, and the same number of negative examples $n$ among {1, 10, 20, 40}. The training time on both data sets is limited to at most 400 epochs. The best models are selected by a grid search and early stopping on validation sets. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "33\n",
      "Q1: What are the two evaluation metrics used for knowledge graph completion?\n",
      "\n",
      "A1: The two evaluation metrics used for knowledge graph completion are Mean Rank and Hits@1/Hits@10.\n",
      "Question : for the text The purpose of knowledge graph completion is to complete a triple $(h, r, t)$ when one of $h, r, t$ is missing, which is used many literature BIBREF7. Two measures are considered as our evaluation metrics: (1) the mean rank of correct entities or relations (Mean Rank); (2) the proportion of correct entities or relations ranked in top1 (Hits@1, for relations) or top 10 (Hits@10, for entities). Following the setting in BIBREF7, we also adopt the two evaluation settings named \"raw\" and \"filter\" in order to avoid misleading behavior..The results of entity and relation predication on FB24K are shown in the Table TABREF33. This results indicates that KANE still outperforms other baselines significantly and consistently. This also verifies the necessity of modeling high-order structural and attribute information of KGs in Knowledge graph embedding models. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "34\n",
      "What is KANE and how does it address the limitations of existing knowledge graph embedding methods?\n",
      "\n",
      "KANE is a Knowledge Graph Attention Network that aggregates all attribute triples with bias and performs embedding propagation based on relation triples to capture both high-order structural and attribute information of KGs in an efficient, explicit and unified manner under the graph convolutional networks framework. KANE addresses the limitations of existing knowledge graph embedding methods by recursively embedding propagation based on relation triples to capture high-order structural information and multi-head attention-based aggregation to learn the weight of each attribute triple through applying the neural attention mechanism.\n",
      "Question : for the text In the past decade, many large-scale Knowledge Graphs (KGs), such as Freebase BIBREF0, DBpedia BIBREF1 and YAGO BIBREF2 have been built to represent human complex knowledge about the real-world in the machine-readable format. The facts in KGs are usually encoded in the form of triples $(\\textit {head entity}, relation, \\textit {tail entity})$ (denoted $(h, r, t)$ in this study) through the Resource Description Framework, e.g.,$(\\textit {Donald Trump}, Born In, \\textit {New York City})$. Figure FIGREF2 shows the subgraph of knowledge graph about the family of Donald Trump. In many KGs, we can observe that some relations indicate attributes of entities, such as the $\\textit {Born}$ and $\\textit {Abstract}$ in Figure FIGREF2, and others indicates the relations between entities (the head entity and tail entity are real world entity). Hence, the relationship in KG can be divided into relations and attributes, and correspondingly two types of triples, namely relation triples and attribute triples BIBREF3. A relation triples in KGs represents relationship between entities, e.g.,$(\\textit {Donald Trump},Father of, \\textit {Ivanka Trump})$, while attribute triples denote a literal attribute value of an entity, e.g.,$(\\textit {Donald Trump},Born, \\textit {\"June 14, 1946\"})$..Knowledge graphs have became important basis for many artificial intelligence applications, such as recommendation system BIBREF4, question answering BIBREF5 and information retrieval BIBREF6, which is attracting growing interests in both academia and industry communities. A common approach to apply KGs in these artificial intelligence applications is through embedding, which provide a simple method to encode both entities and relations into a continuous low-dimensional embedding spaces. Hence, learning distributional representation of knowledge graph has attracted many research attentions in recent years. TransE BIBREF7 is a seminal work in representation learning low-dimensional vectors for both entities and relations. The basic idea behind TransE is that the embedding $\\textbf {t}$ of tail entity should be close to the head entity's embedding $\\textbf {r}$ plus the relation vector $\\textbf {t}$ if $(h, r, t)$ holds, which indicates $\\textbf {h}+\\textbf {r}\\approx \\textbf {t}$. This model provide a flexible way to improve the ability in completing the KGs, such as predicating the missing items in knowledge graph. Since then, several methods like TransH BIBREF8 and TransR BIBREF9, which represent the relational translation in other effective forms, have been proposed. Recent attempts focused on either incorporating extra information beyond KG triples BIBREF10, BIBREF11, BIBREF12, BIBREF13, or designing more complicated strategies BIBREF14, BIBREF15, BIBREF16..While these methods have achieved promising results in KG completion and link predication, existing knowledge graph embedding methods still have room for improvement. First, TransE and its most extensions only take direct relations between entities into consideration. We argue that the high-order structural relationship between entities also contain rich semantic relationships and incorporating these information can improve model performance. For example the fact $\\textit {Donald Trump}\\stackrel{Father of}{\\longrightarrow }\\textit {Ivanka Trump}\\stackrel{Spouse}{\\longrightarrow }\\textit {Jared Kushner} $ indicates the relationship between entity Donald Trump and entity Jared Kushner. Several path-based methods have attempted to take multiple-step relation paths into consideration for learning high-order structural information of KGs BIBREF17, BIBREF18. But note that huge number of paths posed a critical complexity challenge on these methods. In order to enable efficient path modeling, these methods have to make approximations by sampling or applying path selection algorithm. We argue that making approximations has a large impact on the final performance..Second, to the best of our knowledge, most existing knowledge graph embedding methods just leverage relation triples of KGs while ignoring a large number of attribute triples. Therefore, these methods easily suffer from sparseness and incompleteness of knowledge graph. Even worse, structure information usually cannot distinguish the different meanings of relations and entities in different triples. We believe that these rich information encoded in attribute triples can help explore rich semantic information and further improve the performance of knowledge graph. For example, we can learn date of birth and abstraction from values of Born and Abstract about Donald Trump in Figure FIGREF2. There are a huge number of attribute triples in real KGs, for example the statistical results in BIBREF3 shows attribute triples are three times as many as relationship triples in English DBpedia (2016-04). Recent a few attempts try to incorporate attribute triples BIBREF11, BIBREF12. However, these are two limitations existing in these methods. One is that only a part of attribute triples are used in the existing methods, such as only entity description is used in BIBREF12. The other is some attempts try to jointly model the attribute triples and relation triples in one unified optimization problem. The loss of two kinds triples has to be carefully balanced during optimization. For example, BIBREF3 use hyper-parameters to weight the loss of two kinds triples in their models..Considering limitations of existing knowledge graph embedding methods, we believe it is of critical importance to develop a model that can capture both high-order structural and attribute information of KGs in an efficient, explicit and unified manner. Towards this end, inspired by the recent developments of graph convolutional networks (GCN) BIBREF19, which have the potential of achieving the goal but have not been explored much for knowledge graph embedding, we propose Knowledge Graph Attention Networks for Enhancing Knowledge Graph Embedding (KANE). The key ideal of KANE is to aggregate all attribute triples with bias and perform embedding propagation based on relation triples when calculating the representations of given entity. Specifically, two carefully designs are equipped in KANE to correspondingly address the above two challenges: 1) recursive embedding propagation based on relation triples, which updates a entity embedding. Through performing such recursively embedding propagation, the high-order structural information of kGs can be successfully captured in a linear time complexity; and 2) multi-head attention-based aggregation. The weight of each attribute triples can be learned through applying the neural attention mechanism BIBREF20..In experiments, we evaluate our model on two KGs tasks including knowledge graph completion and entity classification. Experimental results on three datasets shows that our method can significantly outperforms state-of-arts methods..The main contributions of this study are as follows:.1) We highlight the importance of explicitly modeling the high-order structural and attribution information of KGs to provide better knowledge graph embedding..2) We proposed a new method KANE, which achieves can capture both high-order structural and attribute information of KGs in an efficient, explicit and unified manner under the graph convolutional networks framework..3) We conduct experiments on three datasets, demonstrating the effectiveness of KANE and its interpretability in understanding the importance of high-order relations. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "35\n",
      "What are relation triples and attribute triples in KGs?\n",
      "Relation triples denote the relation between entities in a KG, while attribute triples describe attributes of entities. Both types of triples provide important information about entities and are taken into consideration in the task of learning entity representations.\n",
      "Question : for the text In this study, wo consider two kinds of triples existing in KGs: relation triples and attribute triples. Relation triples denote the relation between entities, while attribute triples describe attributes of entities. Both relation and attribute triples denotes important information about entity, we will take both of them into consideration in the task of learning representation of entities. We let $I $ denote the set of IRIs (Internationalized Resource Identifier), $B $ are the set of blank nodes, and $L $ are the set of literals (denoted by quoted strings). The relation triples and attribute triples can be formalized as follows:.Definition 1. Relation and Attribute Triples: A set of Relation triples $ T_{R} $ can be represented by $ T_{R} \\subset E \\times R \\times E $, where $E \\subset I \\cup B $ is set of entities, $R \\subset I$ is set of relations between entities. Similarly, $ T_{A} \\subset E \\times R \\times A $ is the set of attribute triples, where $ A \\subset I \\cup B \\cup L $ is the set of attribute values..Definition 2. Knowledge Graph: A KG consists of a combination of relation triples in the form of $ (h, r, t)\\in T_{R} $, and attribute triples in form of $ (h, r, a)\\in T_{A} $. Formally, we represent a KG as $G=(E,R,A,T_{R},T_{A})$, where $E=\\lbrace h,t|(h,r,t)\\in T_{R} \\cup (h,r,a)\\in T_{A}\\rbrace $ is set of entities, $R =\\lbrace r|(h,r,t)\\in T_{R} \\cup (h,r,a)\\in T_{A}\\rbrace $ is set of relations, $A=\\lbrace a|(h,r,a)\\in T_{A}\\rbrace $, respectively..The purpose of this study is try to use embedding-based model which can capture both high-order structural and attribute information of KGs that assigns a continuous representations for each element of triples in the form $ (\\textbf {h}, \\textbf {r}, \\textbf {t})$ and $ (\\textbf {h}, \\textbf {r}, \\textbf {a})$, where Boldfaced $\\textbf {h}\\in \\mathbb {R}^{k}$, $\\textbf {r}\\in \\mathbb {R}^{k}$, $\\textbf {t}\\in \\mathbb {R}^{k}$ and $\\textbf {a}\\in \\mathbb {R}^{k}$ denote the embedding vector of head entity $h$, relation $r$, tail entity $t$ and attribute $a$ respectively..Next, we detail our proposed model which models both high-order structural and attribute information of KGs in an efficient, explicit and unified manner under the graph convolutional networks framework. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "36\n",
      "What are the components of the KANE model that are discussed in detail?\n",
      "\n",
      "The components of the KANE model that are discussed in detail include the overall framework, input embedding of entities, relations and values in KGs, design of embedding propagation layers based on graph attention network, and the loss functions for link predication and entity classification task.\n",
      "Question : for the text In this section, we present the proposed model in detail. We first introduce the overall framework of KANE, then discuss the input embedding of entities, relations and values in KGs, the design of embedding propagation layers based on graph attention network and the loss functions for link predication and entity classification task, respectively. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "37\n",
      "Question 1: What are the two encoders used to model the attribute value in this study?\n",
      "\n",
      "Answer 1: The two encoders used to model the attribute value in this study are the Bag-of-Words Encoder and the LSTM Encoder.\n",
      "Question : for the text The value in attribute triples usually is sentence or a word. To encode the representation of value from its sentence or word, we need to encode the variable-length sentences to a fixed-length vector. In this study, we adopt two different encoders to model the attribute value..Bag-of-Words Encoder. The representation of attribute value can be generated by a summation of all words embeddings of values. We denote the attribute value $a$ as a word sequence $a = w_{1},...,w_{n}$, where $w_{i}$ is the word at position $i$. The embedding of $\\textbf {a}$ can be defined as follows..where $\\textbf {w}_{i}\\in \\mathbb {R}^{k}$ is the word embedding of $w_{i}$..Bag-of-Words Encoder is a simple and intuitive method, which can capture the relative importance of words. But this method suffers in that two strings that contains the same words with different order will have the same representation..LSTM Encoder. In order to overcome the limitation of Bag-of-Word encoder, we consider using LSTM networks to encoder a sequence of words in attribute value into a single vector. The final hidden state of the LSTM networks is selected as a representation of the attribute value..where $f_{lstm}$ is the LSTM network. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "38\n",
      "What is the purpose of attentive embedding propagation in KANE?\n",
      "The purpose of attentive embedding propagation in KANE is to encode the neighborhood of an entity and output a new embedding for that entity by assigning varying levels of importance to its neighboring entities and their relations.\n",
      "Question : for the text Next we describe the details of recursively embedding propagation method building upon the architecture of graph convolution network. Moreover, by exploiting the idea of graph attention network, out method learn to assign varying levels of importance to entity in every entity's neighborhood and can generate attentive weights of cascaded embedding propagation. In this study, embedding propagation layer consists of two mainly components: attentive embedding propagation and embedding aggregation. Here, we start by describing the attentive embedding propagation..Attentive Embedding Propagation: Considering an KG $G$, the input to our layer is a set of entities, relations and attribute values embedding. We use $\\textbf {h}\\in \\mathbb {R}^{k}$ to denote the embedding of entity $h$. The neighborhood of entity $h$ can be described by $\\mathcal {N}_{h} = \\lbrace t,a|(h,r,t)\\in T_{R} \\cup (h,r,a)\\in T_{A}\\rbrace $. The purpose of attentive embedding propagation is encode $\\mathcal {N}_{h}$ and output a vector $\\vec{\\textbf {h}}$ as the new embedding of entity $h$..In order to obtain sufficient expressive power, one learnable linear transformation $\\textbf {W}\\in \\mathbb {R}^{k^{^{\\prime }} \\times k}$ is adopted to transform the input embeddings into higher level feature space. In this study, we take a triple $(h,r,t)$ as example and the output a vector $\\vec{\\textbf {h}}$ can be formulated as follows:.where $\\pi (h,r,t)$ is attention coefficients which indicates the importance of entity's $t$ to entities $h$ ..In this study, the attention coefficients also control how many information being propagated from its neighborhood through the relation. To make attention coefficients easily comparable between different entities, the attention coefficient $\\pi (h,r,t)$ can be computed using a softmax function over all the triples connected with $h$. The softmax function can be formulated as follows:.Hereafter, we implement the attention coefficients $\\pi (h,r,t)$ through a single-layer feedforward neural network, which is formulated as follows:.where the leakyRelu is selected as activation function..As shown in Equation DISPLAY_FORM13, the attention coefficient score is depend on the distance head entity $h$ and the tail entity $t$ plus the relation $r$, which follows the idea behind TransE that the embedding $\\textbf {t}$ of head entity should be close to the tail entity's embedding $\\textbf {r}$ plus the relation vector $\\textbf {t}$ if $(h, r, t)$ holds..Embedding Aggregation. To stabilize the learning process of attention, we perform multi-head attention on final layer. Specifically, we use $m$ attention mechanism to execute the transformation of Equation DISPLAY_FORM11. A aggregators is needed to combine all embeddings of multi-head graph attention layer. In this study, we adapt two types of aggregators:.Concatenation Aggregator concatenates all embeddings of multi-head graph attention, followed by a nonlinear transformation:.where $\\mathop {\\Big |\\Big |}$ represents concatenation, $ \\pi (h,r,t)^{i}$ are normalized attention coefficient computed by the $i$-th attentive embedding propagation, and $\\textbf {W}^{i}$ denotes the linear transformation of input embedding..Averaging Aggregator sums all embeddings of multi-head graph attention and the output embedding in the final is calculated applying averaging:.In order to encode the high-order connectivity information in KGs, we use multiple embedding propagation layers to gathering the deep information propagated from the neighbors. More formally, the embedding of entity $h$ in $l$-th layers can be defined as follows:.After performing $L$ embedding propagation layers, we can get the final embedding of entities, relations and attribute values, which include both high-order structural and attribute information of KGs. Next, we discuss the loss functions of KANE for two different tasks and introduce the learning and optimization detail. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "39\n",
      "What are the two different tasks of KG that our method focuses on?\n",
      "Our method focuses on two different tasks of KG, which include knowledge graph completion and entity classification.\n",
      "Question : for the text Here, we introduce the learning and optimization details for our method. Two different loss functions are carefully designed fro two different tasks of KG, which include knowledge graph completion and entity classification. Next details of these two loss functions are discussed..knowledge graph completion. This task is a classical task in knowledge graph representation learning community. Specifically, two subtasks are included in knowledge graph completion: entity predication and link predication. Entity predication aims to infer the impossible head/tail entities in testing datasets when one of them is missing, while the link predication focus on complete a triple when relation is missing. In this study, we borrow the idea of translational scoring function from TransE, which the embedding $\\textbf {t}$ of tail entity should be close to the head entity's embedding $\\textbf {r}$ plus the relation vector $\\textbf {t}$ if $(h, r, t)$ holds, which indicates $d(h+r,t)= ||\\textbf {h}+\\textbf {r}- \\textbf {t}||$. Specifically, we train our model using hinge-loss function, given formally as.where $\\gamma >0$ is a margin hyper-parameter, $[x ]_{+}$ denotes the positive part of $x$, $T=T_{R} \\cup T_{A}$ is the set of valid triples, and $T^{\\prime }$ is set of corrupted triples which can be formulated as:.Entity Classification. For the task of entity classification, we simple uses a fully connected layers and binary cross-entropy loss (BCE) over sigmoid activation on the output of last layer. We minimize the binary cross-entropy on all labeled entities, given formally as:.where $E_{D}$ is the set of entities indicates have labels, $C$ is the dimension of the output features, which is equal to the number of classes, $y_{ej}$ is the label indicator of entity $e$ for $j$-th class, and $\\sigma (x)$ is sigmoid function $\\sigma (x) = \\frac{1}{1+e^{-x}}$..We optimize these two loss functions using mini-batch stochastic gradient decent (SGD) over the possible $\\textbf {h}$, $\\textbf {r}$, $\\textbf {t}$, with the chin rule that applying to update all parameters. At each step, we update the parameter $\\textbf {h}^{\\tau +1}\\leftarrow \\textbf {h}^{\\tau }-\\lambda \\nabla _{\\textbf {h}}\\mathcal {L}$, where $\\tau $ labels the iteration step and $\\lambda $ is the learning rate. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "40\n",
      "Question 1: What is the task of attribute embedding in KANE?\n",
      "\n",
      "Answer 1: The task of attribute embedding in KANE is to embed every value in attribute triples into a continuous vector space while preserving the semantic information.\n",
      "Question : for the text The process of KANE is illustrated in Figure FIGREF2. We introduce the architecture of KANE from left to right. As shown in Figure FIGREF2, the whole triples of knowledge graph as input. The task of attribute embedding lays is embedding every value in attribute triples into a continuous vector space while preserving the semantic information. To capture both high-order structural information of KGs, we used an attention-based embedding propagation method. This method can recursively propagate the embeddings of entities from an entity's neighbors, and aggregate the neighbors with different weights. The final embedding of entities, relations and values are feed into two different deep neural network for two different tasks including link predication and entity classification. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "41\n",
      "What is the advantage of KANE over existing methods?\n",
      "\n",
      "KANE is advantageous over existing methods because it directly factors high-order relations into the predictive model in linear time, encodes all attribute triples in learning representation of entities, and factors high-order relations and attribute information into the predictive model in an efficient, explicit, and unified manner. This improves the performance of knowledge graph embedding, and all related parameters are tailored for optimizing the embedding objective.\n",
      "Question : for the text In recent years, there are many efforts in Knowledge Graph Embeddings for KGs aiming to encode entities and relations into a continuous low-dimensional embedding spaces. Knowledge Graph Embedding provides a very simply and effective methods to apply KGs in various artificial intelligence applications. Hence, Knowledge Graph Embeddings has attracted many research attentions in recent years. The general methodology is to define a score function for the triples and finally learn the representations of entities and relations by minimizing the loss function $f_r(h,t)$, which implies some types of transformations on $\\textbf {h}$ and $\\textbf {t}$. TransE BIBREF7 is a seminal work in knowledge graph embedding, which assumes the embedding $\\textbf {t}$ of tail entity should be close to the head entity's embedding $\\textbf {r}$ plus the relation vector $\\textbf {t}$ when $(h, r, t)$ holds as mentioned in section “Introduction\". Hence, TransE defines the following loss function:.TransE regarding the relation as a translation between head entity and tail entity is inspired by the word2vec BIBREF21, where relationships between words often correspond to translations in latent feature space. This model achieves a good trade-off between computational efficiency and accuracy in KGs with thousands of relations. but this model has flaws in dealing with one-to-many, many-to-one and many-to-many relations..In order to address this issue, TransH BIBREF8 models a relation as a relation-specific hyperplane together with a translation on it, allowing entities to have distinct representation in different relations. TransR BIBREF9 models entities and relations in separate spaces, i.e., entity space and relation spaces, and performs translation from entity spaces to relation spaces. TransD BIBREF22 captures the diversity of relations and entities simultaneously by defining dynamic mapping matrix. Recent attempts can be divided into two categories: (i) those which tries to incorporate additional information to further improve the performance of knowledge graph embedding, e.g., entity types or concepts BIBREF13, relations paths BIBREF17, textual descriptions BIBREF11, BIBREF12 and logical rules BIBREF23; (ii) those which tries to design more complicated strategies, e.g., deep neural network models BIBREF24..Except for TransE and its extensions, some efforts measure plausibility by matching latent semantics of entities and relations. The basic idea behind these models is that the plausible triples of a KG is assigned low energies. For examples, Distant Model BIBREF25 defines two different projections for head and tail entity in a specific relation, i.e., $\\textbf {M}_{r,1}$ and $\\textbf {M}_{r,2}$. It represents the vectors of head and tail entity can be transformed by these two projections. The loss function is $f_r(h,t)=||\\textbf {M}_{r,1}\\textbf {h}-\\textbf {M}_{r,2}\\textbf {t}||_{1}$..Our KANE is conceptually advantageous to existing methods in that: 1) it directly factors high-order relations into the predictive model in linear time which avoids the labor intensive process of materializing paths, thus is more efficient and convenient to use; 2) it directly encodes all attribute triples in learning representation of entities which can capture rich semantic information and further improve the performance of knowledge graph embedding, and 3) KANE can directly factors high-order relations and attribute information into the predictive model in an efficient, explicit and unified manner, thus all related parameters are tailored for optimizing the embedding objective. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "42\n",
      "Question 1: What are the challenges of drunk-texting prediction mentioned in the paper? \n",
      "Answer 1: One of the challenges of drunk-texting prediction is the selection of negative examples (sober tweets), which is addressed in the paper through hashtag-based supervision to create annotated datasets.\n",
      "Question : for the text In this paper, we introduce automatic drunk-texting prediction as the task of predicting a tweet as drunk or sober. First, we justify the need for drunk-texting prediction as means of identifying risky social behavior arising out of alcohol abuse, and the need to build tools that avoid privacy leaks due to drunk-texting. We then highlight the challenges of drunk-texting prediction: one of the challenges is selection of negative examples (sober tweets). Using hashtag-based supervision, we create three datasets annotated with drunk or sober labels. We then present SVM-based classifiers which use two sets of features: N-gram and stylistic features. Our drunk prediction system obtains a best accuracy of 78.1%. We observe that our stylistic features add negligible value to N-gram features. We use our heldout dataset to compare how our system performs against human annotators. While human annotators achieve an accuracy of 68.8%, our system reaches reasonably close and performs with a best accuracy of 64%..Our analysis of the task and experimental findings make a case for drunk-texting prediction as a useful and feasible NLP application. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "43\n",
      "What approach do the authors use to create their datasets?\n",
      "\n",
      "The authors use hashtag-based supervision to create their datasets, which is similar to tasks like emotion classification. They download tweets using Twitter API and remove non-Unicode characters, hyperlinks, and short tweets. Hashtags used for drunk or sober tweets are also removed so that they only provide labels. The dataset is available on request, and they create three datasets using different strategies for sober tweets.\n",
      "Question : for the text We use hashtag-based supervision to create our datasets, similar to tasks like emotion classification BIBREF8 . The tweets are downloaded using Twitter API (https://dev.twitter.com/). We remove non-Unicode characters, and eliminate tweets that contain hyperlinks and also tweets that are shorter than 6 words in length. Finally, hashtags used to indicate drunk or sober tweets are removed so that they provide labels, but do not act as features. The dataset is available on request. As a result, we create three datasets, each using a different strategy for sober tweets, as follows:.The drunk tweets for Datasets 1 and 2 are the same. Figure FIGREF9 shows a word-cloud for these drunk tweets (with stop words and forms of the word `drunk' removed), created using WordItOut. The size of a word indicates its frequency. In addition to topical words such as `bar', `bottle' and `wine', the word-cloud shows sentiment words such as `love' or `damn', along with profane words..Heuristics other than these hashtags could have been used for dataset creation. For example, timestamps were a good option to account for time at which a tweet was posted. However, this could not be used because user's local times was not available, since very few users had geolocation enabled. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "44\n",
      "Question 1: What are some of the challenges of drunk-texting prediction?\n",
      "Answer 1: Some of the challenges of drunk-texting prediction include accurately classifying a text as drunk or sober, dealing with language ambiguity or sarcasm, and accounting for individual differences in language use and alcohol tolerance.\n",
      "Question : for the text Drunk-texting prediction is the task of classifying a text as drunk or sober. For example, a tweet `Feeling buzzed. Can't remember how the evening went' must be predicted as `drunk', whereas, `Returned from work late today, the traffic was bad' must be predicted as `sober'. The challenges are: generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "45\n",
      "What is an example of a category of errors that occur in predicting drunk tweets?\n",
      "One category of errors that occur is incorrect hashtag supervision, where tweets may be labeled as drunk when they actually describe a drunk episode in retrospective and are therefore sober.\n",
      "Question : for the text Some categories of errors that occur are:.Incorrect hashtag supervision: The tweet `Can't believe I lost my bag last night, literally had everything in! Thanks god the bar man found it' was marked with`#Drunk'. However, this tweet is not likely to be a drunk tweet, but describes a drunk episode in retrospective. Our classifier predicts it as sober..Seemingly sober tweets: Human annotators as well as our classifier could not identify whether `Will you take her on a date? But really she does like you' was drunk, although the author of the tweet had marked it so. This example also highlights the difficulty of drunk-texting prediction..Pragmatic difficulty: The tweet `National dress of Ireland is one's one vomit.. my family is lovely' was correctly identified by our human annotators as a drunk tweet. This tweet contains an element of humour and topic change, but our classifier could not capture it. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "46\n",
      "Question 1: What are the performance metrics shown for the SVM classifiers trained on the two sets of features?\n",
      "\n",
      "Answer 1: The performance metrics shown for the SVM classifiers trained on the two sets of features include accuracy (A), positive/negative precision (PP/NP), and positive/negative recall (PR/NR). The positive class is 'Drunk' and the negative class is 'Sober'.\n",
      "Question : for the text Using the two sets of features, we train SVM classifiers BIBREF11 . We show the five-fold cross-validation performance of our features on Datasets 1 and 2, in Section SECREF17 , and on Dataset H in Section SECREF21 . Section SECREF22 presents an error analysis. Accuracy, positive/negative precision and positive/negative recall are shown as A, PP/NP and PR/NR respectively. `Drunk' forms the positive class, while `Sober' forms the negative class. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "47\n",
      "Question 1: What are some of the stylistic features included in the prediction system?\n",
      "\n",
      "Answer 1: The stylistic features included in the prediction system are POS ratios, discourse connectors, spelling errors, repeated characters, emoticons, sentiment ratio, and LDA-specific unigrams.\n",
      "Question : for the text The complete set of features is shown in Table TABREF7 . There are two sets of features: (a) N-gram features, and (b) Stylistic features. We use unigrams and bigrams as N-gram features- considering both presence and count..Table TABREF7 shows the complete set of stylistic features of our prediction system. POS ratios are a set of features that record the proportion of each POS tag in the dataset (for example, the proportion of nouns/adjectives, etc.). The POS tags and named entity mentions are obtained from NLTK BIBREF9 . Discourse connectors are identified based on a manually created list. Spelling errors are identified using a spell checker by enchant. The repeated characters feature captures a situation in which a word contains a letter that is repeated three or more times, as in the case of happpy. Since drunk-texting is often associated with emotional expression, we also incorporate a set of sentiment-based features. These features include: count/presence of emoticons and sentiment ratio. Sentiment ratio is the proportion of positive and negative words in the tweet. To determine positive and negative words, we use the sentiment lexicon in mpqa. To identify a more refined set of words that correspond to the two classes, we also estimated 20 topics for the dataset by estimating an LDA model BIBREF10 . We then consider top 10 words per topic, for both classes. This results in 400 LDA-specific unigrams that are then used as features. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "48\n",
      "Question 1: What is the key challenge in obtaining an annotated dataset for the automatic prediction of drunk-texting in tweets?\n",
      "\n",
      "Answer 1: The key challenge in obtaining an annotated dataset for drunk-texting prediction is to identify which tweets were written by a drunk user. The authors of the tweets mention if they were drunk at the time of posting a tweet using hashtags, and this forms the basis for the supervision used in creating the datasets.\n",
      "Question : for the text The ubiquity of communication devices has made social media highly accessible. The content on these media reflects a user's day-to-day activities. This includes content created under the influence of alcohol. In popular culture, this has been referred to as `drunk-texting'. In this paper, we introduce automatic `drunk-texting prediction' as a computational task. Given a tweet, the goal is to automatically identify if it was written by a drunk user. We refer to tweets written under the influence of alcohol as `drunk tweets', and the opposite as `sober tweets'..A key challenge is to obtain an annotated dataset. We use hashtag-based supervision so that the authors of the tweets mention if they were drunk at the time of posting a tweet. We create three datasets by using different strategies that are related to the use of hashtags. We then present SVM-based classifiers that use N-gram and stylistic features such as capitalisation, spelling errors, etc. Through our experiments, we make subtle points related to: (a) the performance of our features, (b) how our approach compares against human ability to detect drunk-texting, (c) most discriminative stylistic features, and (d) an error analysis that points to future work. To the best of our knowledge, this is a first study that shows the feasibility of text-based analysis for drunk-texting prediction. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "49\n",
      "What is the potential benefit of an automatic drunk-texting prediction system?\n",
      "\n",
      "An automatic drunk-texting prediction system can be used to identify individuals susceptible to unsociable behaviors associated with alcohol abuse, as well as improve systems aimed to avoid regrettable drunk-texting by monitoring social media text without seeking user input.\n",
      "Question : for the text Past studies show the relation between alcohol abuse and unsociable behaviour such as aggression BIBREF0 , crime BIBREF1 , suicide attempts BIBREF2 , drunk driving BIBREF3 , and risky sexual behaviour BIBREF4 . suicide state that “those responsible for assessing cases of attempted suicide should be adept at detecting alcohol misuse”. Thus, a drunk-texting prediction system can be used to identify individuals susceptible to these behaviours, or for investigative purposes after an incident..Drunk-texting may also cause regret. Mail Goggles prompts a user to solve math questions before sending an email on weekend evenings. Some Android applications avoid drunk-texting by blocking outgoing texts at the click of a button. However, to the best of our knowledge, these tools require a user command to begin blocking. An ongoing text-based analysis will be more helpful, especially since it offers a more natural setting by monitoring stream of social media text and not explicitly seeking user input. Thus, automatic drunk-texting prediction will improve systems aimed to avoid regrettable drunk-texting. To the best of our knowledge, ours is the first study that does a quantitative analysis, in terms of prediction of the drunk state by using textual clues..Several studies have studied linguistic traits associated with emotion expression and mental health issues, suicidal nature, criminal status, etc. BIBREF5 , BIBREF6 . NLP techniques have been used in the past to address social safety and mental health issues BIBREF7 . generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "50\n",
      "Question 1: What is the accuracy for N-gram features in Dataset 1?\n",
      "\n",
      "Answer 1: The accuracy for N-gram features in Dataset 1 is 85.5%.\n",
      "Question : for the text Table TABREF14 shows the performance for five-fold cross-validation for Datasets 1 and 2. In case of Dataset 1, we observe that N-gram features achieve an accuracy of 85.5%. We see that our stylistic features alone exhibit degraded performance, with an accuracy of 75.6%, in the case of Dataset 1. Table TABREF16 shows top stylistic features, when trained on the two datasets. Spelling errors, POS ratios for nouns (POS_NOUN), length and sentiment ratios appear in both lists, in addition to LDA-based unigrams. However, negative recall reduces to a mere 3.2%. This degradation implies that our features capture a subset of drunk tweets and that there are properties of drunk tweets that may be more subtle. When both N-gram and stylistic features are used, there is negligible improvement. The accuracy for Dataset 2 increases from 77.9% to 78.1%. Precision/Recall metrics do not change significantly either. The best accuracy of our classifier is 78.1% for all features, and 75.6% for stylistic features. This shows that text-based clues can indeed be used for drunk-texting prediction. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "1\n",
      "Question 1: What is the average accuracy of the human annotators in marking tweets as drunk or sober in the held-out dataset?\n",
      "\n",
      "Answer 1: The average accuracy of the human annotators in marking tweets as drunk or sober in the held-out dataset is 68.8%.\n",
      "Question : for the text Using held-out dataset H, we evaluate how our system performs in comparison to humans. Three annotators, A1-A3, mark each tweet in the Dataset H as drunk or sober. Table TABREF19 shows a moderate agreement between our annotators (for example, it is 0.42 for A1 and A2). Table TABREF20 compares our classifier with humans. Our human annotators perform the task with an average accuracy of 68.8%, while our classifier (with all features) trained on Dataset 2 reaches 64%. The classifier trained on Dataset 2 is better than which is trained on Dataset 1. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "2\n",
      "Question 1: What is the capital of Australia?\n",
      "Answer 1: Canberra.\n",
      "Question : for the text To build the ILP model, we first need to get the questions terms (qterm) from the question by chunking the question using an in-house chunker based on the postagger from FACTORIE.  generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "3\n",
      "Question 1: What type of knowledge does TupleInf rely on to answer complex science questions?\n",
      "\n",
      "Answer 1: TupleInf does not rely on curated knowledge and can reason over a large, potentially noisy tuple KB to answer complex science questions.\n",
      "Question : for the text We presented a new QA system, TupleInf, that can reason over a large, potentially noisy tuple KB to answer complex questions. Our results show that TupleInf is a new state-of-the-art structured solver for elementary-level science that does not rely on curated knowledge and generalizes to higher grades. Errors due to lossy IE and misalignments suggest future work in incorporating context and distributional measures. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "4\n",
      "Question 1: Which planet is closest to the sun?\n",
      "Answer 1: Mercury.\n",
      "Question : for the text We describe four classes of failures that we observed, and the future work they suggest..Missing Important Words: Which material will spread out to completely fill a larger container? (A)air (B)ice (C)sand (D)water.In this question, we have tuples that support water will spread out and fill a larger container but miss the critical word “completely”. An approach capable of detecting salient question words could help avoid that..Lossy IE: Which action is the best method to separate a mixture of salt and water? ....The IR solver correctly answers this question by using the sentence: Separate the salt and water mixture by evaporating the water. However, TupleInf is not able to answer this question as Open IE is unable to extract tuples from this imperative sentence. While the additional structure from Open IE is useful for more robust matching, converting sentences to Open IE tuples may lose important bits of information..Bad Alignment: Which of the following gases is necessary for humans to breathe in order to live?(A) Oxygen(B) Carbon dioxide(C) Helium(D) Water vapor.TupleInf returns “Carbon dioxide” as the answer because of the tuple (humans; breathe out; carbon dioxide). The chunk “to breathe” in the question has a high alignment score to the “breathe out” relation in the tuple even though they have completely different meanings. Improving the phrase alignment can mitigate this issue..Out of scope: Deer live in forest for shelter. If the forest was cut down, which situation would most likely happen?....Such questions that require modeling a state presented in the question and reasoning over the state are out of scope of our solver. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "5\n",
      "Question 1: How did the authors optimize their ILP model?\n",
      "Answer 1: The authors used the SCIP ILP optimization engine BIBREF21 to optimize their ILP model. They forced the active variable for each answer choice to be one and used the objective function value of the ILP model as the score. They used a 2-core 2.5 GHz Amazon EC2 linux machine with 16 GB RAM for evaluations. They converted curated tables and tuples into the expected format of each solver for evaluation.\n",
      "Question : for the text We use the SCIP ILP optimization engine BIBREF21 to optimize our ILP model. To get the score for each answer choice $a_i$ , we force the active variable for that choice $x_{a_i}$ to be one and use the objective function value of the ILP model as the score. For evaluations, we use a 2-core 2.5 GHz Amazon EC2 linux machine with 16 GB RAM. To evaluate TableILP and TupleInf on curated tables and tuples, we converted them into the expected format of each solver as follows. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "6\n",
      "Question 1: How does TupleInf compare to IR and TableILP in terms of performance on science exams?\n",
      "\n",
      "Answer 1: TupleInf, with only automatically extracted tuples, outperforms TableILP with its original curated knowledge as well as with additional tuples, and its complementary approach to IR leads to an improved ensemble. The comparison was conducted on two question sets, a 4th Grade set (1220 train, 1304 test) and an 8th Grade set (293 train, 282 test), using two knowledge sources, a Sentence corpus (S) and Curated tables (C). The results show that TupleInf is superior to IR and TableILP, with statistical significance based on the Binomial exact test at $p=0.05$.\n",
      "Question : for the text Comparing our method with two state-of-the-art systems for 4th and 8th grade science exams, we demonstrate that (a) TupleInf with only automatically extracted tuples significantly outperforms TableILP with its original curated knowledge as well as with additional tuples, and (b) TupleInf's complementary approach to IR leads to an improved ensemble. Numbers in bold indicate statistical significance based on the Binomial exact test BIBREF20 at $p=0.05$ ..We consider two question sets. (1) 4th Grade set (1220 train, 1304 test) is a 10x larger superset of the NY Regents questions BIBREF6 , and includes professionally written licensed questions. (2) 8th Grade set (293 train, 282 test) contains 8th grade questions from various states..We consider two knowledge sources. The Sentence corpus (S) consists of domain-targeted $~$ 80K sentences and 280 GB of plain text extracted from web pages used by BIBREF6 aristo2016:combining. This corpus is used by the IR solver and also used to create the tuple KB T and on-the-fly tuples $T^{\\prime }_{qa}$ . Additionally, TableILP uses $\\sim $ 70 Curated tables (C) designed for 4th grade NY Regents exams..We compare TupleInf with two state-of-the-art baselines. IR is a simple yet powerful information-retrieval baseline BIBREF6 that selects the answer option with the best matching sentence in a corpus. TableILP is the state-of-the-art structured inference baseline BIBREF9 developed for science questions. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "7\n",
      "Question 1: Which type of reasoning is paramount when using automatically extracted tuples for complex multiple-choice questions?\n",
      "\n",
      "Answer 1: Conjunctive evidence becomes paramount when using automatically extracted tuples for complex multiple-choice questions.\n",
      "Question : for the text Effective question answering (QA) systems have been a long-standing quest of AI research. Structured curated KBs have been used successfully for this task BIBREF0 , BIBREF1 . However, these KBs are expensive to build and typically domain-specific. Automatically constructed open vocabulary (subject; predicate; object) style tuples have broader coverage, but have only been used for simple questions where a single tuple suffices BIBREF2 , BIBREF3 ..Our goal in this work is to develop a QA system that can perform reasoning with Open IE BIBREF4 tuples for complex multiple-choice questions that require tuples from multiple sentences. Such a system can answer complex questions in resource-poor domains where curated knowledge is unavailable. Elementary-level science exams is one such domain, requiring complex reasoning BIBREF5 . Due to the lack of a large-scale structured KB, state-of-the-art systems for this task either rely on shallow reasoning with large text corpora BIBREF6 , BIBREF7 or deeper, structured reasoning with a small amount of automatically acquired BIBREF8 or manually curated BIBREF9 knowledge..Consider the following question from an Alaska state 4th grade science test:.Which object in our solar system reflects light and is a satellite that orbits around one planet? (A) Earth (B) Mercury (C) the Sun (D) the Moon.This question is challenging for QA systems because of its complex structure and the need for multi-fact reasoning. A natural way to answer it is by combining facts such as (Moon; is; in the solar system), (Moon; reflects; light), (Moon; is; satellite), and (Moon; orbits; around one planet)..A candidate system for such reasoning, and which we draw inspiration from, is the TableILP system of BIBREF9 . TableILP treats QA as a search for an optimal subgraph that connects terms in the question and answer via rows in a set of curated tables, and solves the optimization problem using Integer Linear Programming (ILP). We similarly want to search for an optimal subgraph. However, a large, automatically extracted tuple KB makes the reasoning context different on three fronts: (a) unlike reasoning with tables, chaining tuples is less important and reliable as join rules aren't available; (b) conjunctive evidence becomes paramount, as, unlike a long table row, a single tuple is less likely to cover the entire question; and (c) again, unlike table rows, tuples are noisy, making combining redundant evidence essential. Consequently, a table-knowledge centered inference model isn't the best fit for noisy tuples..To address this challenge, we present a new ILP-based model of inference with tuples, implemented in a reasoner called TupleInf. We demonstrate that TupleInf significantly outperforms TableILP by 11.8% on a broad set of over 1,300 science questions, without requiring manually curated tables, using a substantially simpler ILP formulation, and generalizing well to higher grade levels. The gains persist even when both solvers are provided identical knowledge. This demonstrates for the first time how Open IE based QA can be extended from simple lookup questions to an effective system for complex questions. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "8\n",
      "Question 1: What is the difference between retrieval-based web question-answering and science question-answering?\n",
      "\n",
      "Answer 1: Retrieval-based web question-answering involves simple reasoning with a large scale knowledge base, while science question-answering involves complex reasoning with a smaller knowledge base.\n",
      "Question : for the text We discuss two classes of related work: retrieval-based web question-answering (simple reasoning with large scale KB) and science question-answering (complex reasoning with small KB). generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "9\n",
      "Question 1: According to Table 2, which solver outperforms TableILP even when given the same knowledge (C+T)? \n",
      "\n",
      "Answer 1: TupleInf outperforms TableILP even when given the same knowledge (C+T) as shown in Table 2.\n",
      "Question : for the text Table 2 shows that TupleInf, with no curated knowledge, outperforms TableILP on both question sets by more than 11%. The lower half of the table shows that even when both solvers are given the same knowledge (C+T), the improved selection and simplified model of TupleInf results in a statistically significant improvement. Our simple model, TupleInf(C + T), also achieves scores comparable to TableILP on the latter's target Regents questions (61.4% vs TableILP's reported 61.5%) without any specialized rules..Table 3 shows that while TupleInf achieves similar scores as the IR solver, the approaches are complementary (structured lossy knowledge reasoning vs. lossless sentence retrieval). The two solvers, in fact, differ on 47.3% of the training questions. To exploit this complementarity, we train an ensemble system BIBREF6 which, as shown in the table, provides a substantial boost over the individual solvers. Further, IR + TupleInf is consistently better than IR + TableILP. Finally, in combination with IR and the statistical association based PMI solver (that scores 54.1% by itself) of BIBREF6 aristo2016:combining, TupleInf achieves a score of 58.2% as compared to TableILP's ensemble score of 56.7% on the 4th grade set, again attesting to TupleInf's strength. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "10\n",
      "Question 1: What is the capital city of France?\n",
      "\n",
      "Answer 1: Paris.\n",
      "Question : for the text Similar to TableILP, we view the QA task as searching for a graph that best connects the terms in the question (qterms) with an answer choice via the knowledge; see Figure 1 for a simple illustrative example. Unlike standard alignment models used for tasks such as Recognizing Textual Entailment (RTE) BIBREF18 , however, we must score alignments between a set $T_{qa} \\cup T^{\\prime }_{qa}$ of structured tuples and a (potentially multi-sentence) multiple-choice question $qa$ ..The qterms, answer choices, and tuples fields form the set of possible vertices, $\\mathcal {V}$ , of the support graph. Edges connecting qterms to tuple fields and tuple fields to answer choices form the set of possible edges, $\\mathcal {E}$ . The support graph, $G(V, E)$ , is a subgraph of $\\mathcal {G}(\\mathcal {V}, \\mathcal {E})$ where $V$ and $E$ denote “active” nodes and edges, resp. We define the desired behavior of an optimal support graph via an ILP model as follows..Similar to TableILP, we score the support graph based on the weight of the active nodes and edges. Each edge $e(t, h)$ is weighted based on a word-overlap score. While TableILP used WordNet BIBREF19 paths to compute the weight, this measure results in unreliable scores when faced with longer phrases found in Open IE tuples..Compared to a curated KB, it is easy to find Open IE tuples that match irrelevant parts of the questions. To mitigate this issue, we improve the scoring of qterms in our ILP objective to focus on important terms. Since the later terms in a question tend to provide the most critical information, we scale qterm coefficients based on their position. Also, qterms that appear in almost all of the selected tuples tend not to be discriminative as any tuple would support such a qterm. Hence we scale the coefficients by the inverse frequency of the tokens in the selected tuples..Since Open IE tuples do not come with schema and join rules, we can define a substantially simpler model compared to TableILP. This reduces the reasoning capability but also eliminates the reliance on hand-authored join rules and regular expressions used in TableILP. We discovered (see empirical evaluation) that this simple model can achieve the same score as TableILP on the Regents test (target test set used by TableILP) and generalizes better to different grade levels..We define active vertices and edges using ILP constraints: an active edge must connect two active vertices and an active vertex must have at least one active edge. To avoid positive edge coefficients in the objective function resulting in spurious edges in the support graph, we limit the number of active edges from an active tuple, question choice, tuple fields, and qterms (first group of constraints in Table 1 ). Our model is also capable of using multiple tuples to support different parts of the question as illustrated in Figure 1 . To avoid spurious tuples that only connect with the question (or choice) or ignore the relation being expressed in the tuple, we add constraints that require each tuple to connect a qterm with an answer choice (second group of constraints in Table 1 )..We also define new constraints based on the Open IE tuple structure. Since an Open IE tuple expresses a fact about the tuple's subject, we require the subject to be active in the support graph. To avoid issues such as (Planet; orbit; Sun) matching the sample question in the introduction (“Which object $\\ldots $ orbits around a planet”), we also add an ordering constraint (third group in Table 1 )..Its worth mentioning that TupleInf only combines parallel evidence i.e. each tuple must connect words in the question to the answer choice. For reliable multi-hop reasoning using OpenIE tuples, we can add inter-tuple connections to the support graph search, controlled by a small number of rules over the OpenIE predicates. Learning such rules for the Science domain is an open problem and potential avenue of future work. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "11\n",
      "Question 1: How is a tuple defined in the solver?\n",
      "Answer 1: A tuple in the solver is defined as (subject; predicate; objects) with zero or more objects, with the subject, predicate, and objects referred to as fields of the tuple.\n",
      "Question : for the text We first describe the tuples used by our solver. We define a tuple as (subject; predicate; objects) with zero or more objects. We refer to the subject, predicate, and objects as the fields of the tuple. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "12\n",
      "Question 1: What is the process used to retrieve domain-relevant sentences from the text corpora to build the tuple KB?\n",
      "\n",
      "Answer 1: For each test set, the corresponding training questions $Q_\\mathit {tr}$ are used to retrieve domain-relevant sentences from the text corpora (S). Specifically, for each multiple-choice question $(q,A) \\in Q_\\mathit {tr}$ and each choice $a \\in A$, all non-stopword tokens in $q$ and $a$ are used as an ElasticSearch query against S. The top 200 hits are taken, Open IE v4 is run, and the resulting tuples are aggregated over all $a \\in A$ and over all questions in $Q_\\mathit {tr}$ to create the tuple KB (T).\n",
      "Question : for the text We use the text corpora (S) from BIBREF6 aristo2016:combining to build our tuple KB. For each test set, we use the corresponding training questions $Q_\\mathit {tr}$ to retrieve domain-relevant sentences from S. Specifically, for each multiple-choice question $(q,A) \\in Q_\\mathit {tr}$ and each choice $a \\in A$ , we use all non-stopword tokens in $q$ and $a$ as an ElasticSearch query against S. We take the top 200 hits, run Open IE v4, and aggregate the resulting tuples over all $a \\in A$ and over all questions in $Q_\\mathit {tr}$ to create the tuple KB (T). generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "13\n",
      "Question 1: Which tuples are selected from KB and on-the-fly text for a multiple-choice question?\n",
      "Answer 1: To select relevant tuples, we use an inverted index to find the 1,000 tuples that have the most overlapping tokens with the question tokens. We compute the normalized TF-IDF score treating the question as a query and each tuple as a document. We then take the 50 top-scoring tuples from KB and on-the-fly text.\n",
      "Question : for the text Given a multiple-choice question $qa$ with question text $q$ and answer choices A= $\\lbrace a_i\\rbrace $ , we select the most relevant tuples from $T$ and $S$ as follows..Selecting from Tuple KB: We use an inverted index to find the 1,000 tuples that have the most overlapping tokens with question tokens $tok(qa).$ . We also filter out any tuples that overlap only with $tok(q)$ as they do not support any answer. We compute the normalized TF-IDF score treating the question, $q$ as a query and each tuple, $t$ as a document: $\n",
      "&\\textit {tf}(x, q)=1\\; \\textmd {if x} \\in q ; \\textit {idf}(x) = log(1 + N/n_x) \\\\\n",
      "&\\textit {tf-idf}(t, q)=\\sum _{x \\in t\\cap q} idf(x)\n",
      "$ . where $N$ is the number of tuples in the KB and $n_x$ are the number of tuples containing $x$ . We normalize the tf-idf score by the number of tokens in $t$ and $q$ . We finally take the 50 top-scoring tuples $T_{qa}$ ..On-the-fly tuples from text: To handle questions from new domains not covered by the training set, we extract additional tuples on the fly from S (similar to BIBREF17 knowlhunting). We perform the same ElasticSearch query described earlier for building T. We ignore sentences that cover none or all answer choices as they are not discriminative. We also ignore long sentences ( $>$ 300 characters) and sentences with negation as they tend to lead to noisy inference. We then run Open IE on these sentences and re-score the resulting tuples using the Jaccard score due to the lossy nature of Open IE, and finally take the 50 top-scoring tuples $T^{\\prime }_{qa}$ . generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "14\n",
      "Question 1: What do we do with tuples that have multiple objects in TableILP? \n",
      "\n",
      "Answer 1: For each object, we add a triple $(S; P; O_i)$ to the table.\n",
      "Question : for the text We create an additional table in TableILP with all the tuples in $T$ . Since TableILP uses fixed-length $(subject; predicate; object)$ triples, we need to map tuples with multiple objects to this format. For each object, $O_i$ in the input Open IE tuple $(S; P; O_1; O_2 \\ldots )$ , we add a triple $(S; P; O_i)$ to this table. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "15\n",
      "Question 1: How many total medals were won by the United States in the 2016 Olympics?\n",
      "\n",
      "Answer 1: According to our table search and tuple generation method, the United States won a total of 121 medals in the 2016 Olympics.\n",
      "Question : for the text For each question, we select the 7 best matching tables using the tf-idf score of the table w.r.t. the question tokens and top 20 rows from each table using the Jaccard similarity of the row with the question. (same as BIBREF9 tableilp2016). We then convert the table rows into the tuple structure using the relations defined by TableILP. For every pair of cells connected by a relation, we create a tuple with the two cells as the subject and primary object with the relation as the predicate. The other cells of the table are used as additional objects to provide context to the solver. We pick top-scoring 50 tuples using the Jaccard score. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "16\n",
      "Question 1: What organizations supported the project \"Joining Ontologies and Semantics Induced from Text\" (JOIN-T)?\n",
      "\n",
      "Answer 1: The project \"Joining Ontologies and Semantics Induced from Text\" (JOIN-T) was supported by the Deutsche Forschungsgemeinschaft (DFG), RFBR under projects no. 16-37-00203 mol_a and no. 16-37-00354 mol_a, and RFH under project no. 16-04-12019. The research was also supported by the Ministry of Education and Science of the Russian Federation Agreement no. 02.A03.21.0006.\n",
      "Question : for the text We acknowledge the support of the Deutsche Forschungsgemeinschaft (DFG) under the project “Joining Ontologies and Semantics Induced from Text” (JOIN-T), the RFBR under the projects no. 16-37-00203 mol_a and no. 16-37-00354 mol_a, and the RFH under the project no. 16-04-12019. The research was supported by the Ministry of Education and Science of the Russian Federation Agreement no. 02.A03.21.0006. The calculations were carried out using the supercomputer “Uran” at the Krasovskii Institute of Mathematics and Mechanics. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "17\n",
      "What are the language processing tools required for using Watasense?\n",
      "\n",
      "The language processing tools required for using Watasense are a tokenizer, a part-of-speech tagger, lemmatizer, and a sense inventory.\n",
      "Question : for the text In this paper, we presented Watasense, an open source unsupervised word sense disambiguation system that is parameterized only by a word sense inventory. It supports both sparse and dense sense representations. We were able to show that the dense approach substantially boosts the performance of the sparse approach on three different sense inventories for Russian. We recommend using the dense approach in further studies due to its smoothing capabilities that reduce sparseness. In further studies, we will look at the problem of phrase neighbors that influence the sentence vector representations..Finally, we would like to emphasize the fact that Watasense has a simple API for integrating different algorithms for WSD. At the same time, it requires only a basic set of language processing tools to be available: tokenizer, a part-of-speech tagger, lemmatizer, and a sense inventory, which means that low-resourced language can benefit of its usage. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "18\n",
      "Question 1: What is the gold-standard dataset used in the evaluation of the word sense disambiguation methods in Watasense?\n",
      "\n",
      "Answer 1: The gold-standard dataset used in the evaluation of the word sense disambiguation methods in Watasense is the WSD training dataset for Russian created during RUSSE'2018: A Shared Task on Word Sense Induction and Disambiguation for the Russian Language.\n",
      "Question : for the text We evaluate the word sense disambiguation methods in Watasense against three baselines: an unsupervised approach for learning multi-prototype word embeddings called AdaGram BIBREF15 , same sense for all the instances per lemma (One), and one sense per instance (Singletons). The AdaGram model is trained on the combination of RuWac, Lib.Ru, and the Russian Wikipedia with the overall vocabulary size of 2 billion tokens BIBREF1 ..As the gold-standard dataset, we use the WSD training dataset for Russian created during RUSSE'2018: A Shared Task on Word Sense Induction and Disambiguation for the Russian Language BIBREF16 . The dataset has 31 words covered by INLINEFORM0 instances in the bts-rnc subset and 5 words covered by 439 instances in the wiki-wiki subset..The following different sense inventories have been used during the evaluation:.[leftmargin=4mm].Watlink, a word sense network constructed automatically. It uses the synsets induced in an unsupervised way by the Watset[CWnolog, MCL] method BIBREF2 and the semantic relations from such dictionaries as Wiktionary referred as Joint INLINEFORM0 Exp INLINEFORM1 SWN in Ustalov:17:dialogue. This is the only automatically built inventory we use in the evaluation..RuThes, a large-scale lexical ontology for Russian created by a group of expert lexicographers BIBREF17 ..RuWordNet, a semi-automatic conversion of the RuThes lexical ontology into a WordNet-like structure BIBREF18 ..Since the Dense model requires word embeddings, we used the 500-dimensional word vectors from the Russian Distributional Thesaurus BIBREF19 . These vectors are obtained using the Skip-gram approach trained on the lib.rus.ec text corpus. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "19\n",
      "Question 1: What is the evaluation methodology used in the experiments conducted in the text?\n",
      "\n",
      "Answer 1: The experiments in the text are conducted using the evaluation methodology of SemEval 2010 Task 14: Word Sense Induction & Disambiguation.\n",
      "Question : for the text We conduct our experiments using the evaluation methodology of SemEval 2010 Task 14: Word Sense Induction & Disambiguation BIBREF5 . In the gold standard, each word is provided with a set of instances, i.e., the sentences containing the word. Each instance is manually annotated with the single sense identifier according to a pre-defined sense inventory. Each participating system estimates the sense labels for these ambiguous words, which can be viewed as a clustering of instances, according to sense labels. The system's clustering is compared to the gold-standard clustering for evaluation. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "20\n",
      "What was the motivation for developing an unsupervised system for word sense disambiguation instead of a supervised system?\n",
      "\n",
      "The absence of resources that enable a supervised system for under-resourced languages, such as Russian, motivated the development of an unsupervised system for word sense disambiguation called Watasense.\n",
      "Question : for the text Word sense disambiguation (WSD) is a natural language processing task of identifying the particular word senses of polysemous words used in a sentence. Recently, a lot of attention was paid to the problem of WSD for the Russian language BIBREF0 , BIBREF1 , BIBREF2 . This problem is especially difficult because of both linguistic issues – namely, the rich morphology of Russian and other Slavic languages in general – and technical challenges like the lack of software and language resources required for addressing the problem..To address these issues, we present Watasense, an unsupervised system for word sense disambiguation. We describe its architecture and conduct an evaluation on three datasets for Russian. The choice of an unsupervised system is motivated by the absence of resources that would enable a supervised system for under-resourced languages. Watasense is not strictly tied to the Russian language and can be applied to any language for which a tokenizer, part-of-speech tagger, lemmatizer, and a sense inventory are available..The rest of the paper is organized as follows. Section 2 reviews related work. Section 3 presents the Watasense word sense disambiguation system, presents its architecture, and describes the unsupervised word sense disambiguation methods bundled with it. Section 4 evaluates the system on a gold standard for Russian. Section 5 concludes with final remarks. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "21\n",
      "What external clustering measure was used in the original SemEval 2010 Task 14, and why was it not a satisfactory choice?\n",
      "\n",
      "The original SemEval 2010 Task 14 used the V-Measure external clustering measure, but it was not a satisfactory choice because it was maximized by clustering each sentence into its own distinct cluster, which is a 'dummy' singleton baseline achieved by the system deciding that every ambiguous word in every sentence corresponds to a different word sense.\n",
      "Question : for the text The original SemEval 2010 Task 14 used the V-Measure external clustering measure BIBREF5 . However, this measure is maximized by clustering each sentence into his own distinct cluster, i.e., a `dummy' singleton baseline. This is achieved by the system deciding that every ambiguous word in every sentence corresponds to a different word sense. To cope with this issue, we follow a similar study BIBREF1 and use instead of the adjusted Rand index (ARI) proposed by Hubert:85 as an evaluation measure..In order to provide the overall value of ARI, we follow the addition approach used in BIBREF1 . Since the quality measure is computed for each lemma individually, the total value is a weighted sum, namely DISPLAYFORM0 .where INLINEFORM0 is the lemma, INLINEFORM1 is the set of the instances for the lemma INLINEFORM2 , INLINEFORM3 is the adjusted Rand index computed for the lemma INLINEFORM4 . Thus, the contribution of each lemma to the total score is proportional to the number of instances of this lemma. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "22\n",
      "What is the IMS system?\n",
      "\n",
      "The IMS system is a supervised WSD system that uses a support vector machine classifier to infer the particular sense of a word in the sentence given its contextual sentence-level features. It was designed initially for the English language and is among the freely available WSD systems. (Source: The given text)\n",
      "Question : for the text Although the problem of WSD has been addressed in many SemEval campaigns BIBREF3 , BIBREF4 , BIBREF5 , we focus here on word sense disambiguation systems rather than on the research methodologies..Among the freely available systems, IMS (“It Makes Sense”) is a supervised WSD system designed initially for the English language BIBREF6 . The system uses a support vector machine classifier to infer the particular sense of a word in the sentence given its contextual sentence-level features. Pywsd is an implementation of several popular WSD algorithms implemented in a library for the Python programming language. It offers both the classical Lesk algorithm for WSD and path-based algorithms that heavily use the WordNet and similar lexical ontologies. DKPro WSD BIBREF7 is a general-purpose framework for WSD that uses a lexical ontology as the sense inventory and offers the variety of WordNet-based algorithms. Babelfy BIBREF8 is a WSD system that uses BabelNet, a large-scale multilingual lexical ontology available for most natural languages. Due to the broad coverage of BabelNet, Babelfy offers entity linking as part of the WSD functionality..Panchenko:17:emnlp present an unsupervised WSD system that is also knowledge-free: its sense inventory is induced based on the JoBimText framework, and disambiguation is performed by computing the semantic similarity between the context and the candidate senses BIBREF9 . Pelevina:16 proposed a similar approach to WSD, but based on dense vector representations (word embeddings), called SenseGram. Similarly to SenseGram, our WSD system is based on averaging of word embeddings on the basis of an automatically induced sense inventory. A crucial difference, however, is that we induce our sense inventory from synonymy dictionaries and not distributional word vectors. While this requires more manually created resources, a potential advantage of our approach is that the resulting inventory contains less noise. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "23\n",
      "What is the primary reason for the SenseGram-based approach yielding better results in word sense disambiguation?\n",
      "The primary reason for the SenseGram-based approach yielding better results in word sense disambiguation is the implicit handling of similar words due to the averaging of dense word vectors for semantically related words.\n",
      "Question : for the text We compare the evaluation results obtained for the Sparse and Dense approaches with three baselines: the AdaGram model (AdaGram), the same sense for all the instances per lemma (One) and one sense per instance (Singletons). The evaluation results are presented in Table TABREF25 . The columns bts-rnc and wiki-wiki represent the overall value of ARI according to Equation ( EQREF15 ). The column Avg. consists of the weighted average of the datasets w.r.t. the number of instances..We observe that the SenseGram-based approach for word sense disambiguation yields substantially better results in every case (Table TABREF25 ). The primary reason for that is the implicit handling of similar words due to the averaging of dense word vectors for semantically related words. Thus, we recommend using the dense approach in further studies. Although the AdaGram approach trained on a large text corpus showed better results according to the weighted average, this result does not transfer to languages with less available corpus size. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "24\n",
      "What is the purpose of the BaseWSD class? \n",
      "\n",
      "The purpose of the BaseWSD class is to provide a generic interface for Word Sense Disambiguation (WSD) and to encapsulate common routines for data pre-processing.\n",
      "Question : for the text A sentence is represented as a list of spans. A span is a quadruple: INLINEFORM0 , where INLINEFORM1 is the word or the token, INLINEFORM2 is the part of speech tag, INLINEFORM3 is the lemma, INLINEFORM4 is the position of the word in the sentence. These data are provided by tokenizer, part-of-speech tagger, and lemmatizer that are specific for the given language. The WSD results are represented as a map of spans to the corresponding word sense identifiers..The sense inventory is a list of synsets. A synset is represented by three bag of words: the synonyms, the hypernyms, and the union of two former – the bag. Due to the performance reasons, on initialization, an inverted index is constructed to map a word to the set of synsets it is included into..Each word sense disambiguation method extends the BaseWSD class. This class provides the end user with a generic interface for WSD and also encapsulates common routines for data pre-processing. The inherited classes like SparseWSD and DenseWSD should implement the disambiguate_word(...) method that disambiguates the given word in the given sentence. Both classes use the bag representation of synsets on the initialization. As the result, for WSD, not just the synonyms are used, but also the hypernyms corresponding to the synsets. The UML class diagram is presented in Figure FIGREF4 ..Watasense supports two sources of word vectors: it can either read the word vector dataset in the binary Word2Vec format or use Word2Vec-Pyro4, a general-purpose word vector server. The use of a remote word vector server is recommended due to the reduction of memory footprint per each Watasense process. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "25\n",
      "Question 1: What are the two primary activities of the Watasense web interface?\n",
      "Answer 1: The two primary activities of the Watasense web interface are text input and method selection, as well as the display of disambiguation results with part of speech highlighting.\n",
      "Question : for the text  FIGREF2 shows the Web interface of Watasense. It is composed of two primary activities. The first is the text input and the method selection ( FIGREF2 ). The second is the display of the disambiguation results with part of speech highlighting ( FIGREF7 ). Those words with resolved polysemy are underlined; the tooltips with the details are raised on hover. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "26\n",
      "Question 1: What programming language is used to implement Watasense?\n",
      "\n",
      "Answer 1: The Python programming language is used to implement Watasense, along with the scikit-learn and Gensim libraries.\n",
      "Question : for the text Watasense is implemented in the Python programming language using the scikit-learn BIBREF10 and Gensim BIBREF11 libraries. Watasense offers a Web interface (Figure FIGREF2 ), a command-line tool, and an application programming interface (API) for deployment within other applications. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "27\n",
      "What is the difference between the sparse model and dense model in word sense disambiguation?\n",
      "\n",
      "The sparse model uses a sparse vector space model to represent contexts and synsets, while the dense model represents synsets and contexts in a dense, low-dimensional space by averaging word embeddings.\n",
      "Question : for the text We use two different unsupervised approaches for word sense disambiguation. The first, called `sparse model', uses a straightforward sparse vector space model, as widely used in Information Retrieval, to represent contexts and synsets. The second, called `dense model', represents synsets and contexts in a dense, low-dimensional space by averaging word embeddings..In the vector space model approach, we follow the sparse context-based disambiguated method BIBREF12 , BIBREF13 . For estimating the sense of the word INLINEFORM0 in a sentence, we search for such a synset INLINEFORM1 that maximizes the cosine similarity to the sentence vector: DISPLAYFORM0 .where INLINEFORM0 is the set of words forming the synset, INLINEFORM1 is the set of words forming the sentence. On initialization, the synsets represented in the sense inventory are transformed into the INLINEFORM2 -weighted word-synset sparse matrix efficiently represented in the memory using the compressed sparse row format. Given a sentence, a similar transformation is done to obtain the sparse vector representation of the sentence in the same space as the word-synset matrix. Then, for each word to disambiguate, we retrieve the synset containing this word that maximizes the cosine similarity between the sparse sentence vector and the sparse synset vector. Let INLINEFORM3 be the maximal number of synsets containing a word and INLINEFORM4 be the maximal size of a synset. Therefore, disambiguation of the whole sentence INLINEFORM5 requires INLINEFORM6 operations using the efficient sparse matrix representation..In the synset embeddings model approach, we follow SenseGram BIBREF14 and apply it to the synsets induced from a graph of synonyms. We transform every synset into its dense vector representation by averaging the word embeddings corresponding to each constituent word: DISPLAYFORM0 .where INLINEFORM0 denotes the word embedding of INLINEFORM1 . We do the same transformation for the sentence vectors. Then, given a word INLINEFORM2 , a sentence INLINEFORM3 , we find the synset INLINEFORM4 that maximizes the cosine similarity to the sentence: DISPLAYFORM0 .On initialization, we pre-compute the dense synset vectors by averaging the corresponding word embeddings. Given a sentence, we similarly compute the dense sentence vector by averaging the vectors of the words belonging to non-auxiliary parts of speech, i.e., nouns, adjectives, adverbs, verbs, etc. Then, given a word to disambiguate, we retrieve the synset that maximizes the cosine similarity between the dense sentence vector and the dense synset vector. Thus, given the number of dimensions INLINEFORM0 , disambiguation of the whole sentence INLINEFORM1 requires INLINEFORM2 operations. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "28\n",
      "Question 1: Who funded the work mentioned in the text?\n",
      "\n",
      "Answer 1: The work was funded by NSF under grants CCF-1414030 and IIS-1250956 and by grants from Google.\n",
      "Question : for the text This work was funded by NSF under grants CCF-1414030 and IIS-1250956 and by grants from Google. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "29\n",
      "Question 1: What is the WD baseline used for in evaluating Quasar? \n",
      "Answer 1: The WD baseline measures the sum of distances from a candidate to other non-stopword tokens in the passage which are also present in the query. It is used in evaluating Quasar for both Quasar-S and Quasar-T. For Quasar-S, it measures the distance by first aligning the query placeholder to the candidate in the passage and then measuring the offsets between other tokens in the query and their mentions in the passage. For Quasar-T, it tests the Sliding Window (SW) and Sliding Window + Distance (SW+D) baselines proposed.\n",
      "Question : for the text We evaluate several baselines on Quasar, ranging from simple heuristics to deep neural networks. Some predict a single token / entity as the answer, while others predict a span of tokens..MF-i (Maximum Frequency) counts the number of occurrences of each candidate answer in the retrieved context and returns the one with maximum frequency. MF-e is the same as MF-i except it excludes the candidates present in the query. WD (Word Distance) measures the sum of distances from a candidate to other non-stopword tokens in the passage which are also present in the query. For the cloze-style Quasar-S the distances are measured by first aligning the query placeholder to the candidate in the passage, and then measuring the offsets between other tokens in the query and their mentions in the passage. The maximum distance for any token is capped at a specified threshold, which is tuned on the validation set..For Quasar-T we also test the Sliding Window (SW) and Sliding Window + Distance (SW+D) baselines proposed in BIBREF13 . The scores were computed for the list of candidate solutions described in Section \"Context Retrieval\" ..For Quasar-S, since the answers come from a fixed vocabulary of entities, we test language model baselines which predict the most likely entity to appear in a given context. We train three n-gram baselines using the SRILM toolkit BIBREF21 for $n=3,4,5$ on the entire corpus of all Stack Overflow posts. The output predictions are restricted to the output vocabulary of entities..We also train a bidirectional Recurrent Neural Network (RNN) language model (based on GRU units). This model encodes both the left and right context of an entity using forward and backward GRUs, and then concatenates the final states from both to predict the entity through a softmax layer. Training is performed on the entire corpus of Stack Overflow posts, with the loss computed only over mentions of entities in the output vocabulary. This approach benefits from looking at both sides of the cloze in a query to predict the entity, as compared to the single-sided n-gram baselines..Reading comprehension models are trained to extract the answer from the given passage. We test two recent architectures on Quasar using publicly available code from the authors ..The GA Reader BIBREF8 is a multi-layer neural network which extracts a single token from the passage to answer a given query. At the time of writing it had state-of-the-art performance on several cloze-style datasets for QA. For Quasar-S we train and test GA on all instances for which the correct answer is found within the retrieved context. For Quasar-T we train and test GA on all instances where the answer is in the context and is a single token..The BiDAF model BIBREF9 is also a multi-layer neural network which predicts a span of text from the passage as the answer to a given query. At the time of writing it had state-of-the-art performance among published models on the Squad dataset. For Quasar-T we train and test BiDAF on all instances where the answer is in the retrieved context. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "30\n",
      "Question 1: What was the size of Quasar-S's closed vocabulary candidate list?\n",
      "\n",
      "Answer 1: The closed vocabulary candidate list for Quasar-S contained 4874 tags.\n",
      "Question : for the text The list of candidate solutions provided with each record is guaranteed to contain the correct answer to the question. Quasar-S used a closed vocabulary of 4874 tags as its candidate list. Since the questions in Quasar-T are in free-response format, we constructed a separate list of candidate solutions for each question. Since most of the correct answers were noun phrases, we took each sequence of NN* -tagged tokens in the context document, as identified by the Stanford NLP Maxent POS tagger, as the candidate list for each record. If this list did not include the correct answer, it was added to the list. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "31\n",
      "Question 1: What are the two related tasks for QA that the Quasar datasets were presented for?\n",
      "\n",
      "Answer 1: The Quasar datasets were presented for promoting research into two related tasks for QA – searching a large corpus of text for relevant passages, and reading the passages to extract answers.\n",
      "Question : for the text We have presented the Quasar datasets for promoting research into two related tasks for QA – searching a large corpus of text for relevant passages, and reading the passages to extract answers. We have also described baseline systems for the two tasks which perform reasonably but lag behind human performance. While the searching performance improves as we retrieve more context, the reading performance typically goes down. Hence, future work, in addition to improving these components individually, should also focus on joint approaches to optimizing the two on end-task performance. The datasets, including the documents retrieved by our system and the human annotations, are available at https://github.com/bdhingra/quasar. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "32\n",
      "Question 1: How were the context documents generated for Quasar-S?\n",
      "Answer 1: The context documents for Quasar-S were generated in a two-phase fashion, first collecting a large pool of semirelevant text, then filling a temporary index with short or long pseudodocuments from the pool, and finally selecting a set of $N$ top-ranking pseudodocuments (100 short or 20 long) from the temporary index. The pool of text for each question was composed of 50+ question-and-answer threads scraped from http://stackoverflow.com, and the pseudodocuments for the entire corpus were loaded into a disk-based lucene index, each annotated with its thread ID and the tags for the thread. The top $100N$ pseudodocuments were retrieved, and the top $N$ unique pseudodocuments were added to the context document along with their lucene retrieval score.\n",
      "Question : for the text The context document for each record consists of a list of ranked and scored pseudodocuments relevant to the question..Context documents for each query were generated in a two-phase fashion, first collecting a large pool of semirelevant text, then filling a temporary index with short or long pseudodocuments from the pool, and finally selecting a set of $N$ top-ranking pseudodocuments (100 short or 20 long) from the temporary index..For Quasar-S, the pool of text for each question was composed of 50+ question-and-answer threads scraped from http://stackoverflow.com. StackOverflow keeps a running tally of the top-voted questions for each tag in their knowledge base; we used Scrapy to pull the top 50 question posts for each tag, along with any answer-post responses and metadata (tags, authorship, comments). From each thread we pulled all text not marked as code, and split it into sentences using the Stanford NLP sentence segmenter, truncating sentences to 2048 characters. Each sentence was marked with a thread identifier, a post identifier, and the tags for the thread. Long pseudodocuments were either the full post (in the case of question posts), or the full post and its head question (in the case of answer posts), comments included. Short pseudodocuments were individual sentences..To build the context documents for Quasar-S, the pseudodocuments for the entire corpus were loaded into a disk-based lucene index, each annotated with its thread ID and the tags for the thread. This index was queried for each cloze using the following lucene syntax:.[noitemsep] .SHOULD(PHRASE(question text)).SHOULD(BOOLEAN(question text)).MUST(tags:$headtag).where “question text” refers to the sequence of tokens in the cloze question, with the placeholder removed. The first SHOULD term indicates that an exact phrase match to the question text should score highly. The second SHOULD term indicates that any partial match to tokens in the question text should also score highly, roughly in proportion to the number of terms matched. The MUST term indicates that only pseudodocuments annotated with the head tag of the cloze should be considered..The top $100N$ pseudodocuments were retrieved, and the top $N$ unique pseudodocuments were added to the context document along with their lucene retrieval score. Any questions showing zero results for this query were discarded..For Quasar-T, the pool of text for each question was composed of 100 HTML documents retrieved from ClueWeb09. Each question-answer pair was converted to a #combine query in the Indri query language to comply with the ClueWeb09 batch query service, using simple regular expression substitution rules to remove (s/[.(){}<>:*`_]+//g) or replace (s/[,?']+/ /g) illegal characters. Any questions generating syntax errors after this step were discarded. We then extracted the plaintext from each HTML document using Jericho. For long pseudodocuments we used the full page text, truncated to 2048 characters. For short pseudodocuments we used individual sentences as extracted by the Stanford NLP sentence segmenter, truncated to 200 characters..To build the context documents for the trivia set, the pseudodocuments from the pool were collected into an in-memory lucene index and queried using the question text only (the answer text was not included for this step). The structure of the query was identical to the query for Quasar-S, without the head tag filter:.[noitemsep] .SHOULD(PHRASE(question text)).SHOULD(BOOLEAN(question text)).The top $100N$ pseudodocuments were retrieved, and the top $N$ unique pseudodocuments were added to the context document along with their lucene retrieval score. Any questions showing zero results for this query were discarded. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "33\n",
      "Question 1: What is the capital city of Brazil?\n",
      "Answer 1: The capital city of Brazil is Brasília.\n",
      "Question : for the text Each dataset consists of a collection of records with one QA problem per record. For each record, we include some question text, a context document relevant to the question, a set of candidate solutions, and the correct solution. In this section, we describe how each of these fields was generated for each Quasar variant. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "34\n",
      "Question 1: How were the domain experts and non-experts evaluated on their ability to answer questions from the datasets?\n",
      "\n",
      "Answer 1: Domain experts and non-experts were presented with randomly selected questions from the development set and asked to answer them via an online app. The experts were evaluated in a \"closed-book\" setting, while the non-experts were evaluated in an \"open-book\" setting with access to a search engine over the short pseudo-documents extracted for each dataset. They were also asked to provide annotations to categorize the type of each question they were asked, and a label for whether the question was ambiguous.\n",
      "Question : for the text To put the difficulty of the introduced datasets into perspective, we evaluated human performance on answering the questions. For each dataset, we recruited one domain expert (a developer with several years of programming experience for Quasar-S, and an avid trivia enthusiast for Quasar-T) and $1-3$ non-experts. Each volunteer was presented with randomly selected questions from the development set and asked to answer them via an online app. The experts were evaluated in a “closed-book” setting, i.e. they did not have access to any external resources. The non-experts were evaluated in an “open-book” setting, where they had access to a search engine over the short pseudo-documents extracted for each dataset (as described in Section \"Context Retrieval\" ). We decided to use short pseudo-documents for this exercise to reduce the burden of reading on the volunteers, though we note that the long pseudo-documents have greater coverage of answers..We also asked the volunteers to provide annotations to categorize the type of each question they were asked, and a label for whether the question was ambiguous. For Quasar-S the annotators were asked to mark the relation between the head entity (from whose definition the cloze was constructed) and the answer entity. For Quasar-T the annotators were asked to mark the genre of the question (e.g., Arts & Literature) and the entity type of the answer (e.g., Person). When multiple annotators marked the same question differently, we took the majority vote when possible and discarded ties. In total we collected 226 relation annotations for 136 questions in Quasar-S, out of which 27 were discarded due to conflicting ties, leaving a total of 109 annotated questions. For Quasar-T we collected annotations for a total of 144 questions, out of which 12 we marked as ambiguous. In the remaining 132, a total of 214 genres were annotated (a question could be annotated with multiple genres), while 10 questions had conflicting entity-type annotations which we discarded, leaving 122 total entity-type annotations. Figure 3 shows the distribution of these annotations. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "35\n",
      "Question 1: What are the two main approaches for factoid QA?\n",
      "\n",
      "Answer 1: The two main approaches for factoid QA are structured sources, including Knowledge Bases (KBs) such as Freebase, and unstructured sources, such as Wikipedia articles.\n",
      "Question : for the text Factoid Question Answering (QA) aims to extract answers, from an underlying knowledge source, to information seeking questions posed in natural language. Depending on the knowledge source available there are two main approaches for factoid QA. Structured sources, including Knowledge Bases (KBs) such as Freebase BIBREF1 , are easier to process automatically since the information is organized according to a fixed schema. In this case the question is parsed into a logical form in order to query against the KB. However, even the largest KBs are often incomplete BIBREF2 , BIBREF3 , and hence can only answer a limited subset of all possible factoid questions..For this reason the focus is now shifting towards unstructured sources, such as Wikipedia articles, which hold a vast quantity of information in textual form and, in principle, can be used to answer a much larger collection of questions. Extracting the correct answer from unstructured text is, however, challenging, and typical QA pipelines consist of the following two components: (1) searching for the passages relevant to the given question, and (2) reading the retrieved text in order to select a span of text which best answers the question BIBREF4 , BIBREF5 ..Like most other language technologies, the current research focus for both these steps is firmly on machine learning based approaches for which performance improves with the amount of data available. Machine reading performance, in particular, has been significantly boosted in the last few years with the introduction of large-scale reading comprehension datasets such as CNN / DailyMail BIBREF6 and Squad BIBREF7 . State-of-the-art systems for these datasets BIBREF8 , BIBREF9 focus solely on step (2) above, in effect assuming the relevant passage of text is already known..In this paper, we introduce two new datasets for QUestion Answering by Search And Reading – Quasar. The datasets each consist of factoid question-answer pairs and a corresponding large background corpus to facilitate research into the combined problem of retrieval and comprehension. Quasar-S consists of 37,362 cloze-style questions constructed from definitions of software entities available on the popular website Stack Overflow. The answer to each question is restricted to be another software entity, from an output vocabulary of 4874 entities. Quasar-T consists of 43,013 trivia questions collected from various internet sources by a trivia enthusiast. The answers to these questions are free-form spans of text, though most are noun phrases..While production quality QA systems may have access to the entire world wide web as a knowledge source, for Quasar we restrict our search to specific background corpora. This is necessary to avoid uninteresting solutions which directly extract answers from the sources from which the questions were constructed. For Quasar-S we construct the knowledge source by collecting top 50 threads tagged with each entity in the dataset on the Stack Overflow website. For Quasar-T we use ClueWeb09 BIBREF0 , which contains about 1 billion web pages collected between January and February 2009. Figure 1 shows some examples..Unlike existing reading comprehension tasks, the Quasar tasks go beyond the ability to only understand a given passage, and require the ability to answer questions given large corpora. Prior datasets (such as those used in BIBREF4 ) are constructed by first selecting a passage and then constructing questions about that passage. This design (intentionally) ignores some of the subproblems required to answer open-domain questions from corpora, namely searching for passages that may contain candidate answers, and aggregating information/resolving conflicts between candidates from many passages. The purpose of Quasar is to allow research into these subproblems, and in particular whether the search step can benefit from integration and joint training with downstream reading systems..Additionally, Quasar-S has the interesting feature of being a closed-domain dataset about computer programming, and successful approaches to it must develop domain-expertise and a deep understanding of the background corpus. To our knowledge it is one of the largest closed-domain QA datasets available. Quasar-T, on the other hand, consists of open-domain questions based on trivia, which refers to “bits of information, often of little importance\". Unlike previous open-domain systems which rely heavily on the redundancy of information on the web to correctly answer questions, we hypothesize that Quasar-T requires a deeper reading of documents to answer correctly..We evaluate Quasar against human testers, as well as several baselines ranging from naïve heuristics to state-of-the-art machine readers. The best performing baselines achieve $33.6\\%$ and $28.5\\%$ on Quasar-S and Quasar-T, while human performance is $50\\%$ and $60.6\\%$ respectively. For the automatic systems, we see an interesting tension between searching and reading accuracies – retrieving more documents in the search phase leads to a higher coverage of answers, but makes the comprehension task more difficult. We also collect annotations on a subset of the development set questions to allow researchers to analyze the categories in which their system performs well or falls short. We plan to release these annotations along with the datasets, and our retrieved documents for each question. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "36\n",
      "Question 1: What is the capital of France?\n",
      "Answer 1: Paris.\n",
      "Question : for the text Evaluation is straightforward on Quasar-S since each answer comes from a fixed output vocabulary of entities, and we report the average accuracy of predictions as the evaluation metric. For Quasar-T, the answers may be free form spans of text, and the same answer may be expressed in different terms, which makes evaluation difficult. Here we pick the two metrics from BIBREF7 , BIBREF19 . In preprocessing the answer we remove punctuation, white-space and definite and indefinite articles from the strings. Then, exact match measures whether the two strings, after preprocessing, are equal or not. For F1 match we first construct a bag of tokens for each string, followed be preprocessing of each token, and measure the F1 score of the overlap between the two bags of tokens. These metrics are far from perfect for Quasar-T; for example, our human testers were penalized for entering “0” as answer instead of “zero”. However, a comparison between systems may still be meaningful. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "37\n",
      "Question 1: In which categories does Bi-RNN perform comparably to humans in Quasar-S?\n",
      "\n",
      "Answer 1: Bi-RNN performs comparably to humans in Quasar-S for the developed-with and runs-on categories.\n",
      "Question : for the text Figure 5 shows a comparison of the human performance with the best performing baseline for each category of annotated questions. We see consistent differences between the two, except in the following cases. For Quasar-S, Bi-RNN performs comparably to humans for the developed-with and runs-on categories, but much worse in the has-component and is-a categories. For Quasar-T, BiDAF performs comparably to humans in the sports category, but much worse in history & religion and language, or when the answer type is a number or date/time. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "38\n",
      "Question 1: What is the purpose of extracting a subset of questions from the context documents?\n",
      "\n",
      "Answer 1: The purpose of extracting a subset of questions from the context documents is to evaluate the performance of the reading system independently from the search system.\n",
      "Question : for the text Once context documents had been built, we extracted the subset of questions where the answer string, excluded from the query for the two-phase search, was nonetheless present in the context document. This subset allows us to evaluate the performance of the reading system independently from the search system, while the full set allows us to evaluate the performance of Quasar as a whole. We also split the full set into training, validation and test sets. The final size of each data subset after all discards is listed in Table 1 . generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "39\n",
      "Question 1: What does Table 4 in Quasar-S include?\n",
      "\n",
      "Answer 1: Table 4 in Quasar-S includes the definition of all the annotated relations.\n",
      "Question : for the text Table 4 includes the definition of all the annotated relations for Quasar-S. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "40\n",
      "in except\n",
      "40\n",
      "Q1: What is the issue with the labeling of organizations in the training and test data?\n",
      "A1: The organizations are labeled in the train data but not labeled in the test data, causing inconsistency between the datasets.\n",
      "Question : for the text After considering the training and test data, we realized that this data has many problems need to be fixed in the next run experiments. The annotators are not consistent between the training data and the test data, more details are shown as follow:.The organizations are labeled in the train data but not labeled in the test data:.Training data: vietnam⟨ORG⟩ Sở Y_tế ⟨ORG⟩ (Department of Health).Test data: vietnamSở Y_tế (Department of Health).Explanation: vietnam\"Sở Y_tế\" in train and test are the same name of organization entity. However the one in test data is not labeled..The entity has the same meaning but is assigned differently between the train data and the test:.Training data: vietnam⟨MISC⟩ người Việt ⟨MISC⟩ (Vietnamese people).Test data: vietnamdân ⟨LOC⟩ Việt ⟨LOC⟩ (Vietnamese people).Explanation: vietnamBoth \"người Việt\" in train data and \"dân Việt\" in test data are the same meaning, but they are assigned differently..The range of entities are differently between the train data and the test data:.Training data: vietnam⟨LOC⟩ làng Atâu ⟨LOC⟩ (Atâu village).Test data: vietnamlàng ⟨LOC⟩ Hàn_Quốc ⟨LOC⟩ (Korea village).Explanation: The two villages differ only in name, but they are labeled differently in range.Capitalization rules are not unified with a token is considered an entity:.Training data: vietnam⟨ORG⟩ Công_ty Inmasco ⟨ORG⟩ (Inmasco Company).Training data: vietnamcông_ty con (Subsidiaries).Test data: vietnamcông_ty ⟨ORG⟩ Yeon Young Entertainment ⟨ORG⟩ (Yeon Young Entertainment company).Explanation: If it comes to a company with a specific name, it should be labeled vietnam⟨ORG⟩ Công_ty Yeon Young Entertainment ⟨ORG⟩ with \"C\" in capital letters. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "1\n",
      "What is Named Entity Recognition (NER)?\n",
      "NER is responsible for detecting entity elements from raw text and can determine the category in which the element belongs, these categories include the names of persons, organizations, locations, expressions of times, quantities, monetary values and percentages.\n",
      "Question : for the text Named Entity Recognition (NER) is one of information extraction subtasks that is responsible for detecting entity elements from raw text and can determine the category in which the element belongs, these categories include the names of persons, organizations, locations, expressions of times, quantities, monetary values and percentages..The problem of NER is described as follow:.Input: A sentence S consists a sequence of $n$ words: $S= w_1,w_2,w_3,…,w_n$ ($w_i$: the $i^{th}$ word).Output: The sequence of $n$ labels $y_1,y_2,y_3,…,y_n$. Each $y_i$ label represents the category which $w_i$ belongs to..For example, given a sentence:.Input: vietnamGiám đốc điều hành Tim Cook của Apple vừa giới thiệu 2 điện thoại iPhone, đồng hồ thông minh mới, lớn hơn ở sự kiện Flint Center, Cupertino..(Apple CEO Tim Cook introduces 2 new, larger iPhones, Smart Watch at Cupertino Flint Center event).The algorithm will output:.Output: vietnam⟨O⟩Giám đốc điều hành⟨O⟩ ⟨PER⟩Tim Cook⟨PER⟩ ⟨O⟩của⟨O⟩ ⟨ORG⟩Apple⟨ORG⟩ ⟨O⟩vừa giới thiệu 2 điện thoại iPhone, đồng hồ thông minh mới, lớn hơn ở sự kiện⟨O⟩ ⟨ORG⟩Flint Center⟨ORG⟩, ⟨LOC⟩Cupertino⟨LOC⟩..With LOC, PER, ORG is Name of location, person, organization respectively. Note that O means Other (Not a Name entity). We will not denote the O label in the following examples in this article because we only care about name of entities..In this paper, we analyze common errors of the previous state-of-the-art techniques using Deep Neural Network (DNN) on VLSP Corpus. This may contribute to the later researchers the common errors from the results of these state-of-the-art models, then they can rely on to improve the model..Section 2 discusses the related works to this paper. We will present a method for evaluating and analyzing the types of errors in Section 3. The data used for testing and analysis of errors will be introduced in Section 4, we also talk about deep neural network methods and pre-trained word embeddings for experimentation in this section. Section 5 will detail the errors and evaluations. In the end is our contribution to improve the above errors. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "2\n",
      "What are some traditional machine learning methods used for Vietnamese language processing NER systems?\n",
      "\n",
      "Answer 1: Some traditional machine learning methods used for Vietnamese language processing NER systems include Maximum Entropy Markov Model (MEMM), Support Vector Machine (SVM), and Conditional Random Field (CRF).\n",
      "Question : for the text Previously publicly available NER systems do not use DNN, for example, the MITRE Identification Scrubber Toolkit (MIST) BIBREF0, Stanford NER BIBREF1, BANNER BIBREF2 and NERsuite BIBREF3. NER systems for Vietnamese language processing used traditional machine learning methods such as Maximum Entropy Markov Model (MEMM), Support Vector Machine (SVM) and Conditional Random Field (CRF). In particular, most of the toolkits for NER task attempted to use MEMM BIBREF4, and CRF BIBREF5 to solve this problem..Nowadays, because of the increase in data, DNN methods are used a lot. They have archived great results when it comes to NER tasks, for example, Guillaume Lample et al with BLSTM-CRF in BIBREF6 report 90.94 F1 score, Chiu et al with BLSTM-CNN in BIBREF7 got 91.62 F1 score, Xeuzhe Ma and Eduard Hovy with BLSTM-CNN-CRF in BIBREF8 achieved F1 score of 91.21, Thai-Hoang Pham and Phuong Le-Hong with BLSTM-CNN-CRF in BIBREF9 got 88.59% F1 score. These DNN models are also the state-of-the-art models. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "3\n",
      "Question 1: What does a darker blue patch indicate in the weight distribution visualization? \n",
      "Answer 1: A darker blue patch indicates a larger weight relative to other words in the same sentence.\n",
      "Question : for the text In addition to quantitative analysis, it is natural to qualitatively evaluate the performance of the attention mechanism by visualizing the weight distribution of each instance. We randomly picked several instances from the test set in task A, for which the sentence lengths are more moderate for demonstration. These examples are shown in Figure 5 , and categorized into short, long, and noisy sentences for discussion. A darker blue patch refers to a larger weight relative to other words in the same sentence. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "4\n",
      "Question 1: What are some sources of external question-answer pairs that could be used in the tasks? \n",
      "Answer 1: WebQuestion and The SimpleQuestions dataset are some examples of external sources that could be used for the tasks.\n",
      "Question : for the text There are many sources of external question-answer pairs that could be used in our tasks. For example: WebQuestion (was introduced by the authors of SEMPRE system BIBREF18 ) and The SimpleQuestions dataset . All of them are positive examples for our task and we can easily create negative examples from it. Initial experiments indicate that it is very easy to overfit these obvious negative examples. We believe this is because our negative examples are non-informative for our task and just introduce noise..Since the external data seems to hurt the performance, we try to use the in-domain pairs to enhance task B and task C. For task B, if relative question 1 (rel1) and relative question 2 (rel2) are both relevant to the original question, then we add a positive sample (rel1, rel2, 1). If either rel1 and rel2 is irrelevant and the other is relevant, we add a negative sample (rel1, rel2, 0). After doing this, the samples of task B increase from $2,670$ to $11,810$ . By applying this method, the MAP score increased slightly from $0.5723$ to $0.5789$ but the F1 score improved from $0.4334$ to $0.5860$ ..For task C, we used task A's data directly. The results are very similar with a slight improvement on MAP, but large improvement on F1 score from $0.1449$ to $0.2064$ . generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "5\n",
      "Question 1: What additional feature was incorporated into the FNN classifier to further enhance the system?\n",
      "\n",
      "Answer 1: A one hot vector of the original IR ranking was incorporated as an additional feature into the FNN classifier to further enhance the system.\n",
      "Question : for the text To further enhance the system, we incorporate a one hot vector of the original IR ranking as an additional feature into the FNN classifier. Table 3 shows the results. In comparing the models with and without augmented features, we can see large improvement for task B and C. The F1 score for task A degrades slightly but MAP improves. This might be because task A already had a substantial amount of training data. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "6\n",
      "Question 1: What does Table 4 in the text show in terms of the comparison between different models?\n",
      "\n",
      "Answer 1: Table 4 in the text provides the final comparison between different models for the challenge, specifically in terms of their MAP scores. The system that was restricted to the provided training data performed better than a strong feature-rich based system for task A when there was enough training data. However, for task B with limited training data, both the feature-rich based system and the system with provided training data performed worse than the IR system. Finally, for task C, the system with provided training data got comparable results with the feature-rich based system.\n",
      "Question : for the text Table 4 gives the final comparison between different models (we only list the MAP score because it is the official score for the challenge). Since the two baseline models did not use any additional data, in this table our system was also restricted to the provided training data. For task A, we can see that if there is enough training data our single system already performs better than a very strong feature-rich based system. For task B, since only limited training data is given, both feature-rich based system and our system are worse than the IR system. For task C, our system also got comparable results with the feature-rich based system. If we do a simple system combination (average the rank score) between our system and the IR system, the combined system will give large gains on tasks B and C. This implies that our system is complimentary with the IR system. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "7\n",
      "What is the purpose of adding a neural attention mechanism to the RNN encoder framework for community question answering tasks?\n",
      "\n",
      "Adding a neural attention mechanism improves the RNN encoder framework both quantitatively and qualitatively.\n",
      "Question : for the text In this paper, we demonstrate that a general RNN encoder framework can be applied to community question answering tasks. By adding a neural attention mechanism, we showed quantitatively and qualitatively that attention can improve the RNN encoder framework. To deal with a more realistic scenario, we expanded the framework to incorporate metadata as augmented inputs to a FNN classifier, and pretrained models on larger datasets, increasing both stability and performance. Our model is consistently better than or comparable to a strong feature-rich baseline system, and is superior to an IR-based system when there is a reasonable amount of training data..Our model is complimentary with an IR-based system that uses vast amounts of external resources but trained for general purposes. By combining the two systems, it exceeds the feature-rich and IR-based system in all three tasks..Moreover, our approach is also language independent. We have also performed preliminary experiments on the Arabic portion of the SemEval-2016 cQA task. The results are competitive with a hand-tuned strong baseline from SemEval-2015..Future work could proceed in two directions: first, we can enrich the existing system by incorporating available metadata and preprocessing data with morphological normalization and out-of-vocabulary mappings; second, we can reinforce our model by carrying out word-by-word and history-aware attention mechanisms instead of attending only when reading the last word. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "8\n",
      "Question 1: What is the size of the dataset used for task C in the evaluation of the approach? \n",
      "\n",
      "Answer 1: The size of the dataset used for task C in the evaluation of the approach is 26,700 question-comment pairs.\n",
      "Question : for the text We evaluate our approach on all three cQA tasks. We use the cQA datasets provided by the Semeval 2016 task . The cQA data is organized as follows: there are 267 original questions, each question has 10 related question, and each related question has 10 comments. Therefore, for task A, there are a total number of 26,700 question-comment pairs. For task B, there are 2,670 question-question pairs. For task C, there are 26,700 question-comment pairs. The test dataset includes 50 questions, 500 related questions and 5,000 comments which do not overlap with the training set. To evaluate the performance, we use mean average precision (MAP) and F1 score. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "9\n",
      "Question 1: What is the capital of France?\n",
      "\n",
      "Answer 1: The capital of France is Paris.\n",
      "Question : for the text Community question answering (cQA) is a paradigm that provides forums for users to ask or answer questions on any topic with barely any restrictions. In the past decade, these websites have attracted a great number of users, and have accumulated a large collection of question-comment threads generated by these users. However, the low restriction results in a high variation in answer quality, which makes it time-consuming to search for useful information from the existing content. It would therefore be valuable to automate the procedure of ranking related questions and comments for users with a new question, or when looking for solutions from comments of an existing question..Automation of cQA forums can be divided into three tasks: question-comment relevance (Task A), question-question relevance (Task B), and question-external comment relevance (Task C). One might think that classic retrieval models like language models for information retrieval BIBREF0 could solve these tasks. However, a big challenge for cQA tasks is that users are used to expressing similar meanings with different words, which creates gaps when matching questions based on common words. Other challenges include informal usage of language, highly diverse content of comments, and variation in the length of both questions and comments..To overcome these issues, most previous work (e.g. SemEval 2015 BIBREF1 ) relied heavily on additional features and reasoning capabilities. In BIBREF2 , a neural attention-based model was proposed for automatically recognizing entailment relations between pairs of natural language sentences. In this study, we first modify this model for all three cQA tasks. We also extend this framework into a jointly trained model when the external resources are available, i.e. selecting an external comment when we know the question that the external comment answers (Task C)..Our ultimate objective is to classify relevant questions and comments without complicated handcrafted features. By applying RNN-based encoders, we avoid heavily engineered features and learn the representation automatically. In addition, an attention mechanism augments encoders with the ability to attend to past outputs directly. This becomes helpful when encoding longer sequences, since we no longer need to compress all information into a fixed-length vector representation..In our view, existing annotated cQA corpora are generally too small to properly train an end-to-end neural network. To address this, we investigate transfer learning by pretraining the recurrent systems on other corpora, and also generating additional instances from existing cQA corpus. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "10\n",
      "Question 1: What are the components of an LSTM unit?\n",
      "Answer 1: An LSTM unit contains a memory cell with self-connections and three multiplicative gates: input, forget, and output gates. The gates are controlled by sigmoid functions to control the amount of information flow. The LSTM unit takes input vector $x_t$, previous hidden output $h_{t-1}$, and previous cell state $c_{t-1}$ as inputs and produces an updated cell state $c_t$ and hidden output $h_t$.\n",
      "Question : for the text LSTMs have shown great success in many different fields. An LSTM unit contains a memory cell with self-connections, as well as three multiplicative gates to control information flow. Given input vector $x_t$ , previous hidden outputs $h_{t-1}$ , and previous cell state $c_{t-1}$ , LSTM units operate as follows: .$$X &= \\begin{bmatrix}\n",
      "x_t\\\\[0.3em]\n",
      "h_{t-1}\\\\[0.3em]\n",
      "\\end{bmatrix}\\\\\n",
      "i_t &= \\sigma (\\mathbf {W_{iX}}X + \\mathbf {W_{ic}}c_{t-1} + \\mathbf {b_i})\\\\\n",
      "f_t &= \\sigma (\\mathbf {W_{fX}}X + \\mathbf {W_{fc}}c_{t-1} + \\mathbf {b_f})\\\\\n",
      "o_t &= \\sigma (\\mathbf {W_{oX}}X + \\mathbf {W_{oc}}c_{t-1} + \\mathbf {b_o})\\\\\n",
      "c_t &= f_t \\odot c_{t-1} + i_t \\odot tanh(\\mathbf {W_{cX}}X + \\mathbf {b_c})\\\\\n",
      "h_t &= o_t \\odot tanh(c_t)$$   (Eq. 3) .where $i_t$ , $f_t$ , $o_t$ are input, forget, and output gates, respectively. The sigmoid function $\\sigma ()$ is a soft gate function controlling the amount of information flow. $W$ s and $b$ s are model parameters to learn. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "11\n",
      "Question 1: Does the distribution of weights become more uniform in the longer questions investigated in Figure 5?\n",
      "\n",
      "Answer 1: No, despite the longer questions containing 63 words, the distribution of weights does not become more uniform. The model still focuses attention on a small number of hot words and assigns very small weights to some frequently appearing words that carry little information for classification.\n",
      "Question : for the text In Figure 5 , we investigate two examples with longer questions, which both contain 63 words. Interestingly, the distribution of weights does not become more uniform; the model still focuses attention on a small number of hot words, for example, “puppy dog for ... mall” and “hectic driving in doha ... car insurance ... quite costly”. Additionally, some words that appear frequently but carry little information for classification are assigned very small weights, such as I/we/my, is/am, like, and to. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "12\n",
      "Question 1: What models are used to predict relationships between pairs of sentences in this section?\n",
      "\n",
      "Answer 1: The section discusses using long short-term memory (LSTM) units with an associated attention mechanism to encode pairs of sentences into a dense vector and predict relationships. The models are applied to predict question-question similarity, question-comment similarity, and question-external comment similarity.\n",
      "Question : for the text In this section, we first discuss long short-term memory (LSTM) units and an associated attention mechanism. Next, we explain how we can encode a pair of sentences into a dense vector for predicting relationships using an LSTM with an attention mechanism. Finally, we apply these models to predict question-question similarity, question-comment similarity, and question-external comment similarity. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "13\n",
      "Question 1: What is the multitask learning framework used for in task C? \n",
      "\n",
      "Answer 1: The multitask learning framework used in task C is used to jointly predict the relationships of the three pairs - oriQ/relQ, oriQ/relC, and relQ/relC. This framework consists of three separate serialized LSTM-encoders for the three respective object pairs and an FNN that takes the concatenation of the outputs of three encoders as input and predicts the relationships for all three pairs. The overall loss function is a combination of three separate loss functions using a heuristic weight vector that allocates a higher weight to the main task (oriQ-relC relationship prediction), which can improve the main task by leveraging commonality among all tasks.\n",
      "Question : for the text For task C, in addition to an original question (oriQ) and an external comment (relC), the question which relC commented on is also given (relQ). To incorporate this extra information, we consider a multitask learning framework which jointly learns to predict the relationships of the three pairs (oriQ/relQ, oriQ/relC, relQ/relC)..Figure 3 shows our framework: the three lower models are separate serialized LSTM-encoders for the three respective object pairs, whereas the upper model is an FNN that takes as input the concatenation of the outputs of three encoders, and predicts the relationships for all three pairs. More specifically, the output layer consists of three softmax layers where each one is intended to predict the relationship of one particular pair..For the overall loss function, we combine three separate loss functions using a heuristic weight vector $\\beta $ that allocates a higher weight to the main task (oriQ-relC relationship prediction) as follows: .$$\\mathcal {L} = \\beta _1 \\mathcal {L}_1 + \\beta _2 \\mathcal {L}_2 + \\beta _3 \\mathcal {L}_3$$   (Eq. 11) .By doing so, we hypothesize that the related tasks can improve the main task by leveraging commonality among all tasks. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "14\n",
      "Question 1: What is the reason behind the increase in F1 score when using a multitask learning framework for all three tasks in the mentioned section?\n",
      "\n",
      "Answer 1: The other auxiliary tasks have more balanced labels, which helps improve the shared parameters for the main task (task C), resulting in an increased F1 score of 0.1617.\n",
      "Question : for the text As mentioned in Section \"Modeling Question-External Comments\" , we also explored a multitask learning framework that jointly learns to predict the relationships of all three tasks. We set $0.8$ for the main task (task C) and $0.1$ for the other auxiliary tasks. The MAP score did not improve, but F1 increases to $0.1617$ . We believe this is because other tasks have more balanced labels, which improves the shared parameters for task C. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "15\n",
      "Question 1: What is the problem with the traditional RNN encoder-decoder approach?\n",
      "\n",
      "Answer 1: The problem with the traditional RNN encoder-decoder approach is that it compresses all necessary information into a single fixed-length vector, which can be problematic.\n",
      "Question : for the text A traditional RNN encoder-decoder approach BIBREF11 first encodes an arbitrary length input sequence into a fixed-length dense vector that can be used as input to subsequent classification models, or to initialize the hidden state of a secondary decoder. However, the requirement to compress all necessary information into a single fixed length vector can be problematic. A neural attention model BIBREF12 BIBREF13 has been recently proposed to alleviate this issue by enabling the network to attend to past outputs when decoding. Thus, the encoder no longer needs to represent an entire sequence with one vector; instead, it encodes information into a sequence of vectors, and adaptively chooses a subset of the vectors when decoding. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "16\n",
      "Question 1: Can you give an example of noisy content in cQA forums?\n",
      "\n",
      "Answer 1: Yes, Figure 5 in the text is an example of noisy content with excessive usage of question marks. However, our model is robust enough to allocate low weights to such noise symbols and exclude noninformative content.\n",
      "Question : for the text Due to the open nature of cQA forums, some content is noisy. Figure 5 is an example with excessive usage of question marks. Again, our model exhibits its robustness by allocating very low weights to the noise symbols and therefore excludes the noninformative content. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "17\n",
      "Question 1: What is the attention mechanism used in the encoder of the model for cQA tasks?\n",
      "\n",
      "Answer 1: The attention mechanism used in the encoder of the model for cQA tasks allows the second LSTM to attend to the sequence of output vectors from the first LSTM, and hence generate a weighted representation of the first object according to both objects. The weight is computed using an importance model that produces a higher score for $(h_i, h_N)$ if $h_i$ is useful to determine the object pair's relationship. The final input to the classifier includes $h_N$, $h^{\\prime}$, as well as augmented features.\n",
      "Question : for the text In our cQA tasks, the pair of objects are (question, question) or (question, comment), and the relationship is relevant/irrelevant. The left side of Figure 1 shows one intuitive way to predict relationships using RNNs. Parallel LSTMs encode two objects independently, and then concatenate their outputs as an input to a feed-forward neural network (FNN) with a softmax output layer for classification..The representations of the two objects are generated independently in this manner. However, we are more interested in the relationship instead of the object representations themselves. Therefore, we consider a serialized LSTM-encoder model in the right side of Figure 1 that is similar to that in BIBREF2 , but also allows an augmented feature input to the FNN classifier..Figure 2 illustrates our attention framework in more detail. The first LSTM reads one object, and passes information through hidden units to the second LSTM. The second LSTM then reads the other object and generates the representation of this pair after the entire sequence is processed. We build another FNN that takes this representation as input to classify the relationship of this pair..By adding an attention mechanism to the encoder, we allow the second LSTM to attend to the sequence of output vectors from the first LSTM, and hence generate a weighted representation of first object according to both objects. Let $h_N$ be the last output of second LSTM and $M = [h_1, h_2, \\cdots , h_L]$ be the sequence of output vectors of the first object. The weighted representation of the first object is .$$h^{\\prime } = \\sum _{i=1}^{L} \\alpha _i h_i$$   (Eq. 7) .The weight is computed by .$$\\alpha _i = \\dfrac{exp(a(h_i,h_N))}{\\sum _{j=1}^{L}exp(a(h_j,h_N))}$$   (Eq. 8) .where $a()$ is the importance model that produces a higher score for $(h_i, h_N)$ if $h_i$ is useful to determine the object pair's relationship. We parametrize this model using another FNN. Note that in our framework, we also allow other augmented features (e.g., the ranking score from the IR system) to enhance the classifier. So the final input to the classifier will be $h_N$ , $h^{\\prime }$ , as well as augmented features. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "18\n",
      "Question 1: What were the initial results using the RNN encoder for different tasks?\n",
      "\n",
      "Answer 1: Table 2 showed that the attention model always performed better than the RNN without attention, especially for task C. However, the F1 score for the RNN model was very low, even worse than the random baseline for task B. The limited training pairs for task B and the highly imbalanced data for task C were believed to be the reasons for these results. In the following section, methods for addressing these issues will be investigated.\n",
      "Question : for the text Table 2 shows the initial results using the RNN encoder for different tasks. We observe that the attention model always gets better results than the RNN without attention, especially for task C. However, the RNN model achieves a very low F1 score. For task B, it is even worse than the random baseline. We believe the reason is because for task B, there are only 2,670 pairs for training which is very limited training for a reasonable neural network. For task C, we believe the problem is highly imbalanced data. Since the related comments did not directly comment on the original question, more than $90\\%$ of the comments are labeled as irrelevant to the original question. The low F1 (with high precision and low recall) means our system tends to label most comments as irrelevant. In the following section, we investigate methods to address these issues. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "19\n",
      "Question 1: What approaches have recent studies used in community question answering tasks?\n",
      "Answer 1: Recent studies on community question answering tasks have incorporated word vectors, neural networks, and attention mechanisms into their feature extraction systems. They have also utilized LSTM and CNN architectures, and have focused on jointly learning which words in the question to prioritize for analysis. While earlier work relied heavily on feature engineering and linguistic tools, these recent approaches offer greater flexibility and generalizability across different languages and cQA tasks.\n",
      "Question : for the text Earlier work of community question answering relied heavily on feature engineering, linguistic tools, and external resource. BIBREF3 and BIBREF4 utilized rich non-textual features such as answer's profile. BIBREF5 syntactically analyzed the question and extracted name entity features. BIBREF6 demonstrated a textual entailment system can enhance cQA task by casting question answering to logical entailment..More recent work incorporated word vector into their feature extraction system and based on it designed different distance metric for question and answer BIBREF7 BIBREF8 . While these approaches showed effectiveness, it is difficult to generalize them to common cQA tasks since linguistic tools and external resource may be restrictive in other languages and features are highly customized for each cQA task..Very recent work on answer selection also involved the use of neural networks. BIBREF9 used LSTM to construct a joint vector based on both the question and the answer and then converted it into a learning to rank problem. BIBREF10 proposed several convolutional neural network (CNN) architectures for cQA. Our method differs in that RNN encoder is applied here and by adding attention mechanism we jointly learn which words in question to focus and hence available to conduct qualitative analysis. During classification, we feed the extracted vector into a feed-forward neural network directly instead of using mean/max pooling on top of each time steps. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "20\n",
      "Q1: What datasets did the authors consider for pretraining the neural network?\n",
      "A1: The authors considered the Stanford natural language inference (SNLI) corpus and an in-domain dataset (referred to as Task A) for pretraining the neural network.\n",
      "Question : for the text One way to improve models trained on limited data is to use external data to pretrain the neural network. We therefore considered two different datasets for this task..Cross-domain: The Stanford natural language inference (SNLI) corpus BIBREF17 has a huge amount of cleaned premise and hypothesis pairs. Unfortunately the pairs are for a different task. The relationship between the premise and hypothesis may be similar to the relation between questions and comments, but may also be different..In-domain: since task A seems has reasonable performance, and the network is also well-trained, we could use it directly to initialize task B..To utilize the data, we first trained the model on each auxiliary data (SNLI or Task A) and then removed the softmax layer. After that, we retrain the network using the target data with a softmax layer that was randomly initialized..For task A, the SNLI cannot improve MAP or F1 scores. Actually it slightly hurts the performance. We surmise that it is probably because the domain is different. Further investigation is needed: for example, we could only use the parameter for embedding layers etc. For task B, the SNLI yields a slight improvement on MAP ( $0.2\\%$ ), and Task A could give ( $1.2\\%$ ) on top of that. No improvement was observed on F1. For task C, pretraining by task A is also better than using SNLI (task A is $1\\%$ better than the baseline, while SNLI is almost the same)..In summary, the in-domain pretraining seems better, but overall, the improvement is less than we expected, especially for task B, which only has very limited target data. We will not make a conclusion here since more investigation is needed. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "21\n",
      "Question 1: What is the best place for snorkeling in Qatar?\n",
      "\n",
      "Answer 1: According to the comments, the best place for snorkeling in Qatar is off the coast of Dukhan.\n",
      "Question : for the text Figure 5 illustrates two cQA examples whose questions are relatively short. The comments corresponding to these questions are “...snorkeling two days ago off the coast of dukhan...” and “the doha international airport...”. We can observe that our model successfully learns to focus on the most representative part of the question pertaining to classifying the relationship, which is \"place for snorkeling\" for the first example and “place can ... visited in qatar” for the second example. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "22\n",
      "What is the attentional encoder layer?\n",
      "Answer 1: The attentional encoder layer is a parallelizable and interactive alternative of LSTM used to compute the hidden states of the input embeddings. It contains two submodules, namely Multi-Head Attention (MHA) and Point-wise Convolution Transformation (PCT).\n",
      "Question : for the text The attentional encoder layer is a parallelizable and interactive alternative of LSTM and is applied to compute the hidden states of the input embeddings. This layer consists of two submodules: the Multi-Head Attention (MHA) and the Point-wise Convolution Transformation (PCT)..Multi-Head Attention (MHA) is the attention that can perform multiple attention function in parallel. Different from Transformer BIBREF13 , we use Intra-MHA for introspective context words modeling and Inter-MHA for context-perceptive target words modeling, which is more lightweight and target is modeled according to a given context..An attention function maps a key sequence INLINEFORM0 and a query sequence INLINEFORM1 to an output sequence INLINEFORM2 : DISPLAYFORM0 . where INLINEFORM0 denotes the alignment function which learns the semantic relevance between INLINEFORM1 and INLINEFORM2 : DISPLAYFORM0 . where INLINEFORM0 are learnable weights..MHA can learn n_head different scores in parallel child spaces and is very powerful for alignments. The INLINEFORM0 outputs are concatenated and projected to the specified hidden dimension INLINEFORM1 , namely, DISPLAYFORM0 . where “ INLINEFORM0 ” denotes vector concatenation, INLINEFORM1 , INLINEFORM2 is the output of the INLINEFORM3 -th head attention and INLINEFORM4 ..Intra-MHA, or multi-head self-attention, is a special situation for typical attention mechanism that INLINEFORM0 . Given a context embedding INLINEFORM1 , we can get the introspective context representation INLINEFORM2 by: DISPLAYFORM0 . The learned context representation INLINEFORM0 is aware of long-term dependencies..Inter-MHA is the generally used form of attention mechanism that INLINEFORM0 is different from INLINEFORM1 . Given a context embedding INLINEFORM2 and a target embedding INLINEFORM3 , we can get the context-perceptive target representation INLINEFORM4 by: DISPLAYFORM0 .After this interactive procedure, each given target word INLINEFORM0 will have a composed representation selected from context embeddings INLINEFORM1 . Then we get the context-perceptive target words modeling INLINEFORM2 ..A Point-wise Convolution T ransformation (PCT) can transform contextual information gathered by the MHA. Point-wise means that the kernel sizes are 1 and the same transformation is applied to every single token belonging to the input. Formally, given a input sequence INLINEFORM0 , PCT is defined as: DISPLAYFORM0 . where INLINEFORM0 stands for the ELU activation, INLINEFORM1 is the convolution operator, INLINEFORM2 and INLINEFORM3 are the learnable weights of the two convolutional kernels, INLINEFORM4 and INLINEFORM5 are biases of the two convolutional kernels..Given INLINEFORM0 and INLINEFORM1 , PCTs are applied to get the output hidden states of the attentional encoder layer INLINEFORM2 and INLINEFORM3 by: DISPLAYFORM0  generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "23\n",
      "Question 1: What technique was employed in the proposed model for the targeted sentiment classification task?\n",
      "\n",
      "Answer 1: The proposed model employed attention based encoders for the modeling between context and target for the targeted sentiment classification task.\n",
      "Question : for the text In this work, we propose an attentional encoder network for the targeted sentiment classification task. which employs attention based encoders for the modeling between context and target. We raise the the label unreliability issue add a label smoothing regularization to encourage the model to be less confident with fuzzy labels. We also apply pre-trained BERT to this task and obtain new state-of-the-art results. Experiments and analysis demonstrate the effectiveness and lightweight of the proposed model. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "24\n",
      "Question 1: What are the two datasets used for the experiments in the study?\n",
      "\n",
      "Answer 1: The two datasets used for the experiments in the study are the SemEval 2014 Task 4 BIBREF15 dataset composed of Restaurant reviews and Laptop reviews, and the ACL 14 Twitter dataset gathered by Dong et al. dong2014adaptive. Both datasets are labeled with three sentiment polarities: positive, neutral, and negative.\n",
      "Question : for the text We conduct experiments on three datasets: SemEval 2014 Task 4 BIBREF15 dataset composed of Restaurant reviews and Laptop reviews, and ACL 14 Twitter dataset gathered by Dong et al. dong2014adaptive. These datasets are labeled with three sentiment polarities: positive, neutral and negative. Table TABREF31 shows the number of training and test instances in each category..Word embeddings in AEN-GloVe do not get updated in the learning process, but we fine-tune pre-trained BERT in AEN-BERT. Embedding dimension INLINEFORM0 is 300 for GloVe and is 768 for pre-trained BERT. Dimension of hidden states INLINEFORM1 is set to 300. The weights of our model are initialized with Glorot initialization BIBREF16 . During training, we set label smoothing parameter INLINEFORM2 to 0.2 BIBREF14 , the coefficient INLINEFORM3 of INLINEFORM4 regularization item is INLINEFORM5 and dropout rate is 0.1. Adam optimizer BIBREF17 is applied to update all the parameters. We adopt the Accuracy and Macro-F1 metrics to evaluate the performance of the model. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "25\n",
      "Question 1: What is INLINEFORM0 in the text?\n",
      "\n",
      "Answer 1: INLINEFORM0 refers to the pre-trained GloVe BIBREF12 embedding matrix used to map each word INLINEFORM3 to its corresponding embedding vector INLINEFORM4 in the given text.\n",
      "Question : for the text Let INLINEFORM0 to be the pre-trained GloVe BIBREF12 embedding matrix, where INLINEFORM1 is the dimension of word vectors and INLINEFORM2 is the vocabulary size. Then we map each word INLINEFORM3 to its corresponding embedding vector INLINEFORM4 , which is a column in the embedding matrix INLINEFORM5 ..BERT embedding uses the pre-trained BERT to generate word vectors of sequence. In order to facilitate the training and fine-tuning of BERT model, we transform the given context and target to “[CLS] + context + [SEP]” and “[CLS] + target + [SEP]” respectively. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "26\n",
      "What is targeted sentiment classification and what is its objective?\n",
      "Answer 1: Targeted sentiment classification is a task that aims to determine the sentiment polarities of a sentence for certain “opinion targets” that appear in the sentence. The objective is to classify the sentiment as either negative, neutral, or positive for each target.\n",
      "Question : for the text Targeted sentiment classification is a fine-grained sentiment analysis task, which aims at determining the sentiment polarities (e.g., negative, neutral, or positive) of a sentence over “opinion targets” that explicitly appear in the sentence. For example, given a sentence “I hated their service, but their food was great”, the sentiment polarities for the target “service” and “food” are negative and positive respectively. A target is usually an entity or an entity aspect..In recent years, neural network models are designed to automatically learn useful low-dimensional representations from targets and contexts and obtain promising results BIBREF0 , BIBREF1 . However, these neural network models are still in infancy to deal with the fine-grained targeted sentiment classification task..Attention mechanism, which has been successfully used in machine translation BIBREF2 , is incorporated to enforce the model to pay more attention to context words with closer semantic relations with the target. There are already some studies use attention to generate target-specific sentence representations BIBREF3 , BIBREF4 , BIBREF5 or to transform sentence representations according to target words BIBREF6 . However, these studies depend on complex recurrent neural networks (RNNs) as sequence encoder to compute hidden semantics of texts..The first problem with previous works is that the modeling of text relies on RNNs. RNNs, such as LSTM, are very expressive, but they are hard to parallelize and backpropagation through time (BPTT) requires large amounts of memory and computation. Moreover, essentially every training algorithm of RNN is the truncated BPTT, which affects the model's ability to capture dependencies over longer time scales BIBREF7 . Although LSTM can alleviate the vanishing gradient problem to a certain extent and thus maintain long distance information, this usually requires a large amount of training data. Another problem that previous studies ignore is the label unreliability issue, since neutral sentiment is a fuzzy sentimental state and brings difficulty for model learning. As far as we know, we are the first to raise the label unreliability issue in the targeted sentiment classification task..This paper propose an attention based model to solve the problems above. Specifically, our model eschews recurrence and employs attention as a competitive alternative to draw the introspective and interactive semantics between target and context words. To deal with the label unreliability issue, we employ a label smoothing regularization to encourage the model to be less confident with fuzzy labels. We also apply pre-trained BERT BIBREF8 to this task and show our model enhances the performance of basic BERT model. Experimental results on three benchmark datasets show that the proposed model achieves competitive performance and is a lightweight alternative of the best RNN based models..The main contributions of this work are presented as follows: generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "27\n",
      "Question 1: What is the advantage of using pre-trained BERT on small-data tasks?\n",
      "\n",
      "Answer 1: Using pre-trained BERT on small-data tasks such as sentiment analysis can result in substantial accuracy improvements, as shown in the comparison between BERT-SPC and AEN-BERT in Table TABREF34. However, further fine-tuning on the specific task is necessary to release the true power of BERT, as the pre-trained knowledge is not domain-specific.\n",
      "Question : for the text Table TABREF34 shows the performance comparison of AEN with other models. BERT-SPC and AEN-BERT obtain substantial accuracy improvements, which shows the power of pre-trained BERT on small-data task. The overall performance of AEN-BERT is better than BERT-SPC, which suggests that it is important to design a downstream network customized to a specific task. As the prior knowledge in the pre-trained BERT is not specific to any particular domain, further fine-tuning on the specific task is necessary for releasing the true power of BERT..The overall performance of TD-LSTM is not good since it only makes a rough treatment of the target words. ATAE-LSTM, IAN and RAM are attention based models, they stably exceed the TD-LSTM method on Restaurant and Laptop datasets. RAM is better than other RNN based models, but it does not perform well on Twitter dataset, which might because bidirectional LSTM is not good at modeling small and ungrammatical text..Feature-based SVM is still a competitive baseline, but relying on manually-designed features. Rec-NN gets the worst performances among all neural network baselines as dependency parsing is not guaranteed to work well on ungrammatical short texts such as tweets and comments. Like AEN, MemNet also eschews recurrence, but its overall performance is not good since it does not model the hidden semantic of embeddings, and the result of the last attention is essentially a linear combination of word embeddings. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "28\n",
      "What is the difference in performance between AEN-GloVe and AEN-GloVe w/o LSR? \n",
      "\n",
      "The accuracy of AEN-GloVe w/o LSR drops significantly on all three datasets compared to AEN-GloVe, which shows that LSR is important for good performance. This could be attributed to the unreliability of training samples with neutral sentiment.\n",
      "Question : for the text As shown in Table TABREF34 , the performances of AEN-GloVe ablations are incomparable with AEN-GloVe in both accuracy and macro-F1 measure. This result shows that all of these discarded components are crucial for a good performance. Comparing the results of AEN-GloVe and AEN-GloVe w/o LSR, we observe that the accuracy of AEN-GloVe w/o LSR drops significantly on all three datasets. We could attribute this phenomenon to the unreliability of the training samples with neutral sentiment. The overall performance of AEN-GloVe and AEN-GloVe-BiLSTM is relatively close, AEN-GloVe performs better on the Restaurant dataset. More importantly, AEN-GloVe has fewer parameters and is easier to parallelize..To figure out whether the proposed AEN-GloVe is a lightweight alternative of recurrent models, we study the model size of each model on the Restaurant dataset. Statistical results are reported in Table TABREF37 . We implement all the compared models base on the same source code infrastructure, use the same hyperparameters, and run them on the same GPU ..RNN-based and BERT-based models indeed have larger model size. ATAE-LSTM, IAN, RAM, and AEN-GloVe-BiLSTM are all attention based RNN models, memory optimization for these models will be more difficult as the encoded hidden states must be kept simultaneously in memory in order to perform attention mechanisms. MemNet has the lowest model size as it only has one shared attention layer and two linear layers, it does not calculate hidden states of word embeddings. AEN-GloVe's lightweight level ranks second, since it takes some more parameters than MemNet in modeling hidden states of sequences. As a comparison, the model size of AEN-GloVe-BiLSTM is more than twice that of AEN-GloVe, but does not bring any performance improvements. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "29\n",
      "What are the seven baseline models used to evaluate the performance of AEN-GloVe?\n",
      "Answer 1: The seven baseline models used to evaluate the performance of AEN-GloVe are Feature-based SVM, Rec-NN, MemNet, TD-LSTM, ATAE-LSTM, IAN, and RAM.\n",
      "Question : for the text In order to comprehensively evaluate and analysis the performance of AEN-GloVe, we list 7 baseline models and design 4 ablations of AEN-GloVe. We also design a basic BERT-based model to evaluate the performance of AEN-BERT.. .Non-RNN based baselines:. INLINEFORM0 Feature-based SVM BIBREF18 is a traditional support vector machine based model with extensive feature engineering.. INLINEFORM0 Rec-NN BIBREF0 firstly uses rules to transform the dependency tree and put the opinion target at the root, and then learns the sentence representation toward target via semantic composition using Recursive NNs.. INLINEFORM0 MemNet BIBREF19 uses multi-hops of attention layers on the context word embeddings for sentence representation to explicitly captures the importance of each context word.. .RNN based baselines:. INLINEFORM0 TD-LSTM BIBREF1 extends LSTM by using two LSTM networks to model the left context with target and the right context with target respectively. The left and right target-dependent representations are concatenated for predicting the sentiment polarity of the target.. INLINEFORM0 ATAE-LSTM BIBREF3 strengthens the effect of target embeddings, which appends the target embeddings with each word embeddings and use LSTM with attention to get the final representation for classification.. INLINEFORM0 IAN BIBREF4 learns the representations of the target and context with two LSTMs and attentions interactively, which generates the representations for targets and contexts with respect to each other.. INLINEFORM0 RAM BIBREF5 strengthens MemNet by representing memory with bidirectional LSTM and using a gated recurrent unit network to combine the multiple attention outputs for sentence representation.. .AEN-GloVe ablations:. INLINEFORM0 AEN-GloVe w/o PCT ablates PCT module.. INLINEFORM0 AEN-GloVe w/o MHA ablates MHA module.. INLINEFORM0 AEN-GloVe w/o LSR ablates label smoothing regularization.. INLINEFORM0 AEN-GloVe-BiLSTM replaces the attentional encoder layer with two bidirectional LSTM.. .Basic BERT-based model:. INLINEFORM0 BERT-SPC feeds sequence “[CLS] + context + [SEP] + target + [SEP]” into the basic BERT model for sentence pair classification task. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "30\n",
      "Question 1: What is done with the previous outputs before projecting them into the space of targeted classes? \n",
      "\n",
      "Answer 1: The previous outputs are averaged pooled and concatenated to form a final comprehensive representation before being projected into the space of the targeted classes using a full connected layer.\n",
      "Question : for the text We get the final representations of the previous outputs by average pooling, concatenate them as the final comprehensive representation INLINEFORM0 , and use a full connected layer to project the concatenated vector into the space of the targeted INLINEFORM1 classes. DISPLAYFORM0 . where INLINEFORM0 is the predicted sentiment polarity distribution, INLINEFORM1 and INLINEFORM2 are learnable parameters. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "31\n",
      "Question 1: What are the two types of embedding used in the AEN model?\n",
      "\n",
      "Answer 1: The AEN model uses two types of embedding: GloVe embedding and BERT embedding. The models are named AEN-GloVe and AEN-BERT accordingly.\n",
      "Question : for the text Given a context sequence INLINEFORM0 and a target sequence INLINEFORM1 , where INLINEFORM2 is a sub-sequence of INLINEFORM3 . The goal of this model is to predict the sentiment polarity of the sentence INLINEFORM4 over the target INLINEFORM5 ..Figure FIGREF9 illustrates the overall architecture of the proposed Attentional Encoder Network (AEN), which mainly consists of an embedding layer, an attentional encoder layer, a target-specific attention layer, and an output layer. Embedding layer has two types: GloVe embedding and BERT embedding. Accordingly, the models are named AEN-GloVe and AEN-BERT. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "32\n",
      "What is the purpose of Label Smoothing Regularization (LSR) in the loss function?\n",
      "\n",
      "LSR is used to penalize low entropy output distributions and reduce overfitting by preventing a network from assigning the full probability to each training example during training. It replaces the 0 and 1 targets for a classifier with smoothed values like 0.1 or 0.9. The objective function to be optimized is the cross-entropy loss with LSR regularization.\n",
      "Question : for the text Since neutral sentiment is a very fuzzy sentimental state, training samples which labeled neutral are unreliable. We employ a Label Smoothing Regularization (LSR) term in the loss function. which penalizes low entropy output distributions BIBREF14 . LSR can reduce overfitting by preventing a network from assigning the full probability to each training example during training, replaces the 0 and 1 targets for a classifier with smoothed values like 0.1 or 0.9..For a training sample INLINEFORM0 with the original ground-truth label distribution INLINEFORM1 , we replace INLINEFORM2 with DISPLAYFORM0 . where INLINEFORM0 is the prior distribution over labels , and INLINEFORM1 is the smoothing parameter. In this paper, we set the prior label distribution to be uniform INLINEFORM2 ..LSR is equivalent to the KL divergence between the prior label distribution INLINEFORM0 and the network's predicted distribution INLINEFORM1 . Formally, LSR term is defined as: DISPLAYFORM0 .The objective function (loss function) to be optimized is the cross-entropy loss with INLINEFORM0 and INLINEFORM1 regularization, which is defined as: DISPLAYFORM0 . where INLINEFORM0 is the ground truth represented as a one-hot vector, INLINEFORM1 is the predicted sentiment distribution vector given by the output layer, INLINEFORM2 is the coefficient for INLINEFORM3 regularization term, and INLINEFORM4 is the parameter set. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "33\n",
      "Question 1: What is the main difference between traditional machine learning methods and neural network methods for targeted sentiment classification? \n",
      "\n",
      "Answer 1: Traditional machine learning methods involve extracting a set of features and relying heavily on manual feature engineering, while neural network methods use low-dimensional word vectors to encode sentences and do not require handcrafted features, allowing for richer semantic information to be utilized.\n",
      "Question : for the text The research approach of the targeted sentiment classification task including traditional machine learning methods and neural networks methods..Traditional machine learning methods, including rule-based methods BIBREF9 and statistic-based methods BIBREF10 , mainly focus on extracting a set of features like sentiment lexicons features and bag-of-words features to train a sentiment classifier BIBREF11 . The performance of these methods highly depends on the effectiveness of the feature engineering works, which are labor intensive..In recent years, neural network methods are getting more and more attention as they do not need handcrafted features and can encode sentences with low-dimensional word vectors where rich semantic information stained. In order to incorporate target words into a model, Tang et al. tang2016effective propose TD-LSTM to extend LSTM by using two single-directional LSTM to model the left context and right context of the target word respectively. Tang et al. tang2016aspect design MemNet which consists of a multi-hop attention mechanism with an external memory to capture the importance of each context word concerning the given target. Multiple attention is paid to the memory represented by word embeddings to build higher semantic information. Wang et al. wang2016attention propose ATAE-LSTM which concatenates target embeddings with word representations and let targets participate in computing attention weights. Chen et al. chen2017recurrent propose RAM which adopts multiple-attention mechanism on the memory built with bidirectional LSTM and nonlinearly combines the attention results with gated recurrent units (GRUs). Ma et al. ma2017interactive propose IAN which learns the representations of the target and context with two attention networks interactively. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "34\n",
      "Question 1: What is the purpose of employing another MHA after obtaining the introspective context representation and context-perceptive target representation?\n",
      "\n",
      "Answer 1: The purpose of employing another MHA is to obtain the target-specific context representation, which is achieved by using the multi-head attention function with independent parameters.\n",
      "Question : for the text After we obtain the introspective context representation INLINEFORM0 and the context-perceptive target representation INLINEFORM1 , we employ another MHA to obtain the target-specific context representation INLINEFORM2 by: DISPLAYFORM0 . The multi-head attention function here also has its independent parameters. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "35\n",
      "Question 1: What organizations supported the research mentioned in the text? \n",
      "\n",
      "Answer 1: The research was supported by the MSIT (Ministry of Science ICT), Korea, under the National Program for Excellence in SW (2015-0-00910) and Artificial Intelligence Contact Center Solution (2018-0-00605) supervised by the IITP (Institute for Information & Communications Technology Planning & Evaluation).\n",
      "Question : for the text This research was supported by the MSIT (Ministry of Science ICT), Korea, under (National Program for Excellence in SW) (2015-0-00910) and (Artificial Intelligence Contact Center Solution) (2018-0-00605) supervised by the IITP(Institute for Information & Communications Technology Planning & Evaluation)  generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "36\n",
      "What is JESSI? \n",
      "\n",
      "JESSI is a system developed for the SemEval 2019 Task 9: Suggestion Mining from Online Reviews and Forums. It is a combination of language and translation models which provide joint encoders for stable suggestion inference.\n",
      "Question : for the text We presented JESSI (Joint Encoders for Stable Suggestion Inference), our system for the SemEval 2019 Task 9: Suggestion Mining from Online Reviews and Forums. JESSI builds upon jointly combined encoders, borrowing pre-trained knowledge from a language model BERT and a translation model CoVe. We found that BERT alone performs bad and unstably when tested on out-of-domain samples. We mitigate the problem by appending an RNN-based sentence encoder above BERT, and jointly combining a CNN-based encoder. Results from the shared task show that JESSI performs competitively among participating models, obtaining second place on Subtask A with an F-Score of 77.78%. It also performs well on Subtask B, with an F-Score of 79.59%, even without using any additional external data. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "37\n",
      "Question 1: What are the model names used for Subtask A and Subtask B in the experiments?\n",
      "Answer 1: The model name used for Subtask A is JESSI-A and for Subtask B is JESSI-B.\n",
      "Question : for the text In this section, we show our results and experiments. We denote JESSI-A as our model for Subtask A (i.e., BERT INLINEFORM0 CNN+CNN INLINEFORM1 Att), and JESSI-B as our model for Subtask B (i.e., BERT INLINEFORM2 BiSRU+CNN INLINEFORM3 Att+DomAdv). The performance of the models is measured and compared using the F1-score. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "38\n",
      "Question 1: What is the main challenge in suggestion mining, according to the organizers of the SemEval 2019 Task 9?\n",
      "\n",
      "Answer 1: According to the organizers, the main challenges in suggestion mining are the sparse occurrences of suggestions, figurative expressions, different domains, and complex sentences.\n",
      "Question : for the text Opinion mining BIBREF0 is a huge field that covers many NLP tasks ranging from sentiment analysis BIBREF1 , aspect extraction BIBREF2 , and opinion summarization BIBREF3 , among others. Despite the vast literature on opinion mining, the task on suggestion mining has given little attention. Suggestion mining BIBREF4 is the task of collecting and categorizing suggestions about a certain product. This is important because while opinions indirectly give hints on how to improve a product (e.g. analyzing reviews), suggestions are direct improvement requests (e.g. tips, advice, recommendations) from people who have used the product..To this end, BIBREF5 organized a shared task specifically on suggestion mining called SemEval 2019 Task 9: Suggestion Mining from Online Reviews and Forums. The shared task is composed of two subtasks, Subtask A and B. In Subtask A, systems are tasked to predict whether a sentence of a certain domain (i.e. electronics) entails a suggestion or not given a training data of the same domain. In Subtask B, systems are tasked to do suggestion prediction of a sentence from another domain (i.e. hotels). Organizers observed four main challenges: (a) sparse occurrences of suggestions; (b) figurative expressions; (c) different domains; and (d) complex sentences. While previous attempts BIBREF6 , BIBREF4 , BIBREF7 made use of human-engineered features to solve this problem, the goal of the shared task is to leverage the advancements seen on neural networks, by providing a larger dataset to be used on data-intensive models to achieve better performance..This paper describes our system JESSI (Joint Encoders for Stable Suggestion Inference). JESSI is built as a combination of two neural-based encoders using multiple pre-trained word embeddings, including BERT BIBREF8 , a pre-trained deep bidirectional transformer that is recently reported to perform exceptionally well across several tasks. The main intuition behind JESSI comes from our finding that although BERT gives exceptional performance gains when applied to in-domain samples, it becomes unstable when applied to out-of-domain samples, even when using a domain adversarial training BIBREF9 module. This problem is mitigated using two tricks: (1) jointly training BERT with a CNN-based encoder, and (2) using an RNN-based encoder on top of BERT before feeding to the classifier..JESSI is trained using only the datasets given on the shared task, without using any additional external data. Despite this, JESSI performs second on Subtask A with an F1 score of 77.78% among 33 other team submissions. It also performs well on Subtask B with an F1 score of 79.59%. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "39\n",
      "Question 1: What are the four important components of JESSI model? \n",
      "Answer 1: The four important components of JESSI model are a BERT-based encoder, a CNN-based encoder, an MLP classifier, and a domain adversarial training module.\n",
      "Question : for the text We present our model JESSI, which stands for Joint Encoders for Stable Suggestion Inference, shown in Figure FIGREF4 . Given a sentence INLINEFORM0 , JESSI returns a binary suggestion label INLINEFORM1 . JESSI consists of four important components: (1) A BERT-based encoder that leverages general knowledge acquired from a large pre-trained language model, (2) A CNN-based encoder that learns task-specific sentence representations, (3) an MLP classifier that predicts the label given the joint encodings, and (4) a domain adversarial training module that prevents the model to distinguish between the two domains. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "40\n",
      "Question 1: Which classic literary work is labeled as \"Romance\"?\n",
      "\n",
      "Answer 1: The classic literary work labeled as \"Romance\" in Table TABREF26 is Jane Austen's \"Pride and Prejudice\" (passage 1).\n",
      "Question : for the text Table TABREF26 shows sample passages from classic titles with corresponding labels. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "41\n",
      "Question 1: What is Emotional Arc and how was it developed? \n",
      "\n",
      "Answer 1: Emotional Arc is a recent advance in understanding emotions in narratives, developed by using lexicons and unsupervised learning methods based on unlabelled passages from titles in Project Gutenberg.\n",
      "Question : for the text Using the categorical basic emotion model BIBREF3, BIBREF4, BIBREF5 studied creating lexicons from tweets for use in emotion analysis. Recently, BIBREF1, BIBREF6 and BIBREF2 proposed shared-tasks for multi-class emotion analysis based on tweets..Fewer works have been reported on understanding emotions in narratives. Emotional Arc BIBREF7 is one recent advance in this direction. The work used lexicons and unsupervised learning methods based on unlabelled passages from titles in Project Gutenberg..For labelled datasets on narratives, BIBREF8 provided a sentence-level annotated corpus of childrens' stories and BIBREF9 provided phrase-level annotations on selected Project Gutenberg titles..To the best of our knowledge, the dataset in this work is the first to provide multi-class emotion labels on passages, selected from both Project Gutenberg and modern narratives. The dataset is available upon request for non-commercial, research only purposes. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "42\n",
      "Question 1: Which algorithm achieved the best performance in the benchmark experiments?\n",
      "\n",
      "Answer 1: Bidirectional Encoder Representations from Transformers (BERT) achieved the best performance with a 0.604 micro-F1 score among all the benchmark experiments.\n",
      "Question : for the text We performed benchmark experiments on the dataset using several different algorithms. In all experiments, we have discarded the data labelled with Surprise and Disgust..We pre-processed the data by using the SpaCy pipeline. We masked out named entities with entity-type specific placeholders to reduce the chance of benchmark models utilizing named entities as a basis for classification..Benchmark results are shown in Table TABREF17. The dataset is approximately balanced after discarding the Surprise and Disgust classes. We report the average micro-F1 scores, with 5-fold cross validation for each technique..We provide a brief overview of each benchmark experiment below. Among all of the benchmarks, Bidirectional Encoder Representations from Transformers (BERT) BIBREF11 achieved the best performance with a 0.604 micro-F1 score..Overall, we observed that deep-learning based techniques performed better than lexical based methods. This suggests that a method which attends to context and themes could do well on the dataset. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "43\n",
      "Question 1: What were the methods used for computing bag-of-words-based benchmarks?\n",
      "Answer 1: The methods used for computing bag-of-words-based benchmarks were classification with TF-IDF + Linear SVM, classification with Depeche++ Emotion lexicons BIBREF12 + Linear SVM, classification with NRC Emotion lexicons BIBREF13, BIBREF14 + Linear SVM and a combination of TF-IDF and NRC Emotion lexicons (TF-NRC + SVM).\n",
      "Question : for the text We computed bag-of-words-based benchmarks using the following methods:.Classification with TF-IDF + Linear SVM (TF-IDF + SVM).Classification with Depeche++ Emotion lexicons BIBREF12 + Linear SVM (Depeche + SVM).Classification with NRC Emotion lexicons BIBREF13, BIBREF14 + Linear SVM (NRC + SVM).Combination of TF-IDF and NRC Emotion lexicons (TF-NRC + SVM) generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "44\n",
      "Question 1: What technique has been adapted to improve interpretability and performance in RNN-based text classification?\n",
      "\n",
      "Answer 1: Self-attention has been adapted to improve interpretability and performance in RNN-based text classification.\n",
      "Question : for the text One challenge with RNN-based solutions for text classification is finding the best way to combine word-level representations into higher-level representations..Self-attention BIBREF19, BIBREF20, BIBREF21 has been adapted to text classification, providing improved interpretability and performance. We used BIBREF20 as the basis of this benchmark..The benchmark used a layered Bi-directional RNN (60 units) with GRU cells and a dense layer. Both self-attention layers were 60 units in size and cross-entropy was used as the cost function..Note that we have omitted the orthogonal regularizer term, since this dataset is relatively small compared to the traditional datasets used for training such a model. We did not observe any significant performance gain while using the regularizer term in our experiments. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "45\n",
      "Question 1: What type of model was used for classification and what was used as features?\n",
      "\n",
      "Answer 1: Simple classification models with learned embeddings were used. The Doc2Vec model BIBREF15 was trained using the dataset, and the embedding document vectors were used as features for a linear SVM classifier.\n",
      "Question : for the text We also used simple classification models with learned embeddings. We trained a Doc2Vec model BIBREF15 using the dataset and used the embedding document vectors as features for a linear SVM classifier. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "46\n",
      "Question 1: What is the benefit of using the ELMo model in NLP tasks?\n",
      "\n",
      "Answer 1: The benefit of using the ELMo model in NLP tasks is that its unsupervised nature allows it to utilize a large amount of available unlabelled data in order to learn better representations of words. This has resulted in recent success in a number of NLP tasks.\n",
      "Question : for the text Deep Contextualized Word Representations (ELMo) BIBREF22 have shown recent success in a number of NLP tasks. The unsupervised nature of the language model allows it to utilize a large amount of available unlabelled data in order to learn better representations of words..We used the pre-trained ELMo model (v2) available on Tensorhub for this benchmark. We fed the word embeddings of ELMo as input into a one layer Bi-directional RNN (16 units) with GRU cells (with dropout) and a dense layer. Cross-entropy was used as the cost function. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "47\n",
      "Question 1: What is the average micro-F1 score achieved by using the fine-tuning procedure on pre-trained uncased BERT$_\\textrm {{\\scriptsize LARGE}}$ for a multi-class passage classification task?\n",
      "\n",
      "Answer 1: The average micro-F1 score achieved by using the fine-tuning procedure on pre-trained uncased BERT$_\\textrm {{\\scriptsize LARGE}}$ for a multi-class passage classification task is 60.4%.\n",
      "Question : for the text Bidirectional Encoder Representations from Transformers (BERT) BIBREF11 has achieved state-of-the-art results on several NLP tasks, including sentence classification..We used the fine-tuning procedure outlined in the original work to adapt the pre-trained uncased BERT$_\\textrm {{\\scriptsize LARGE}}$ to a multi-class passage classification task. This technique achieved the best result among our benchmarks, with an average micro-F1 score of 60.4%. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "48\n",
      "Question 1: What type of neural network was used for the benchmark and how many BiLSTMs were used?\n",
      "\n",
      "Answer 1: A Hierarchical RNN was used for the benchmark and two BiLSTMs with 256 units each were used to model sentences and documents.\n",
      "Question : for the text For this benchmark, we considered a Hierarchical RNN, following BIBREF16. We used two BiLSTMs BIBREF17 with 256 units each to model sentences and documents. The tokens of a sentence were processed independently of other sentence tokens. For each direction in the token-level BiLSTM, the last outputs were concatenated and fed into the sentence-level BiLSTM as inputs..The outputs of the BiLSTM were connected to 2 dense layers with 256 ReLU units and a Softmax layer. We initialized tokens with publicly available embeddings trained with GloVe BIBREF18. Sentence boundaries were provided by SpaCy. Dropout was applied to the dense hidden layers during training. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "49\n",
      "Question 1: What is the potential future direction for this work?\n",
      "Answer 1: The potential future direction for this work includes incorporating common-sense knowledge into emotion analysis and using few-shot learning to improve performance on underrepresented emotions. Additionally, future datasets could focus on the multi-emotion complexities of human language and their contextual interactions.\n",
      "Question : for the text We introduce DENS, a dataset for multi-class emotion analysis from long-form narratives in English. We provide a number of benchmark results based on models ranging from bag-of-word models to methods based on pre-trained language models (ELMo and BERT)..Our benchmark results demonstrate that this dataset provides a novel challenge in emotion analysis. The results also demonstrate that attention-based models could significantly improve performance on classification tasks such as emotion analysis..Interesting future directions for this work include: 1. incorporating common-sense knowledge into emotion analysis to capture semantic context and 2. using few-shot learning to bootstrap and improve performance of underrepresented emotions..Finally, as narrative passages often involve interactions between multiple emotions, one avenue for future datasets could be to focus on the multi-emotion complexities of human language and their contextual interactions. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "50\n",
      "Question 1: What is the purpose of annotating the dataset?\n",
      "Answer 1: The purpose of annotating the dataset is to add additional information, such as labels or tags, to the data in order to make it easier to use and analyze for certain applications or research purposes.\n",
      "Question : for the text In this section, we describe the process used to collect and annotate the dataset. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "1\n",
      "Question 1: What is the average length of a passage in the dataset?\n",
      "\n",
      "Answer 1: The average length of a passage in the dataset is 86 words.\n",
      "Question : for the text The dataset contains a total of 9710 passages, with an average of 6.24 sentences per passage, 16.16 words per sentence, and an average length of 86 words..The vocabulary size is 28K (when lowercased). It contains over 1600 unique titles across multiple categories, including 88 titles (1520 passages) from Project Gutenberg. All of the modern narratives were written after the year 2000, with notable amount of themes in coming-of-age, strong-female-lead, and LGBTQ+. The genre distribution is listed in Table TABREF8..In the final dataset, 21.0% of the data has consensus between all annotators, 73.5% has majority agreement, and 5.48% has labels assigned after consultation with in-house annotators..The distribution of data points over labels with top lexicons (lower-cased, normalized) is shown in Table TABREF9. Note that the Disgust category is very small and should be discarded. Furthermore, we suspect that the data labelled as Surprise may be noisier than other categories and should be discarded as well..Table TABREF10 shows a few examples labelled data from classic titles. More examples can be found in Table TABREF26 in the Appendix SECREF27. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "2\n",
      "What qualifications did annotators need to have to label the emotional content of the passages?\n",
      "\n",
      "Annotators were required to have a \"master\" MTurk qualification to label the emotional content of the passages.\n",
      "Question : for the text MTurk was set up using the standard sentiment template and instructed the crowd annotators to `pick the best/major emotion embodied in the passage'..We further provided instructions to clarify the intensity of an emotion, such as: “Rage/Annoyance is a form of Anger”, “Serenity/Ecstasy is a form of Joy”, and “Love includes Romantic/Family/Friendship”, along with sample passages..We required all annotators have a `master' MTurk qualification. Each passage was labelled by 3 unique annotators. Only passages with a majority agreement between annotators were accepted as valid. This is equivalent to a Fleiss's $\\kappa $ score of greater than $0.4$..For passages without majority agreement between annotators, we consolidated their labels using in-house data annotators who are experts in narrative content. A passage is accepted as valid if the in-house annotator's label matched any one of the MTurk annotators' labels. The remaining passages are discarded. We provide the fraction of annotator agreement for each label in the dataset..Though passages may lose some emotional context when read independently of the complete narrative, we believe annotator agreement on our dataset supports the assertion that small excerpts can still convey coherent emotions..During the annotation process, several annotators had suggested for us to include additional emotions such as confused, pain, and jealousy, which are common to narratives. As they were not part of the original Plutchik’s wheel, we decided to not include them. An interesting future direction is to study the relationship between emotions such as ‘pain versus sadness’ or ‘confused versus surprise’ and improve the emotion model for narratives. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "3\n",
      "Question 1: How were the modern narratives selected for this dataset?\n",
      "\n",
      "Answer 1: The modern narratives were sampled based on popularity from Wattpad.\n",
      "Question : for the text We selected both classic and modern narratives in English for this dataset. The modern narratives were sampled based on popularity from Wattpad. We parsed selected narratives into passages, where a passage is considered to be eligible for annotation if it contained between 40 and 200 tokens..In long-form narratives, many non-conversational passages are intended for transition or scene introduction, and may not carry any emotion. We divided the eligible passages into two parts, and one part was pruned using selected emotion-rich but ambiguous lexicons such as cry, punch, kiss, etc.. Then we mixed this pruned part with the unpruned part for annotation in order to reduce the number of neutral passages. See Appendix SECREF25 for the lexicons used. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "4\n",
      "What emotions are included in the final annotation categories for the dataset?\n",
      "\n",
      "The final annotation categories for the dataset are Joy, Sadness, Anger, Fear, Anticipation, Surprise, Love, Disgust, and Neutral.\n",
      "Question : for the text The dataset is annotated based on a modified Plutchik’s wheel of emotions..The original Plutchik’s wheel consists of 8 primary emotions: Joy, Sadness, Anger, Fear, Anticipation, Surprise, Trust, Disgust. In addition, more complex emotions can be formed by combing two basic emotions. For example, Love is defined as a combination of Joy and Trust (Fig. 1)..The intensity of an emotion is also captured in Plutchik's wheel. For example, the primary emotion of Anger can vary between Annoyance (mild) and Rage (intense)..We conducted an initial survey based on 100 stories with a significant fraction sampled from the romance genre. We asked readers to identify the major emotion exhibited in each story from a choice of the original 8 primary emotions..We found that readers have significant difficulty in identifying Trust as an emotion associated with romantic stories. Hence, we modified our annotation scheme by removing Trust and adding Love. We also added the Neutral category to denote passages that do not exhibit any emotional content..The final annotation categories for the dataset are: Joy, Sadness, Anger, Fear, Anticipation, Surprise, Love, Disgust, Neutral. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "5\n",
      "Question 1: What is the Dataset for Emotions of Narrative Sequences (DENS) used for?\n",
      "\n",
      "Answer 1: The Dataset for Emotions of Narrative Sequences (DENS) is used for emotion analysis in character dialogues and narratives in storytelling. It consists of self-contained passages from long-form fictional narratives from both classic literature and modern stories in English, annotated with one of 9 classes and an indicator for annotator agreement.\n",
      "Question : for the text Humans experience a variety of complex emotions in daily life. These emotions are heavily reflected in our language, in both spoken and written forms..Many recent advances in natural language processing on emotions have focused on product reviews BIBREF0 and tweets BIBREF1, BIBREF2. These datasets are often limited in length (e.g. by the number of words in tweets), purpose (e.g. product reviews), or emotional spectrum (e.g. binary classification)..Character dialogues and narratives in storytelling usually carry strong emotions. A memorable story is often one in which the emotional journey of the characters resonates with the reader. Indeed, emotion is one of the most important aspects of narratives. In order to characterize narrative emotions properly, we must move beyond binary constraints (e.g. good or bad, happy or sad)..In this paper, we introduce the Dataset for Emotions of Narrative Sequences (DENS) for emotion analysis, consisting of passages from long-form fictional narratives from both classic literature and modern stories in English. The data samples consist of self-contained passages that span several sentences and a variety of subjects. Each sample is annotated by using one of 9 classes and an indicator for annotator agreement. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "6\n",
      "Question 1: Who is being thanked for the donation of a Titan X GPU?\n",
      "Answer 1: The NVIDIA Corporation is being thanked for the donation of a Titan X GPU.\n",
      "Question : for the text We thank the NVIDIA Corporation for the donation of a Titan X GPU. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "7\n",
      "Question 1: What was the difference in phone error rate (PER) between the CTC system used in this study and the one used by Graves et al. BIBREF29?\n",
      "\n",
      "Answer 1: Graves et al. BIBREF29 obtained a better phone error rate (PER) of 18.6% compared to the 19.9% achieved by the CTC system used in this study. This difference may be due to the use of prefix decoding with beam search in the former system.\n",
      "Question : for the text Table 1 shows the baseline results of SRNN and CTC models using two different kinds of features. The FBANK features are 120-dimensional with delta and delta-delta coefficients, and the fMLLR features are 40-dimensional, which were obtained from a Kaldi baseline system. We used a 3-layer bidirectional LSTMs for feature extraction, and we used the greedy best path decoding algorithm for both models. Our SRNN and CTC achieved comparable phone error rate (PER) for both kinds of features. However, for the CTC system, Graves et al. BIBREF29 obtained a better result, using about the same size of neural network (3 hidden layers with 250 hidden units of bidirectional LSTMs), compared to ours (18.6% vs. 19.9%). Apart from the implementation difference of using different code bases, Graves et al. BIBREF29 applied the prefix decoding with beam search, which may have lower search error than our best path decoding algorithm. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "8\n",
      "What approach did the authors investigate for speech recognition in their paper?\n",
      "\n",
      "The authors investigated multitask learning with CTC and SCRF for speech recognition in their paper.\n",
      "Question : for the text We investigated multitask learning with CTC and SCRF for speech recognition in this paper. Using an RNN encoder for feature extraction, both CTC and SCRF can be trained end-to-end, and the two models can be trained together by interpolating the two loss functions. From experiments on the TIMIT dataset, the multitask learning approach improved the recognition accuracies of both CTC and SCRF acoustic models. We also showed that CTC can be used to pretrain the RNN encoder, speeding up the training of the joint model. In the future, we will study the multitask learning approach for larger-scale speech recognition tasks, where the CTC pretraining approach may be more helpful to overcome the problem of high computational cost. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "9\n",
      "What is the key difference between CTC and SCRF?\n",
      "\n",
      "CTC normalizes the probabilistic distribution at the frame level, whereas SCRF does not.\n",
      "Question : for the text CTC also directly computes the conditional probability $P(y \\mid X)$ , with the key difference from SCRF in that it normalizes the probabilistic distribution at the frame level. To address the problem of length mismatch between the input and output sequences, CTC allows repetitions of output labels and introduces a special blank token ( $-$ ), which represents the probability of not emitting any label at a particular time step. The conditional probability is then obtained by summing over all the probabilities of all the paths that corresponding to $y$ after merging the repeated labels and removing the blank tokens, i.e., P(y X) = (y) P(X), where $\\Psi (y)$ denotes the set of all possible paths that correspond to $y$ after repetitions of labels and insertions of the blank token. Now the length of $\\pi $ is the same as $X$ , the probability $P(\\pi \\mid X)$ is then approximated by the independence assumption as P(X) t=1T P(t xt), where $\\pi _t $ ranges over $\\mathcal {Y}\\cup \\lbrace -\\rbrace $ , and $-$0 can be computed using the softmax function. The training criterion for CTC is to maximize the conditional probability of the ground truth labels, which is equivalent to minimizing the negative log likelihood: Lctc = -P(y X), which can be reformulated as the CE criterion. More details regarding the computation of the loss and the backpropagation algorithm to train CTC models can be found in BIBREF23 . generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "10\n",
      "Question 1: What toolkit was used to implement the SRNN and CTC models in the experiments?\n",
      "\n",
      "Answer 1: The DyNet toolkit was used to implement the SRNN and CTC models in the experiments. (BIBREF25)\n",
      "Question : for the text Our experiments were performed on the TIMIT database, and both the SRNN and CTC models were implemented using the DyNet toolkit BIBREF25 . We followed the standard protocol of the TIMIT dataset, and our experiments were based on the Kaldi recipe BIBREF26 . We used the core test set as our evaluation set, which has 192 utterances. Our models were trained with 48 phonemes, and their predictions were converted to 39 phonemes before scoring. The dimension of $\\mathbf {u}_j$ was fixed to be 64, and the dimension of $\\mathbf {w}$ in Eq. ( 2 ) is also 64. We set the initial SGD learning rate to be 0.1, and we exponentially decay the learning rate by 0.75 when the validation error stopped decreasing. We also subsampled the acoustic sequence by a factor of 4 using the hierarchical RNN as in BIBREF12 . Our models were trained with dropout regularization BIBREF27 , using a specific implementation for recurrent networks BIBREF28 . The dropout rate was 0.2 unless specified otherwise. Our models were randomly initialized with the same random seed. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "11\n",
      "Question 1: What is the drawback of the previous implementation of the SRNN model?\n",
      "\n",
      "Answer 1: The drawback of the previous implementation of the SRNN model is the large memory cost, as it requires storing the array of hidden states for all the possible segments, resulting in a memory cost of O(T^2H), where T is the length of the input sequence and H is the dimension of an RNN hidden state.\n",
      "Question : for the text SRNN uses an RNN to learn segmental level acoustic embeddings. Given the input sequence ${X} = ({x}_1, \\cdots , {x}_T)$ , and we need to compute the embedding vector $\\bar{x}_j$ in Eq. ( 2 ) corresponding to the segment ${e}_j = \\langle s_j, n_j\\rangle $ . Since the segment boundaries are known, it is straightforward to employ an RNN to map the segment into a vector as [ l hsj.hsj+1. $\\vdots $ .hnj ] = [ l RNN(h0, xsj).RNN(hsj, xsj+1). $\\vdots $ .RNN(hnj-1, xnj) ] where ${h}_0$ denotes the initial hidden state, which is initialized to be zero. RNN( $\\cdot $ ) denotes the nonlinear recurrence operation used in an RNN, which takes the previous hidden state and the feature vector at the current timestep as inputs, and produce an updated hidden state vector. Given the recurrent hidden states, the embedding vector can be simply defined as $\\bar{x}_j= {h}_{n_j}$ as in our previous work BIBREF12 . However, the drawback of this implementation is the large memory cost, as we need to store the array of hidden states $({h}_{s_j}, \\cdots , {h}_{n_j})$ for all the possible segments $\\langle s_j, n_j\\rangle $ . If we denote $H$ as the dimension of an RNN hidden state, the memory cost will be on the order of $O(T^2H)$ , where $T$ is the length of $X$ . It is especially problematic for the joint model as the CTC model requires additional memory space. In this work, we adopt another approach that requires much less memory. In this approach, we use an RNN to read the whole input sequence as [ c h1.h2. $\\vdots $ .hT ] = [ l RNN(h0, x1).RNN(h1, x2). $\\vdots $ .RNN(hT-1, xT) ] and we define the embedding vector for segment ${e} = \\langle k, t\\rangle $ as xj = [ c hsj.hnj ] In this case, we only provide the context information for the feature function $\\Phi (\\cdot )$ to extract segmental features. We refer this approach as context-aware embedding. Since we only need to read the input sequence once, the memory requirement is on the order of $O(TH)$ , which is much smaller. The cost, however, is the slightly degradation of the recognition accuracy. This model is illustrated by Figure 1 ..The feature function $\\Phi (\\cdot )$ also requires a vector representation of the label $y_j$ . This embedding vector can be obtained using a linear embedding matrix, following common practice for RNN language models. More specifically, $y_j$ is first represented as a one-hot vector ${v}_j$ , and it is then mapped into a continuous space by a linear embedding matrix ${M}$ as .$${u}_j = {M v}_j$$   (Eq. 4) .Given the acoustic embedding $\\bar{x}_j$ and label embedding $u_j$ , the feature function $\\Phi (\\cdot )$ can be represented as (yj, ej, xj) = (W1uj + W2xj + b), where $\\sigma $ denotes a non-linear activation function (e.g., sigmoid or tanh); $W_1, W_2$ and $b$ are weight matrices and a bias vector. Eq. ( \"Connectionist Temporal Classification \" ) corresponds to one layer of non-linear transformation. In fact, it is straightforward to stack multiple nonlinear layers in this feature function. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "12\n",
      "What are the three architectures proposed for end-to-end learning in speech recognition?\n",
      "\n",
      "The three architectures proposed for end-to-end learning in speech recognition are connectionist temporal classification (CTC), sequence-to-sequence with attention model, and neural network segmental conditional random field (SCRF).\n",
      "Question : for the text State-of-the-art speech recognition accuracy has significantly improved over the past few years since the application of deep neural networks BIBREF0 , BIBREF1 . Recently, it has been shown that with the application of both neural network acoustic model and language model, an automatic speech recognizer can approach human-level accuracy on the Switchboard conversational speech recognition benchmark using around 2,000 hours of transcribed data BIBREF2 . While progress is mainly driven by well engineered neural network architectures and a large amount of training data, the hidden Markov model (HMM) that has been the backbone for speech recognition for decades is still playing a central role. Though tremendously successful for the problem of speech recognition, the HMM-based pipeline factorizes the whole system into several components, and building these components separately may be less computationally efficient when developing a large-scale system from thousands to hundred of thousands of examples BIBREF3 ..Recently, along with hybrid HMM/NN frameworks for speech recognition, there has been increasing interest in end-to-end training approaches. The key idea is to directly map the input acoustic frames to output characters or words without the intermediate alignment to context-dependent phones used by HMMs. In particular, three architectures have been proposed for the goal of end-to-end learning: connectionist temporal classification (CTC) BIBREF4 , BIBREF5 , BIBREF6 , BIBREF7 , sequence-to-sequence with attention model BIBREF8 , BIBREF9 , BIBREF10 , and neural network segmental conditional random field (SCRF) BIBREF11 , BIBREF12 . These end-to-end models simplify the pipeline of speech recognition significantly. They do not require intermediate alignment or segmentation like HMMs, instead, the alignment or segmentation is marginalized out during training for CTC and SCRF or inferred by the attention mechanism. In terms of the recognition accuracy, however, the end-to-end models usually lag behind their HMM-based counterparts. Though CTC has been shown to outperform HMM systems BIBREF13 , the improvement is based on the use of context-dependent phone targets and a very large amount of training data. Therefore, it has almost the same system complexity as HMM acoustic models. When the training data is less abundant, it has been shown that the accuracy of CTC systems degrades significantly BIBREF14 ..However, end-to-end models have the flexibility to be combined to mitigate their individual weaknesses. For instance, multitask learning with attention models has been investigated for machine translation BIBREF15 , and Mandarin speech recognition using joint Character-Pinyin training BIBREF16 . In BIBREF17 , Kim et al. proposed a multitask learning approach to train a joint attention model and a CTC model using a shared encoder. They showed that the CTC auxiliary task can help the attention model to overcome the misalignment problem in the initial few epochs, and speed up the convergence of the attention model. Another nice property of the multitask learning approach is that the joint model can still be trained end-to-end. Inspired by this work, we study end-to-end training of a joint CTC and SCRF model using an interpolated loss function. The key difference of our study from BIBREF17 is that the two loss functions of the CTC and attention models are locally normalized for each output token, and they are both trained using the cross entropy criterion. However, the SCRF loss function is normalized at the sequence-level, which is similar to the sequence discriminative training objective function for HMMs. From this perspective, the interpolation of CTC and SCRF loss functions is analogous to the sequence discriminative training of HMMs with CE regularization to overcome overfitting, where a sequence-level loss is also interpolated with a frame-level loss, e.g., BIBREF18 . Similar to the observations in BIBREF17 , we demonstrate that the joint training approach improves the recognition accuracies of both CTC and SCRF acoustic models. Further, we also show that CTC can be used to pretrain the neural network feature extractor to speed up the convergence of the joint model. Experiments were performed on the TIMIT database. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "13\n",
      "Question 1: How do we train the two models jointly in this work?\n",
      "\n",
      "Answer 1: We can simply interpolate the CTC and SCRF loss functions as L = Lctc + (1-λ)Lscrf with λ being the interpolation weight. The two models share the same neural network for feature extraction, which is an RNN with LSTM units. Other types of neural architecture may be considered in future work.\n",
      "Question : for the text Training the two models jointly is trivial. We can simply interpolate the CTC and SCRF loss functions as L = Lctc + (1-)Lscrf, where $\\lambda \\in [0, 1]$ is the interpolation weight. The two models share the same neural network for feature extraction. In this work, we focus on the RNN with long short-term memory (LSTM) BIBREF24 units for feature extraction. Other types of neural architecture, e.g., convolutional neural network (CNN) or combinations of CNN and RNN, may be considered in future work. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "14\n",
      "Q: Why is it difficult to train a speech recognition model directly by maximizing the conditional probability when segmentation labels are unknown in the training set?\n",
      "\n",
      "A: It is difficult to train a speech recognition model directly by maximizing the conditional probability when segmentation labels are unknown in the training set because we cannot train the model directly by maximizing the conditional probability. This is because we need to marginalize out the segmentation variable, which is impractical to compute due to the exponential number of possible segmentations.\n",
      "Question : for the text For speech recognition, the segmentation labels ${E}$ are usually unknown in the training set. In this case, we cannot train the model directly by maximizing the conditional probability in Eq. ( \"Segmental Conditional Random Fields\" ). However, the problem can be addressed by marginalizing out the segmentation variable as Lscrf = - P(y X).= - E P(y, E X).= - E j f ( yj, ej, xj ) Z(X, y) + Z(X), where $Z({X}, {y})$ denotes the summation over all the possible segmentations when only ${y}$ is observed. To simplify notation, the objective function $\\mathcal {L}_{\\mathit {scrf}}$ is defined here with only one training utterance..However, the number of possible segmentations is exponential in the length of ${X}$ , which makes the naïve computation of both $Z({X}, {y})$ and $Z({X})$ impractical. To address this problem, a dynamic programming algorithm can be applied, which can reduce the computational complexity to $O(T^2\\cdot |\\mathcal {Y}|)$ BIBREF22 . The computational cost can be further reduced by limiting the maximum length of all the possible segments. The reader is referred to BIBREF12 for further details including the decoding algorithm. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "15\n",
      "What is the reason for the improvement in recognition accuracy seen with multitask learning for SRNN and CTC acoustic models?\n",
      "The improvement in recognition accuracies of both SRNN and CTC acoustic models with multitask learning may be due to the regularization effect of the joint training loss.\n",
      "Question : for the text Table 2 shows results of multitask learning for CTC and SRNN using the interpolated loss in Eq. ( \"Joint Training Loss\" ). We only show results of using LSTMs with 250 dimensional hidden states. The interpolation weight was set to be 0.5. In our experiments, tuning the interpolation weight did not further improve the recognition accuracy. From Table 2 , we can see that multitask learning improves recognition accuracies of both SRNN and CTC acoustic models, which may due to the regularization effect of the joint training loss. The improvement for FBANK features is much larger than fMLLR features. In particular, with multitask learning, the recognition accuracy of our CTC system with best path decoding is comparable to the results obtained by Graves et al. BIBREF29 with beam search decoding..One of the major drawbacks of SCRF models is their high computational cost. In our experiments, the CTC model is around 3–4 times faster than the SRNN model that uses the same RNN encoder. The joint model by multitask learning is slightly more expensive than the stand-alone SRNN model. To cut down the computational cost, we investigated if CTC can be used to pretrain the RNN encoder to speed up the training of the joint model. This is analogous to sequence training of HMM acoustic models, where the network is usually pretrained by the frame-level CE criterion. Figure 2 shows the convergence curves of the joint model with and without CTC pretraining, and we see pretraining indeed improves the convergence speed of the joint model. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "16\n",
      "What is SCRF and how is it used in speech recognition?\n",
      "SCRF is a variant of the linear-chain CRF model where each output token corresponds to a segment of input tokens instead of a single input instance. In speech recognition, SCRF defines the sequence-level conditional probability with the auxiliary segment labels ${E} = ({e}_1, \\cdots , {e}_J) $ and uses an end-to-end training approach for SCRFs, where $\\Phi (\\cdot )$ is defined with neural networks and the segmental level features are learned by RNNs, which is referred to as the segmental RNN (SRNN).\n",
      "Question : for the text SCRF is a variant of the linear-chain CRF model where each output token corresponds to a segment of input tokens instead of a single input instance. In the context of speech recognition, given a sequence of input vectors of $T$ frames ${X} = ( {x}_1, \\cdots , {x}_T )$ and its corresponding sequence of output labels ${y} = ( y_1, \\cdots , y_J)$ , the zero-order linear-chain CRF defines the sequence-level conditional probability as P(y X) = 1Z(X) t=1T f ( yt, xt ), where $Z({X})$ denotes the normalization term, and $T=J$ . Extension to higher order models is straightforward, but it is usually computationally much more expensive. The model defined in Eq. ( \"Segmental Conditional Random Fields\" ) requires the length of ${X}$ and ${y}$ to be equal, which makes it inappropriate for speech recognition because the lengths of the input and output sequences are not equal. For the case where $T\\ge J$ as in speech recognition, SCRF defines the sequence-level conditional probability with the auxiliary segment labels ${E} = ({e}_1, \\cdots , {e}_J) $ as P(y, E X) = 1Z(X) j=1J f ( yj, ej, xj ), where $\\mathbf {e}_j = \\langle s_{j}, n_{j} \\rangle $ is a tuple of the beginning ( ${X} = ( {x}_1, \\cdots , {x}_T )$0 ) and the end ( ${X} = ( {x}_1, \\cdots , {x}_T )$1 ) time tag for the segment of ${X} = ( {x}_1, \\cdots , {x}_T )$2 , and ${X} = ( {x}_1, \\cdots , {x}_T )$3 while ${X} = ( {x}_1, \\cdots , {x}_T )$4 ; ${X} = ( {x}_1, \\cdots , {x}_T )$5 and ${X} = ( {x}_1, \\cdots , {x}_T )$6 denotes the vocabulary set; ${X} = ( {x}_1, \\cdots , {x}_T )$7 is the embedding vector of the segment corresponding to the token ${X} = ( {x}_1, \\cdots , {x}_T )$8 . In this case, ${X} = ( {x}_1, \\cdots , {x}_T )$9 sums over all the possible ${y} = ( y_1, \\cdots , y_J)$0 pairs, i.e., .$$Z({X}) = \\sum _{y,E} \\prod _{j=1}^J \\exp f \\left( y_j, {e}_j, \\bar{x}_j \\right).$$   (Eq. 1) .Similar to other CRFs, the function $f(\\cdot )$ is defined as .$$f \\left( y_j, {e}_j, \\bar{x}_t \\right) = \\mathbf {w}^\\top \\Phi (y_j, {e}_j, \\bar{x}_j),$$   (Eq. 2) .where $\\Phi (\\cdot )$ denotes the feature function, and $\\mathbf {w}$ is the weight vector. Most of conventional approaches for SCRF-based acoustic models use a manually defined feature function $\\Phi (\\cdot )$ , where the features and segment boundary information are provided by an auxiliary system BIBREF19 , BIBREF20 . In BIBREF21 , BIBREF12 , we proposed an end-to-end training approach for SCRFs, where $\\Phi (\\cdot )$ was defined with neural networks, and the segmental level features were learned by RNNs. The model was referred to as the segmental RNN (SRNN), and it will be used as the implementation of the SCRF acoustic model for multitask learning in this study. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "17\n",
      "Question 1: What is the black-box injection approach used in the current work?\n",
      "\n",
      "Answer 1: The black-box injection approach is used in the current work to inject knowledge to the input of a trained NMT system, without having access to its internals or its training procedure. This is done to influence the output of the NMT system in order to produce the desired target-side morphology when the information is not available in the source sentence.\n",
      "Question : for the text Our goal is to supply an NMT system with knowledge regarding the speaker and interlocutor of first-person sentences, in order to produce the desired target-side morphology when the information is not available in the source sentence. The approach we take in the current work is that of black-box injection, in which we attempt to inject knowledge to the input in order to influence the output of a trained NMT system, without having access to its internals or its training procedure as proposed by vanmassenhove-hardmeier-way:2018:EMNLP..We are motivated by recent work by BIBREF4 who showed that NMT systems learn to track coreference chains when presented with sufficient discourse context. We conjecture that there are enough sentence-internal pronominal coreference chains appearing in the training data of large-scale NMT systems, such that state-of-the-art NMT systems can and do track sentence-internal coreference. We devise a wrapper method to make use of this coreference tracking ability by introducing artificial antecedents that unambiguously convey the desired gender and number properties of the speaker and audience..More concretely, a sentence such as “I love you” is ambiguous with respect to the gender of the speaker and the gender and number of the audience. However, sentences such as “I love you, she told him” are unambiguous given the coreference groups {I, she} and {you, him} which determine I to be feminine singular and you to be masculine singular. We can thus inject the desired information by prefixing a sentence with short generic sentence fragment such as “She told him:” or “She told them that”, relying on the NMT system's coreference tracking abilities to trigger the correctly marked translation, and then remove the redundant translated prefix from the generated target sentence. We observed that using a parataxis construction (i.e. “she said to him:”) almost exclusively results in target-side parataxis as well (in 99.8% of our examples), making it easy to identify and strip the translated version from the target side. Moreover, because the parataxis construction is grammatically isolated from the rest of the sentence, it can be stripped without requiring additional changes or modification to the rest of the sentence, ensuring grammaticality. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "18\n",
      "Question 1: How does the proposed gender-aware translation method compare to the Google Translate baseline on the English-French test set?\n",
      "\n",
      "Answer 1: The proposed gender-aware translation method (using Google Translate and given prefixes) was evaluated on the English-French test set proposed by vanmassenhove-hardmeier-way:2018:EMNLP. The results showed that Google Translate outperformed the proposed system, as it was trained on a different corpus and used more complex machine translation models. However, using the proposed method further improved the BLEU score.\n",
      "Question : for the text Closely related to our work, vanmassenhove-hardmeier-way:2018:EMNLP proposed a method and an English-French test set to evaluate gender-aware translation, based on the Europarl corpus BIBREF7 . We evaluate our method (using Google Translate and the given prefixes) on their test set to see whether it is applicable to another language pair and domain. Table shows the results of our approach vs. their published results and the Google Translate baseline. As may be expected, Google Translate outperforms their system as it is trained on a different corpus and may use more complex machine translation models. Using our method improves the BLEU score even further. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "19\n",
      "Question 1: What is the proposed method for injecting gender and number information into a pre-trained NMT model?\n",
      "\n",
      "Answer 1: The proposed method for injecting gender and number information into a pre-trained NMT model involves injecting such information in a black-box setting. This method was shown to improve machine translation accuracy by 2.3 BLEU in an English-to-Hebrew translation setting where the speaker and audience gender can be inferred. Additionally, a fine-grained syntactic analysis demonstrated the efficacy of the method in controlling the realization of first and second-person pronouns, verbs, and adjectives related to them. Future work may explore the automatic generation of the injected context or the use of cross-sentence context to infer the injected information.\n",
      "Question : for the text We highlight the problem of translating between languages with different morphological systems, in which the target translation must contain gender and number information that is not available in the source. We propose a method for injecting such information into a pre-trained NMT model in a black-box setting. We demonstrate the effectiveness of this method by showing an improvement of 2.3 BLEU in an English-to-Hebrew translation setting where the speaker and audience gender can be inferred. We also perform a fine-grained syntactic analysis that shows how our method enables to control the morphological realization of first and second-person pronouns, together with verbs and adjectives related to them. In future work we would like to explore automatic generation of the injected context, or the use of cross-sentence context to infer the injected information. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "20\n",
      "Question 1: Which machine translation system did the researchers use for their experiments?\n",
      "\n",
      "Answer 1: The researchers used Google's machine translation system (GMT), accessed through its Cloud API, for their experiments.\n",
      "Question : for the text To demonstrate our method in a black-box setting, we focus our experiments on Google's machine translation system (GMT), accessed through its Cloud API. To test the method on real-world sentences, we consider a monologue from the stand-up comedy show “Sarah Silverman: A Speck of Dust”. The monologue consists of 1,244 English sentences, all by a female speaker conveyed to a plural, gender-neutral audience. Our parallel corpora consists of the 1,244 English sentences from the transcript, and their corresponding Hebrew translations based on the Hebrew subtitles. We translate the monologue one sentence at a time through the Google Cloud API. Eyeballing the results suggest that most of the translations use the incorrect, but default, masculine and singular forms for the speaker and the audience, respectively. We expect that by adding the relevant condition of “female speaking to an audience” we will get better translations, affecting both the gender of the speaker and the number of the audience..To verify this, we experiment with translating the sentences with the following variations: No Prefix—The baseline translation as returned by the GMT system. “He said:”—Signaling a male speaker. We expect to further skew the system towards masculine forms. “She said:”—Signaling a female speaker and unknown audience. As this matches the actual speaker's gender, we expect an improvement in translation of first-person pronouns and verbs with first-person pronouns as subjects. “I said to them:”—Signaling an unknown speaker and plural audience. “He said to them:”—Masculine speaker and plural audience. “She said to them:”—Female speaker and plural audience—the complete, correct condition. We expect the best translation accuracy on this setup. “He/she said to him/her”—Here we set an (incorrect) singular gender-marked audience, to investigate our ability to control the audience morphology. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "21\n",
      "Question 1: What is the aim of the black-box method presented in the text?\n",
      "\n",
      "Answer 1: The aim of the black-box method presented in the text is to influence the interpretation chosen by an NMT system in ambiguous cases related to gender, number, and case by providing pre-defined textual hints about the gender and number of the speaker and the audience. This method is effective in resolving many of the ambiguities and improves translation quality.\n",
      "Question : for the text A common way for marking information about gender, number, and case in language is morphology, or the structure of a given word in the language. However, different languages mark such information in different ways – for example, in some languages gender may be marked on the head word of a syntactic dependency relation, while in other languages it is marked on the dependent, on both, or on none of them BIBREF0 . This morphological diversity creates a challenge for machine translation, as there are ambiguous cases where more than one correct translation exists for the same source sentence. For example, while the English sentence “I love language” is ambiguous with respect to the gender of the speaker, Hebrew marks verbs for the gender of their subject and does not allow gender-neutral translation. This allows two possible Hebrew translations – one in a masculine and the other in a feminine form. As a consequence, a sentence-level translator (either human or machine) must commit to the gender of the speaker, adding information that is not present in the source. Without additional context, this choice must be done arbitrarily by relying on language conventions, world knowledge or statistical (stereotypical) knowledge..Indeed, the English sentence “I work as a doctor” is translated into Hebrew by Google Translate using the masculine verb form oved, indicating a male speaker, while “I work as a nurse” is translated with the feminine form ovedet, indicating a female speaker (verified on March 2019). While this is still an issue, there have been recent efforts to reduce it for specific language pairs..We present a simple black-box method to influence the interpretation chosen by an NMT system in these ambiguous cases. More concretely, we construct pre-defined textual hints about the gender and number of the speaker and the audience (the interlocutors), which we concatenate to a given input sentence that we would like to translate accordingly. We then show that a black-box NMT system makes the desired morphological decisions according to the given hint, even when no other evidence is available on the source side. While adding those hints results in additional text on the target side, we show that it is simple to remove, leaving only the desired translation..Our method is appealing as it only requires simple pre-and-post processing of the inputs and outputs, without considering the system internals, or requiring specific annotated data and training procedure as in previous work BIBREF1 . We show that in spite of its simplicity, it is effective in resolving many of the ambiguities and improves the translation quality in up to 2.3 BLEU when given the correct hints, which may be inferred from text metadata or other sources. Finally, we perform a fine-grained syntactic analysis of the translations generated using our method which shows its effectiveness. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "22\n",
      "Question 1: What is the role of morphology in translation systems?\n",
      "\n",
      "Answer 1: Morphology plays a crucial role in translation systems as it is responsible for generating correct target-language morphology during the translation process. Translation systems must have knowledge of both the source-side and target-side morphology to accurately convey the intended message. Current state-of-the-art translation systems do capture many aspects of natural language, including morphology, when a relevant context is available, but may resort to statistical guessing when there is a lack of context.\n",
      "Question : for the text Different languages use different morphological features marking different properties on different elements. For example, English marks for number, case, aspect, tense, person, and degree of comparison. However, English does not mark gender on nouns and verbs. Even when a certain property is marked, languages differ in the form and location of the marking BIBREF0 . For example, marking can occur on the head of a syntactic dependency construction, on its argument, on both (requiring agreement), or on none of them. Translation systems must generate correct target-language morphology as part of the translation process. This requires knowledge of both the source-side and target-side morphology. Current state-of-the-art translation systems do capture many aspects of natural language, including morphology, when a relevant context is available BIBREF2 , BIBREF3 , but resort to “guessing” based on the training-data statistics when it is not. Complications arise when different languages convey different kinds of information in their morphological systems. In such cases, a translation system may be required to remove information available in the source sentence, or to add information not available in it, where the latter can be especially tricky. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "23\n",
      "Question 1: How did the researchers test their method's outputs on multiple languages? \n",
      "\n",
      "Answer 1: The researchers ran their pre-and post-processing steps with Google Translate using examples sourced from native speakers of different languages. For every example, they had an English sentence and two translations in the corresponding language, one in masculine and one in feminine form.\n",
      "Question : for the text To test our method’s outputs on multiple languages, we run our pre-and post-processing steps with Google Translate using examples we sourced from native speakers of different languages. For every example we have an English sentence and two translations in the corresponding language, one in masculine and one in feminine form. Not all examples are using the same source English sentence as different languages mark different information. Table shows that for these specific examples our method worked on INLINEFORM0 of the languages we had examples for, while for INLINEFORM1 languages both translations are masculine, and for 1 language both are feminine. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "24\n",
      "Q: What is the BLEU score and how does it relate to the analysis performed in the study?\n",
      "A: The BLEU score is an indication of how close the automated translation is to the reference translation, but it does not tell us exactly what changed concerning the gender and number properties being controlled. The study performs a finer-grained analysis focusing on the relation between the injected speaker and audience information, and the morphological realizations of the corresponding elements.\n",
      "Question : for the text The BLEU score is an indication of how close the automated translation is to the reference translation, but does not tell us what exactly changed concerning the gender and number properties we attempt to control. We perform a finer-grained analysis focusing on the relation between the injected speaker and audience information, and the morphological realizations of the corresponding elements. We parse the translations and the references using a Hebrew dependency parser. In addition to the parse structure, the parser also performs morphological analysis and tagging of the individual tokens. We then perform the following analysis..Speaker's Gender Effects: We search for first-person singular pronouns with subject case (ani, unmarked for gender, corresponding to the English I), and consider the gender of its governing verb (or adjectives in copular constructions such as `I am nice'). The possible genders are `masculine', `feminine' and `both', where the latter indicates a case where the none-diacriticized written form admits both a masculine and a feminine reading. We expect the gender to match the ones requested in the prefix..Interlocutors' Gender and Number Effects: We search for second-person pronouns and consider their gender and number. For pronouns in subject position, we also consider the gender and number of their governing verbs (or adjectives in copular constructions). For a singular audience, we expect the gender and number to match the requested ones. For a plural audience, we expect the masculine-plural forms..Results: Speaker. Figure FIGREF3 shows the result for controlling the morphological properties of the speaker ({he, she, I} said). It shows the proportion of gender-inflected verbs for the various conditions and the reference. We see that the baseline system severely under-predicts the feminine form of verbs as compared to the reference. The “He said” conditions further decreases the number of feminine verbs, while the “I said” conditions bring it back to the baseline level. Finally, the “She said” prefixes substantially increase the number of feminine-marked verbs, bringing the proportion much closer to that of the reference (though still under-predicting some of the feminine cases)..Results: Audience. The chart in Figure FIGREF3 shows the results for controlling the number of the audience (...to them vs nothing). It shows the proportion of singular vs. plural second-person pronouns on the various conditions. It shows a similar trend: the baseline system severely under-predicts the plural forms with respect to the reference translation, while adding the “to them” condition brings the proportion much closer to that of the reference. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "25\n",
      "Question 1: What is the multi-bleu.perl script used for?\n",
      "Answer 1: The multi-bleu.perl script is used to compare different conditions by comparing BLEU BIBREF5 with respect to the reference Hebrew translations in the Moses toolkit.\n",
      "Question : for the text We compare the different conditions by comparing BLEU BIBREF5 with respect to the reference Hebrew translations. We use the multi-bleu.perl script from the Moses toolkit BIBREF6 . Table shows BLEU scores for the different prefixes. The numbers match our expectations: Generally, providing an incorrect speaker and/or audience information decreases the BLEU scores, while providing the correct information substantially improves it - we see an increase of up to 2.3 BLEU over the baseline. We note the BLEU score improves in all cases, even when given the wrong gender of either the speaker or the audience. We hypothesise this improvement stems from the addition of the word “said” which hints the model to generate a more “spoken” language which matches the tested scenario. Providing correct information for both speaker and audience usually helps more than providing correct information to either one of them individually. The one outlier is providing “She” for the speaker and “her” for the audience. While this is not the correct scenario, we hypothesise it gives an improvement in BLEU as it further reinforces the female gender in the sentence. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "26\n",
      "Question 1: What did the protocol for evaluating the morphological competence of MT systems show? \n",
      "\n",
      "Answer 1: The protocol presented in burlot:hal-01618387 showed that current translation systems only manage to capture some morphological phenomena correctly.\n",
      "Question : for the text E17-1101 showed that given input with author traits like gender, it is possible to retain those traits in Statistical Machine Translation (SMT) models. W17-4727 showed that incorporating morphological analysis in the decoder improves NMT performance for morphologically rich languages. burlot:hal-01618387 presented a new protocol for evaluating the morphological competence of MT systems, indicating that current translation systems only manage to capture some morphological phenomena correctly. Regarding the application of constraints in NMT, N16-1005 presented a method for controlling the politeness level in the generated output. DBLP:journals/corr/FiclerG17aa showed how to guide a neural text generation system towards style and content parameters like the level of professionalism, subjective/objective, sentiment and others. W17-4811 showed that incorporating more context when translating subtitles can improve the coherence of the generated translations. Most closely to our work, vanmassenhove-hardmeier-way:2018:EMNLP also addressed the missing gender information by training proprietary models with a gender-indicating-prefix. We differ from this work by treating the problem in a black-box manner, and by addressing additional information like the number of the speaker and the gender and number of the audience. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "27\n",
      "Question 1: Who financially supported the work discussed in the text?\n",
      "Answer 1: The Ministry of Education and Science of the Russian Federation provided financial support for the work, through Contract 14.575.21.0132 (IDRFMEFI57517X0132).\n",
      "Question : for the text This work was financially supported by the Ministry of Education and Science of the Russian Federation, Contract 14.575.21.0132 (IDRFMEFI57517X0132). generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "28\n",
      "What dataset was used for training and development in the experiments? \n",
      "\n",
      "The experiments used conversational speech from IARPA Babel Turkish Language Pack (LDC2016S10), which contains about 80 hours of transcribed speech for training and 10 hours for development.\n",
      "Question : for the text For all experiments we used conversational speech from IARPA Babel Turkish Language Pack (LDC2016S10). This corpus contains about 80 hours of transcribed speech for training and 10 hours for development. The dataset is rather small compared to widely used benchmarks for conversational speech: English Switchboard corpus (300 hours, LDC97S62) and Fisher dataset (2000 hours, LDC2004S13 and LDC2005S13)..As targets we use 32 symbols: 29 lowercase characters of Turkish alphabet BIBREF19 , apostrophe, space and special 〈blank〉 character that means “no output”. Thus we do not use any prior linguistic knowledge and also avoid OOV problem as the system can construct new words directly..All models are trained with CTC-loss. Input features are 40 mel-scaled log filterbank enegries (FBanks) computed every 10 ms with 25 ms window, concatenated with deltas and delta-deltas (120 features in vector). We also tried to use spectrogram and experimented with different normalization techniques..For decoding we used character-based beam search BIBREF20 with 3-gram language model build with SRILM package BIBREF21 finding sequence of characters INLINEFORM0 that maximizes the following objective BIBREF9 : INLINEFORM1 .where INLINEFORM0 is language model weight and INLINEFORM1 is word insertion penalty..For all experiments we used INLINEFORM0 , INLINEFORM1 , and performed decoding with beam width equal to 100 and 2000, which is not very large compared to 7000 and more active hypotheses used in traditional WFST decoders (e.g. many Kaldi recipes do decoding with INLINEFORM2 )..To compare with other published results BIBREF18 , BIBREF22 we used Sclite BIBREF23 scoring package to measure results of decoding with beam width 2000, that takes into account incomplete words and spoken noise in reference texts and doesn't penalize model if it incorrectly recognize these pieces..Also we report WER (word error rate) for simple argmax decoder (taking labels with maximum output on each time step and than applying CTC decoding rule – collapse repeated labels and remove “blanks”). generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "29\n",
      "Question 1: What was the WER achieved by the final system in the low-resource ASR task using Babel Turkish dataset?\n",
      "\n",
      "Answer 1: The final system achieved 45.8% WER using in-domain data only, which is the best published result for Turkish end-to-end systems.\n",
      "Question : for the text In this paper we explored different end-to-end architectures in low-resource ASR task using Babel Turkish dataset. We considered different ways to improve performance and proposed promising CTC-loss modification that uses segmentation during training. Our final system achieved 45.8% WER using in-domain data only, which is the best published result for Turkish end-to-end systems. Our work also shows than well-tuned end-to-end system can achieve results very close to traditional DNN-HMM systems even for low-resource languages. In future work we plan to further investigate different loss modifications (Gram-CTC, ASG) and try to use RNN-Transducers and multi-task learning. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "30\n",
      "Question 1: What are the different neural network architectures used in the study?\n",
      "\n",
      "Answer 1: The study explored the behavior of multi-layer bidirectional LSTM networks, a fully-convolutional architecture similar to Wav2Letter, and a DeepSpeech-like architecture developed by Salesforce.\n",
      "Question : for the text We tried to explore the behavior of different neural network architectures in case when rather small data is available. We used multi-layer bidirectional LSTM networks, tried fully-convolutional architecture similar to Wav2Letter BIBREF8 and explored DeepSpeech-like architecture developed by Salesforce (DS-SF) BIBREF14 ..The convolutional model consists of 11 convolutional layers with batch normalization after each layer. The DeepSpeech-like architecture consists of 5-layers residual network with depth-wise separable convolutions followed by 4-layer bidirectional Gated Recurrent Unit (GRU) as described in BIBREF14 ..Our baseline bidirectional LSTM is 6-layers network with 320 hidden units per direction as in BIBREF18 . Also we tried to use bLSTM to label every second frame (20 ms) concatenating every first output from first layer with second and taking this as input for second model layer..The performance of our baseline models is shown in Table TABREF6 . generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "31\n",
      "What are the three main categories of end-to-end systems for speech recognition and how are they different from traditional ASR training paradigms?\n",
      "\n",
      "Answer 1: The three main categories of end-to-end systems for speech recognition are Connectionist Temporal Classification (CTC), Sequence-to-sequence models with attention mechanism, and RNN-Transducers. These systems work differently from traditional ASR training paradigms because they use global training with acoustic data and reference texts, and in some cases, simplify or exclude the decoding process. Additionally, these systems can construct new words using graphemes or subword units, while traditional DNN-HMM systems are limited with language model vocabulary.\n",
      "Question : for the text Although development of the first speech recognition systems began half a century ago, there has been a significant increase of the accuracy of ASR systems and number of their applications for the recent ten years, even for low-resource languages BIBREF0 , BIBREF1 ..This is mainly due to widespread applying of deep learning and very effective performance of neural networks in hybrid recognition systems (DNN-HMM). However, for last few years there has been a trend to change traditional ASR training paradigm. End-to-end training systems gradually displace complex multistage learning process (including training of GMMs BIBREF2 , clustering of allophones’ states, aligning of speech to clustered senones, training neural networks with cross-entropy loss, followed by retraining with sequence-discriminative criterion). The new approach implies training the system in one global step, working only with acoustic data and reference texts, and significantly simplifies or even completely excludes in some cases the decoding process. It also avoids the problem of out-of-vocabulary words (OOV), because end-to-end system, trained with parts of the words as targets, can construct new words itself using graphemes or subword units, while traditional DNN-HMM systems are limited with language model vocabulary..The whole variety of end-to-end systems can be divided into 3 main categories: Connectionist Temporal Classification (CTC) BIBREF3 ; Sequence-to-sequence models with attention mechanism BIBREF4 ; RNN-Transducers BIBREF5 ..Connectionist Temporal Classification (CTC) approach uses loss functions that utilize all possible alignments between reference text and audio data. Targets for CTC-based system can be phonemes, graphemes, syllables and other subword units and even whole words. However, a lot more data is usually required to train such systems well, compared to traditional hybrid systems..Sequence-to-sequence models are used to map entire input sequences to output sequences without any assumptions about their alignment. The most popular architecture for sequence-to-sequence models is encoder-decoder model with attention. Encoder and decoder are usually constructed using recurrent neural networks, basic attention mechanism calculates energy weights that emphasize importance of encoder vectors for decoding on this step, and then sums all these vectors with energy weights. Encoder-decoder models with attention mechanism show results close to traditional DNN-HMM systems and in some cases surpass them, but for a number of reasons their usage is still rather limited. First of all, this is related to the fact, that such systems show best results when the duration of real utterances is close to the duration of utterances from training data. However, when the duration difference increases, the performance degrades significantly BIBREF4 ..Moreover, the entire utterance must be preprocessed by encoder before start of decoder's work. This is the reason, why it is hard to apply the approach to recognize long recordings or streaming audio. Segmenting long recordings into shorter utterances solves the duration issue, but leads to a context break, and eventually negatively affects recognition accuracy. Secondly, the computational complexity of encoder-decoder models is high because of recurrent networks usage, so these models are rather slow and hard to parallelize..The idea of RNN-Transducer is an extension of CTC and provides the ability to model inner dependencies separately and jointly between elements of both input (audio frames) and output (phonemes and other subword units) sequences. Despite of mathematical elegance, such systems are very complicated and hard to implement, so they are still rarely used, although several impressive results were obtained using this technique..CTC-based approach is easier to implement, better scaled and has many “degrees of freedom”, which allows to significantly improve baseline systems and achieve results close to state-of-the-art. Moreover, CTC-based systems are well compatible with traditional WFST-decoders and can be easily integrated with conventional ASR systems..Besides, as already mentioned, CTC-systems are rather sensitive to the amount of training data, so it is very relevant to study how to build effective CTC-based recognition system using a small amount of training samples. It is especially actual for low-resource languages, where we have only a few dozen hours of speech. Building ASR system for low-resource languages is one of the aims of international Babel program, funded by the Intelligence Advanced Research Projects Activity (IARPA). Within the program extensive research was carried out, resulting in creation of a number of modern ASR systems for low-resource languages. Recently, end-to-end approaches were applied to this task, showing expectedly worse results than traditional systems, although the difference is rather small..In this paper we explore a number of ways to improve end-to-end CTC-based systems in low-resource scenarios using the Turkish language dataset from the IARPA Babel collection. In the next section we describe in more details different versions of CTC-systems and their application for low-resource speech recognition. Section 3 describes the experiments and their results. Section 4 summarizes the results and discusses possible ways for further work. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "32\n",
      "What technique was developed to help the model converge faster for CTC-loss?\n",
      "\n",
      "Sortagrad was developed to help the model converge faster for CTC-loss by using shorter segments at the beginning of training.\n",
      "Question : for the text It is known that CTC-loss is very unstable for long utterances BIBREF3 , and smaller utterances are more useful for this task. Some techniques were developed to help model converge faster, e.g. sortagrad BIBREF10 (using shorter segments at the beginning of training)..To compute CTC-loss we use all possible alignments between audio features and reference text, but only some of the alignments make sense. Traditional DNN-HMM systems also use iterative training with finding best alignment and then training neural network to approximate this alignment. Therefore, we propose the following algorithm to use segmentation during training:.compute CTC-alignment (find the sequence of targets with minimal loss that can be mapped to real targets by collapsing repeated characters and removing blanks).perform greedy decoding (argmax on each step).find “well-recognized” words with INLINEFORM0 ( INLINEFORM1 is a hyperparameter): segment should start and end with space; word is “well-recognized” when argmax decoding is equal to computed alignment.if the word is “well-recognized”, divide the utterance into 5 segments: left segment before space, left space, the word, right space and right segment.compute CTC-loss for all this segments separately and do back-propagation as usual.The results of training with this criterion are shown in Table TABREF13 . The proposed criterion doesn't lead to consistent improvement while decoding with large beam width (2000), but shows significant improvement when decoding with smaller beam (100). We plan to further explore utilizing alignment information during training. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "33\n",
      "What is the CTC loss and how does it relate to ASR systems based on neural networks? \n",
      "\n",
      "The CTC loss is the total probability of a label sequence given an observation sequence, taking into account all possible alignments induced by a given word sequence. It can be efficiently computed using the Forward-Backward algorithm, and is differentiable with respect to the posterior probabilities of any output sequence element observation given the time frame. This allows for ASR systems based on neural networks that estimate these posteriors to be trained using conventional error back-propagation gradient descent. Furthermore, the CTC loss accumulates information about the whole output sequence, making it an alternative to traditional fine-tuning of neural network acoustic models using sequence-discriminative criteria.\n",
      "Question : for the text Development of CTC-based systems originates from the paper BIBREF3 where CTC loss was introduced. This loss is a total probability of labels sequence given observation sequence, which takes into account all possible alignments induced by a given words sequence..Although a number of possible alignments increases exponentially with sequences’ lengths, there is an efficient algorithm to compute CTC loss based on dynamic programming principle (known as Forward-Backward algorithm). This algorithm operates with posterior probabilities of any output sequence element observation given the time frame and CTC loss is differentiable with respect to these probabilities..Therefore, if an acoustic model is based on the neural network which estimates these posteriors, its training may be performed with a conventional error back-propagation gradient descent BIBREF6 . Training of ASR system based on such a model does not require an explicit alignment of input utterance to the elements of output sequence and thus may be performed in end-to-end fashion. It is also important that CTC loss accumulates the information about the whole output sequence, and hence its optimization is in some sense an alternative to the traditional fine-tuning of neural network acoustic models by means of sequence-discriminative criteria such as sMBR BIBREF7 etc. The implementation of CTC is conventionally based on RNN/LSTM networks, including bidirectional ones as acoustic models, since they are known to model long context effectively..The important component of CTC is a special “blank” symbol which fills in gaps between meaningful elements of output sequence to equalize its length to the number of frames in the input sequence. It corresponds to a separate output neuron, and blank symbols are deleted from the recognized sequence to obtain the final result. In BIBREF8 a modification of CTC loss was proposed, referred as Auto SeGmentation criterion (ASG loss), which does not use blank symbols. Instead of using “blank”, a simple transition probability model for an output symbols is introduced. This leads to a significant simplification and speedup of computations. Moreover, the improved recognition results compared to basic CTC loss were obtained..DeepSpeech BIBREF9 developed by Baidu Inc. was one of the first systems that demonstrated an effectiveness of CTC-based speech recognition in LVCSR tasks. Being trained on 2300 hours of English Conversational Telephone Speech data, it demonstrated state-of-the-art results on Hub5'00 evaluation set. Research in this direction continued and resulted in DeepSpeech2 architecture BIBREF10 , composed of both convolutional and recurrent layers. This system demonstrates improved accuracy of recognition of both English and Mandarin speech. Another successful example of applying CTC to LVCSR tasks is EESEN system BIBREF11 . It integrates an RNN-based model trained with CTC criterion to the conventional WFST-based decoder from the Kaldi toolkit BIBREF12 . The paper BIBREF13 shows that end-to-end systems may be successfully built from convolutional layers only instead of recurrent ones. It was demonstrated that using Gated Convolutional Units (GLU-CNNs) and training with ASG-loss leads to the state-of-the-art results on the LibriSpeech database (960 hours of training data)..Recently, a new modification of DeepSpeech2 architecture was proposed in BIBREF14 . Several lower convolutional layers were replaced with a deep residual network with depth-wise separable convolutions. This modification along with using strong regularization and data augmentation techniques leads to the results close to DeepSpeech2 in spite of significantly lower amount of data used for training. Indeed, one of the models was trained with only 80 hours of speech data (which were augmented with noisy and speed-perturbed versions of original data)..These results suggest that CTC can be successfully applied for the training of ASR systems for low-resource languages, in particular, for those included in Babel research program (the amount of training data for them is normally 40 to 80 hours of speech)..Currently, Babel corpus contains data for more than 20 languages, and for most of them quite good traditional ASR system were built BIBREF15 , BIBREF16 , BIBREF17 . In order to improve speech recognition accuracy for a given language, data from other languages is widely used as well. It can be used to train multilingual system via multitask learning or to obtain high-level multilingual representations, usually bottleneck features, extracted from a pre-trained multilingual network..One of the first attempts to build ASR system for low-resource BABEL languages using CTC-based end-to-end training was made recently BIBREF18 . Despite the obtained results are somewhat worse compared to the state-of-the-art traditional systems, they still demonstrate that CTC-based approach is viable for building low-resource ASR systems. The aim of our work is to investigate some ways to improve the obtained results. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "34\n",
      "Question 1: What is the best published end-to-end result on the Babel Turkish dataset using in-domain data?\n",
      "\n",
      "Answer 1: The best published end-to-end result on the Babel Turkish dataset using in-domain data is 45.8% WER, achieved by the best model trained with speed and volume perturbation BIBREF24.\n",
      "Question : for the text To train our best model we chose the best network from our experiments (6-layer bLSTM with 1024 hidden units), trained it with Adam optimizer and fine-tuned with SGD with momentum using exponential learning rate decay. The best model trained with speed and volume perturbation BIBREF24 achieved 45.8% WER, which is the best published end-to-end result on Babel Turkish dataset using in-domain data. For comparison, WER of model trained using in-domain data in BIBREF18 is 53.1%, using 4 additional languages (including English Switchboard dataset) – 48.7%. It is also not far from Kaldi DNN-HMM system BIBREF22 with 43.8% WER. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "35\n",
      "Question 1: Did using variance with mean normalization (CMVN) improve the task?\n",
      "\n",
      "Answer 1: No, we found using variance with mean normalization (CMVN) unnecessary for the task.\n",
      "Question : for the text We explored different normalization techniques. FBanks with cepstral mean normalization (CMN) perform better than raw FBanks. We found using variance with mean normalization (CMVN) unnecessary for the task. Using deltas and delta-deltas improves model, so we used them in other experiments. Models trained with spectrogram features converge slower and to worse minimum, but the difference when using CMN is not very big compared to FBanks. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "36\n",
      "Question 1: What was observed regarding the performance of 6-layer bLSTM models with varying numbers of hidden units?\n",
      "\n",
      "Answer 1: It was observed that models with 512 and 768 hidden units were worse than those with 320, but the model with 1024 hidden units performed significantly better than the others. Additionally, it was found that the 6-layer model outperformed other models.\n",
      "Question : for the text Experiments with varying number of hidden units of 6-layer bLSTM models are presented in Table TABREF17 . Models with 512 and 768 hidden units are worse than with 320, but model with 1024 hidden units is significantly better than others. We also observed that model with 6 layers performs better than others. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "37\n",
      "Question 1: How does the new approach for tackling multi-span questions work?\n",
      "\n",
      "Answer 1: The new approach for tackling multi-span questions involves individually tagging each token with a categorical tag and relying on the tokens' contextual representation to bridge the information gap resulting from the tokens being tagged individually.\n",
      "Question : for the text In this work, we introduced a new approach for tackling multi-span questions in reading comprehension datasets. This approach is based on individually tagging each token with a categorical tag, relying on the tokens' contextual representation to bridge the information gap resulting from the tokens being tagged individually..First, we show that integrating this new approach into an existing model, NABERT+, does not hinder performance on other questions types, while substantially improving the results on multi-span questions. Later, we compare our results to the current state-of-the-art on multi-span questions. We show that our model has a clear advantage in handling multi-span questions, with a 29.7 absolute improvement in EM, and a 15.1 absolute improvement in F1. Furthermore, we show that our model slightly eclipses the current state-of-the-art results on the entire DROP dataeset. Finally, we present some ablation studies, analyzing the benefit gained from individual components of our model..We believe that combining our tag-based approach for handling multi-span questions with current successful techniques for handling single-span questions could prove beneficial in finding better, more holistic ways, of tackling span questions in general. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "38\n",
      "Question 1: What is the alternative approach to optimizing the average likelihood for each individual span? \n",
      "\n",
      "Answer 1: The alternative approach is to only take into account the most likely tag sequence instead of considering all possible tag sequences. This provides the model more flexibility during training and the ability to focus on more \"correct\" tag sequences.\n",
      "Question : for the text Currently, For each individual span, we optimize the average likelihood over all its possible tag sequences (see Section SECREF9). A different approach could be not taking each possible tag sequence into account but only the most likely one. This could provide the model more flexibility during training and the ability to focus on the more \"correct\" tag sequences. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "39\n",
      "Question 1: Did the model consider the representation of all sub-tokens in the tagging task?\n",
      "\n",
      "Answer 1: No, the model only considered the representation of the first wordpiece sub-token.\n",
      "Question : for the text As mentioned in Section SECREF5, we only considered the representation of the first wordpiece sub-token in our model. It would be interesting to see how different approaches to utilize the other sub-tokens' representations in the tagging task affect the results. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "40\n",
      "Question 1: What is the purpose of the DROP dataset?\n",
      "\n",
      "Answer 1: The purpose of the DROP dataset is to present questions that require more complex reasoning in order to answer than that of previous datasets, in a hope to push the field towards a more comprehensive analysis of paragraphs of text.\n",
      "Question : for the text The task of reading comprehension, where systems must understand a single passage of text well enough to answer arbitrary questions about it, has seen significant progress in the last few years. With models reaching human performance on the popular SQuAD dataset BIBREF0, and with much of the most popular reading comprehension datasets having been solved BIBREF1, BIBREF2, a new dataset, DROP BIBREF3, was recently published..DROP aimed to present questions that require more complex reasoning in order to answer than that of previous datasets, in a hope to push the field towards a more comprehensive analysis of paragraphs of text. In addition to questions whose answers are a single continuous span from the paragraph text (questions of a type already included in SQuAD), DROP introduced additional types of questions. Among these new types were questions that require simple numerical reasoning, i.e questions whose answer is the result of a simple arithmetic expression containing numbers from the passage, and questions whose answers consist of several spans taken from the paragraph or the question itself, what we will denote as \"multi-span questions\"..Of all the existing models that tried to tackle DROP, only one model BIBREF4 directly targeted multi-span questions in a manner that wasn't just a by-product of the model's overall performance. In this paper, we propose a new method for tackling multi-span questions. Our method takes a different path from that of the aforementioned model. It does not try to generalize the existing approach for tackling single-span questions, but instead attempts to attack this issue with a new, tag-based, approach. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "41\n",
      "Question 1: What is the capital of France?\n",
      "\n",
      "Answer 1: Paris.\n",
      "Question : for the text Problem statement. Given a pair $(x^P,x^Q)$ of a passage and a question respectively, both comprised of tokens from a vocabulary $V$, we wish to predict an answer $y$. The answer could be either a collection of spans from the input, or a number, supposedly arrived to by performing arithmetic reasoning on the input. We want to estimate $p(y;x^P,x^Q)$..The basic structure of our model is shared with NABERT+, which in turn is shared with that of NAQANET (the model initially released with DROP). Consequently, meticulously presenting every part of our model would very likely prove redundant. As a reasonable compromise, we will introduce the shared parts with more brevity, and will go into greater detail when presenting our contributions. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "42\n",
      "Sorry, I am an AI language model and I am incapable of generating questions and answers without a context or prompt. Kindly provide me with more information or context, and I will be glad to assist you further.\n",
      "Question : for the text A subset of questions that wasn't directly dealt with by the base models (NAQANET, NABERT+) is questions that have an answer which is composed of multiple non-continuous spans. We suggest a head that will be able to deal with both single-span and multi-span questions..To model an answer which is a collection of spans, the multi-span head uses the $\\mathtt {BIO}$ tagging format BIBREF8: $\\mathtt {B}$ is used to mark the beginning of a span, $\\mathtt {I}$ is used to mark the inside of a span and $\\mathtt {O}$ is used to mark tokens not included in a span. In this way, we get a sequence of chunks that can be decoded to a final answer - a collection of spans..As words are broken up by the wordpiece tokenization for BERT, we decided on only considering the representation of the first sub-token of the word to tag, following the NER task from BIBREF2..For the $i$-th token of an input, the probability to be assigned a $\\text{tag} \\in \\left\\lbrace {\\mathtt {B},\\mathtt {I},\\mathtt {O}} \\right\\rbrace $ is computed as generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "43\n",
      "Question 1: How does the model handle multi-span questions?\n",
      "\n",
      "Answer 1: The model has a head capable of outputting correct answers for multi-span questions. Unlike other models, it does not settle on using \"semi-correct answers\" and deliberately encourages the model to provide partial answers for multi-span questions to improve their F1 score.\n",
      "Question : for the text Assume there are $K$ answer heads in the model and their weights denoted by $\\theta $. For each pair $(x^P,x^Q)$ we assume a latent categorical random variable $z\\in \\left\\lbrace 1,\\ldots \\,K\\right\\rbrace $ such that the probability of an answer $y$ is.where each component of the mixture corresponds to an output head such that.Note that a head is not always capable of producing the correct answer $y_\\text{gold}$ for each type of question, in which case $p\\left(y_\\text{gold} \\vert z ; x^{P},x^{Q},\\theta \\right)=0$. For example, the arithmetic head, whose output is always a single number, cannot possibly produce a correct answer for a multi-span question..For a multi-span question with an answer composed of $l$ spans, denote $y_{{\\text{gold}}_{\\textit {MS}}}=\\left\\lbrace y_{{\\text{gold}}_1}, \\ldots , y_{{\\text{gold}}_l} \\right\\rbrace $. NAQANET and NABERT+ had no head capable of outputting correct answers for multi-span questions. Instead of ignoring them in training, both models settled on using \"semi-correct answers\": each $y_\\text{gold} \\in y_{{\\text{gold}}_{\\textit {MS}}}$ was considered to be a correct answer (only in training). By deliberately encouraging the model to provide partial answers for multi-span questions, they were able to improve the corresponding F1 score. As our model does have a head with the ability to answer multi-span questions correctly, we didn't provide the aforementioned semi-correct answers to any of the other heads. Otherwise, we would have skewed the predictions of the head predictor and effectively mislead the other heads to believe they could predict correct answers for multi-span questions. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "44\n",
      "Question 1: What are the summary vectors in BERT-based QA models?\n",
      "\n",
      "Answer 1: The summary vectors are two fixed-size learned representations of the question and the passage, which serve as an input for some of the heads. They are computed by defining BERT's output on a $(x^{P},x^{Q})$ input as $\\mathbf {T}$, letting $\\mathbf {T}^{P}$ and $\\mathbf {T}^{Q}$ be subsequences of $\\mathbf {T}$ that correspond to $x^P$ and $x^Q$ respectively, and creating learned linear layers $\\mathbf {W}^P \\in \\mathbb {R}^\\texttt {Bdim}$ and $\\mathbf {W}^Q \\in \\mathbb {R}^\\texttt {Bdim}$ to compute the summary vectors.\n",
      "Question : for the text Before going over the answer heads, two additional components should be introduced - the summary vectors, and the head predictor..Summary vectors. The summary vectors are two fixed-size learned representations of the question and the passage, which serve as an input for some of the heads. To create the summary vectors, first define $\\mathbf {T}$ as BERT's output on a $(x^{P},x^{Q})$ input. Then, let $\\mathbf {T}^{P}$ and $\\mathbf {T}^{Q}$ be subsequences of T that correspond to $x^P$ and $x^Q$ respectively. Finally, let us also define Bdim as the dimension of the tokens in $\\mathbf {T}$ (e.g 768 for BERTbase), and have $\\mathbf {W}^P \\in \\mathbb {R}^\\texttt {Bdim}$ and $\\mathbf {W}^Q \\in \\mathbb {R}^\\texttt {Bdim}$ as learned linear layers. Then, the summary vectors are computed as:.Head predictor. A learned categorical variable with its number of outcomes equal to the number of answer heads in the model. Used to assign probabilities for using each of the heads in prediction..where FFN is a two-layer feed-forward network with RELU activation..Passage span. Define $\\textbf {W}^S \\in \\mathbb {R}^\\texttt {Bdim}$ and $\\textbf {W}^E \\in \\mathbb {R}^\\texttt {Bdim}$ as learned vectors. Then the probabilities of the start and end positions of a passage span are computed as.Question span. The probabilities of the start and end positions of a question span are computed as.where $\\textbf {e}^{|\\textbf {T}^Q|}\\otimes \\textbf {h}^P$ repeats $\\textbf {h}^P$ for each component of $\\textbf {T}^Q$..Count. Counting is treated as a multi-class prediction problem with the numbers 0-9 as possible labels. The label probabilities are computed as.Arithmetic. As in NAQNET, this head obtains all of the numbers from the passage, and assigns a plus, minus or zero (\"ignore\") for each number. As BERT uses wordpiece tokenization, some numbers are broken up into multiple tokens. Following NABERT+, we chose to represent each number by its first wordpiece. That is, if $\\textbf {N}^i$ is the set of tokens corresponding to the $i^\\text{th}$ number, we define a number representation as $\\textbf {h}_i^N = \\textbf {N}^i_0$..The selection of the sign for each number is a multi-class prediction problem with options $\\lbrace 0, +, -\\rbrace $, and the probabilities for the signs are given by.As for NABERT+'s two additional arithmetic features, we decided on using only the standard numbers, as the benefits from using templates were deemed inconclusive. Note that unlike the single-span heads, which are related to our introduction of a multi-span head, the arithmetic and count heads were not intended to play a significant role in our work. We didn't aim to improve results on these types of questions, perhaps only as a by-product of improving the general reading comprehension ability of our model. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "45\n",
      "Sorry, as an AI language model, I cannot generate a question and answer without any context. Please provide me with more information or context so I can give you a relevant response.\n",
      "Question : for the text To train our model, we try to maximize the log-likelihood of the correct answer $p(y_\\text{gold};x^{P},x^{Q},\\theta )$ as defined in Section SECREF2. If no head is capable of predicting the gold answer, the sample is skipped..We enumerate over every answer head $z\\in \\left\\lbrace \\textit {PS}, \\textit {QS}, \\textit {C}, \\textit {A}, \\textit {MS}\\right\\rbrace $ (Passage Span, Question Span, Count, Arithmetic, Multi-Span) to compute each of the objective's addends:.Note that we are in a weakly supervised setup: the answer type is not given, and neither is the correct arithmetic expression required for deriving some answers. Therefore, it is possible that $y_\\text{gold}$ could be derived by more than one way, even from the same head, with no indication of which is the \"correct\" one..We use the weakly supervised training method used in NABERT+ and NAQANET. Based on BIBREF9, for each head we find all the executions that evaluate to the correct answer and maximize their marginal likelihood ..For a datapoint $\\left(y, x^{P}, x^{Q} \\right)$ let $\\chi ^z$ be the set of all possible ways to get $y$ for answer head $z\\in \\left\\lbrace \\textit {PS}, \\textit {QS}, \\textit {C}, \\textit {A}, \\textit {MS}\\right\\rbrace $. Then, as in NABERT+, we have.Finally, for the arithmetic head, let $\\mu $ be the set of all the standard numbers and the numbers from the passage, and let $\\mathbf {\\chi }^{\\textit {A}}$ be the set of correct sign assignments to these numbers. Then, we have generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "46\n",
      "Question 1: How does the model handle questions with a large number of correct tag sequences?\n",
      "\n",
      "Answer 1: For questions with a large number of correct tag sequences between 10,000 and 100,000,000, beam search is used to generate the top-k predictions of the training model, and then incorrect sequences are filtered out. This enables the optimization to be done with respect to answers that are more compatible with the model. If no correct tag sequences were predicted within the top-k, the tag sequence that has all of the answer spans marked is used instead.\n",
      "Question : for the text The number of correct tag sequences can be expressed by.where $s$ is the number of spans in the answer and $\\#_i$ is the number of times the $i^\\text{th}$ span appears in the text..For questions with a reasonable amount of correct tag sequences, we generate all of them before the training starts. However, there is a small group of questions for which the amount of such sequences is between 10,000 and 100,000,000 - too many to generate and train on. In such cases, inspired by BIBREF9, instead of just using an arbitrary subset of the correct sequences, we use beam search to generate the top-k predictions of the training model, and then filter out the incorrect sequences. Compared to using an arbitrary subset, using these sequences causes the optimization to be done with respect to answers more compatible with the model. If no correct tag sequences were predicted within the top-k, we use the tag sequence that has all of the answer spans marked. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "47\n",
      "Question 1: What are the correct tag sequences for the multi-span answer [\"cat\", \"dog\"] in the text \"I have a cat and a dog\"?\n",
      "\n",
      "Answer 1: The correct tag sequences for the multi-span answer [\"cat\", \"dog\"] in the text \"I have a cat and a dog\" are $\\mathtt {O\\,B\\,O\\,B}$, $\\mathtt {O\\,B\\,B\\,O}$, and $\\mathtt {O\\,B\\,B\\,B}$.\n",
      "Question : for the text Since a given multi-span answer is a collection of spans, it is required to obtain its matching tag sequences in order to compute the training objective..In what we consider to be a correct tag sequence, each answer span will be marked at least once. Due to the weakly supervised setup, we consider all the question/passage spans that match the answer spans as being correct. To illustrate, consider the following simple example. Given the text \"X Y Z Z\" and the correct multi-span answer [\"Y\", \"Z\"], there are three correct tag sequences: $\\mathtt {O\\,B\\,B\\,B}$,$\\quad $ $\\mathtt {O\\,B\\,B\\,O}$,$\\quad $ $\\mathtt {O\\,B\\,O\\,B}$. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "48\n",
      "Question 1: What is denoted by ${\\chi }^{\\textit {MS}}$?\n",
      "Answer 1: ${\\chi }^{\\textit {MS}}$ denotes the set of correct tag sequences.\n",
      "\n",
      "Question 2: How is the likelihood of a tag sequence approximated?\n",
      "Answer 2: The likelihood of a tag sequence is approximated by assuming independence between the sequence's positions and multiplying the likelihoods of all the correct tags in the sequence. \n",
      "\n",
      "Question 3: What is the notation used for a correct tag sequence when the concatenation of a question and a passage is $m$ tokens long?\n",
      "Answer 3: A correct tag sequence is denoted as $\\left(\\text{tag}_1,\\ldots ,\\text{tag}_m\\right)$.\n",
      "Question : for the text Denote by ${\\chi }^{\\textit {MS}}$ the set of correct tag sequences. If the concatenation of a question and a passage is $m$ tokens long, then denote a correct tag sequence as $\\left(\\text{tag}_1,\\ldots ,\\text{tag}_m\\right)$..We approximate the likelihood of a tag sequence by assuming independence between the sequence's positions, and multiplying the likelihoods of all the correct tags in the sequence. Then, we have generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "49\n",
      "What is the objective of predicting the most likely sequence given the BIO constraints?\n",
      "\n",
      "Answer 1: The objective of predicting the most likely sequence given the BIO constraints is to generate a valid BIO sequence of length m based on the outputs $\\textbf {p}_{i}^{{\\text{tag}}_{i}}$.\n",
      "Question : for the text Based on the outputs $\\textbf {p}_{i}^{{\\text{tag}}_{i}}$ we would like to predict the most likely sequence given the $\\mathtt {BIO}$ constraints. Denote $\\textit {validSeqs}$ as the set of all $\\mathtt {BIO}$ sequences of length $m$ that are valid according to the rules specified in Section SECREF5. The $\\mathtt {BIO}$ tag sequence to predict is then.We considered the following approaches: generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "50\n",
      "Question 1: Why is using beam search a reasonable approach for tag sequence prediction with BIO tags? \n",
      "\n",
      "Answer 1: Using beam search for tag sequence prediction with BIO tags is a reasonable approach because past tag predictions only affect future tag predictions from the last B prediction and as long as the best tag to predict is I. Additionally, considering the frequency and length of correct spans in the question and the passage, there's effectively no effect of past sequence's positions on future ones. Lastly, at each prediction step, there are no more than 3 tags to consider, making beam search a very reasonable approach for near-optimal results with small beam width values.\n",
      "Question : for the text Due to our use of $\\mathtt {BIO}$ tags and their constraints, observe that past tag predictions only affect future tag predictions from the last $\\mathtt {B}$ prediction and as long as the best tag to predict is $\\mathtt {I}$. Considering the frequency and length of the correct spans in the question and the passage, effectively there's no effect of past sequence's positions on future ones, other than a very few positions ahead. Together with the fact that at each prediction step there are no more than 3 tags to consider, it means using beam search to get the most likely sequence is very reasonable and even allows near-optimal results with small beam width values. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "1\n",
      "Q: Does greedy tagging enforce the BIO constraints?\n",
      "A: No, greedy tagging does not enforce the BIO constraints, but the multi-span head's training objective adheres to the constraints. The predictions are expected to mostly adhere to the constraints as well, and any violations must be corrected post-prediction.\n",
      "Question : for the text Notice that greedy tagging does not enforce the $\\mathtt {BIO}$ constraints. However, since the multi-span head's training objective adheres to the $\\mathtt {BIO}$ constraints via being given the correct tag sequences, we can expect that even with greedy tagging the predictions will mostly adhere to these constraints as well. In case there are violations, their amendment is required post-prediction. Albeit faster, greedy tagging resulted in a small performance hit, as seen in Table TABREF26. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "2\n",
      "Question: What is the natural candidate for getting the most likely sequence?\n",
      "\n",
      "Answer: The natural candidate for getting the most likely sequence is Viterbi decoding.\n",
      "Question : for the text A natural candidate for getting the most likely sequence is Viterbi decoding, BIBREF10 with transition probabilities learned by a $\\mathtt {BIO}$ constrained Conditional Random Field (CRF) BIBREF11. However, further inspection of our sequence's properties reveals that such a computational effort is probably not necessary, as explained in following paragraphs. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "3\n",
      "Question 1: What tokenizer is used for tokenizing the passage, question, and answer texts?\n",
      "Answer 1: The BERT uncased wordpiece tokenizer from huggingface is used for tokenizing the passage, question, and answer texts.\n",
      "Question : for the text We tokenize the passage, question, and all answer texts using the BERT uncased wordpiece tokenizer from huggingface. The tokenization resulting from each $(x^P,x^Q)$ input pair is truncated at 512 tokens so it can be fed to BERT as an input. However, before tokenizing the dataset texts, we perform additional preprocessing as listed below. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "4\n",
      "Question 1: What was the precision issue encountered during analysis of the training process?\n",
      "\n",
      "Answer 1: The precision issue encountered during analysis of the training process was that the value resulting from an arithmetic operation would not always yield the exact result due to floating point precision limitations.\n",
      "Question : for the text Although we previously stated that we aren't focusing on improving arithmetic performance, while analyzing the training process we encountered two arithmetic-related issues that could be resolved rather quickly: a precision issue and a number extraction issue. Regarding precision, we noticed that while either generating expressions for the arithmetic head, or using the arithmetic head to predict a numeric answer, the value resulting from an arithmetic operation would not always yield the exact result due to floating point precision limitations. For example, $5.8 + 6.6 = 12.3999...$ instead of $12.4$. This issue has caused a significant performance hit of about 1.5 points for both F1 and EM and was fixed by simply rounding numbers to 5 decimal places, assuming that no answer requires a greater precision. Regarding number extraction, we noticed that some numeric entities, required in order to produce a correct answer, weren't being extracted from the passage. Examples include ordinals (121st, 189th) and some \"per-\" units (1,580.7/km2, 1050.95/month). generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "5\n",
      "Question 1: What was one issue with the HTML entities in the original dataset?\n",
      "\n",
      "Answer 1: The raw dataset included almost a thousand HTML entities that did not get parsed properly, resulting in things like \"&#160;\" instead of a simple space.\n",
      "Question : for the text The raw dataset included almost a thousand of HTML entities that did not get parsed properly, e.g \"&#160;\" instead of a simple space. In addition, we fixed some quirks that were introduced by the original Wikipedia parsing method. For example, when encountering a reference to an external source that included a specific page from that reference, the original parser ended up introducing a redundant \":<PAGE NUMBER>\" into the parsed text. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "6\n",
      "Question 1: Who founded Microsoft and what was their role in the company?\n",
      "\n",
      "Answer 1: The founder of Microsoft was Bill Gates, who served as its CEO for many years.\n",
      "Question : for the text The training dataset contains multi-span questions with answers that are clearly incorrect, with examples shown in Table TABREF22. In order to mitigate this, we applied an answer-cleaning technique using a pretrained Named Entity Recognition (NER) model BIBREF12 in the following manner: (1) Pre-define question prefixes whose answer spans are expected to contain only a specific entity type and filter the matching questions. (2) For a given answer of a filtered question, remove any span that does not contain at least one token of the expected type, where the types are determined by applying the NER model on the passage. For example, if a question starts with \"who scored\", we expect that any valid span will include a person entity ($\\mathtt {PER}$). By applying such rules, we discovered that at least 3% of the multi-span questions in the training dataset included incorrect spans. As our analysis of prefixes wasn't exhaustive, we believe that this method could yield further gains. Table TABREF22 shows a few of our cleaning method results, where we perfectly clean the first two questions, and partially clean a third question. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "7\n",
      "Question 1: What is the passage span head in NAQANet used for?\n",
      "Answer 1: The passage span head in NAQANet is designed for producing answers that consist of a single span from the passage, dealing with the same type of questions introduced in SQuAD.\n",
      "Question : for the text Numerically-aware QANet (NAQANet) BIBREF3 was the model released with DROP. It uses QANET BIBREF5, at the time the best-performing published model on SQuAD 1.1 BIBREF0 (without data augmentation or pretraining), as the encoder. On top of QANET, NAQANet adds four different output layers, which we refer to as \"heads\". Each of these heads is designed to tackle a specific question type from DROP, where these types where identified by DROP's authors post-creation of the dataset. These four heads are (1) Passage span head, designed for producing answers that consist of a single span from the passage. This head deals with the type of questions already introduced in SQuAD. (2) Question span head, for answers that consist of a single span from the question. (3) Arithmetic head, for answers that require adding or subtracting numbers from the passage. (4) Count head, for answers that require counting and sorting entities from the text. In addition, to determine which head should be used to predict an answer, a 4-way categorical variable, as per the number of heads, is trained. We denote this categorical variable as the \"head predictor\"..Numerically-aware BERT (NABERT+) BIBREF6 introduced two main improvements over NAQANET. The first was to replace the QANET encoder with BERT. This change alone resulted in an absolute improvement of more than eight points in both EM and F1 metrics. The second improvement was to the arithmetic head, consisting of the addition of \"standard numbers\" and \"templates\". Standard numbers were predefined numbers which were added as additional inputs to the arithmetic head, regardless of their occurrence in the passage. Templates were an attempt to enrich the head's arithmetic capabilities, by adding the ability of doing simple multiplications and divisions between up to three numbers..MTMSN BIBREF4 is the first, and only model so far, that specifically tried to tackle the multi-span questions of DROP. Their approach consisted of two parts. The first was to train a dedicated categorical variable to predict the number of spans to extract. The second was to generalize the single-span head method of extracting a span, by utilizing the non-maximum suppression (NMS) algorithm BIBREF7 to find the most probable set of non-overlapping spans. The number of spans to extract was determined by the aforementioned categorical variable..Additionally, MTMSN introduced two new other, non span-related, components. The first was a new \"negation\" head, meant to deal with questions deemed as requiring logical negation (e.g. \"How many percent were not German?\"). The second was improving the arithmetic head by using beam search to re-rank candidate arithmetic expressions. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "8\n",
      "Question 1: How did NER span cleaning affect the performance of multi-span questions?\n",
      "Answer 1: NER span cleaning improved the performance of multi-span questions by 5.4 EM.\n",
      "Question : for the text In order to analyze the effect of each of our changes, we conduct ablation studies on the development set, depicted in Table TABREF26..Not using the simple preprocessing from Section SECREF17 resulted in a 2.5 point decrease in both EM and F1. The numeric questions were the most affected, with their performance dropping by 3.5 points. Given that number questions make up about 61% of the dataset, we can deduce that our improved number handling is responsible for about a 2.1 point gain, while the rest could be be attributed to the improved Wikipedia parsing..Although NER span cleaning (Section SECREF23) affected only 3% of the multi-span questions, it provided a solid improvement of 5.4 EM in multi-span questions and 1.5 EM in single-span questions. The single-span improvement is probably due to the combination of better multi-span head learning as a result of fixing multi-span questions and the fact that the multi-span head can answer single-span questions as well..Not using the single-span heads results in a slight drop in multi-span performance, and a noticeable drop in single-span performance. However when performing the same comparison between our large models (see Table TABREF24), this performance gap becomes significantly smaller..As expected, not using the multi-span head causes the multi-span performance to plummet. Note that for this ablation test the single-span heads were permitted to train on multi-span questions..Compared to using greedy decoding in the prediction of multi-span questions, using beam search results in a small improvement. We used a beam with of 5, and didn't perform extensive tuning of the beam width. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "9\n",
      "Question 1: Did the large models perform better than the base models according to the table?\n",
      "Answer 1: Yes, the large models demonstrated a significant improvement across all metrics compared to the base models, as shown in Table TABREF24.\n",
      "Question : for the text Table TABREF24 shows the results on DROP's development set. Compared to our base models, our large models exhibit a substantial improvement across all metrics. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "10\n",
      "Question 1: How does our best model compare to MTMSNlarge in terms of EM and F1?\n",
      "\n",
      "Answer 1: Our best model, large-squad, exhibits a huge improvement of 29.7 in EM and 15.1 in F1 compared to MTMSNlarge.\n",
      "Question : for the text Notice that different BERTlarge models were used, so the comparison is less direct. Overall, our large models exhibits similar results to those of MTMSNlarge..For multi-span questions we achieve a significantly better performance. While a breakdown of metrics was only available for MTMSNlarge, notice that even when comparing these metrics to our base model, we still achieve a 12.2 absolute improvement in EM, and a 2.3 improvement in F1. All that, while keeping in mind we compare a base model to a large model (for reference, note the 8 point improvement between MTMSNbase and MTMSNlarge in both EM and F1). Our best model, large-squad, exhibits a huge improvement of 29.7 in EM and 15.1 in F1 compared to MTMSNlarge..When comparing single-span performance, our best model exhibits slightly better results, but it should be noted that it retains the single-span heads from NABERT+, while in MTMSN they have one head to predict both single-span and multi-span answers. For a fairer comparison, we trained our model with the single-span heads removed, where our multi-span head remained the only head aimed for handling span questions. With this no-single-span-heads setting, while our multi-span performance even improved a bit, our single-span performance suffered a slight drop, ending up trailing by 0.8 in EM and 0.6 in F1 compared to MTMSN. Therefore, it could prove beneficial to try and analyze the reasons behind each model's (ours and MTMSN) relative advantages, and perhaps try to combine them into a more holistic approach of tackling span questions. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "11\n",
      "Question 1: Did the base model perform better than the NABERT+ baseline in every metric?\n",
      "\n",
      "Answer 1: Yes, according to the text, the base model surpassed the NABERT+ baseline in every metric.\n",
      "Question : for the text We can see that our base model surpasses the NABERT+ baseline in every metric. The major improvement in multi-span performance was expected, as our multi-span head was introduced specifically to tackle this type of questions. For the other types, most of the improvement came from better preprocessing. A more detailed discussion could be found in Section (SECREF36). generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "12\n",
      "Question 1: According to the results on DROP's test set, was the model developed by the team the best overall?\n",
      "Answer 1: Yes, the model developed by the team was the best overall as of the time of writing, not just on multi-span questions, according to the results shown in Table TABREF25.\n",
      "Question : for the text Table TABREF25 shows the results on DROP's test set, with our model being the best overall as of the time of writing, and not just on multi-span questions. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "13\n",
      "Question 1: What was the base model used for the implementation?\n",
      "\n",
      "Answer 1: The base model used for the implementation was bert-base-uncased.\n",
      "Question : for the text The starting point for our implementation was the NABERT+ model, which in turn was based on allenai's NAQANET. Our implementation can be found on GitHub. All three models utilize the allennlp framework. The pretrained BERT models were supplied by huggingface. For our base model we used bert-base-uncased. For our large models we used the standard bert-large-uncased-whole-word-masking and the squad fine-tuned bert-large-uncased- whole-word-masking-finetuned-squad..Due to limited computational resources, we did not perform any hyperparameter searching. We preferred to focus our efforts on the ablation studies, in hope to gain further insights on the effect of the components that we ourselves introduced. For ease of performance comparison, we followed NABERT+'s training settings: we used the BERT Adam optimizer from huggingface with default settings and a learning rate of $1e^{-5}$. The only difference was that we used a batch size of 12. We trained our base model for 20 epochs. For the large models we used a batch size of 3 with a learning rate of $5e^{-6}$ and trained for 5 epochs, except for the model without the single-span heads that was trained with a batch size of 2 for 7 epochs. F1 was used as our validation metric. All models were trained on a single GPU with 12-16GB of memory. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "14\n",
      "Question 1: Who supported the work described in the text?\n",
      "\n",
      "Answer 1: The work was supported in part by The Israeli Science Foundation (grant number 1555/15).\n",
      "Question : for the text The work was supported in part by The Israeli Science Foundation (grant number 1555/15). generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "15\n",
      "Question 1: What is the purpose of training Aspect-based Sentiment Classifier (ABSC) using sentence-level sentiment signals?\n",
      "\n",
      "Answer 1: The purpose of training ABSC using sentence-level sentiment signals is to demonstrate the procedure given above.\n",
      "Question : for the text We demonstrate the procedure given above by training Aspect-based Sentiment Classifier (ABSC) using sentence-level sentiment signals. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "16\n",
      "Question 1: What is the aspect-based sentiment classification (ABSC) task, and what does it involve?\n",
      "\n",
      "Answer 1: The ABSC task involves determining the sentiment, either positive, negative or neutral, expressed towards a particular aspect in a given sentence. The aspect may correspond to a sub-sequence of the sentence, and the sentiment label is assigned to the aspect. This task is defined in SemEval-2015 and SemEval-2016 shared tasks, and requires identifying the sentiment towards one or more aspects in a sentence.\n",
      "Question : for the text In the aspect-based sentiment classification (ABSC) task, we are given a sentence and an aspect, and need to determine the sentiment that is expressed towards the aspect. For example the sentence “Excellent food, although the interior could use some help.“ has two aspects: food and interior, a positive sentiment is expressed about the food, but a negative sentiment is expressed about the interior. A sentence INLINEFORM0 , may contain 0 or more aspects INLINEFORM1 , where each aspect corresponds to a sub-sequence of the original sentence, and has an associated sentiment label (Neg, Pos, or Neu). Concretely, we follow the task definition in the SemEval-2015 and SemEval-2016 shared tasks BIBREF23 , BIBREF24 , in which the relevant aspects are given and the task focuses on finding the sentiment label of the aspects..While sentence-level sentiment labels are relatively easy to obtain, aspect-level annotation are much more scarce, as demonstrated in the small datasets of the SemEval shared tasks. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "17\n",
      "What is the neural architecture used for both sentence classification and fragment classification?\n",
      "The same BiLSTM based neural architecture is used for both sentence classification and fragment classification.\n",
      "Question : for the text We model the ABSC problem by associating each (sentence,aspect) pair with a sentence-fragment, and constructing a neural classifier from fragments to sentiment labels. We heuristically decompose a sentence into fragments. We use the same BiLSTM based neural architecture for both sentence classification and fragment classification..We now describe the procedure we use to associate a sentence fragment with each (sentence,aspect) pairs. The shared tasks data associates each aspect with a pivot-phrase INLINEFORM0 , where pivot phrase INLINEFORM1 is defined as a pre-determined sequence of words that is contained within the sentence. For a sentence INLINEFORM2 , a set of pivot phrases INLINEFORM3 and a specific pivot phrase INLINEFORM4 , we consult the constituency parse tree of INLINEFORM5 and look for tree nodes that satisfy the following conditions:.The node governs the desired pivot phrase INLINEFORM0 ..The node governs either a verb (VB, VBD, VBN, VBG, VBP, VBZ) or an adjective (JJ, JJR, JJS), which is different than any INLINEFORM0 ..The node governs a minimal number of pivot phrases from INLINEFORM0 , ideally only INLINEFORM1 ..We then select the highest node in the tree that satisfies all conditions. The span governed by this node is taken as the fragment associated with aspect INLINEFORM0 . The decomposition procedure is demonstrated in Figure FIGREF22 ..When aspect-level information is given, we take the pivot-phrases to be the requested aspects. When aspect-level information is not available, we take each noun in the sentence to be a pivot-phrase..Our classification model is a simple 1-layer BiLSTM encoder (a concatenation of the last states of a forward and a backward running LSTMs) followed by a linear-predictor. The encoder is fed either a complete sentence or a sentence fragment. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "18\n",
      "Question 1: What is the proposed transfer learning method based on for training aspect-based sentiment classifiers? \n",
      "\n",
      "Answer 1: The proposed transfer learning method is based on expectation regularization (XR) and is effective in situations where there is a conditional relationship between the labels of a source task with supervision and a target task without supervision.\n",
      "Question : for the text We presented a transfer learning method based on expectation regularization (XR), and demonstrated its effectiveness for training aspect-based sentiment classifiers using sentence-level supervision. The method achieves state-of-the-art results for the task, and is also effective for improving on top of a strong pre-trained Bert model. The proposed method provides an additional data-efficient tool in the modeling arsenal, which can be applied on its own or together with another training method, in situations where there is a conditional relations between the labels of a source task for which we have supervision, and a target task for which we don't..While we demonstrated the approach on the sentiment domain, the required conditional dependence between task labels is present in many situations. Other possible application of the method includes training language identification of tweets given geo-location supervision (knowing the geographical region gives a prior on languages spoken), training predictors for renal failure from textual medical records given classifier for diabetes (there is a strong correlation between the two conditions), training a political affiliation classifier from social media tweets based on age-group classifiers, zip-code information, or social-status classifiers (there are known correlations between all of these to political affiliation), training hate-speech detection based on emotion detection, and so on. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "19\n",
      "What is the main objective of Expectation Regularization (XR) in training a model for a target task based on the output of a related source-task classifier?\n",
      "\n",
      "The main objective of XR is to move from a fully supervised situation where each data-point has an associated label, to a setup where sets of data points are associated with corresponding label proportions. In using XR to train a target task, the aim is to fit the label distribution over the sets based on the output of a related source-task classifier, rather than using fully labeled data.\n",
      "Question : for the text Expectation Regularization (XR) BIBREF0 is a lightly supervised learning method, in which the model is trained to fit the conditional probabilities of labels given features. In the context of NLP, XR was used by BIBREF20 to train twitter-user attribute prediction using hundreds of noisy distributional expectations based on census demographics. Here, we suggest using XR to train a target task (aspect-level sentiment) based on the output of a related source-task classifier (sentence-level sentiment)..The main idea of XR is moving from a fully supervised situation in which each data-point INLINEFORM0 has an associated label INLINEFORM1 , to a setup in which sets of data points INLINEFORM2 are associated with corresponding label proportions INLINEFORM3 over that set..Formally, let INLINEFORM0 be a set of data points, INLINEFORM1 be a set of INLINEFORM2 class labels, INLINEFORM3 be a set of sets where INLINEFORM4 for every INLINEFORM5 , and let INLINEFORM6 be the label distribution of set INLINEFORM7 . For example, INLINEFORM8 would indicate that 70% of data points in INLINEFORM9 are expected to have class 0, 20% are expected to have class 1 and 10% are expected to have class 2. Let INLINEFORM10 be a parameterized function with parameters INLINEFORM11 from INLINEFORM12 to a vector of conditional probabilities over labels in INLINEFORM13 . We write INLINEFORM14 to denote the probability assigned to the INLINEFORM15 th event (the conditional probability of INLINEFORM16 given INLINEFORM17 )..A typically objective when training on fully labeled data of INLINEFORM0 pairs is to maximize likelihood of labeled data using the cross entropy loss, INLINEFORM1 .Instead, in XR our data comes in the form of pairs INLINEFORM0 of sets and their corresponding expected label proportions, and we aim to optimize INLINEFORM1 to fit the label distribution INLINEFORM2 over INLINEFORM3 , for all INLINEFORM4 ..As counting the number of predicted class labels over a set INLINEFORM0 leads to a non-differentiable objective, BIBREF0 suggest to relax it and use instead the model's posterior distribution INLINEFORM1 over the set: DISPLAYFORM0 DISPLAYFORM1 .where INLINEFORM0 indicates the INLINEFORM1 th entry in INLINEFORM2 . Then, we would like to set INLINEFORM3 such that INLINEFORM4 and INLINEFORM5 are close. BIBREF0 suggest to use KL-divergence for this. KL-divergence is composed of two parts: INLINEFORM6 INLINEFORM7 .Since INLINEFORM0 is constant, we only need to minimize INLINEFORM1 , therefore the loss function becomes: DISPLAYFORM0 .Notice that computing INLINEFORM0 requires summation over INLINEFORM1 for the entire set INLINEFORM2 , which can be prohibitive. We present batched approximation (Section SECREF19 ) to overcome this.. BIBREF0 find that XR might find a degenerate solution. For example, in a three class classification task, where INLINEFORM0 , it might find a solution such that INLINEFORM1 for every instance, as a result, every instance will be classified the same. To avoid this, BIBREF0 suggest to penalize flat distributions by using a temperature coefficient T likewise: DISPLAYFORM0 .Where z is a feature vector and W and b are the linear classifier parameters. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "20\n",
      "What effect does increasing the amount of unlabeled data have on the XR training?\n",
      "\n",
      "Increasing the amount of unlabeled data improves the results of XR training. For instance, an unannotated corpus of INLINEFORM0 sentences is sufficient to surpass the results of the INLINEFORM1 sentence-level trained classifier, and more unannotated data further improves the results.\n",
      "Question : for the text In each experiment in this section we estimate the proportions using the SemEval-2015 train set..How does the XR training scale with the amount of unlabeled data? Figure FIGREF54 a shows the macro-F1 scores on the entire SemEval-2016 dataset, with different unlabeled corpus sizes (measured in number of sentences). An unannotated corpus of INLINEFORM0 sentences is sufficient to surpass the results of the INLINEFORM1 sentence-level trained classifier, and more unannotated data further improves the results..Our method requires a sentence level classifier INLINEFORM0 to label both the target-task corpus and the unlabeled corpus. How does the quality of this classifier affect the overall XR training? We vary the amount of supervision used to train INLINEFORM1 from 0 sentences (assigning the same label to all sentences), to 100, 1000, 5000 and 10000 sentences. We again measure macro-F1 on the entire SemEval 2016 corpus..The results in Figure FIGREF54 b show that when using the prior distributions of aspects (0), the model struggles to learn from this signal, it learns mostly to predict the majority class, and hence reaches very low F1 scores of 35.28. The more data given to the sentence level classifier, the better the potential results will be when training with our method using the classifier labels, with a classifiers trained on 100,1000,5000 and 10000 labeled sentences, we get a F1 scores of 53.81, 58.84, 61.81, 65.58 respectively. Improvements in the source task classifier's quality clearly contribute to the target task accuracy..The Stochastic Batched XR algorithm (Algorithm SECREF12 ) samples a batch of INLINEFORM0 examples at each step to estimate the posterior label distribution used in the loss computation. How does the size of INLINEFORM1 affect the results? We use INLINEFORM2 fragments in our main experiments, but smaller values of INLINEFORM3 reduce GPU memory load and may train better in practice. We tested our method with varying values of INLINEFORM4 on a sample of INLINEFORM5 , using batches that are composed of fragments of 5, 25, 100, 450, 1000 and 4500 sentences. The results are shown in Figure FIGREF54 c. Setting INLINEFORM6 result in low scores. Setting INLINEFORM7 yields better F1 score but with high variance across runs. For INLINEFORM8 fragments the results begin to stabilize, we also see a slight decrease in F1-scores with larger batch sizes. We attribute this drop despite having better estimation of the gradients to the general trend of larger batch sizes being harder to train with stochastic gradient methods. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "21\n",
      "What is the proposed approach for transfer learning in tasks with correlated labels?\n",
      "The proposed approach is to use the XR framework for transfer learning, where the label information for task A is used to train a classifier for performing task B instead, without a direct supervision signal for task B. This allows for accurate aspect-level sentiment classification using only a medium-sized sentiment corpus with sentence-level labels and a large collection of un-annotated text from the same distribution. Additionally, the XR loss can be used on top of pre-trained models or fine-tuned to small-scale annotated data.\n",
      "Question : for the text Data annotation is a key bottleneck in many data driven algorithms. Specifically, deep learning models, which became a prominent tool in many data driven tasks in recent years, require large datasets to work well. However, many tasks require manual annotations which are relatively hard to obtain at scale. An attractive alternative is lightly supervised learning BIBREF1 , BIBREF2 , BIBREF3 , BIBREF4 , BIBREF5 , BIBREF6 , BIBREF7 , BIBREF8 , in which the objective function is supplemented by a set of domain-specific soft-constraints over the model's predictions on unlabeled data. For example, in label regularization BIBREF0 the model is trained to fit the true label proportions of an unlabeled dataset. Label regularization is special case of expectation regularization (XR) BIBREF0 , in which the model is trained to fit the conditional probabilities of labels given features..In this work we consider the case of correlated tasks, in the sense that knowing the labels for task A provides information on the expected label composition of task B. We demonstrate the approach using sentence-level and aspect-level sentiment analysis, which we use as a running example: knowing that a sentence has positive sentiment label (task A), we can expect that most aspects within this sentence (task B) will also have positive label. While this expectation may be noisy on the individual example level, it holds well in aggregate: given a set of positively-labeled sentences, we can robustly estimate the proportion of positively-labeled aspects within this set. For example, in a random set of positive sentences, we expect to find 90% positive aspects, while in a set of negative sentences, we expect to find 70% negative aspects. These proportions can be easily either guessed or estimated from a small set..We propose a novel application of the XR framework for transfer learning in this setup. We present an algorithm (Sec SECREF12 ) that, given a corpus labeled for task A (sentence-level sentiment), learns a classifier for performing task B (aspect-level sentiment) instead, without a direct supervision signal for task B. We note that the label information for task A is only used at training time. Furthermore, due to the stochastic nature of the estimation, the task A labels need not be fully accurate, allowing us to make use of noisy predictions which are assigned by an automatic classifier (Sections SECREF12 and SECREF4 ). In other words, given a medium-sized sentiment corpus with sentence-level labels, and a large collection of un-annotated text from the same distribution, we can train an accurate aspect-level sentiment classifier..The XR loss allows us to use task A labels for training task B predictors. This ability seamlessly integrates into other semi-supervised schemes: we can use the XR loss on top of a pre-trained model to fine-tune the pre-trained representation to the target task, and we can also take the model trained using XR loss and plentiful data and fine-tune it to the target task using the available small-scale annotated data. In Section SECREF56 we explore these options and show that our XR framework improves the results also when applied on top of a pre-trained Bert-based model BIBREF9 ..Finally, to make the XR framework applicable to large-scale deep-learning setups, we propose a stochastic batched approximation procedure (Section SECREF19 ). Source code is available at https://github.com/MatanBN/XRTransfer. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "22\n",
      "Question 1: What is lightly supervised learning?\n",
      "\n",
      "Answer 1: Lightly supervised learning is a method of training classifiers using a set of domain-specific soft-constraints over the model's predictions on unlabeled data. It is an effective way to supplement small annotated datasets.\n",
      "Question : for the text An effective way to supplement small annotated datasets is to use lightly supervised learning, in which the objective function is supplemented by a set of domain-specific soft-constraints over the model's predictions on unlabeled data. Previous work in lightly-supervised learning focused on training classifiers by using prior knowledge of label proportions BIBREF2 , BIBREF3 , BIBREF10 , BIBREF0 , BIBREF11 , BIBREF12 , BIBREF7 , BIBREF13 , BIBREF14 , BIBREF15 , BIBREF16 , BIBREF8 or prior knowledge of features label associations BIBREF1 , BIBREF17 , BIBREF18 , BIBREF19 , BIBREF20 . In the context of NLP, BIBREF17 suggested to use distributional similarities of words to train sequence models for part-of-speech tagging and a classified ads information extraction task. BIBREF19 used background lexical information in terms of word-class associations to train a sentiment classifier. BIBREF21 , BIBREF22 suggested to exploit the bilingual correlations between a resource rich language and a resource poor language to train a classifier for the resource poor language in a lightly supervised manner. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "23\n",
      "What is the difference between BiLSTM-XR-Dev and BiLSTM-XR? \n",
      "\n",
      "BiLSTM-XR-Dev uses validation set of SemEval-2015 to estimate label proportions for each sentence-level label, while BiLSTM-XR estimates the proportion based on the entire aspect-based training set.\n",
      "Question : for the text Table TABREF44 compares these baselines to three XR conditions..The first condition, BiLSTM-XR-Dev, performs XR training on the automatically-labeled sentence-level dataset. The only access it has to aspect-level annotation is for estimating the proportions of labels for each sentence-level label, which is done based on the validation set of SemEval-2015 (i.e., 20% of the train set). The XR setting is very effective: without using any in-task data, this model already surpasses all other models, both supervised and semi-supervised, except for the BIBREF35 , BIBREF34 models which achieve higher F1 scores. We note that in contrast to XR, the competing models have complete access to the supervised aspect-based labels. The second condition, BiLSTM-XR, is similar but now the model is allowed to estimate the conditional label proportions based on the entire aspect-based training set (the classifier still does not have direct access to the labels beyond the aggregate proportion information). This improves results further, showing the importance of accurately estimating the proportions. Finally, in BiLSTM-XR+Finetuning, we follow the XR training with fully supervised fine-tuning on the small labeled dataset, using the attention-based model of BIBREF35 . This achieves the best results, and surpasses also the semi-supervised BIBREF35 baseline on accuracy, and matching it on F1..We report significance tests for the robustness of the method under random parameter initialization. Our reported numbers are averaged over five random initialization. Since the datasets are unbalanced w.r.t the label distribution, we report both accuracy and macro-F1..The XR training is also more stable than the other semi-supervised baselines, achieving substantially lower standard deviations across different runs. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "24\n",
      "What pre-training methods were compared in the experiment? \n",
      "\n",
      "The experiment compared pre-training by training the BiLSTM model to predict noisy sentence-level predictions, and using the pre-trained Bert representation.\n",
      "Question : for the text The XR training can be performed also over pre-trained representations. We experiment with two pre-training methods: (1) pre-training by training the BiLSTM model to predict the noisy sentence-level predictions. (2) Using the pre-trained Bert representation BIBREF9 . For (1), we compare the effect of pre-train on unlabeled corpora of sizes of INLINEFORM0 , INLINEFORM1 and INLINEFORM2 sentences. Results in Figure FIGREF54 d show that this form of pre-training is effective for smaller unlabeled corpora but evens out for larger ones..For the Bert experiments, we experiment with the Bert-base model with INLINEFORM1 sets, 30 epochs for XR training or sentence level fine-tuning and 15 epochs for aspect based fine-tuning, on each training method we evaluated the model on the dev set after each epoch and the best model was chosen. We compare the following setups:.-Bert INLINEFORM0 Aspect Based Finetuning: pretrained bert model finetuned to the aspect based task..-Bert INLINEFORM0 : A pretrained bert model finetuned to the sentence level task on the INLINEFORM1 sentences, and tested by predicting fragment-level sentiment..-Bert INLINEFORM0 INLINEFORM1 INLINEFORM2 Aspect Based Finetuning: pretrained bert model finetuned to the sentence level task, and finetuned again to the aspect based one..-Bert INLINEFORM0 XR: pretrained bert model followed by XR training using our method..-Bert INLINEFORM0 XR INLINEFORM1 Aspect Based Finetuning: pretrained bert followed by XR training and then fine-tuned to the aspect level task..The results are presented in Table TABREF55 . As before, aspect-based fine-tuning is beneficial for both SemEval-16 and SemEval-15. Training a BiLSTM with XR surpasses pre-trained bert models and using XR training on top of the pre-trained Bert models substantially increases the results even further. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "25\n",
      "Question 1: What is the relationship between sentence-level sentiment and aspect-level sentiment?\n",
      "\n",
      "Answer 1: While sentence-level sentiment does not necessarily determine the sentiment of individual aspects, it is predictive of the proportion of sentiment labels of the fragments within a sentence. This means that positively-labeled sentences are likely to have more positive aspects and fewer negative ones, whereas negatively-labeled sentences are likely to have more negative aspects and fewer positive ones. This relationship is ideal for performing XR training.\n",
      "Question : for the text We observe that while the sentence-level sentiment does not determine the sentiment of individual aspects (a positive sentence may contain negative remarks about some aspects), it is very predictive of the proportion of sentiment labels of the fragments within a sentence. Positively labeled sentences are likely to have more positive aspects and fewer negative ones, and vice-versa for negatively-labeled sentences. While these proportions may vary on the individual sentence level, we expect them to be stable when aggregating fragments from several sentences: when considering a large enough sample of fragments that all come from positively labeled sentences, we expect the different samples to have roughly similar label proportions to each other. This situation is idealy suited for performing XR training, as described in section SECREF12 ..The application to ABSC is almost straightforward, but is complicated a bit by the decomposition of sentences into fragments: each sentence level decision now corresponds to multiple fragment-level decisions. Thus, we apply the sentence-level (task A) classifier INLINEFORM0 on the aspect-level corpus INLINEFORM1 by applying it on the sentence level and then associating the predicted sentence labels with each of the fragments, resulting in fragment-level labeling. Similarly, when we apply INLINEFORM2 to the unlabeled data INLINEFORM3 we again do it at the sentence level, but the sets INLINEFORM4 are composed of fragments, not sentences: INLINEFORM5 .We then apply algorithm SECREF12 as is: at each step of training we sample a source label INLINEFORM0 Pos,Neg,Neu INLINEFORM1 , sample INLINEFORM2 fragments from INLINEFORM3 , and use the XR loss to fit the expected fragment-label proportions over these INLINEFORM4 fragments to INLINEFORM5 . Figure FIGREF21 illustrates the procedure. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "26\n",
      "What is the proposed solution to the computational challenge of the summation over the entire set of examples in equation ( EQREF5 )?\n",
      "\n",
      "The proposed solution is a stochastic batched approximation in which random subsets of the constraint set are used to match the expected label posterior distribution instead of the full constraint set.\n",
      "Question : for the text  BIBREF0 and following work take the base classifier INLINEFORM0 to be a logistic regression classifier, for which they manually derive gradients for the XR loss and train with LBFGs BIBREF25 . However, nothing precludes us from using an arbitrary neural network instead, as long as it culminates in a softmax layer..One complicating factor is that the computation of INLINEFORM0 in equation ( EQREF5 ) requires a summation over INLINEFORM1 for the entire set INLINEFORM2 , which in our setup may contain hundreds of thousands of examples, making gradient computation and optimization impractical. We instead proposed a stochastic batched approximation in which, instead of requiring that the full constraint set INLINEFORM3 will match the expected label posterior distribution, we require that sufficiently large random subsets of it will match the distribution. At each training step we compute the loss and update the gradient with respect to a different random subset. Specifically, in each training step we sample a random pair INLINEFORM4 , sample a random subset INLINEFORM5 of INLINEFORM6 of size INLINEFORM7 , and compute the local XR loss of set INLINEFORM8 : DISPLAYFORM0 .where INLINEFORM0 is computed by summing over the elements of INLINEFORM1 rather than of INLINEFORM2 in equations ( EQREF5 –2). The stochastic batched XR training algorithm is given in Algorithm SECREF12 . For large enough INLINEFORM3 , the expected label distribution of the subset is the same as that of the complete set. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "27\n",
      "Question 1: What is the purpose of applying Algorithm SECREF12 in the training procedure?\n",
      "\n",
      "Answer 1: The purpose of applying Algorithm SECREF12 is to train a classifier for the target task using input pairs and the XR loss. This method uses expected label proportions over the target task given predicted labels of the source task to train a target-class classifier.\n",
      "Question : for the text [t!] Inputs: A dataset INLINEFORM0 , batch size INLINEFORM1 , differentiable classifier INLINEFORM2 [H] not converged INLINEFORM3 random( INLINEFORM4 ) INLINEFORM5 random-choice( INLINEFORM6 , INLINEFORM7 ) INLINEFORM8 INLINEFORM9 INLINEFORM10 INLINEFORM11 Compute loss INLINEFORM12 (eq (4)) Compute gradients and update INLINEFORM13 INLINEFORM14 Stochastic Batched XR.Consider two classification tasks over a shared input space, a source task INLINEFORM0 from INLINEFORM1 to INLINEFORM2 and a target task INLINEFORM3 from INLINEFORM4 to INLINEFORM5 , which are related through a conditional distribution INLINEFORM6 . In other words, a labeling decision for task INLINEFORM7 induces an expected label distribution over the task INLINEFORM8 . For a set of datapoints INLINEFORM9 that share a source label INLINEFORM10 , we expect to see a target label distribution of INLINEFORM11 ..Given a large unlabeled dataset INLINEFORM0 , a small labeled dataset for the target task INLINEFORM1 , classifier INLINEFORM2 (or sufficient training data to train one) for the source task, we wish to use INLINEFORM3 and INLINEFORM4 to train a good classifier INLINEFORM5 for the target task. This can be achieved using the following procedure..Apply INLINEFORM0 to INLINEFORM1 , resulting in a noisy source-side labels INLINEFORM2 for the target task..Estimate the conditional probability INLINEFORM0 table using MLE estimates over INLINEFORM1 INLINEFORM2 .where INLINEFORM0 is a counting function over INLINEFORM1 ..Apply INLINEFORM0 to the unlabeled data INLINEFORM1 resulting in labels INLINEFORM2 . Split INLINEFORM3 into INLINEFORM4 sets INLINEFORM5 according to the labeling induced by INLINEFORM6 : INLINEFORM7 .Use Algorithm SECREF12 to train a classifier for the target task using input pairs INLINEFORM0 and the XR loss..In words, by using XR training, we use the expected label proportions over the target task given predicted labels of the source task, to train a target-class classifier. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "28\n",
      "Question 1: From where has MS received funding?\n",
      "Answer 1: MS has received funding from the European Research Council (ERC) under the European Union's Horizon 2020 research and innovation programme (grant agreement No 771113).\n",
      "Question : for the text MS has received funding from the European Research Council (ERC) under the European Union's Horizon 2020 research and innovation programme (grant agreement No 771113). generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "29\n",
      "Question 1: Which type of attention has dominated previous SIGMORPHON shared tasks?\n",
      "\n",
      "Answer 1: Neural sequence-to-sequence models with soft attention have dominated previous SIGMORPHON shared tasks.\n",
      "Question : for the text We include four neural sequence-to-sequence models mapping lemma into inflected word forms: soft attention BIBREF13, non-monotonic hard attention BIBREF14, monotonic hard attention and a variant with offset-based transition distribution BIBREF15. Neural sequence-to-sequence models with soft attention BIBREF13 have dominated previous SIGMORPHON shared tasks BIBREF16. BIBREF14 instead models the alignment between characters in the lemma and the inflected word form explicitly with hard attention and learns this alignment and transduction jointly. BIBREF15 shows that enforcing strict monotonicity with hard attention is beneficial in tasks such as morphological inflection where the transduction is mostly monotonic. The encoder is a biLSTM while the decoder is a left-to-right LSTM. All models use multiplicative attention and have roughly the same number of parameters. In the model, a morphological tag is fed to the decoder along with target character embeddings to guide the decoding. During the training of the hard attention model, dynamic programming is applied to marginalize all latent alignments exactly. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "30\n",
      "Question 1: How does the neural model in BIBREF18 address exposure bias?\n",
      "\n",
      "Answer 1: The model in BIBREF18 accounts for exposure bias through the application of maximum likelihood (MLE) and by using jackknifing to expose the lemmatizer to the errors made by the tagger model during training.\n",
      "Question : for the text BIBREF18: This is a state-of-the-art neural model that also performs joint morphological tagging and lemmatization, but also accounts for the exposure bias with the application of maximum likelihood (MLE). The model stitches the tagger and lemmatizer together with the use of jackknifing BIBREF19 to expose the lemmatizer to the errors made by the tagger model during training. The morphological tagger is based on a character-level biLSTM embedder that produces the embedding for a word, and a word-level biLSTM tagger that predicts a morphological tag sequence for each word in the sentence. The lemmatizer is a neural sequence-to-sequence model BIBREF15 that uses the decoded morphological tag sequence from the tagger as an additional attribute. The model uses hard monotonic attention instead of standard soft attention, along with a dynamic programming based training scheme. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "31\n",
      "Question 1: What kind of model is the Lemming model and what does it do?\n",
      "\n",
      "Answer 1: The Lemming model is a log-linear model that performs joint morphological tagging and lemmatization.\n",
      "Question : for the text BIBREF17: The Lemming model is a log-linear model that performs joint morphological tagging and lemmatization. The model is globally normalized with the use of a second order linear-chain CRF. To efficiently calculate the partition function, the choice of lemmata are pruned with the use of pre-extracted edit trees. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "32\n",
      "What type of evaluation did the SIGMORPHON 2019 shared task provide on systems for inflection and analysis? \n",
      "\n",
      "The SIGMORPHON 2019 shared task provided a type-level evaluation on 100 language pairs in 79 languages and a token-level evaluation on 107 treebanks in 66 languages for systems for inflection and analysis.\n",
      "Question : for the text The SIGMORPHON 2019 shared task provided a type-level evaluation on 100 language pairs in 79 languages and a token-level evaluation on 107 treebanks in 66 languages, of systems for inflection and analysis. On task 1 (low-resource inflection with cross-lingual transfer), 14 systems were submitted, while on task 2 (lemmatization and morphological feature analysis), 16 systems were submitted. All used neural network models, completing a trend in past years' shared tasks and other recent work on morphology..In task 1, gains from cross-lingual training were generally modest, with gains positively correlating with the linguistic similarity of the two languages..In the second task, several methods were implemented by multiple groups, with the most successful systems implementing variations of multi-headed attention, multi-level encoding, multiple decoders, and ELMo and BERT contextual embeddings..We have released the training, development, and test sets, and expect these datasets to provide a useful benchmark for future research into learning of inflectional morphology and string-to-string transduction. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "33\n",
      "Question 1: What is included in the basic data for each language?\n",
      "\n",
      "Answer 1: The basic data for each language consists of triples of the form (lemma, feature bundle, inflected form), where the first feature in the bundle always specifies the core part of speech (e.g., verb). All features in the bundle are coded according to the UniMorph Schema, a cross-linguistically consistent universal morphological feature set.\n",
      "Question : for the text For each language, the basic data consists of triples of the form (lemma, feature bundle, inflected form), as in tab:sub1data. The first feature in the bundle always specifies the core part of speech (e.g., verb). For each language pair, separate files contain the high- and low-resource training examples..All features in the bundle are coded according to the UniMorph Schema, a cross-linguistically consistent universal morphological feature set BIBREF8, BIBREF9. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "34\n",
      "Question 1: What procedure was used to extract tables in Wiktionary for each language?\n",
      "\n",
      "Answer 1: Tables were extracted using a template annotation procedure described in BIBREF10.\n",
      "Question : for the text For each of the Wiktionary languages, Wiktionary provides a number of tables, each of which specifies the full inflectional paradigm for a particular lemma. As in the previous iteration, tables were extracted using a template annotation procedure described in BIBREF10. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "35\n",
      "Question 1: From which source was the data extracted for all but four languages in the study?\n",
      "\n",
      "Answer 1: The data for all but four languages (Basque, Kurmanji, Murrinhpatha, and Sorani) was extracted from English Wiktionary, a large multi-lingual crowd-sourced dictionary with morphological paradigms for many lemmata.\n",
      "Question : for the text We presented data in 100 language pairs spanning 79 unique languages. Data for all but four languages (Basque, Kurmanji, Murrinhpatha, and Sorani) are extracted from English Wiktionary, a large multi-lingual crowd-sourced dictionary with morphological paradigms for many lemmata. 20 of the 100 language pairs are either distantly related or unrelated; this allows speculation into the relative importance of data quantity and linguistic relatedness. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "36\n",
      "Question 1: What changes were adopted to increase compatibility?\n",
      "Answer 1: Some changes were adopted to increase compatibility, including correcting some annotation errors created while scraping Wiktionary for the 2018 task and standardizing Romanian t-cedilla and t-comma to t-comma (as well as s-cedilla and s-comma).\n",
      "Question : for the text We further adopted some changes to increase compatibility. Namely, we corrected some annotation errors created while scraping Wiktionary for the 2018 task, and we standardized Romanian t-cedilla and t-comma to t-comma. (The same was done with s-cedilla and s-comma.) generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "37\n",
      "Question 1: How were the training, development, and test sets sampled for each language's collection of paradigms?\n",
      "\n",
      "Answer 1: The data were sampled in the same fashion as in 2018, and probability distributions were constructed over the (lemma, feature bundle, inflected form) triples in the full dataset. 12,000 triples were then sampled without replacement from this distribution, with the first 100 used as training data for low-resource settings and the first 10,000 used as high-resource training sets. The final 2000 were randomly shuffled and split in half to obtain development and test sets of 1000 forms each, with shuffling performed to ensure similarity between the two sets.\n",
      "Question : for the text From each language's collection of paradigms, we sampled the training, development, and test sets as in 2018. Crucially, while the data were sampled in the same fashion, the datasets are distinct from those used for the 2018 shared task..Our first step was to construct probability distributions over the (lemma, feature bundle, inflected form) triples in our full dataset. For each triple, we counted how many tokens the inflected form has in the February 2017 dump of Wikipedia for that language. To distribute the counts of an observed form over all the triples that have this token as its form, we follow the method used in the previous shared task BIBREF1, training a neural network on unambiguous forms to estimate the distribution over all, even ambiguous, forms. We then sampled 12,000 triples without replacement from this distribution. The first 100 were taken as training data for low-resource settings. The first 10,000 were used as high-resource training sets. As these sets are nested, the highest-count triples tend to appear in the smaller training sets..The final 2000 triples were randomly shuffled and then split in half to obtain development and test sets of 1000 forms each. The final shuffling was performed to ensure that the development set is similar to the test set. By contrast, the development and test sets tend to contain lower-count triples than the training set. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "38\n",
      "Question 1: What is the source of data for task 2?\n",
      "Answer 1: The data for task 2 come from the Universal Dependencies treebanks BIBREF11, which provides pre-defined training, development, and test splits and annotations in a unified annotation schema for morphosyntax and dependency relationships.\n",
      "Question : for the text Our data for task 2 come from the Universal Dependencies treebanks BIBREF11, which provides pre-defined training, development, and test splits and annotations in a unified annotation schema for morphosyntax and dependency relationships. Unlike the 2018 cloze task which used UD data, we require no manual data preparation and are able to leverage all 107 monolingual treebanks. As is typical, data are presented in CoNLL-U format, although we modify the morphological feature and lemma fields. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "39\n",
      "Question 1: What was done to the morphological annotations for the 2019 shared task?\n",
      "\n",
      "Answer 1: The morphological annotations for the 2019 shared task were converted to the UniMorph schema according to a deterministic mapping provided by BIBREF12, which increases agreement across languages. The part of speech was also moved into the bundle of morphological features.\n",
      "Question : for the text The morphological annotations for the 2019 shared task were converted to the UniMorph schema BIBREF10 according to BIBREF12, who provide a deterministic mapping that increases agreement across languages. This also moves the part of speech into the bundle of morphological features. We do not attempt to individually correct any errors in the UD source material. Further, some languages received additional pre-processing. In the Finnish data, we removed morpheme boundaries that were present in the lemmata (e.g., puhe#kieli $\\mapsto $ puhekieli `spoken+language'). Russian lemmata in the GSD treebank were presented in all uppercase; to match the 2018 shared task, we lowercased these. In development and test data, all fields except for form and index within the sentence were struck. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "40\n",
      "Question 1: What is an interesting future topic related to inflection mentioned in the text? \n",
      "\n",
      "Answer 1: Departing from orthographic representation and using more IPA-like representations, i.e. transductions over pronunciations, is an interesting future topic related to inflection mentioned in the text.\n",
      "Question : for the text In general, the application of typology to natural language processing BIBREF23, BIBREF24 provides an interesting avenue for multilinguality. Further, our shared task was designed to only leverage a single helper language, though many may exist with lexical or morphological overlap with the target language. Techniques like those of BIBREF25 may aid in designing universal inflection architectures. Neither task this year included unannotated monolingual corpora. Using such data is well-motivated from an L1-learning point of view, and may affect the performance of low-resource data settings..In the case of inflection an interesting future topic could involve departing from orthographic representation and using more IPA-like representations, i.e. transductions over pronunciations. Different languages, in particular those with idiosyncratic orthographies, may offer new challenges in this respect..Only one team tried to learn inflection in a multilingual setting—i.e. to use all training data to train one model. Such transfer learning is an interesting avenue of future research, but evaluation could be difficult. Whether any cross-language transfer is actually being learned vs. whether having more data better biases the networks to copy strings is an evaluation step to disentangle..Creating new data sets that accurately reflect learner exposure (whether L1 or L2) is also an important consideration in the design of future shared tasks. One pertinent facet of this is information about inflectional categories—often the inflectional information is insufficiently prescribed by the lemma, as with the Romanian verbal inflection classes or nominal gender in German..As we move toward multilingual models for morphology, it becomes important to understand which representations are critical or irrelevant for adapting to new languages; this may be probed in the style of BIBREF27, and it can be used as a first step toward designing systems that avoid catastrophic forgetting as they learn to inflect new languages BIBREF28..Future directions for Task 2 include exploring cross-lingual analysis—in stride with both Task 1 and BIBREF29—and leveraging these analyses in downstream tasks. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "41\n",
      "What is the focus of this year's shared task in the field of linguistics?\n",
      "\n",
      "The focus of this year's shared task in the field of linguistics is on encouraging the construction of strong morphological systems that perform two related but different inflectional tasks: creating morphological inflectors for a large number of under-resourced languages and lemmatizing each word and tagging them with a morphosyntactic description.\n",
      "Question : for the text While producing a sentence, humans combine various types of knowledge to produce fluent output—various shades of meaning are expressed through word selection and tone, while the language is made to conform to underlying structural rules via syntax and morphology. Native speakers are often quick to identify disfluency, even if the meaning of a sentence is mostly clear..Automatic systems must also consider these constraints when constructing or processing language. Strong enough language models can often reconstruct common syntactic structures, but are insufficient to properly model morphology. Many languages implement large inflectional paradigms that mark both function and content words with a varying levels of morphosyntactic information. For instance, Romanian verb forms inflect for person, number, tense, mood, and voice; meanwhile, Archi verbs can take on thousands of forms BIBREF0. Such complex paradigms produce large inventories of words, all of which must be producible by a realistic system, even though a large percentage of them will never be observed over billions of lines of linguistic input. Compounding the issue, good inflectional systems often require large amounts of supervised training data, which is infeasible in many of the world's languages..This year's shared task is concentrated on encouraging the construction of strong morphological systems that perform two related but different inflectional tasks. The first task asks participants to create morphological inflectors for a large number of under-resourced languages, encouraging systems that use highly-resourced, related languages as a cross-lingual training signal. The second task welcomes submissions that invert this operation in light of contextual information: Given an unannotated sentence, lemmatize each word, and tag them with a morphosyntactic description. Both of these tasks extend upon previous morphological competitions, and the best submitted systems now represent the state of the art in their respective tasks. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "42\n",
      "Question 1: How many submissions were received for task 1 and task 2 in the SIGMORPHON 2019 shared task, and how many teams participated?\n",
      "\n",
      "Answer 1: The SIGMORPHON 2019 shared task received 30 submissions in total, with 14 submissions for task 1 and 16 submissions for task 2. These submissions were made by 23 participating teams.\n",
      "Question : for the text The SIGMORPHON 2019 shared task received 30 submissions—14 for task 1 and 16 for task 2—from 23 teams. In addition, the organizers' baseline systems were evaluated. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "43\n",
      "Question 1: Which team trained a combined system by projecting the high-resource language data into the low-resource language data and vice versa?\n",
      "\n",
      "Answer 1: The University of Alberta (UAlberta) trained a combined system by projecting the high-resource language data into the low-resource language data and vice versa.\n",
      "Question : for the text Five teams participated in the first Task, with a variety of methods aimed at leveraging the cross-lingual data to improve system performance..The University of Alberta (UAlberta) performed a focused investigation on four language pairs, training cognate-projection systems from external cognate lists. Two methods were considered: one which trained a high-resource neural encoder-decoder, and projected the test data into the HRL, and one that projected the HRL data into the LRL, and trained a combined system. Results demonstrated that certain language pairs may be amenable to such methods..The Tuebingen University submission (Tuebingen) aligned source and target to learn a set of edit-actions with both linear and neural classifiers that independently learned to predict action sequences for each morphological category. Adding in the cross-lingual data only led to modest gains..AX-Semantics combined the low- and high-resource data to train an encoder-decoder seq2seq model; optionally also implementing domain adaptation methods to focus later epochs on the target language..The CMU submission first attends over a decoupled representation of the desired morphological sequence before using the updated decoder state to attend over the character sequence of the lemma. Secondly, in order to reduce the bias of the decoder's language model, they hallucinate two types of data that encourage common affixes and character copying. Simply allowing the model to learn to copy characters for several epochs significantly out-performs the task baseline, while further improvements are obtained through fine-tuning. Making use of an adversarial language discriminator, cross lingual gains are highly-correlated to linguistic similarity, while augmenting the data with hallucinated forms and multiple related target language further improves the model..The system from IT-IST also attends separately to tags and lemmas, using a gating mechanism to interpolate the importance of the individual attentions. By combining the gated dual-head attention with a SparseMax activation function, they are able to jointly learn stem and affix modifications, improving significantly over the baseline system..The relative system performance is described in tab:sub2team, which shows the average per-language accuracy of each system. The table reflects the fact that some teams submitted more than one system (e.g. Tuebingen-1 & Tuebingen-2 in the table). generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "44\n",
      "What approach did Ohio State University use for Task 2?\n",
      "\n",
      "Ohio State University used a dual decoder approach, where the first decoder predicted features based on the word embedding and the second decoder predicted the sequence of tags using a GRU seq2seq. They also used the baseline lemmatizer for lemmatization.\n",
      "Question : for the text Nine teams submitted system papers for Task 2, with several interesting modifications to either the baseline or other prior work that led to modest improvements..Charles-Saarland achieved the highest overall tagging accuracy by leveraging multi-lingual BERT embeddings fine-tuned on a concatenation of all available languages, effectively transporting the cross-lingual objective of Task 1 into Task 2. Lemmas and tags are decoded separately (with a joint encoder and separate attention); Lemmas are a sequence of edit-actions, while tags are calculated jointly. (There is no splitting of tags into features; tags are atomic.).CBNU instead lemmatize using a transformer network, while performing tagging with a multilayer perceptron with biaffine attention. Input words are first lemmatized, and then pipelined to the tagger, which produces atomic tag sequences (i.e., no splitting of features)..The team from Istanbul Technical University (ITU) jointly produces lemmatic edit-actions and morphological tags via a two level encoder (first word embeddings, and then context embeddings) and separate decoders. Their system slightly improves over the baseline lemmatization, but significantly improves tagging accuracy..The team from the University of Groningen (RUG) also uses separate decoders for lemmatization and tagging, but uses ELMo to initialize the contextual embeddings, leading to large gains in performance. Furthermore, joint training on related languages further improves results..CMU approaches tagging differently than the multi-task decoding we've seen so far (baseline is used for lemmatization). Making use of a hierarchical CRF that first predicts POS (that is subsequently looped back into the encoder), they then seek to predict each feature separately. In particular, predicting POS separately greatly improves results. An attempt to leverage gold typological information led to little gain in the results; experiments suggest that the system is already learning the pertinent information..The team from Ohio State University (OHIOSTATE) concentrates on predicting tags; the baseline lemmatizer is used for lemmatization. To that end, they make use of a dual decoder that first predicts features given only the word embedding as input; the predictions are fed to a GRU seq2seq, which then predicts the sequence of tags..The UNT HiLT+Ling team investigates a low-resource setting of the tagging, by using parallel Bible data to learn a translation matrix between English and the target language, learning morphological tags through analogy with English..The UFAL-Prague team extends their submission from the UD shared task (multi-layer LSTM), replacing the pretrained embeddings with BERT, to great success (first in lemmatization, 2nd in tagging). Although they predict complete tags, they use the individual features to regularize the decoder. Small gains are also obtained from joining multi-lingual corpora and ensembling..CUNI–Malta performs lemmatization as operations over edit actions with LSTM and ReLU. Tagging is a bidirectional LSTM augmented by the edit actions (i.e., two-stage decoding), predicting features separately..The Edinburgh system is a character-based LSTM encoder-decoder with attention, implemented in OpenNMT. It can be seen as an extension of the contextual lemmatization system Lematus BIBREF20 to include morphological tagging, or alternatively as an adaptation of the morphological re-inflection system MED BIBREF21 to incorporate context and perform analysis rather than re-inflection. Like these systems it uses a completely generic encoder-decoder architecture with no specific adaptation to the morphological processing task other than the form of the input. In the submitted version of the system, the input is split into short chunks corresponding to the target word plus one word of context on either side, and the system is trained to output the corresponding lemmas and tags for each three-word chunk..Several teams relied on external resources to improve their lemmatization and feature analysis. Several teams made use of pre-trained embeddings. CHARLES-SAARLAND-2 and UFALPRAGUE-1 used pretrained contextual embeddings (BERT) provided by Google BIBREF22. CBNU-1 used a mix of pre-trained embeddings from the CoNLL 2017 shared task and fastText. Further, some teams trained their own embeddings to aid performance. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "45\n",
      "What is the goal of the first task in the annotation of resources for low-resource languages?\n",
      "\n",
      "The goal of the first task is to perform morphological inflection in a low-resource language using knowledge transferred from a high-resource language that is genetically related to the low-resource language, in order to aid downstream tasks like machine translation in low-resource settings.\n",
      "Question : for the text Annotated resources for the world's languages are not distributed equally—some languages simply have more as they have more native speakers willing and able to annotate more data. We explore how to transfer knowledge from high-resource languages that are genetically related to low-resource languages..The first task iterates on last year's main task: morphological inflection BIBREF1. Instead of giving some number of training examples in the language of interest, we provided only a limited number in that language. To accompany it, we provided a larger number of examples in either a related or unrelated language. Each test example asked participants to produce some other inflected form when given a lemma and a bundle of morphosyntactic features as input. The goal, thus, is to perform morphological inflection in the low-resource language, having hopefully exploited some similarity to the high-resource language. Models which perform well here can aid downstream tasks like machine translation in low-resource settings. All datasets were resampled from UniMorph, which makes them distinct from past years..The mode of the task is inspired by BIBREF2, who fine-tune a model pre-trained on a high-resource language to perform well on a low-resource language. We do not, though, require that models be trained by fine-tuning. Joint modeling or any number of methods may be explored instead. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "46\n",
      "Question 1: How do they score the output of each system?\n",
      "Answer 1: They score the output of each system in terms of its predictions' exact-match accuracy and the average Levenshtein distance between the predictions and their corresponding true forms.\n",
      "Question : for the text We score the output of each system in terms of its predictions' exact-match accuracy and the average Levenshtein distance between the predictions and their corresponding true forms. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "47\n",
      "Question 1: What languages are being utilized in the model discussed in the text?\n",
      "\n",
      "Answer 1: The model will use Asturian as the target language and Spanish as the source language.\n",
      "Question : for the text The model will have access to type-level data in a low-resource target language, plus a high-resource source language. We give an example here of Asturian as the target language with Spanish as the source language.. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "48\n",
      "Question 1: What was required of systems in the second task of the CoNLL-SIGMORPHON Shared Task in 2018?\n",
      "Answer 1: Systems had to complete an inflectional cloze task given only the sentential context and the desired lemma, in order to predict the plural form of a word.\n",
      "Question : for the text Although inflection of words in a context-agnostic manner is a useful evaluation of the morphological quality of a system, people do not learn morphology in isolation..In 2018, the second task of the CoNLL–SIGMORPHON Shared Task BIBREF1 required submitting systems to complete an inflectional cloze task BIBREF3 given only the sentential context and the desired lemma – an example of the problem is given in the following lines: A successful system would predict the plural form “dogs”. Likewise, a Spanish word form ayuda may be a feminine noun or a third-person verb form, which must be disambiguated by context...This year's task extends the second task from last year. Rather than inflect a single word in context, the task is to provide a complete morphological tagging of a sentence: for each word, a successful system will need to lemmatize and tag it with a morphsyntactic description (MSD)..width=.Context is critical—depending on the sentence, identical word forms realize a large number of potential inflectional categories, which will in turn influence lemmatization decisions. If the sentence were instead “The barking dogs kept us up all night”, “barking” is now an adjective, and its lemma is also “barking”. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "49\n",
      "Question 1: Who provided partial support for the research mentioned in the text?\n",
      "\n",
      "Answer 1: The European Union's Horizon 2020 research and innovation programme provided partial support for the research mentioned in the text under grant agreement No. 688147 (MuMMER project).\n",
      "Question : for the text This research was partially supported by the European Union's Horizon 2020 research and innovation programme under grant agreement No. 688147 (MuMMER project). generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "50\n",
      "Question 1: What is the evaluation result of HERMIT NLU compared to other NLU tools?\n",
      "Answer 1: HERMIT NLU out-performs state-of-the-art NLU tools such as Rasa, Dialogflow, LUIS and Watson, even without specific fine-tuning of the model, based on a 25K sentences NLU-Benchmark evaluation.\n",
      "Question : for the text In this paper we presented HERMIT NLU, a hierarchical multi-task architecture for semantic parsing sentences for cross-domain spoken dialogue systems. The problem is addressed using a seq2seq model employing BiLSTM encoders and self-attention mechanisms and followed by CRF tagging layers. We evaluated HERMIT on a 25K sentences NLU-Benchmark and out-perform state-of-the-art NLU tools such as Rasa, Dialogflow, LUIS and Watson, even without specific fine-tuning of the model. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "1\n",
      "Question 1: What was the purpose of conducting empirical evaluations in assessing the proposed architecture? \n",
      "\n",
      "Answer 1: The purpose of conducting empirical evaluations was to assess the effectiveness of the proposed architecture and compare it against existing off-the-shelf tools.\n",
      "Question : for the text In order to assess the effectiveness of the proposed architecture and compare against existing off-the-shelf tools, we run several empirical evaluations. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "2\n",
      "Question 1: What were the characteristics of the datasets tested on the system? \n",
      "Answer 1: The datasets tested on the system were different in size and complexity of the addressed language.\n",
      "Question : for the text We tested the system on two datasets, different in size and complexity of the addressed language. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "3\n",
      "Question 1: How many utterances are included in the NLU-Benchmark dataset?\n",
      "\n",
      "Answer 1: There are 25,716 utterances included in the NLU-Benchmark dataset, which is publicly available and annotated with targeted scenario, action, and entities involved.\n",
      "Question : for the text The first (publicly available) dataset, NLU-Benchmark (NLU-BM), contains $25,716$ utterances annotated with targeted Scenario, Action, and involved Entities. For example, “schedule a call with Lisa on Monday morning” is labelled to contain a calendar scenario, where the set_event action is instantiated through the entities [event_name: a call with Lisa] and [date: Monday morning]. The Intent is then obtained by concatenating scenario and action labels (e.g., calendar_set_event). This dataset consists of multiple home assistant task domains (e.g., scheduling, playing music), chit-chat, and commands to a robot BIBREF7. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "4\n",
      "Question 1: What is the purpose of the ROMULUS dataset?\n",
      "\n",
      "Answer 1: The ROMULUS dataset is being developed for modelling user utterances to open-domain conversational systems for robotic platforms that are expected to handle different interaction situations/patterns. It consists of 1,431 sentences for which dialogue acts, semantic frames, and corresponding frame elements are provided. The dataset addresses different subsections of linguistic phenomena such as chit-chat, command interpretation, and requests for information. The dataset provides a richer representation of the sentence's semantics, making the tasks more complex and challenging.\n",
      "Question : for the text The second dataset, ROMULUS, is composed of $1,431$ sentences, for each of which dialogue acts, semantic frames, and corresponding frame elements are provided. This dataset is being developed for modelling user utterances to open-domain conversational systems for robotic platforms that are expected to handle different interaction situations/patterns – e.g., chit-chat, command interpretation. The corpus is composed of different subsections, addressing heterogeneous linguistic phenomena, ranging from imperative instructions (e.g., “enter the bedroom slowly, turn left and turn the lights off ”) to complex requests for information (e.g., “good morning I want to buy a new mobile phone is there any shop nearby?”) or open-domain chit-chat (e.g., “nope thanks let's talk about cinema”). A considerable number of utterances in the dataset is collected through Human-Human Interaction studies in robotic domain ($\\approx $$70\\%$), though a small portion has been synthetically generated for balancing the frame distribution..Note that while the NLU-BM is designed to have at most one intent per utterance, sentences are here tagged following the IOB2 sequence labelling scheme (see example of Figure TABREF5), so that multiple dialogue acts, frames, and frame elements can be defined at the same time for the same utterance. For example, three dialogue acts are identified within the sentence [good morning]$_{\\textsc {Opening}}$ [I want to buy a new mobile phone]$_{\\textsc {Inform}}$ [is there any shop nearby?]$_{\\textsc {Req\\_info}}$. As a result, though smaller, the ROMULUS dataset provides a richer representation of the sentence's semantics, making the tasks more complex and challenging. These observations are highlighted by the statistics in Table TABREF13, that show an average number of dialogue acts, frames and frame elements always greater than 1 (i.e., $1.33$, $1.41$ and $3.54$, respectively). generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "5\n",
      "What are the limitations of the seq2seq approach in the proposed architecture for NLU in conversational systems?\n",
      " \n",
      "The limitations of the seq2seq approach in the proposed architecture for NLU in conversational systems are on the Frame Semantics side as label sequences are linear structures, which are not suitable for representing nested predicates, a common problem in Natural Language. The current approach is being revised through the application of bilinear models to address this limitation.\n",
      "Question : for the text The experimental evaluation reported in this section provides different insights. The proposed architecture addresses the problem of NLU in wide-coverage conversational systems, modelling semantics through multiple Dialogue Acts and Frame-like structures in an end-to-end fashion. In addition, its hierarchical structure, which reflects the complexity of the single tasks, allows providing rich representations across the whole network. In this respect, we can affirm that the architecture successfully tackles the multi-task problem, with results that are promising in terms of usability and applicability of the system in real scenarios..However, a thorough evaluation in the wild must be carried out, to assess to what extent the system is able to handle complex spoken language phenomena, such as repetitions, disfluencies, etc. To this end, a real scenario evaluation may open new research directions, by addressing new tasks to be included in the multi-task architecture. This is supported by the scalable nature of the proposed approach. Moreover, following BIBREF3, corpora providing different annotations can be exploited within the same multi-task network..We also empirically showed how the same architectural design could be applied to a dataset addressing similar problems. In fact, a comparison with off-the-shelf tools shows the benefits provided by the hierarchical structure, with better overall performance better than any current solution. An ablation study has been performed, assessing the contribution provided by the different components of the network. The results show how the shortcut connections help in the more fine-grained tasks, successfully encoding richer representations. CRFs help when longer spans are being predicted, more present in the upstream tasks..Finally, the seq2seq design allowed obtaining a multi-label approach, enabling the identification of multiple spans in the same utterance that might evoke different dialogue acts/frames. This represents a novelty for NLU in conversational systems, as such a problem has always been tackled as a single-intent detection. However, the seq2seq approach carries also some limitations, especially on the Frame Semantics side. In fact, label sequences are linear structures, not suitable for representing nested predicates, a tough and common problem in Natural Language. For example, in the sentence “I want to buy a new mobile phone”, the [to buy a new mobile phone] span represents both the Desired_event frame element of the Desiring frame and a Commerce_buy frame at the same time. At the moment of writing, we are working on modeling nested predicates through the application of bilinear models. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "6\n",
      "Question 1: What backend was used to implement the models in the HERMIT system?\n",
      "Answer 1: The models in the HERMIT system were implemented using Keras and Tensorflow as backend.\n",
      "Question : for the text All the models are implemented with Keras BIBREF28 and Tensorflow BIBREF29 as backend, and run on a Titan Xp. Experiments are performed in a 10-fold setting, using one fold for tuning and one for testing. However, since HERMIT is designed to operate on dialogue acts, semantic frames and frame elements, the best hyperparameters are obtained over the ROMULUS dataset via a grid search using early stopping, and are applied also to the NLU-BM models. This guarantees fairness towards other systems, that do not perform any fine-tuning on the training data. We make use of pre-trained 1024-dim ELMo embeddings BIBREF26 as word vector representations without re-training the weights. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "7\n",
      "What is the NLU-Benchmark dataset and which NLU services were compared to HERMIT on this dataset?\n",
      "Answer 1: The NLU-Benchmark dataset provided by BIBREF7 was used to compare HERMIT to off-the-shelf NLU services, namely Rasa, Dialogflow, LUIS, and Watson. The dataset was aligned so that Scenarios are treated as DAs, Actions as FRs and Entities as ARs for HERMIT. Micro-averaged Precision, Recall, and F1 were computed following the original paper to assure consistency, and the statistical significance was evaluated through the Wilcoxon signed-rank test.\n",
      "Question : for the text This section shows the results obtained on the NLU-Benchmark (NLU-BM) dataset provided by BIBREF7, by comparing HERMIT to off-the-shelf NLU services, namely: Rasa, Dialogflow, LUIS and Watson. In order to apply HERMIT to NLU-BM annotations, these have been aligned so that Scenarios are treated as DAs, Actions as FRs and Entities as ARs..To make our model comparable against other approaches, we reproduced the same folds as in BIBREF7, where a resized version of the original dataset is used. Table TABREF11 shows some statistics of the NLU-BM and its reduced version. Moreover, micro-averaged Precision, Recall and F1 are computed following the original paper to assure consistency. TP, FP and FN of intent labels are obtained as in any other multi-class task. An entity is instead counted as TP if there is an overlap between the predicted and the gold span, and their labels match..Experimental results are reported in Table TABREF21. The statistical significance is evaluated through the Wilcoxon signed-rank test. When looking at the intent F1, HERMIT performs significantly better than Rasa $[Z=-2.701, p = .007]$ and LUIS $[Z=-2.807, p = .005]$. On the contrary, the improvements w.r.t. Dialogflow $[Z=-1.173, p = .241]$ do not seem to be significant. This is probably due to the high variance obtained by Dialogflow across the 10 folds. Watson is by a significant margin the most accurate system in recognising intents $[Z=-2.191, p = .028]$, especially due to its Precision score..The hierarchical multi-task architecture of HERMIT seems to contribute strongly to entity tagging accuracy. In fact, in this task it performs significantly better than Rasa $[Z=-2.803, p = .005]$, Dialogflow $[Z=-2.803, p = .005]$, LUIS $[Z=-2.803, p = .005]$ and Watson $[Z=-2.805, p = .005]$, with improvements from $7.08$ to $35.92$ of F1..Following BIBREF7, we then evaluated a metric that combines intent and entities, computed by simply summing up the two confusion matrices (Table TABREF23). Results highlight the contribution of the entity tagging task, where HERMIT outperforms the other approaches. Paired-samples t-tests were conducted to compare the HERMIT combined F1 against the other systems. The statistical analysis shows a significant improvement over Rasa $[Z=-2.803, p = .005]$, Dialogflow $[Z=-2.803, p = .005]$, LUIS $[Z=-2.803, p = .005]$ and Watson $[Z=-2.803, p = .005]$. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "8\n",
      "What was the impact of removing the self-attention mechanism in the HERMIT architecture?\n",
      "\n",
      "Removing the self-attention mechanism from the HERMIT architecture slightly decreased the performance on all tasks, with a small inclination towards the upstream tasks. However, the impact of the self-attention mechanism was distributed across all tasks, indicating that it is helpful for identifying pivoting keywords for predicting intent.\n",
      "Question : for the text In order to assess the contributions of the HERMIT's components, we performed an ablation study. The results are obtained on the NLU-BM, following the same setup as in Section SECREF16..Results are shown in Table TABREF25. The first row refers to the complete architecture, while –SA shows the results of HERMIT without the self-attention mechanism. Then, from this latter we further remove shortcut connections (– SA/CN) and CRF taggers (– SA/CRF). The last row (– SA/CN/CRF) shows the results of a simple architecture, without self-attention, shortcuts, and CRF. Though not significant, the contribution of the several architectural components can be observed. The contribution of self-attention is distributed across all the tasks, with a small inclination towards the upstream ones. This means that while the entity tagging task is mostly lexicon independent, it is easier to identify pivoting keywords for predicting the intent, e.g. the verb “schedule” triggering the calendar_set_event intent. The impact of shortcut connections is more evident on entity tagging. In fact, the effect provided by shortcut connections is that the information flowing throughout the hierarchical architecture allows higher layers to encode richer representations (i.e., original word embeddings + latent semantics from the previous task). Conversely, the presence of the CRF tagger affects mainly the lower levels of the hierarchical architecture. This is not probably due to their position in the hierarchy, but to the way the tasks have been designed. In fact, while the span of an entity is expected to cover few tokens, in intent recognition (i.e., a combination of Scenario and Action recognition) the span always covers all the tokens of an utterance. CRF therefore preserves consistency of IOB2 sequences structure. However, HERMIT seems to be the most stable architecture, both in terms of standard deviation and task performance, with a good balance between intent and entity recognition. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "9\n",
      "Question 1: What evaluation metrics were used for the experiments on the ROMULUS dataset?\n",
      "\n",
      "Answer 1: The evaluation metrics used for the experiments on the ROMULUS dataset were span F1, computed using the CoNLL-2000 shared task evaluation script, and the Exact Match (EM) accuracy of the entire sequence of labels, in addition to the evaluation metrics used in BIBREF7.\n",
      "Question : for the text In this section we report the experiments performed on the ROMULUS dataset (Table TABREF27). Together with the evaluation metrics used in BIBREF7, we report the span F1, computed using the CoNLL-2000 shared task evaluation script, and the Exact Match (EM) accuracy of the entire sequence of labels. It is worth noticing that the EM Combined score is computed as the conjunction of the three individual predictions – e.g., a match is when all the three sequences are correct..Results in terms of EM reflect the complexity of the different tasks, motivating their position within the hierarchy. Specifically, dialogue act identification is the easiest task ($89.31\\%$) with respect to frame ($82.60\\%$) and frame element ($79.73\\%$), due to the shallow semantics it aims to catch. However, when looking at the span F1, its score ($89.42\\%$) is lower than the frame element identification task ($92.26\\%$). What happens is that even though the label set is smaller, dialogue act spans are supposed to be longer than frame element ones, sometimes covering the whole sentence. Frame elements, instead, are often one or two tokens long, that contribute in increasing span based metrics. Frame identification is the most complex task for several reasons. First, lots of frame spans are interlaced or even nested; this contributes to increasing the network entropy. Second, while the dialogue act label is highly related to syntactic structures, frame identification is often subject to the inherent ambiguity of language (e.g., get can evoke both Commerce_buy and Arriving). We also report the metrics in BIBREF7 for consistency. For dialogue act and frame tasks, scores provide just the extent to which the network is able to detect those labels. In fact, the metrics do not consider any span information, essential to solve and evaluate our tasks. However, the frame element scores are comparable to the benchmark, since the task is very similar..Overall, getting back to the combined EM accuracy, HERMIT seems to be promising, with the network being able to reproduce all the three gold sequences for almost $70\\%$ of the cases. The importance of this result provides an idea of the architecture behaviour over the entire pipeline. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "10\n",
      "What are the limitations in the current approach of integrating the corpus of 5M sentences?\n",
      "\n",
      "The limitations in the current approach include assessing the network's capability in handling typical phenomena of spontaneous spoken language input, such as repetitions and disfluencies, and the fact that the seq2seq scheme does not deal with nested predicates, a common aspect of Natural Language. There is currently no architecture that implements an end-to-end network for FrameNet based semantic parsing.\n",
      "Question : for the text We have started integrating a corpus of 5M sentences of real users chit-chatting with our conversational agent, though at the time of writing they represent only $16\\%$ of the current dataset..As already pointed out in Section SECREF28, there are some limitations in the current approach that need to be addressed. First, we have to assess the network's capability in handling typical phenomena of spontaneous spoken language input, such as repetitions and disfluencies BIBREF30. This may open new research directions, by including new tasks to identify/remove any kind of noise from the spoken input. Second, the seq2seq scheme does not deal with nested predicates, a common aspect of Natural Language. To the best of our knowledge, there is no architecture that implements an end-to-end network for FrameNet based semantic parsing. Following previous work BIBREF2, one of our future goals is to tackle such problems through hierarchical multi-task architectures that rely on bilinear models. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "11\n",
      "Q1: What is HERMIT and how does it differ from previous approaches to Natural Language Understanding for Spoken Dialogue Systems?\n",
      "\n",
      "A1: HERMIT is a HiERarchical MultI-Task Natural Language Understanding architecture designed for effective semantic parsing of domain-independent user utterances. Unlike previous approaches, HERMIT is a cross-domain, multi-task architecture that is capable of recognizing multiple intents/frames in an utterance. Additionally, HERMIT shows better performance compared to current state-of-the-art commercial systems.\n",
      "Question : for the text Research in Conversational AI (also known as Spoken Dialogue Systems) has applications ranging from home devices to robotics, and has a growing presence in industry. A key problem in real-world Dialogue Systems is Natural Language Understanding (NLU) – the process of extracting structured representations of meaning from user utterances. In fact, the effective extraction of semantics is an essential feature, being the entry point of any Natural Language interaction system. Apart from challenges given by the inherent complexity and ambiguity of human language, other challenges arise whenever the NLU has to operate over multiple domains. In fact, interaction patterns, domain, and language vary depending on the device the user is interacting with. For example, chit-chatting and instruction-giving for executing an action are different processes in terms of language, domain, syntax and interaction schemes involved. And what if the user combines two interaction domains: “play some music, but first what's the weather tomorrow”?.In this work, we present HERMIT, a HiERarchical MultI-Task Natural Language Understanding architecture, designed for effective semantic parsing of domain-independent user utterances, extracting meaning representations in terms of high-level intents and frame-like semantic structures. With respect to previous approaches to NLU for SDS, HERMIT stands out for being a cross-domain, multi-task architecture, capable of recognising multiple intents/frames in an utterance. HERMIT also shows better performance with respect to current state-of-the-art commercial systems. Such a novel combination of requirements is discussed below. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "12\n",
      "Question 1: What is the purpose of the three layers of knowledge in modeling meaning for a cross-domain dialogue agent?\n",
      "\n",
      "Answer 1: The three layers of knowledge in modeling meaning for a cross-domain dialogue agent are dialogue acts, frames, and frame arguments. The purpose of these layers is to properly capture the intent of the user and handle heterogeneous types of conversation. Frames and arguments can be mapped to domain-dependent intents and slots or to Frame Semantics' BIBREF0 structures, which allow handling of heterogeneous domains and language.\n",
      "Question : for the text A cross-domain dialogue agent must be able to handle heterogeneous types of conversation, such as chit-chatting, giving directions, entertaining, and triggering domain/task actions. A domain-independent and rich meaning representation is thus required to properly capture the intent of the user. Meaning is modelled here through three layers of knowledge: dialogue acts, frames, and frame arguments. Frames and arguments can be in turn mapped to domain-dependent intents and slots, or to Frame Semantics' BIBREF0 structures (i.e. semantic frames and frame elements, respectively), which allow handling of heterogeneous domains and language. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "13\n",
      "Question 1: What is the granularity of knowledge that can be extracted from an utterance in NLU? \n",
      "\n",
      "Answer 1: The granularity of knowledge that can be extracted from an utterance in NLU can be multi-dialogue act and multi-intent, which means that an utterance's meaning can be modeled through multiple dialogue acts and intents at the same time. This is necessary as utterance semantics is often rich and expressive, and approximating meaning to a single user intent is often not enough to convey the required information.\n",
      "Question : for the text Another degree of complexity in NLU is represented by the granularity of knowledge that can be extracted from an utterance. Utterance semantics is often rich and expressive: approximating meaning to a single user intent is often not enough to convey the required information. As opposed to the traditional single-dialogue act and single-intent view in previous work BIBREF4, BIBREF5, BIBREF6, HERMIT operates on a meaning representation that is multi-dialogue act and multi-intent. In fact, it is possible to model an utterance's meaning through multiple dialogue acts and intents at the same time. For example, the user would be able both to request tomorrow's weather and listen to his/her favourite music with just a single utterance..A further requirement is that for practical application the system should be competitive with state-of-the-art: we evaluate HERMIT's effectiveness by running several empirical investigations. We perform a robust test on a publicly available NLU-Benchmark (NLU-BM) BIBREF7 containing 25K cross-domain utterances with a conversational agent. The results obtained show a performance higher than well-known off-the-shelf tools (i.e., Rasa, DialogueFlow, LUIS, and Watson). The contribution of the different network components is then highlighted through an ablation study. We also test HERMIT on the smaller Robotics-Oriented MUltitask Language UnderStanding (ROMULUS) corpus, annotated with Dialogue Acts and Frame Semantics. HERMIT produces promising results for the application in a real scenario. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "14\n",
      "Question 1: What is HERMIT?\n",
      "\n",
      "Answer 1: HERMIT is a hierarchical multi-task neural architecture that is able to deal with the tasks of tagging dialogue acts, frame-like structures, and their arguments in parallel. It is based on self-attention mechanisms, seq2seq bi-directional Long-Short Term Memory (BiLSTM) encoders, and CRF tagging layers. HERMIT is hierarchical in the sense that information output from earlier layers flows through the network, feeding following layers to solve downstream dependent tasks.\n",
      "Question : for the text Deriving such a multi-layered meaning representation can be approached through a multi-task learning approach. Multi-task learning has found success in several NLP problems BIBREF1, BIBREF2, especially with the recent rise of Deep Learning. Thanks to the possibility of building complex networks, handling more tasks at once has been proven to be a successful solution, provided that some degree of dependence holds between the tasks. Moreover, multi-task learning allows the use of different datasets to train sub-parts of the network BIBREF3. Following the same trend, HERMIT is a hierarchical multi-task neural architecture which is able to deal with the three tasks of tagging dialogue acts, frame-like structures, and their arguments in parallel. The network, based on self-attention mechanisms, seq2seq bi-directional Long-Short Term Memory (BiLSTM) encoders, and CRF tagging layers, is hierarchical in the sense that information output from earlier layers flows through the network, feeding following layers to solve downstream dependent tasks. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "15\n",
      "What is the multi-task learning approach used in tackling the process of extracting a semantic interpretation?\n",
      "The multi-task learning approach used in tackling the process of extracting a complete semantic interpretation includes modelling each task as a seq2seq problem, assigning each token with a task-specific label according to the IOB2 notation, and drawing task labels from the set of classes defined for DAs, FRs, and ARs.\n",
      "Question : for the text The identification of Dialogue Acts (henceforth DAs) is required to drive the dialogue manager to the next dialogue state. General frame structures (FRs) provide a reference framework to capture user intents, in terms of required or desired actions that a conversational agent has to perform. Depending on the level of abstraction required by an application, these can be interpreted as more domain-dependent paradigms like intent, or to shallower representations, such as semantic frames, as conceived in FrameNet BIBREF23. From this perspective, semantic frames represent a versatile abstraction that can be mapped over an agent's capabilities, allowing also the system to be easily extended with new functionalities without requiring the definition of new ad-hoc structures. Similarly, frame arguments (ARs) act as slots in a traditional intent-slots scheme, or to frame elements for semantic frames..In our work, the whole process of extracting a complete semantic interpretation as required by the system is tackled with a multi-task learning approach across DAs, FRs, and ARs. Each of these tasks is modelled as a seq2seq problem, where a task-specific label is assigned to each token of the sentence according to the IOB2 notation BIBREF24, with “B-” marking the Beginning of the chunk, “I-” the tokens Inside the chunk while “O-” is assigned to any token that does not belong to any chunk. Task labels are drawn from the set of classes defined for DAs, FRs, and ARs. Figure TABREF5 shows an example of the tagging layers over the sentence Where can I find Starbucks?, where Frame Semantics has been selected as underlying reference theory. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "16\n",
      "Question 1: What is the main motivation behind the proposed architecture?\n",
      "\n",
      "Answer 1: The main motivation behind the proposed architecture is the dependence among the three tasks of identifying DAs, FRs, and ARs, and the relationship between tagging frame and arguments.\n",
      "Question : for the text The central motivation behind the proposed architecture is that there is a dependence among the three tasks of identifying DAs, FRs, and ARs. The relationship between tagging frame and arguments appears more evident, as also developed in theories like Frame Semantics – although it is defined independently by each theory. However, some degree of dependence also holds between the DAs and FRs. For example, the FrameNet semantic frame Desiring, expressing a desire of the user for an event to occur, is more likely to be used in the context of an Inform DA, which indicates the state of notifying the agent with an information, other than in an Instruction. This is clearly visible in interactions like “I'd like a cup of hot chocolate” or “I'd like to find a shoe shop”, where the user is actually notifying the agent about a desire of hers/his..In order to reflect such inter-task dependence, the classification process is tackled here through a hierarchical multi-task learning approach. We designed a multi-layer neural network, whose architecture is shown in Figure FIGREF7, where each layer is trained to solve one of the three tasks, namely labelling dialogue acts ($DA$ layer), semantic frames ($FR$ layer), and frame elements ($AR$ layer). The layers are arranged in a hierarchical structure that allows the information produced by earlier layers to be fed to downstream tasks..The network is mainly composed of three BiLSTM BIBREF25 encoding layers. A sequence of input words is initially converted into an embedded representation through an ELMo embeddings layer BIBREF26, and is fed to the $DA$ layer. The embedded representation is also passed over through shortcut connections BIBREF1, and concatenated with both the outputs of the $DA$ and $FR$ layers. Self-attention layers BIBREF27 are placed after the $DA$ and $FR$ BiLSTM encoders. Where $w_t$ is the input word at time step $t$ of the sentence $\\textbf {\\textrm {w}} = (w_1, ..., w_T)$, the architecture can be formalised by:.where $\\oplus $ represents the vector concatenation operator, $e_t$ is the embedding of the word at time $t$, and $\\textbf {\\textrm {s}}^{L}$ = ($s_1^L$, ..., $s_T^L$) is the embedded sequence output of each $L$ layer, with $L = \\lbrace DA, FR, AR\\rbrace $. Given an input sentence, the final sequence of labels $\\textbf {y}^L$ for each task is computed through a CRF tagging layer, which operates on the output of the $DA$ and $FR$ self-attention, and of the $AR$ BiLSTM embedding, so that:.where a$^{DA}$, a$^{FR}$ are attended embedded sequences. Due to shortcut connections, layers in the upper levels of the architecture can rely both on direct word embeddings as well as the hidden representation $a_t^L$ computed by a previous layer. Operationally, the latter carries task specific information which, combined with the input embeddings, helps in stabilising the classification of each CRF layer, as shown by our experiments. The network is trained by minimising the sum of the individual negative log-likelihoods of the three CRF layers, while at test time the most likely sequence is obtained through the Viterbi decoding over the output scores of the CRF layer. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "17\n",
      "Question 1: What is the main difference between the NLU-BM dataset and previous datasets used in natural language understanding research?\n",
      "\n",
      "Answer 1: The NLU-BM dataset is more challenging as it includes a higher number of domains/scenarios, intents, and slots than previous datasets used in NLU research. Additionally, the proposed multi-task hierarchical architecture is trained to solve multiple tasks, each tackled with a seq2seq classification using a CRF output layer.\n",
      "Question : for the text Much research on Natural (or Spoken, depending on the input) Language Understanding has been carried out in the area of Spoken Dialogue Systems BIBREF8, where the advent of statistical learning has led to the application of many data-driven approaches BIBREF9. In recent years, the rise of deep learning models has further improved the state-of-the-art. Recurrent Neural Networks (RNNs) have proven to be particularly successful, especially uni- and bi-directional LSTMs and Gated Recurrent Units (GRUs). The use of such deep architectures has also fostered the development of joint classification models of intents and slots. Bi-directional GRUs are applied in BIBREF10, where the hidden state of each time step is used for slot tagging in a seq2seq fashion, while the final state of the GRU is used for intent classification. The application of attention mechanisms in a BiLSTM architecture is investigated in BIBREF5, while the work of BIBREF11 explores the use of memory networks BIBREF12 to exploit encoding of historical user utterances to improve the slot-filling task. Seq2seq with self-attention is applied in BIBREF13, where the classified intent is also used to guide a special gated unit that contributes to the slot classification of each token..One of the first attempts to jointly detect domains in addition to intent-slot tagging is the work of BIBREF4. An utterance syntax is encoded through a Recursive NN, and it is used to predict the joined domain-intent classes. Syntactic features extracted from the same network are used in the per-word slot classifier. The work of BIBREF6 applies the same idea of BIBREF10, this time using a context-augmented BiLSTM, and performing domain-intent classification as a single joint task. As in BIBREF11, the history of user utterances is also considered in BIBREF14, in combination with a dialogue context encoder. A two-layer hierarchical structure made of a combination of BiLSTM and BiGRU is used for joint classification of domains and intents, together with slot tagging. BIBREF15 apply multi-task learning to the dialogue domain. Dialogue state tracking, dialogue act and intent classification, and slot tagging are jointly learned. Dialogue states and user utterances are encoded to provide hidden representations, which jointly affect all the other tasks..Many previous systems are trained and compared over the ATIS (Airline Travel Information Systems) dataset BIBREF16, which covers only the flight-booking domain. Some of them also use bigger, not publicly available datasets, which appear to be similar to the NLU-BM in terms of number of intents and slots, but they cover no more than three or four domains. Our work stands out for its more challenging NLU setting, since we are dealing with a higher number of domains/scenarios (18), intents (64) and slots (54) in the NLU-BM dataset, and dialogue acts (11), frames (58) and frame elements (84) in the ROMULUS dataset. Moreover, we propose a multi-task hierarchical architecture, where each layer is trained to solve one of the three tasks. Each of these is tackled with a seq2seq classification using a CRF output layer, as in BIBREF3..The NLU problem has been studied also on the Interactive Robotics front, mostly to support basic dialogue systems, with few dialogue states and tailored for specific tasks, such as semantic mapping BIBREF17, navigation BIBREF18, BIBREF19, or grounded language learning BIBREF20. However, the designed approaches, either based on formal languages or data-driven, have never been shown to scale to real world scenarios. The work of BIBREF21 makes a step forward in this direction. Their model still deals with the single `pick and place' domain, covering no more than two intents, but it is trained on several thousands of examples, making it able to manage more unstructured language. An attempt to manage a higher number of intents, as well as more variable language, is represented by the work of BIBREF22 where the sole Frame Semantics is applied to represent user intents, with no Dialogue Acts. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "18\n",
      "Question 1: What are the components of the QA-DQN agent proposed in the baseline?\n",
      "\n",
      "Answer 1: The QA-DQN agent proposed in the baseline consists of three components - an encoder, an action generator, and a question answerer. The encoder reads observation string and question string to generate attention aggregated hidden representations, which are used by the action generator to output commands to interact with iMRC. The question answerer takes the current information at game step to generate head and tail pointers for answering the question.\n",
      "Question : for the text As a baseline, we propose QA-DQN, an agent that adopts components from QANet BIBREF18 and adds an extra command generation module inspired by LSTM-DQN BIBREF19..As illustrated in Figure FIGREF6, the agent consists of three components: an encoder, an action generator, and a question answerer. More precisely, at a game step $t$, the encoder reads observation string $o_t$ and question string $q$ to generate attention aggregated hidden representations $M_t$. Using $M_t$, the action generator outputs commands (defined in previous sections) to interact with iMRC. If the generated command is stop or the agent is forced to stop, the question answerer takes the current information at game step $t$ to generate head and tail pointers for answering the question; otherwise, the information gathering procedure continues..In this section, we describe the high-level model structure and training strategies of QA-DQN. We refer readers to BIBREF18 for detailed information. We will release datasets and code in the near future. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "19\n",
      "Question 1: What is the purpose of the ablation study mentioned in the text?\n",
      "Answer 1: The ablation study aims to explore the possibility of an agent exploring an iMRC game only via search queries by limiting the agent to Ctrl+F and stop commands.\n",
      "Question : for the text As mentioned above, an agent might bypass Ctrl+F actions and explore an iMRC game only via next commands. We study this possibility in an ablation study, where we limit the agent to the Ctrl+F and stop commands. In this setting, an agent is forced to explore by means of search a queries. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "20\n",
      "Question 1: Why is a queue used in the iMRC approach?\n",
      "\n",
      "Answer 1: The queue is used to store strings that have been observed recently, to help overcome the limitation of answering questions based on only one sentence. It provides an explicit memory mechanism for the QA-DQN approach.\n",
      "Question : for the text In iMRC, some questions may not be easily answerable based only on observation of a single sentence. To overcome this limitation, we provide an explicit memory mechanism to QA-DQN. Specifically, we use a queue to store strings that have been observed recently. The queue has a limited size of slots (we use queues of size [1, 3, 5] in this work). This prevents the agent from issuing next commands until the environment has been observed fully, in which case our task would degenerate to the standard MRC setting. The memory slots are reset episodically. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "21\n",
      "Question 1: What is the purpose of the sufficient information reward in QA-DQN?\n",
      "\n",
      "Answer 1: The sufficient information reward is designed to encourage and guide the behavior of the agent to find and stop at the sentence that contains the answer in the iMRC task. It is assigned if the agent halts at game step k and the answer is a sub-string of o_k.\n",
      "Question : for the text Because the question answerer in QA-DQN is a pointing model, its performance relies heavily on whether the agent can find and stop at the sentence that contains the answer. We design a heuristic reward to encourage and guide this behavior. In particular, we assign a reward if the agent halts at game step $k$ and the answer is a sub-string of $o_k$ (if larger memory slots are used, we assign this reward if the answer is a sub-string of the memory at game step $k$). We denote this reward as the sufficient information reward, since, if an agent sees the answer, it should have a good chance of having gathered sufficient information for the question (although this is not guaranteed)..Note this sufficient information reward is part of the design of QA-DQN, whereas the question answering score is the only metric used to evaluate an agent's performance on the iMRC task. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "22\n",
      "Question 1: What does $t$ represent in the interaction between an agent and the iMRC environment?\n",
      "\n",
      "Answer 1: $t$ represents one round of interaction between an agent and the iMRC environment.\n",
      "Question : for the text In this section, we use game step $t$ to denote one round of interaction between an agent with the iMRC environment. We use $o_t$ to denote text observation at game step $t$ and $q$ to denote question text. We use $L$ to refer to a linear transformation. $[\\cdot ;\\cdot ]$ denotes vector concatenation. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "23\n",
      "The action generator in this text takes $M_t$ as input and estimates Q-values for all possible actions, consisting of three MLPs. The size of $L_{shared}$ is $95 \\times 150$, and $L_{action}$ has an output size of 4 or 2 depending on the number of actions available. The size of $L_{ctrlf}$ is determined by the dataset's vocabulary size. The overall Q-value is the sum of the two components, which generates one question and answer and presents it in the format Question 1: Answer 1:\n",
      "Question : for the text The action generator takes $M_t$ as input and estimates Q-values for all possible actions. As described in previous section, when an action is a Ctrl+F command, it is composed of two tokens (the token “Ctrl+F” and the query token). Therefore, the action generator consists of three MLPs:.Here, the size of $L_{shared} \\in \\mathbb {R}^{95 \\times 150}$; $L_{action}$ has an output size of 4 or 2 depending on the number of actions available; the size of $L_{ctrlf}$ is the same as the size of a dataset's vocabulary size (depending on different query type settings, we mask out words in the vocabulary that are not query candidates). The overall Q-value is simply the sum of the two components: generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "24\n",
      "Question 1: What is included in the embedding layer of the encoder?\n",
      "\n",
      "Answer 1: The embedding layer includes both word- and character-level embeddings. Word embeddings are initialized by fastText vectors trained on Common Crawl and character embeddings are initialized by random vectors. A convolutional layer with 96 kernels of size 5 is used to aggregate the sequence of characters. A max pooling layer on the character dimension and a multi-layer perceptron of size 96 are used to aggregate the concatenation of word- and character-level representations, with a highway network on top of the MLP.\n",
      "Question : for the text The encoder consists of an embedding layer, two stacks of transformer blocks (denoted as encoder transformer blocks and aggregation transformer blocks), and an attention layer..In the embedding layer, we aggregate both word- and character-level embeddings. Word embeddings are initialized by the 300-dimension fastText BIBREF20 vectors trained on Common Crawl (600B tokens), and are fixed during training. Character embeddings are initialized by 200-dimension random vectors. A convolutional layer with 96 kernels of size 5 is used to aggregate the sequence of characters. We use a max pooling layer on the character dimension, then a multi-layer perceptron (MLP) of size 96 is used to aggregate the concatenation of word- and character-level representations. A highway network BIBREF21 is used on top of this MLP. The resulting vectors are used as input to the encoding transformer blocks..Each encoding transformer block consists of four convolutional layers (with shared weights), a self-attention layer, and an MLP. Each convolutional layer has 96 filters, each kernel's size is 7. In the self-attention layer, we use a block hidden size of 96 and a single head attention mechanism. Layer normalization and dropout are applied after each component inside the block. We add positional encoding into each block's input. We use one layer of such an encoding block..At a game step $t$, the encoder processes text observation $o_t$ and question $q$ to generate context-aware encodings $h_{o_t} \\in \\mathbb {R}^{L^{o_t} \\times H_1}$ and $h_q \\in \\mathbb {R}^{L^{q} \\times H_1}$, where $L^{o_t}$ and $L^{q}$ denote length of $o_t$ and $q$ respectively, $H_1$ is 96..Following BIBREF18, we use a context-query attention layer to aggregate the two representations $h_{o_t}$ and $h_q$. Specifically, the attention layer first uses two MLPs to map $h_{o_t}$ and $h_q$ into the same space, with the resulting representations denoted as $h_{o_t}^{\\prime } \\in \\mathbb {R}^{L^{o_t} \\times H_2}$ and $h_q^{\\prime } \\in \\mathbb {R}^{L^{q} \\times H_2}$, in which, $H_2$ is 96..Then, a tri-linear similarity function is used to compute the similarities between each pair of $h_{o_t}^{\\prime }$ and $h_q^{\\prime }$ items:.where $\\odot $ indicates element-wise multiplication and $w$ is trainable parameter vector of size 96..We apply softmax to the resulting similarity matrix $S$ along both dimensions, producing $S^A$ and $S^B$. Information in the two representations are then aggregated as.where $h_{oq}$ is aggregated observation representation..On top of the attention layer, a stack of aggregation transformer blocks is used to further map the observation representations to action representations and answer representations. The configuration parameters are the same as the encoder transformer blocks, except there are two convolution layers (with shared weights), and the number of blocks is 7..Let $M_t \\in \\mathbb {R}^{L^{o_t} \\times H_3}$ denote the output of the stack of aggregation transformer blocks, in which $H_3$ is 96. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "25\n",
      "Question 1: What are $M_{head}$ and $M_{tail}$ in the context of the extra transformer stacks added on top of the encoder?\n",
      "\n",
      "Answer 1: $M_{head}$ and $M_{tail}$ are the outputs of the two extra transformer stacks added on top of the encoder to compute head and tail positions.\n",
      "Question : for the text Following BIBREF18, we append two extra stacks of aggregation transformer blocks on top of the encoder to compute head and tail positions:.Here, $M_{head}$ and $M_{tail}$ are outputs of the two extra transformer stacks, $L_0$, $L_1$, $L_2$ and $L_3$ are trainable parameters with output size 150, 150, 1 and 1, respectively. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "26\n",
      "What optimization step rule is used for both parts of the training pipeline and what is the learning rate?\n",
      "\n",
      "Adam BIBREF22 is used as the optimization step rule for both parts of the training pipeline and the learning rate is set to 0.00025.\n",
      "Question : for the text In this section, we describe our training strategy. We split the training pipeline into two parts for easy comprehension. We use Adam BIBREF22 as the step rule for optimization in both parts, with the learning rate set to 0.00025. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "27\n",
      "Question 1: What algorithm is used to train the interactive information-gathering behavior of QA-DQN in iMRC games?\n",
      "\n",
      "Answer 1: The Rainbow algorithm, which combines several extensions to the Deep Q-Learning algorithm, is used to train the interactive information-gathering behavior of QA-DQN in iMRC games.\n",
      "Question : for the text iMRC games are interactive environments. We use an RL training algorithm to train the interactive information-gathering behavior of QA-DQN. We adopt the Rainbow algorithm proposed by BIBREF23, which integrates several extensions to the original Deep Q-Learning algorithm BIBREF24. Rainbox exhibits state-of-the-art performance on several RL benchmark tasks (e.g., Atari games)..During game playing, we use a mini-batch of size 10 and push all transitions (observation string, question string, generated command, reward) into a replay buffer of size 500,000. We do not compute losses directly using these transitions. After every 5 game steps, we randomly sample a mini-batch of 64 transitions from the replay buffer, compute loss, and update the network..Detailed hyper-parameter settings for action generation are shown in Table TABREF38. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "28\n",
      "Question 1: What is the method used to train the question answerer in the system? \n",
      "\n",
      "Answer 1: The method used to train the question answerer in the system is supervised learning and Negative Log-Likelihood (NLL) loss with a dropout rate of 0.1. The training transitions consist of observation string, question string, and ground-truth answer positions.\n",
      "Question : for the text Similarly, we use another replay buffer to store question answering transitions (observation string when interaction stops, question string, ground-truth answer)..Because both iSQuAD and iNewsQA are converted from datasets that provide ground-truth answer positions, we can leverage this information and train the question answerer with supervised learning. Specifically, we only push question answering transitions when the ground-truth answer is in the observation string. For each transition, we convert the ground-truth answer head- and tail-positions from the SQuAD and NewsQA datasets to positions in the current observation string. After every 5 game steps, we randomly sample a mini-batch of 64 transitions from the replay buffer and train the question answerer using the Negative Log-Likelihood (NLL) loss. We use a dropout rate of 0.1. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "29\n",
      "What is the proposed direction explored in the work? \n",
      "Answer 1: The proposed direction explored in the work is to convert MRC datasets into interactive environments for neural MRC systems for better information-seeking behavior.\n",
      "Question : for the text In this work, we propose and explore the direction of converting MRC datasets into interactive environments. We believe interactive, information-seeking behavior is desirable for neural MRC systems when knowledge sources are partially observable and/or too large to encode in their entirety — for instance, when searching for information on the internet, where knowledge is by design easily accessible to humans through interaction..Despite being restricted, our proposed task presents major challenges to existing techniques. iMRC lies at the intersection of NLP and RL, which is arguably less studied in existing literature. We hope to encourage researchers from both NLP and RL communities to work toward solving this task..For our baseline, we adopted an off-the-shelf, top-performing MRC model and RL method. Either component can be replaced straightforwardly with other methods (e.g., to utilize a large-scale pretrained language model)..Our proposed setup and baseline agent presently use only a single word with the query command. However, a host of other options should be considered in future work. For example, multi-word queries with fuzzy matching are more realistic. It would also be interesting for an agent to generate a vector representation of the query in some latent space. This vector could then be compared with precomputed document representations (e.g., in an open domain QA dataset) to determine what text to observe next, with such behavior tantamount to learning to do IR..As mentioned, our idea for reformulating existing MRC datasets as partially observable and interactive environments is straightforward and general. Almost all MRC datasets can be used to study interactive, information-seeking behavior through similar modifications. We hypothesize that such behavior can, in turn, help in solving real-world MRC problems involving search. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "30\n",
      "What are the three factors studied in relation to their effects on iMRC and the performance of the QA-DQN agent?\n",
      "The three factors studied are different Ctrl+F strategies, enabled vs. disabled next and previous actions, and different memory slot sizes.\n",
      "Question : for the text In this study, we focus on three factors and their effects on iMRC and the performance of the QA-DQN agent:.different Ctrl+F strategies, as described in the action space section;.enabled vs. disabled next and previous actions;.different memory slot sizes..Below we report the baseline agent's training performance followed by its generalization performance on test data. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "31\n",
      "Question 1: What is the importance of information seeking in solving iMRC tasks according to the study?\n",
      "\n",
      "Answer 1: According to the study, information seeking is an important factor in solving iMRC tasks as it plays a crucial role in navigating to important sentences that contain the answers. The study also suggests that an interactive agent that can better navigate to important sentences is likely to achieve better performance on iMRC tasks.\n",
      "Question : for the text To study QA-DQN's ability to generalize, we select the best performing agent in each experimental setting on the validation set and report their performance on the test set. The agent's test performance is reported in Table TABREF41. In addition, to support our claim that the challenging part of iMRC tasks is information seeking rather than answering questions given sufficient information, we also report the $\\text{F}_1$ score of an agent when it has reached the piece of text that contains the answer, which we denote as $\\text{F}_{1\\text{info}}$..From Table TABREF41 (and validation curves provided in appendix) we can observe that QA-DQN's performance during evaluation matches its training performance in most settings. $\\text{F}_{1\\text{info}}$ scores are consistently higher than the overall $\\text{F}_1$ scores, and they have much less variance across different settings. This supports our hypothesis that information seeking play an important role in solving iMRC tasks, whereas question answering given necessary information is relatively straightforward. This also suggests that an interactive agent that can better navigate to important sentences is very likely to achieve better performance on iMRC tasks. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "32\n",
      "Question 1: What is one of the challenges faced by RL agents in mastering multiple games at the same time?\n",
      "\n",
      "Answer 1: One of the challenges faced by RL agents in mastering multiple games at the same time is that it remains difficult for them to do so. Each document-question pair can be considered a unique game, and since there are hundreds of thousands of them, it becomes a complicated task for RL agents.\n",
      "Question : for the text It remains difficult for RL agents to master multiple games at the same time. In our case, each document-question pair can be considered a unique game, and there are hundred of thousands of them. Therefore, as is common practice in the RL literature, we study an agent's training curves..Due to the space limitations, we select several representative settings to discuss in this section and provide QA-DQN's training and evaluation curves for all experimental settings in the Appendix. We provide the agent's sufficient information rewards (i.e., if the agent stopped at a state where the observation contains the answer) during training in Appendix as well..Figure FIGREF36 shows QA-DQN's training performance ($\\text{F}_1$ score) when next and previous actions are available. Figure FIGREF40 shows QA-DQN's training performance ($\\text{F}_1$ score) when next and previous actions are disabled. Note that all training curves are averaged over 3 runs with different random seeds and all evaluation curves show the one run with max validation performance among the three..From Figure FIGREF36, we can see that the three Ctrl+F strategies show similar difficulty levels when next and previous are available, although QA-DQN works slightly better when selecting a word from the question as query (especially on iNewsQA). However, from Figure FIGREF40 we observe that when next and previous are disabled, QA-DQN shows significant advantage when selecting a word from the question as query. This may due to the fact that when an agent must use Ctrl+F to navigate within documents, the set of question words is a much smaller action space in contrast to the other two settings. In the 4-action setting, an agent can rely on issuing next and previous actions to reach any sentence in a document..The effect of action space size on model performance is particularly clear when using a datasets' entire vocabulary as query candidates in the 2-action setting. From Figure FIGREF40 (and figures with sufficient information rewards in the Appendix) we see QA-DQN has a hard time learning in this setting. As shown in Table TABREF16, both datasets have a vocabulary size of more than 100k. This is much larger than in the other two settings, where on average the length of questions is around 10. This suggests that the methods with better sample efficiency are needed to act in more realistic problem settings with huge action spaces..Experiments also show that a larger memory slot size always helps. Intuitively, with a memory mechanism (either implicit or explicit), an agent could make the environment closer to fully observed by exploring and memorizing observations. Presumably, a larger memory may further improve QA-DQN's performance, but considering the average number of sentences in each iSQuAD game is 5, a memory with more than 5 slots will defeat the purpose of our study of partially observable text environments..Not surprisingly, QA-DQN performs worse in general on iNewsQA, in all experiments. As shown in Table TABREF16, the average number of sentences per document in iNewsQA is about 6 times more than in iSQuAD. This is analogous to games with larger maps in the RL literature, where the environment is partially observable. A better exploration (in our case, jumping) strategy may help QA-DQN to master such harder games. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "33\n",
      "Question 1: What problem do the authors of the paper propose to address with their interactive MRC (iMRC) approach?\n",
      "\n",
      "Answer 1: The authors propose to address the problem of MRC models finding answers through shallow pattern matching by shifting the focus of MRC data away from static, fully observable documents and towards interactive versions of existing MRC tasks, where the information needed to answer a question must be gathered sequentially. They propose to make the MRC problem harder, but believe that the added demands of iMRC more closely match web-level QA and may lead to deeper comprehension of documents' content.\n",
      "Question : for the text Many machine reading comprehension (MRC) datasets have been released in recent years BIBREF0, BIBREF1, BIBREF2, BIBREF3, BIBREF4 to benchmark a system's ability to understand and reason over natural language. Typically, these datasets require an MRC model to read through a document to answer a question about information contained therein..The supporting document is, more often than not, static and fully observable. This raises concerns, since models may find answers simply through shallow pattern matching; e.g., syntactic similarity between the words in questions and documents. As pointed out by BIBREF5, for questions starting with when, models tend to predict the only date/time answer in the supporting document. Such behavior limits the generality and usefulness of MRC models, and suggests that they do not learn a proper `understanding' of the intended task. In this paper, to address this problem, we shift the focus of MRC data away from `spoon-feeding' models with sufficient information in fully observable, static documents. Instead, we propose interactive versions of existing MRC tasks, whereby the information needed to answer a question must be gathered sequentially..The key idea behind our proposed interactive MRC (iMRC) is to restrict the document context that a model observes at one time. Concretely, we split a supporting document into its component sentences and withhold these sentences from the model. Given a question, the model must issue commands to observe sentences in the withheld set; we equip models with actions such as Ctrl+F (search for token) and stop for searching through partially observed documents. A model searches iteratively, conditioning each command on the input question and the sentences it has observed previously. Thus, our task requires models to `feed themselves' rather than spoon-feeding them with information. This casts MRC as a sequential decision-making problem amenable to reinforcement learning (RL)..As an initial case study, we repurpose two well known, related corpora with different difficulty levels for our interactive MRC task: SQuAD and NewsQA. Table TABREF2 shows some examples of a model performing interactive MRC on these datasets. Naturally, our reframing makes the MRC problem harder; however, we believe the added demands of iMRC more closely match web-level QA and may lead to deeper comprehension of documents' content..The main contributions of this work are as follows:.We describe a method to make MRC datasets interactive and formulate the new task as an RL problem..We develop a baseline agent that combines a top performing MRC model and a state-of-the-art RL optimization algorithm and test it on our iMRC tasks..We conduct experiments on several variants of iMRC and discuss the significant challenges posed by our setting. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "34\n",
      "What distinguishes the model developed in this work from previous skip-reading methods? \n",
      "\n",
      "Answer 1: The model developed in this work enables an agent to jump through a document in a more dynamic manner, in some sense combining aspects of skip-reading and re-reading. It can jump forward, backward, or to an arbitrary position, depending on the query. This distinguishes it from previous skip-reading methods which only consider jumping over a few consecutive tokens and the skipping operations are usually unidirectional.\n",
      "Question : for the text Skip-reading BIBREF6, BIBREF7, BIBREF8 is an existing setting in which MRC models read partial documents. Concretely, these methods assume that not all tokens in the input sequence are useful, and therefore learn to skip irrelevant tokens based on the current input and their internal memory. Since skipping decisions are discrete, the models are often optimized by the REINFORCE algorithm BIBREF9. For example, the structural-jump-LSTM proposed in BIBREF10 learns to skip and jump over chunks of text. In a similar vein, BIBREF11 designed a QA task where the model reads streaming data unidirectionally, without knowing when the question will be provided. Skip-reading approaches are limited in that they only consider jumping over a few consecutive tokens and the skipping operations are usually unidirectional. Based on the assumption that a single pass of reading may not provide sufficient information, multi-pass reading methods have also been studied BIBREF12, BIBREF13..Compared to skip-reading and multi-turn reading, our work enables an agent to jump through a document in a more dynamic manner, in some sense combining aspects of skip-reading and re-reading. For example, it can jump forward, backward, or to an arbitrary position, depending on the query. This also distinguishes the model we develop in this work from ReasoNet BIBREF13, where an agent decides when to stop unidirectional reading..Recently, BIBREF14 propose DocQN, which is a DQN-based agent that leverages the (tree) structure of documents and navigates across sentences and paragraphs. The proposed method has been shown to outperform vanilla DQN and IR baselines on TriviaQA dataset. The main differences between our work and DocQA include: iMRC does not depend on extra meta information of documents (e.g., title, paragraph title) for building document trees as in DocQN; our proposed environment is partially-observable, and thus an agent is required to explore and memorize the environment via interaction; the action space in our setting (especially for the Ctrl+F command as defined in later section) is arguably larger than the tree sampling action space in DocQN..Closely related to iMRC is work by BIBREF15, in which the authors introduce a collection of synthetic tasks to train and test information-seeking capabilities in neural models. We extend that work by developing a realistic and challenging text-based task..Broadly speaking, our approach is also linked to the optimal stopping problem in the literature Markov decision processes (MDP) BIBREF16, where at each time-step the agent either continues or stops and accumulates reward. Here, we reformulate conventional QA tasks through the lens of optimal stopping, in hopes of improving over the shallow matching behaviors exhibited by many MRC systems. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "35\n",
      "Question 1: What is the structure of the iSQuAD and iNewsQA datasets?\n",
      "\n",
      "Answer 1: Every data-point consists of a paragraph, a question, and an answer that is a word span defined by head and tail positions in the paragraph. The iSQuAD and iNewsQA datasets were built based on SQuAD v1.1 and NewsQA, respectively, where NewsQA is more difficult due to its larger vocabulary, more difficult questions, and longer source documents. The paragraphs are first split into a list of sentences, and agents interact with the dataset by issuing commands to reveal hidden sentences progressively to gather the information needed to answer a question within a limited number of interaction steps.\n",
      "Question : for the text We build the iSQuAD and iNewsQA datasets based on SQuAD v1.1 BIBREF0 and NewsQA BIBREF1. Both original datasets share similar properties. Specifically, every data-point consists of a tuple, $\\lbrace p, q, a\\rbrace $, where $p$ represents a paragraph, $q$ a question, and $a$ is the answer. The answer is a word span defined by head and tail positions in $p$. NewsQA is more difficult than SQuAD because it has a larger vocabulary, more difficult questions, and longer source documents..We first split every paragraph $p$ into a list of sentences $\\mathcal {S} = \\lbrace s_1, s_2, ..., s_n\\rbrace $, where $n$ stands for number of sentences in $p$. Given a question $q$, rather than showing the entire paragraph $p$, we only show an agent the first sentence $s_1$ and withhold the rest. The agent must issue commands to reveal the hidden sentences progressively and thereby gather the information needed to answer question $q$..An agent decides when to stop interacting and output an answer, but the number of interaction steps is limited. Once an agent has exhausted its step budget, it is forced to answer the question. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "36\n",
      "Question 1: What are the four actions the agent can issue during the information gathering phase of iMRC?\n",
      "Answer 1: The four actions the agent can issue during the information gathering phase of iMRC are previous, next, Ctrl+F $<$query$>$, and stop.\n",
      "Question : for the text To better describe the action space of iMRC, we split an agent's actions into two phases: information gathering and question answering. During the information gathering phase, the agent interacts with the environment to collect knowledge. It answers questions with its accumulated knowledge in the question answering phase..Information Gathering: At step $t$ of the information gathering phase, the agent can issue one of the following four actions to interact with the paragraph $p$, where $p$ consists of $n$ sentences and where the current observation corresponds to sentence $s_k,~1 \\le k \\le n$:.previous: jump to $ \\small {\\left\\lbrace \\begin{array}{ll} s_n & \\text{if $k = 1$,}\\\\ s_{k-1} & \\text{otherwise;} \\end{array}\\right.} $.next: jump to $ \\small {\\left\\lbrace \\begin{array}{ll} s_1 & \\text{if $k = n$,}\\\\ s_{k+1} & \\text{otherwise;} \\end{array}\\right.} $.Ctrl+F $<$query$>$: jump to the sentence that contains the next occurrence of “query”;.stop: terminate information gathering phase..Question Answering: We follow the output format of both SQuAD and NewsQA, where an agent is required to point to the head and tail positions of an answer span within $p$. Assume that at step $t$ the agent stops interacting and the observation $o_t$ is $s_k$. The agent points to a head-tail position pair in $s_k$. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "37\n",
      "Question 1: What evaluation metric is used to compare predicted answers against ground-truth in iMRC?\n",
      "\n",
      "Answer 1: The $\\text{F}_1$ score is used to compare predicted answers against ground-truth in iMRC.\n",
      "Question : for the text Since iMRC involves both MRC and RL, we adopt evaluation metrics from both settings. First, as a question answering task, we use $\\text{F}_1$ score to compare predicted answers against ground-truth, as in previous works. When there exist multiple ground-truth answers, we report the max $\\text{F}_1$ score. Second, mastering multiple games remains quite challenging for RL agents. Therefore, we evaluate an agent's performance during both its training and testing phases. During training, we report training curves averaged over 3 random seeds. During test, we follow common practice in supervised learning tasks where we report the agent's test performance corresponding to its best validation performance . generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "38\n",
      "Q: What is the objective of the agent in the iMRC game?\n",
      "A: The objective of the agent in the iMRC game is to maximize the expected discounted sum of rewards, E[Σt γt rt].\n",
      "Question : for the text As described in the previous section, we convert MRC tasks into sequential decision-making problems (which we will refer to as games). These can be described naturally within the reinforcement learning (RL) framework. Formally, tasks in iMRC are partially observable Markov decision processes (POMDP) BIBREF17. An iMRC data-point is a discrete-time POMDP defined by $(S, T, A, \\Omega , O, R, \\gamma )$, where $\\gamma \\in [0, 1]$ is the discount factor and the other elements are described in detail below..Environment States ($S$): The environment state at turn $t$ in the game is $s_t \\in S$. It contains the complete internal information of the game, much of which is hidden from the agent. When an agent issues an action $a_t$, the environment transitions to state $s_{t+1}$ with probability $T(s_{t+1} | s_t, a_t)$). In this work, transition probabilities are either 0 or 1 (i.e., deterministic environment)..Actions ($A$): At each game turn $t$, the agent issues an action $a_t \\in A$. We will elaborate on the action space of iMRC in the action space section..Observations ($\\Omega $): The text information perceived by the agent at a given game turn $t$ is the agent's observation, $o_t \\in \\Omega $, which depends on the environment state and the previous action with probability $O(o_t|s_t)$. In this work, observation probabilities are either 0 or 1 (i.e., noiseless observation). Reward Function ($R$): Based on its actions, the agent receives rewards $r_t = R(s_t, a_t)$. Its objective is to maximize the expected discounted sum of rewards $E \\left[\\sum _t \\gamma ^t r_t \\right]$. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "39\n",
      "Question 1: What is the deadline for the AAAI conference?\n",
      "\n",
      "Answer 1: The deadline for the AAAI conference is not specified in the given text.\n",
      "Question : for the text Given the question “When is the deadline of AAAI?”, as a human, one might try searching “AAAI” on a search engine, follow the link to the official AAAI website, then search for keywords “deadline” or “due date” on the website to jump to a specific paragraph. Humans have a deep understanding of questions because of their significant background knowledge. As a result, the keywords they use to search are not limited to what appears in the question..Inspired by this observation, we study 3 query types for the Ctrl+F $<$query$>$ command..One token from the question: the setting with smallest action space. Because iMRC deals with Ctrl+F commands by exact string matching, there is no guarantee that all sentences are accessible from question tokens only..One token from the union of the question and the current observation: an intermediate level where the action space is larger..One token from the dataset vocabulary: the action space is huge (see Table TABREF16 for statistics of SQuAD and NewsQA). It is guaranteed that all sentences in all documents are accessible through these tokens. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "40\n",
      "What is MMHS150K?\n",
      "MMHS150K is the biggest available hate speech dataset, and the first one composed of multimodal data, namely tweets formed by image and text, created for the task of hate speech detection on multimodal publications.\n",
      "Question : for the text In this work we have explored the task of hate speech detection on multimodal publications. We have created MMHS150K, to our knowledge the biggest available hate speech dataset, and the first one composed of multimodal data, namely tweets formed by image and text. We have trained different textual, visual and multimodal models with that data, and found out that, despite the fact that images are useful for hate speech detection, the multimodal models do not outperform the textual models. Finally, we have analyzed the challenges of the proposed task and dataset. Given that most of the content in Social Media nowadays is multimodal, we truly believe on the importance of pushing forward this research. The code used in this work is available in . generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "41\n",
      "Question 1: What is the definition of hate speech according to Twitter's hateful conduct policy?\n",
      "\n",
      "Answer 1: According to Twitter's hateful conduct policy, hate speech refers to violence against or direct attack or threat towards other individuals or groups based on their race, ethnicity, national origin, sexual orientation, gender, gender identity, religious affiliation, age, disability, or serious disease.\n",
      "Question : for the text Social Media platforms such as Facebook, Twitter or Reddit have empowered individuals' voices and facilitated freedom of expression. However they have also been a breeding ground for hate speech and other types of online harassment. Hate speech is defined in legal literature as speech (or any form of expression) that expresses (or seeks to promote, or has the capacity to increase) hatred against a person or a group of people because of a characteristic they share, or a group to which they belong BIBREF0. Twitter develops this definition in its hateful conduct policy as violence against or directly attack or threaten other people on the basis of race, ethnicity, national origin, sexual orientation, gender, gender identity, religious affiliation, age, disability, or serious disease..In this work we focus on hate speech detection. Due to the inherent complexity of this task, it is important to distinguish hate speech from other types of online harassment. In particular, although it might be offensive to many people, the sole presence of insulting terms does not itself signify or convey hate speech. And, the other way around, hate speech may denigrate or threaten an individual or a group of people without the use of any profanities. People from the african-american community, for example, often use the term nigga online, in everyday language, without malicious intentions to refer to folks within their community, and the word cunt is often used in non hate speech publications and without any sexist purpose. The goal of this work is not to discuss if racial slur, such as nigga, should be pursued. The goal is to distinguish between publications using offensive terms and publications attacking communities, which we call hate speech..Modern social media content usually include images and text. Some of these multimodal publications are only hate speech because of the combination of the text with a certain image. That is because, as we have stated, the presence of offensive terms does not itself signify hate speech, and the presence of hate speech is often determined by the context of a publication. Moreover, users authoring hate speech tend to intentionally construct publications where the text is not enough to determine they are hate speech. This happens especially in Twitter, where multimodal tweets are formed by an image and a short text, which in many cases is not enough to judge them. In those cases, the image might give extra context to make a proper judgement. Fig. FIGREF5 shows some of such examples in MMHS150K..The contributions of this work are as follows:.[noitemsep,leftmargin=*].We propose the novel task of hate speech detection in multimodal publications, collect, annotate and publish a large scale dataset..We evaluate state of the art multimodal models on this specific task and compare their performance with unimodal detection. Even though images are proved to be useful for hate speech detection, the proposed multimodal models do not outperform unimodal textual models..We study the challenges of the proposed task, and open the field for future research. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "42\n",
      "Question 1: What is the objective of building a hate speech detector that leverages both textual and visual data?\n",
      "\n",
      "Answer 1: The objective is to detect hate speech publications based on the context given by both textual and visual data, and to study how the multimodal context can improve the performance compared to an unimodal context. Different models, such as the Feature Concatenation Model, Spatial Concatenation Model, and Textual Kernels Model, were evaluated to achieve this goal.\n",
      "Question : for the text The objective of this work is to build a hate speech detector that leverages both textual and visual data and detects hate speech publications based on the context given by both data modalities. To study how the multimodal context can boost the performance compared to an unimodal context we evaluate different models: a Feature Concatenation Model (FCM), a Spatial Concatenation Model (SCM) and a Textual Kernels Model (TKM). All of them are CNN+RNN models with three inputs: the tweet image, the tweet text and the text appearing in the image (if any). generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "43\n",
      "Question 1: What is the dimensionality of the feature vector after the three fully connected layers in the FCM architecture?\n",
      "\n",
      "Answer 1: The dimensionality of the feature vector is reduced to two, which is the number of classes, in the last classification layer after being processed by three fully connected layers of decreasing dimensionality $(2348, 1024, 512)$ with following corresponding batch normalization and ReLu layers.\n",
      "Question : for the text The image is fed to the Inception v3 architecture and the 2048 dimensional feature vector after the last average pooling layer is used as the visual representation. This vector is then concatenated with the 150 dimension vectors of the LSTM last word hidden states of the image text and the tweet text, resulting in a 2348 feature vector. This vector is then processed by three fully connected layers of decreasing dimensionality $(2348, 1024, 512)$ with following corresponding batch normalization and ReLu layers until the dimensions are reduced to two, the number of classes, in the last classification layer. The FCM architecture is illustrated in Fig. FIGREF26. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "44\n",
      "Question 1: What is the visual representation used in the SCM, and how is it processed?\n",
      "Answer 1: The SCM uses the $8\\times 8\\times 2048$ feature map after the last Inception module as its visual representation. This feature map is concatenated with 150 dimension vectors encoding the tweet text and the tweet image text at each spatial location. This multimodal feature map is then processed by two Inception-E blocks, followed by dropout and average pooling, and finally, three fully connected layers are used to reduce the dimensionality until the classification layer.\n",
      "Question : for the text Instead of using the latest feature vector before classification of the Inception v3 as the visual representation, in the SCM we use the $8\\times 8\\times 2048$ feature map after the last Inception module. Then we concatenate the 150 dimension vectors encoding the tweet text and the tweet image text at each spatial location of that feature map. The resulting multimodal feature map is processed by two Inception-E blocks BIBREF28. After that, dropout and average pooling are applied and, as in the FCM model, three fully connected layers are used to reduce the dimensionality until the classification layer. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "45\n",
      "Q: What is the purpose of the TKM design in comparison to concatenation models?\n",
      "\n",
      "A: The purpose of the TKM design is to capture interactions between the two modalities more expressively than concatenation models.\n",
      "Question : for the text The TKM design, inspired by BIBREF20 and BIBREF21, aims to capture interactions between the two modalities more expressively than concatenation models. As in SCM we use the $8\\times 8\\times 2048$ feature map after the last Inception module as the visual representation. From the 150 dimension vector encoding the tweet text, we learn $K_t$ text dependent kernels using independent fully connected layers that are trained together with the rest of the model. The resulting $K_t$ text dependent kernels will have dimensionality of $1\\times 1\\times 2048$. We do the same with the feature vector encoding the image text, learning $K_{it}$ kernels. The textual kernels are convolved with the visual feature map in the channel dimension at each spatial location, resulting in a $8\\times 8\\times (K_i+K_{it})$ multimodal feature map, and batch normalization is applied. Then, as in the SCM, the 150 dimension vectors encoding the tweet text and the tweet image text are concatenated at each spatial dimension. The rest of the architecture is the same as in SCM: two Inception-E blocks, dropout, average pooling and three fully connected layers until the classification layer. The number of tweet textual kernels $K_t$ and tweet image textual kernels $K_it$ is set to $K_t = 10$ and $K_it = 5$. The TKM architecture is illustrated in Fig. FIGREF29. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "46\n",
      "Question 1: What type of loss is used to train the multimodal models and do they use weight balancing?\n",
      "\n",
      "Answer 1: The multimodal models are trained with a Cross-Entropy loss with Softmax activations and an ADAM optimizer with an initial learning rate of $1e-4$. Weight balancing is used to compensate for the high class imbalance in the dataset.\n",
      "Question : for the text We train the multimodal models with a Cross-Entropy loss with Softmax activations and an ADAM optimizer with an initial learning rate of $1e-4$. Our dataset suffers from a high class imbalance, so we weight the contribution to the loss of the samples to totally compensate for it. One of the goals of this work is to explore how every one of the inputs contributes to the classification and to prove that the proposed model can learn concurrences between visual and textual data useful to improve the hate speech classification results on multimodal data. To do that we train different models where all or only some inputs are available. When an input is not available, we set it to zeros, and we do the same when an image has no text. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "47\n",
      "Question 1: How is the text in the image used in determining if a publication is hate speech or not?\n",
      "\n",
      "Answer 1: The text in the image is extracted and inputted to the multimodal models alongside the tweet text to assist in determining if a publication is hate speech or not. The model could learn different relations between the image text and the area in the image where it appears, allowing for interpretation of the text in a different way depending on its location.\n",
      "Question : for the text The text in the image can also contain important information to decide if a publication is hate speech or not, so we extract it and also input it to our model. To do so, we use Google Vision API Text Detection module BIBREF27. We input the tweet text and the text from the image separately to the multimodal models, so it might learn different relations between them and between them and the image. For instance, the model could learn to relate the image text with the area in the image where the text appears, so it could learn to interpret the text in a different way depending on the location where it is written in the image. The image text is also encoded by the LSTM as the hidden state after processing its last word. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "48\n",
      "Question 1: What is the CNN architecture used as the image features extractor during training?\n",
      "\n",
      "Answer 1: The CNN architecture used as the image features extractor during training is an Imagenet pre-trained Google Inception v3 architecture.\n",
      "Question : for the text All images are resized such that their shortest size has 500 pixels. During training, online data augmentation is applied as random cropping of $299\\times 299$ patches and mirroring. We use a CNN as the image features extractor which is an Imagenet BIBREF24 pre-trained Google Inception v3 architecture BIBREF25. The fine-tuning process of the Inception v3 layers aims to modify its weights to extract the features that, combined with the textual information, are optimal for hate speech detection. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "49\n",
      "Question 1: What type of word embeddings are used as input representations for the LSTM model in hate/not hate classification?\n",
      "\n",
      "Answer 1: GloVe BIBREF26 embeddings are used as word input representations for the single layer LSTM model with a 150-dimensional hidden state in hate/not hate classification. The pre-trained model has been trained in two billion tweets to ensure that the model can produce word embeddings for slang and other words typically used in Twitter.\n",
      "Question : for the text We train a single layer LSTM with a 150-dimensional hidden state for hate / not hate classification. The input dimensionality is set to 100 and GloVe BIBREF26 embeddings are used as word input representations. Since our dataset is not big enough to train a GloVe word embedding model, we used a pre-trained model that has been trained in two billion tweets. This ensures that the model will be able to produce word embeddings for slang and other words typically used in Twitter. To process the tweets text before generating the word embeddings, we use the same pipeline as the model authors, which includes generating symbols to encode Twitter special interactions such as user mentions (@user) or hashtags (#hashtag). To encode the tweet text and input it later to multimodal models, we use the LSTM hidden state after processing the last tweet word. Since the LSTM has been trained for hate speech classification, it extracts the most useful information for this task from the text, which is encoded in the hidden state after inputting the last tweet word. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "50\n",
      "What is one dataset mentioned in the article for hate speech detection?\n",
      "One dataset mentioned is DT, which consists of 24,783 tweets annotated as hate, offensive language or neither.\n",
      "Question : for the text The literature on detecting hate speech on online textual publications is extensive. Schmidt and Wiegand BIBREF1 recently provided a good survey of it, where they review the terminology used over time, the features used, the existing datasets and the different approaches. However, the field lacks a consistent dataset and evaluation protocol to compare proposed methods. Saleem et al. BIBREF2 compare different classification methods detecting hate speech in Reddit and other forums. Wassem and Hovy BIBREF3 worked on hate speech detection on twitter, published a manually annotated dataset and studied its hate distribution. Later Wassem BIBREF4 extended the previous published dataset and compared amateur and expert annotations, concluding that amateur annotators are more likely than expert annotators to label items as hate speech. Park and Fung BIBREF5 worked on Wassem datasets and proposed a classification method using a CNN over Word2Vec BIBREF6 word embeddings, showing also classification results on racism and sexism hate sub-classes. Davidson et al. BIBREF7 also worked on hate speech detection on twitter, publishing another manually annotated dataset. They test different classifiers such as SVMs and decision trees and provide a performance comparison. Malmasi and Zampieri BIBREF8 worked on Davidson's dataset improving his results using more elaborated features. ElSherief et al. BIBREF9 studied hate speech on twitter and selected the most frequent terms in hate tweets based on Hatebase, a hate expression repository. They propose a big hate dataset but it lacks manual annotations, and all the tweets containing certain hate expressions are considered hate speech. Zhang et al. BIBREF10 recently proposed a more sophisticated approach for hate speech detection, using a CNN and a GRU BIBREF11 over Word2Vec BIBREF6 word embeddings. They show experiments in different datasets outperforming previous methods. Next, we summarize existing hate speech datasets:.[noitemsep,leftmargin=*].RM BIBREF10: Formed by $2,435$ tweets discussing Refugees and Muslims, annotated as hate or non-hate..DT BIBREF7: Formed by $24,783$ tweets annotated as hate, offensive language or neither. In our work, offensive language tweets are considered as non-hate..WZ-LS BIBREF5: A combination of Wassem datasets BIBREF4, BIBREF3 labeled as racism, sexism, neither or both that make a total of $18,624$ tweets..Semi-Supervised BIBREF9: Contains $27,330$ general hate speech Twitter tweets crawled in a semi-supervised manner..Although often modern social media publications include images, not too many contributions exist that exploit visual information. Zhong et al. BIBREF12 worked on classifying Instagram images as potential cyberbullying targets, exploiting both the image content, the image caption and the comments. However, their visual information processing is limited to the use of features extracted by a pre-trained CNN, the use of which does not achieve any improvement. Hosseinmardi et al. BIBREF13 also address the problem of detecting cyberbullying incidents on Instagram exploiting both textual and image content. But, again, their visual information processing is limited to use the features of a pre-trained CNN, and the improvement when using visual features on cyberbullying classification is only of 0.01%. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "1\n",
      "Question 1: What is the approach used in Visual Question Answering (VQA) to merge both data modalities?\n",
      "Answer 1: The approach in VQA is to model very precise correlations between the image and the question representations in order to merge both data modalities and decide which answer is correct.\n",
      "Question : for the text A typical task in multimodal visual and textual analysis is to learn an alignment between feature spaces. To do that, usually a CNN and a RNN are trained jointly to learn a joint embedding space from aligned multimodal data. This approach is applied in tasks such as image captioning BIBREF14, BIBREF15 and multimodal image retrieval BIBREF16, BIBREF17. On the other hand, instead of explicitly learning an alignment between two spaces, the goal of Visual Question Answering (VQA) is to merge both data modalities in order to decide which answer is correct. This problem requires modeling very precise correlations between the image and the question representations. The VQA task requirements are similar to our hate speech detection problem in multimodal publications, where we have a visual and a textual input and we need to combine both sources of information to understand the global context and make a decision. We thus take inspiration from the VQA literature for the tested models. Early VQA methods BIBREF18 fuse textual and visual information by feature concatenation. Later methods, such as Multimodal Compact Bilinear pooling BIBREF19, utilize bilinear pooling to learn multimodal features. An important limitation of these methods is that the multimodal features are fused in the latter model stage, so the textual and visual relationships are modeled only in the last layers. Another limitation is that the visual features are obtained by representing the output of the CNN as a one dimensional vector, which losses the spatial information of the input images. In a recent work, Gao et al. BIBREF20 propose a feature fusion scheme to overcome these limitations. They learn convolution kernels from the textual information –which they call question-guided kernels– and convolve them with the visual information in an earlier stage to get the multimodal features. Margffoy-Tuay et al. BIBREF21 use a similar approach to combine visual and textual information, but they address a different task: instance segmentation guided by natural language queries. We inspire in these latest feature fusion works to build the models for hate speech detection. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "2\n",
      "What are some reasons why the proposed multimodal models did not perform well in detecting hate speech compared to the textual models?\n",
      "The proposed multimodal models did not perform well in detecting hate speech compared to the textual models due to a few reasons including noisy data, the complexity and diversity of multimodal relations, and a small set of multimodal examples.\n",
      "Question : for the text Table TABREF31 shows the F-score, the Area Under the ROC Curve (AUC) and the mean accuracy (ACC) of the proposed models when different inputs are available. $TT$ refers to the tweet text, $IT$ to the image text and $I$ to the image. It also shows results for the LSTM, for the Davison method proposed in BIBREF7 trained with MMHS150K, and for random scores. Fig. FIGREF32 shows the Precision vs Recall plot and the ROC curve (which plots the True Positive Rate vs the False Positive Rate) of the different models..First, notice that given the subjectivity of the task and the discrepancies between annotators, getting optimal scores in the evaluation metrics is virtually impossible. However, a system with relatively low metric scores can still be very useful for hate speech detection in a real application: it will fire on publications for which most annotators agree they are hate, which are often the stronger attacks. The proposed LSTM to detect hate speech when only text is available, gets similar results as the method presented in BIBREF7, which we trained with MMHS150K and the same splits. However, more than substantially advancing the state of the art on hate speech detection in textual publications, our key purpose in this work is to introduce and work on its detection on multimodal publications. We use LSTM because it provides a strong representation of the tweet texts..The FCM trained only with images gets decent results, considering that in many publications the images might not give any useful information for the task. Fig. FIGREF33 shows some representative examples of the top hate and not hate scored images of this model. Many hate tweets are accompanied by demeaning nudity images, being sexist or homophobic. Other racist tweets are accompanied by images caricaturing black people. Finally, MEMES are also typically used in hate speech publications. The top scored images for not hate are portraits of people belonging to minorities. This is due to the use of slur inside these communities without an offensive intention, such as the word nigga inside the afro-american community or the word dyke inside the lesbian community. These results show that images can be effectively used to discriminate between offensive and non-offensive uses of those words..Despite the model trained only with images proves that they are useful for hate speech detection, the proposed multimodal models are not able to improve the detection compared to the textual models. Besides the different architectures, we have tried different training strategies, such as initializing the CNN weights with a model already trained solely with MMHS150K images or using dropout to force the multimodal models to use the visual information. Eventually, though, these models end up using almost only the text input for the prediction and producing very similar results to those of the textual models. The proposed multimodal models, such as TKM, have shown good performance in other tasks, such as VQA. Next, we analyze why they do not perform well in this task and with this data:.[noitemsep,leftmargin=*].Noisy data. A major challenge of this task is the discrepancy between annotations due to subjective judgement. Although this affects also detection using only text, its repercussion is bigger in more complex tasks, such as detection using images or multimodal detection..Complexity and diversity of multimodal relations. Hate speech multimodal publications employ a lot of background knowledge which makes the relations between visual and textual elements they use very complex and diverse, and therefore difficult to learn by a neural network..Small set of multimodal examples. Fig. FIGREF5 shows some of the challenging multimodal hate examples that we aimed to detect. But although we have collected a big dataset of $150K$ tweets, the subset of multimodal hate there is still too small to learn the complex multimodal relations needed to identify multimodal hate. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "3\n",
      "Question 1: What makes the MMHS150K dataset different from existing hate speech datasets?\n",
      "\n",
      "Answer 1: Unlike existing hate speech datasets that only contain textual data, the MMHS150K dataset is multimodal, containing both text and accompanying images. Additionally, a reference benchmark for hate speech detection did not exist before this dataset was created. The dataset is comprised of 150,000 manually annotated tweets and is available online.\n",
      "Question : for the text Existing hate speech datasets contain only textual data. Moreover, a reference benchmark does not exists. Most of the published datasets are crawled from Twitter and distributed as tweet IDs but, since Twitter removes reported user accounts, an important amount of their hate tweets is no longer accessible. We create a new manually annotated multimodal hate speech dataset formed by $150,000$ tweets, each one of them containing text and an image. We call the dataset MMHS150K, and made it available online . In this section, we explain the dataset creation steps. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "4\n",
      "Question 1: How many tweets were labeled as hate tweets in the dataset?\n",
      "\n",
      "Answer 1: 36,978 tweets were labeled as hate tweets in the dataset, which were divided into subcategories such as racist, sexist, homophobic, religion-based hate and other hate tweets.\n",
      "Question : for the text We annotate the gathered tweets using the crowdsourcing platform Amazon Mechanical Turk. There, we give the workers the definition of hate speech and show some examples to make the task clearer. We then show the tweet text and image and we ask them to classify it in one of 6 categories: No attacks to any community, racist, sexist, homophobic, religion based attacks or attacks to other communities. Each one of the $150,000$ tweets is labeled by 3 different workers to palliate discrepancies among workers..We received a lot of valuable feedback from the annotators. Most of them had understood the task correctly, but they were worried because of its subjectivity. This is indeed a subjective task, highly dependent on the annotator convictions and sensitivity. However, we expect to get cleaner annotations the more strong the attack is, which are the publications we are more interested on detecting. We also detected that several users annotate tweets for hate speech just by spotting slur. As already said previously, just the use of particular words can be offensive to many people, but this is not the task we aim to solve. We have not included in our experiments those hits that were made in less than 3 seconds, understanding that it takes more time to grasp the multimodal context and make a decision..We do a majority voting between the three annotations to get the tweets category. At the end, we obtain $112,845$ not hate tweets and $36,978$ hate tweets. The latest are divided in $11,925$ racist, $3,495$ sexist, $3,870$ homophobic, 163 religion-based hate and $5,811$ other hate tweets (Fig. FIGREF17). In this work, we do not use hate sub-categories, and stick to the hate / not hate split. We separate balanced validation ($5,000$) and test ($10,000$) sets. The remaining tweets are used for training..We also experimented using hate scores for each tweet computed given the different votes by the three annotators instead of binary labels. The results did not present significant differences to those shown in the experimental part of this work, but the raw annotations will be published nonetheless for further research..As far as we know, this dataset is the biggest hate speech dataset to date, and the first multimodal hate speech dataset. One of its challenges is to distinguish between tweets using the same key offensive words that constitute or not an attack to a community (hate speech). Fig. FIGREF18 shows the percentage of hate and not hate tweets of the top keywords. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "5\n",
      "Question 1: What method was used to ensure all instances in the hate speech database contain both visual and textual information?\n",
      "\n",
      "Answer 1: The TextFCN Fully Convolutional Network was used to produce a pixel wise text probability map of an image, and empirical thresholds were set to discard images with a significant total text probability. This method filtered out 23% of the collected tweets to ensure all instances contain both visual and textual information.\n",
      "Question : for the text We aim to create a multimodal hate speech database where all the instances contain visual and textual information that we can later process to determine if a tweet is hate speech or not. But a considerable amount of the images of the selected tweets contain only textual information, such as screenshots of other tweets. To ensure that all the dataset instances contain both visual and textual information, we remove those tweets. To do that, we use TextFCN BIBREF22, BIBREF23 , a Fully Convolutional Network that produces a pixel wise text probability map of an image. We set empirical thresholds to discard images that have a substantial total text probability, filtering out $23\\%$ of the collected tweets. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "6\n",
      "What measures did the researchers take to filter the tweets before gathering them?\n",
      "\n",
      "The researchers filtered out retweets, tweets containing less than three words and tweets containing porn related terms from their selection of real-time tweets gathered using the Twitter API. They also kept the ones that included images and selected tweets containing any of the 51 Hatebase terms that are more common in hate speech tweets, as studied in BIBREF9.\n",
      "Question : for the text We used the Twitter API to gather real-time tweets from September 2018 until February 2019, selecting the ones containing any of the 51 Hatebase terms that are more common in hate speech tweets, as studied in BIBREF9. We filtered out retweets, tweets containing less than three words and tweets containing porn related terms. From that selection, we kept the ones that included images and downloaded them. Twitter applies hate speech filters and other kinds of content control based on its policy, although the supervision is based on users' reports. Therefore, as we are gathering tweets from real-time posting, the content we get has not yet passed any filter. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "7\n",
      "Question 1: Who supported the work presented in the text?\n",
      "\n",
      "Answer 1: The work presented in the text is supported by the National Natural Science Foundation of China (No. 61602479, No. 61303172, No. 61403385) and the Strategic Priority Research Program of the Chinese Academy of Sciences (Grant No. XDB02070005).\n",
      "Question : for the text We would like to thank reviewers for their comments, and acknowledge Kaggle and BioASQ for making the datasets available. This work is supported by the National Natural Science Foundation of China (No. 61602479, No. 61303172, No. 61403385) and the Strategic Priority Research Program of the Chinese Academy of Sciences (Grant No. XDB02070005). generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "8\n",
      "What are some of the baseline clustering methods used in the experiment?\n",
      "K-means, Skip-thought Vectors, Recursive Neural Network, Paragraph Vector, Average Embedding, Latent Semantic Analysis, Laplacian Eigenmaps, and Locality Preserving Indexing were the baseline clustering methods used in the experiment.\n",
      "Question : for the text In our experiment, some widely used text clustering methods are compared with our approach. Besides K-means, Skip-thought Vectors, Recursive Neural Network and Paragraph Vector based clustering methods, four baseline clustering methods are directly based on the popular unsupervised dimensionality reduction methods as described in Section SECREF11 . We further compare our approach with some other non-biased neural networks, such as bidirectional RNN. More details are listed as follows:.K-means K-means BIBREF42 on original keyword features which are respectively weighted with term frequency (TF) and term frequency-inverse document frequency (TF-IDF)..Skip-thought Vectors (SkipVec) This baseline BIBREF35 gives an off-the-shelf encoder to produce highly generic sentence representations. The encoder is trained using a large collection of novels and provides three encoder modes, that are unidirectional encoder (SkipVec (Uni)) with 2,400 dimensions, bidirectional encoder (SkipVec (Bi)) with 2,400 dimensions and combined encoder (SkipVec (Combine)) with SkipVec (Uni) and SkipVec (Bi) of 2,400 dimensions each. K-means is employed on the these vector representations respectively..Recursive Neural Network (RecNN) In BIBREF6 , the tree structure is firstly greedy approximated via unsupervised recursive autoencoder. Then, semi-supervised recursive autoencoders are used to capture the semantics of texts based on the predicted structure. In order to make this recursive-based method completely unsupervised, we remove the cross-entropy error in the second phrase to learn vector representation and subsequently employ K-means on the learned vectors of the top tree node and the average of all vectors in the tree..Paragraph Vector (Para2vec) K-means on the fixed size feature vectors generated by Paragraph Vector (Para2vec) BIBREF25 which is an unsupervised method to learn distributed representation of words and paragraphs. In our experiments, we use the open source software released by Mesnil et al. BIBREF43 ..Average Embedding (AE) K-means on the weighted average vectors of the word embeddings which are respectively weighted with TF and TF-IDF. The dimension of average vectors is equal to and decided by the dimension of word vectors used in our experiments..Latent Semantic Analysis (LSA) K-means on the reduced subspace vectors generated by Singular Value Decomposition (SVD) method. The dimension of subspace is default set to the number of clusters, we also iterate the dimensions ranging from 10:10:200 to get the best performance, that is 10 on SearchSnippets, 20 on StackOverflow and 20 on Biomedical in our experiments..Laplacian Eigenmaps (LE) This baseline, using Laplacian Eigenmaps and subsequently employing K-means algorithm, is well known as spectral clustering BIBREF44 . The dimension of subspace is default set to the number of clusters BIBREF18 , BIBREF38 , we also iterate the dimensions ranging from 10:10:200 to get the best performance, that is 20 on SearchSnippets, 70 on StackOverflow and 30 on Biomedical in our experiments..Locality Preserving Indexing (LPI) This baseline, projecting the texts into a lower dimensional semantic space, can discover both the geometric and discriminating structures of the original feature space BIBREF38 . The dimension of subspace is default set to the number of clusters BIBREF38 , we also iterate the dimensions ranging from 10:10:200 to get the best performance, that is 20 on SearchSnippets, 80 on StackOverflow and 30 on Biomedical in our experiments..bidirectional RNN (bi-RNN) We replace the CNN model in our framework as in Figure FIGREF5 with some bi-RNN models. Particularly, LSTM and GRU units are used in the experiments. In order to generate the fixed-length document representation from the variable-length vector sequences, for both bi-LSTM and bi-GRU based clustering methods, we further utilize three pooling methods: last pooling (using the last hidden state), mean pooling and element-wise max pooling. These pooling methods are respectively used in the previous works BIBREF45 , BIBREF27 , BIBREF46 and BIBREF9 . For regularization, the training gradients of all parameters with an INLINEFORM0 2 norm larger than 40 are clipped to 40, as the previous work BIBREF47 . generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "9\n",
      "Question 1: How does the proposed framework for short text clustering differ from traditional approaches?\n",
      "Answer 1: The proposed framework for short text clustering uses deep feature representation learned from self-taught convolutional neural networks, and does not require the use of external tags/labels or complicated NLP pre-processing. It is also flexible, allowing for traditional dimension reduction approaches to enhance performance.\n",
      "Question : for the text With the emergence of social media, short text clustering has become an increasing important task. This paper explores a new perspective to cluster short texts based on deep feature representation learned from the proposed self-taught convolutional neural networks. Our framework can be successfully accomplished without using any external tags/labels and complicated NLP pre-processing, and and our approach is a flexible framework, in which the traditional dimension reduction approaches could be used to get performance enhancement. Our extensive experimental study on three short text datasets shows that our approach can achieve a significantly better performance. In the future, how to select and incorporate more effective semantic features into the proposed framework would call for more research. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "10\n",
      "Question 1: What is the size of the development set for the three public short text datasets used in the experiments?\n",
      "\n",
      "Answer 1: The development set for these datasets was randomly selected as 10% of the data.\n",
      "Question : for the text We test our proposed approach on three public short text datasets. The summary statistics and semantic topics of these datasets are described in Table TABREF24 and Table TABREF25 ..SearchSnippets. This dataset was selected from the results of web search transaction using predefined phrases of 8 different domains by Phan et al. BIBREF41 ..StackOverflow. We use the challenge data published in Kaggle.com. The raw dataset consists 3,370,528 samples through July 31st, 2012 to August 14, 2012. In our experiments, we randomly select 20,000 question titles from 20 different tags as in Table TABREF25 ..Biomedical. We use the challenge data published in BioASQ's official website. In our experiments, we randomly select 20, 000 paper titles from 20 different MeSH major topics as in Table TABREF25 . As described in Table TABREF24 , the max length of selected paper titles is 53..For these datasets, we randomly select 10% of data as the development set. Since SearchSnippets has been pre-processed by Phan et al. BIBREF41 , we do not further process this dataset. In StackOverflow, texts contain lots of computer terminology, and symbols and capital letters are meaningful, thus we do not do any pre-processed procedures. For Biomedical, we remove the symbols and convert letters into lower case. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "11\n",
      "What is the purpose of the Dynamic Convolutional Neural Network (DCNN)?\n",
      "\n",
      "The purpose of the Dynamic Convolutional Neural Network (DCNN) is to transform raw input text into a powerful representation for text classification through a series of basic operations, including wide one-dimensional convolution, folding, and dynamic max pooling. The DCNN is a popular deep convolutional neural network that has been successfully used for completely supervised learning tasks in text classification.\n",
      "Question : for the text In this section, we briefly review one popular deep convolutional neural network, Dynamic Convolutional Neural Network (DCNN) BIBREF10 as an instance of CNN in the following sections, which as the foundation of our proposed method has been successfully proposed for the completely supervised learning task, text classification..Taking a neural network with two convolutional layers in Figure FIGREF9 as an example, the network transforms raw input text to a powerful representation. Particularly, each raw text vector INLINEFORM0 is projected into a matrix representation INLINEFORM1 by looking up a word embedding INLINEFORM2 , where INLINEFORM3 is the length of one text. We also let INLINEFORM4 and INLINEFORM5 denote the weights of the neural networks. The network defines a transformation INLINEFORM6 INLINEFORM7 which transforms an input raw text INLINEFORM8 to a INLINEFORM9 -dimensional deep representation INLINEFORM10 . There are three basic operations described as follows:.Wide one-dimensional convolution This operation INLINEFORM0 is applied to an individual row of the sentence matrix INLINEFORM1 , and yields a resulting matrix INLINEFORM2 , where INLINEFORM3 is the width of convolutional filter..Folding In this operation, every two rows in a feature map are simply summed component-wisely. For a map of INLINEFORM0 rows, folding returns a map of INLINEFORM1 rows, thus halving the size of the representation and yielding a matrix feature INLINEFORM2 . Note that folding operation does not introduce any additional parameters..Dynamic INLINEFORM0 -max pooling Assuming the pooling parameter as INLINEFORM1 , INLINEFORM2 -max pooling selects the sub-matrix INLINEFORM3 of the INLINEFORM4 highest values in each row of the matrix INLINEFORM5 . For dynamic INLINEFORM6 -max pooling, the pooling parameter INLINEFORM7 is dynamically selected in order to allow for a smooth extraction of higher-order and longer-range features BIBREF10 . Given a fixed pooling parameter INLINEFORM8 for the topmost convolutional layer, the parameter INLINEFORM9 of INLINEFORM10 -max pooling in the INLINEFORM11 -th convolutional layer can be computed as follows: DISPLAYFORM0 .where INLINEFORM0 is the total number of convolutional layers in the network. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "12\n",
      "What is the Skip-thought model?\n",
      "\n",
      "The Skip-thought model is an unsupervised learning approach that trains an encoder-decoder model to reconstruct surrounding sentences of an encoded sentence, resulting in a distributed sentence encoder that can be used for various tasks, such as text classification and similarity matching.\n",
      "Question : for the text Recently, there is a revival of interest in DNN and many researchers have concentrated on using Deep Learning to learn features. Hinton and Salakhutdinov BIBREF21 use DAE to learn text representation. During the fine-tuning procedure, they use backpropagation to find codes that are good at reconstructing the word-count vector..More recently, researchers propose to use external corpus to learn a distributed representation for each word, called word embedding BIBREF22 , to improve DNN performance on NLP tasks. The Skip-gram and continuous bag-of-words models of Word2vec BIBREF23 propose a simple single-layer architecture based on the inner product between two word vectors, and Pennington et al. BIBREF24 introduce a new model for word representation, called GloVe, which captures the global corpus statistics..In order to learn the compact representation vectors of sentences, Le and Mikolov BIBREF25 directly extend the previous Word2vec BIBREF23 by predicting words in the sentence, which is named Paragraph Vector (Para2vec). Para2vec is still a shallow window-based method and need a larger corpus to yield better performance. More neural networks utilize word embedding to capture true meaningful syntactic and semantic regularities, such as RecNN BIBREF6 , BIBREF7 and RNN BIBREF8 . However, RecNN exhibits high time complexity to construct the textual tree, and RNN, using the layer computed at the last word to represent the text, is a biased model. Recently, Long Short-Term Memory (LSTM) BIBREF26 and Gated Recurrent Unit (GRU) BIBREF27 , as sophisticated recurrent hidden units of RNN, has presented its advantages in many sequence generation problem, such as machine translation BIBREF28 , speech recognition BIBREF29 , and text conversation BIBREF30 . While, CNN is better to learn non-biased implicit features which has been successfully exploited for many supervised NLP learning tasks as described in Section SECREF1 , and various CNN based variants are proposed in the recent works, such as Dynamic Convolutional Neural Network (DCNN) BIBREF10 , Gated Recursive Convolutional Neural Network (grConv) BIBREF31 and Self-Adaptive Hierarchical Sentence model (AdaSent) BIBREF32 ..In the past few days, Visin et al. BIBREF33 have attempted to replace convolutional layer in CNN to learn non-biased features for object recognition with four RNNs, called ReNet, that sweep over lower-layer features in different directions: (1) bottom to top, (2) top to bottom, (3) left to right and (4) right to left. However, ReNet does not outperform state-of-the-art convolutional neural networks on any of the three benchmark datasets, and it is also a supervised learning model for classification. Inspired by Skip-gram of word2vec BIBREF34 , BIBREF23 , Skip-thought model BIBREF35 describe an approach for unsupervised learning of a generic, distributed sentence encoder. Similar as Skip-gram model, Skip-thought model trains an encoder-decoder model that tries to reconstruct the surrounding sentences of an encoded sentence and released an off-the-shelf encoder to extract sentence representation. Even some researchers introduce continuous Skip-gram and negative sampling to CNN for learning visual representation in an unsupervised manner BIBREF36 . This paper, from a new perspective, puts forward a general self-taught CNN framework which can flexibly couple various semantic features and achieve a good performance on one unsupervised learning task, short text clustering. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "13\n",
      "Question 1: What metrics are used to measure the clustering performance in text analysis?\n",
      "Answer 1: The accuracy (ACC) and the normalized mutual information metric (NMI) are used to measure the clustering performance in text analysis.\n",
      "Question : for the text The clustering performance is evaluated by comparing the clustering results of texts with the tags/labels provided by the text corpus. Two metrics, the accuracy (ACC) and the normalized mutual information metric (NMI), are used to measure the clustering performance BIBREF38 , BIBREF48 . Given a text INLINEFORM0 , let INLINEFORM1 and INLINEFORM2 be the obtained cluster label and the label provided by the corpus, respectively. Accuracy is defined as: DISPLAYFORM0 .where, INLINEFORM0 is the total number of texts, INLINEFORM1 is the indicator function that equals one if INLINEFORM2 and equals zero otherwise, and INLINEFORM3 is the permutation mapping function that maps each cluster label INLINEFORM4 to the equivalent label from the text data by Hungarian algorithm BIBREF49 ..Normalized mutual information BIBREF50 between tag/label set INLINEFORM0 and cluster set INLINEFORM1 is a popular metric used for evaluating clustering tasks. It is defined as follows: DISPLAYFORM0 .where, INLINEFORM0 is the mutual information between INLINEFORM1 and INLINEFORM2 , INLINEFORM3 is entropy and the denominator INLINEFORM4 is used for normalizing the mutual information to be in the range of [0, 1]. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "14\n",
      "What is the value of the learning rate in the model?\n",
      "\n",
      "The value of the learning rate in the model is 0.01.\n",
      "Question : for the text The most of parameters are set uniformly for these datasets. Following previous study BIBREF38 , the number of nearest neighbors in Eqn. ( EQREF15 ) is fixed to 15 when constructing the graph structures for LE and LPI. For CNN model, the networks has two convolutional layers. The widths of the convolutional filters are both 3. The value of INLINEFORM0 for the top INLINEFORM1 -max pooling in Eqn. ( EQREF10 ) is 5. The number of feature maps at the first convolutional layer is 12, and 8 feature maps at the second convolutional layer. Both those two convolutional layers are followed by a folding layer. We further set the dimension of word embeddings INLINEFORM2 as 48. Finally, the dimension of the deep feature representation INLINEFORM3 is fixed to 480. Moreover, we set the learning rate INLINEFORM4 as 0.01 and the mini-batch training size as 200. The output size INLINEFORM5 in Eqn. ( EQREF19 ) is set same as the best dimensions of subspace in the baseline method, as described in Section SECREF37 ..For initial centroids have significant impact on clustering results when utilizing the K-means algorithms, we repeat K-means for multiple times with random initial centroids (specifically, 100 times for statistical significance) as Huang BIBREF48 . The all subspace vectors are normalized to 1 before applying K-means and the final results reported are the average of 5 trials with all clustering methods on three text datasets. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "15\n",
      "What is the main problem with using Term Frequency-Inverse Document Frequency (TF-IDF) for short text clustering?\n",
      "The main problem with using TF-IDF for short text clustering is that it cannot work well in a short text setting due to the data sparsity problem where most words only occur once in each short text.\n",
      "Question : for the text Short text clustering is of great importance due to its various applications, such as user profiling BIBREF0 and recommendation BIBREF1 , for nowaday's social media dataset emerged day by day. However, short text clustering has the data sparsity problem and most words only occur once in each short text BIBREF2 . As a result, the Term Frequency-Inverse Document Frequency (TF-IDF) measure cannot work well in short text setting. In order to address this problem, some researchers work on expanding and enriching the context of data from Wikipedia BIBREF3 or an ontology BIBREF4 . However, these methods involve solid Natural Language Processing (NLP) knowledge and still use high-dimensional representation which may result in a waste of both memory and computation time. Another way to overcome these issues is to explore some sophisticated models to cluster short texts. For example, Yin and Wang BIBREF5 proposed a Dirichlet multinomial mixture model-based approach for short text clustering. Yet how to design an effective model is an open question, and most of these methods directly trained based on Bag-of-Words (BoW) are shallow structures which cannot preserve the accurate semantic similarities..Recently, with the help of word embedding, neural networks demonstrate their great performance in terms of constructing text representation, such as Recursive Neural Network (RecNN) BIBREF6 , BIBREF7 and Recurrent Neural Network (RNN) BIBREF8 . However, RecNN exhibits high time complexity to construct the textual tree, and RNN, using the hidden layer computed at the last word to represent the text, is a biased model where later words are more dominant than earlier words BIBREF9 . Whereas for the non-biased models, the learned representation of one text can be extracted from all the words in the text with non-dominant learned weights. More recently, Convolution Neural Network (CNN), as the most popular non-biased model and applying convolutional filters to capture local features, has achieved a better performance in many NLP applications, such as sentence modeling BIBREF10 , relation classification BIBREF11 , and other traditional NLP tasks BIBREF12 . Most of the previous works focus CNN on solving supervised NLP tasks, while in this paper we aim to explore the power of CNN on one unsupervised NLP task, short text clustering..We systematically introduce a simple yet surprisingly powerful Self-Taught Convolutional neural network framework for Short Text Clustering, called STC INLINEFORM0 . An overall architecture of our proposed approach is illustrated in Figure FIGREF5 . We, inspired by BIBREF13 , BIBREF14 , utilize a self-taught learning framework into our task. In particular, the original raw text features are first embedded into compact binary codes INLINEFORM1 with the help of one traditional unsupervised dimensionality reduction function. Then text matrix INLINEFORM2 projected from word embeddings are fed into CNN model to learn the deep feature representation INLINEFORM3 and the output units are used to fit the pre-trained binary codes INLINEFORM4 . After obtaining the learned features, K-means algorithm is employed on them to cluster texts into clusters INLINEFORM5 . Obviously, we call our approach “self-taught” because the CNN model is learnt from the pseudo labels generated from the previous stage, which is quite different from the term “self-taught” in BIBREF15 . Our main contributions can be summarized as follows:.This work is an extension of our conference paper BIBREF16 , and they differ in the following aspects. First, we put forward a general a self-taught CNN framework in this paper which can flexibly couple various semantic features, whereas the conference version can be seen as a specific example of this work. Second, in this paper we use a new short text dataset, Biomedical, in the experiment to verify the effectiveness of our approach. Third, we put much effort on studying the influence of various different semantic features integrated in our self-taught CNN framework, which is not involved in the conference paper..For the purpose of reproducibility, we make the datasets and software used in our experiments publicly available at the website..The remainder of this paper is organized as follows: In Section SECREF2 , we first briefly survey several related works. In Section SECREF3 , we describe the proposed approach STC INLINEFORM0 and implementation details. Experimental results and analyses are presented in Section SECREF4 . Finally, conclusions are given in the last Section. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "16\n",
      "Question 1: What approach was used to perform clustering on the short texts?\n",
      "Answer 1: The trained deep neural network was first used to obtain semantic representations, and then the traditional K-means algorithm was employed to perform clustering on the short texts.\n",
      "Question : for the text With the given short texts, we first utilize the trained deep neural network to obtain the semantic representations INLINEFORM0 , and then employ traditional K-means algorithm to perform clustering. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "17\n",
      "Question 1: What is the Adagrad update rule used for in the training of the network with mini-batches?\n",
      "\n",
      "Answer 1: The Adagrad update rule is used for gradient-based optimization in the training of the network with mini-batches.\n",
      "Question : for the text The last layer of CNN is an output layer as follows: DISPLAYFORM0 .where, INLINEFORM0 is the deep feature representation, INLINEFORM1 is the output vector and INLINEFORM2 is weight matrix..In order to incorporate the latent semantic features INLINEFORM0 , we first binary the real-valued vectors INLINEFORM1 to the binary codes INLINEFORM2 by setting the threshold to be the media vector INLINEFORM3 . Then, the output vector INLINEFORM4 is used to fit the binary codes INLINEFORM5 via INLINEFORM6 logistic operations as follows: DISPLAYFORM0 .All parameters to be trained are defined as INLINEFORM0 . DISPLAYFORM0 .Given the training text collection INLINEFORM0 , and the pre-trained binary codes INLINEFORM1 , the log likelihood of the parameters can be written down as follows: DISPLAYFORM0 .Following the previous work BIBREF10 , we train the network with mini-batches by back-propagation and perform the gradient-based optimization using the Adagrad update rule BIBREF39 . For regularization, we employ dropout with 50% rate to the penultimate layer BIBREF10 , BIBREF40 . generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "18\n",
      "Question 1: What are the three components of the proposed framework?\n",
      "\n",
      "Answer 1: The three components of the proposed framework are deep convolutional neural network (CNN), unsupervised dimensionality reduction function, and K-means module.\n",
      "Question : for the text Assume that we are given a dataset of INLINEFORM0 training texts denoted as: INLINEFORM1 , where INLINEFORM2 is the dimensionality of the original BoW representation. Denote its tag set as INLINEFORM3 and the pre-trained word embedding set as INLINEFORM4 , where INLINEFORM5 is the dimensionality of word vectors and INLINEFORM6 is the vocabulary size. In order to learn the INLINEFORM7 -dimensional deep feature representation INLINEFORM8 from CNN in an unsupervised manner, some unsupervised dimensionality reduction methods INLINEFORM9 are employed to guide the learning of CNN model. Our goal is to cluster these texts INLINEFORM10 into clusters INLINEFORM11 based on the learned deep feature representation while preserving the semantic consistency..As depicted in Figure FIGREF5 , the proposed framework consist of three components, deep convolutional neural network (CNN), unsupervised dimensionality reduction function and K-means module. In the rest sections, we first present the first two components respectively, and then give the trainable parameters and the objective function to learn the deep feature representation. Finally, the last section describe how to perform clustering on the learned features. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "19\n",
      "Question 1: What datasets are word vectors trained on using the publicly available word2vec tool? \n",
      "\n",
      "Answer 1: Word vectors are trained on Google News setting, Wikipedia dumps for SearchSnippets, the whole corpus of the StackOverflow dataset for StackOverflow, and all titles and abstracts from 2014 training articles for Biomedical.\n",
      "Question : for the text We use the publicly available word2vec tool to train word embeddings, and the most parameters are set as same as Mikolov et al. BIBREF23 to train word vectors on Google News setting, except of vector dimensionality using 48 and minimize count using 5. For SearchSnippets, we train word vectors on Wikipedia dumps. For StackOverflow, we train word vectors on the whole corpus of the StackOverflow dataset described above which includes the question titles and post contents. For Biomedical, we train word vectors on all titles and abstracts of 2014 training articles. The coverage of these learned vectors on three datasets are listed in Table TABREF32 , and the words not present in the set of pre-trained words are initialized randomly. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "20\n",
      "Question 1: What are the two perspectives from which related work is reviewed in this section?\n",
      "Answer 1: The related work is reviewed from the following two perspectives: short text clustering and deep neural networks.\n",
      "Question : for the text In this section, we review the related work from the following two perspectives: short text clustering and deep neural networks. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "21\n",
      "What is the general observation from the evaluation of the proposed approaches and baseline methods in Table TABREF43 and Table TABREF44?\n",
      "The general observation is that BoW based approaches and SkipVec based approaches do not perform well. RecNN based approaches do better, and Para2vec makes a comparable performance with most baselines. The evaluation clearly demonstrates the superiority of the proposed methods STC.\n",
      "Question : for the text In Table TABREF43 and Table TABREF44 , we report the ACC and NMI performance of our proposed approaches and four baseline methods, K-means, SkipVec, RecNN and Para2vec based clustering methods. Intuitively, we get a general observation that (1) BoW based approaches, including K-means (TF) and K-means (TF-IDF), and SkipVec based approaches perform not well; (2) RecNN based approaches, both RecNN (Ave.) and RecNN (Top+Ave.), do better; (3) Para2vec makes a comparable performance with the most baselines; and (4) the evaluation clearly demonstrate the superiority of our proposed methods STC INLINEFORM0 . It is an expected results. For SkipVec based approaches, the off-the-shelf encoders are trained on the BookCorpus datasets BIBREF51 , and then applied to our datasets to extract the sentence representations. The SkipVec encoders can produce generic sentence representations but may not perform well for specific datasets, in our experiments, StackOverflow and Biomedical datasets consist of many computer terms and medical terms, such as “ASP.NET”, “XML”, “C#”, “serum” and “glycolytic”. When we take a more careful look, we find that RecNN (Top) does poorly, even worse than K-means (TF-IDF). The reason maybe that although recursive neural models introduce tree structure to capture compositional semantics, the vector of the top node mainly captures a biased semantic while the average of all vectors in the tree nodes, such as RecNN (Ave.), can be better to represent sentence level semantic. And we also get another observation that, although our proposed STC INLINEFORM1 -LE and STC INLINEFORM2 -LPI outperform both BoW based and RecNN based approaches across all three datasets, STC INLINEFORM3 -AE and STC INLINEFORM4 -LSA do just exhibit some similar performances as RecNN (Ave.) and RecNN (Top+Ave.) do in the datasets of StackOverflow and Biomedical..We further replace the CNN model in our framework as in Figure FIGREF5 with some other non-biased models, such as bi-LSTM and bi-GRU, and report the results in Table TABREF46 and Table TABREF47 . As an instance, the binary codes generated from LPI are used to guide the learning of bi-LSTM/bi-GRU models. From the results, we can see that bi-GRU and bi-LSTM based clustering methods do equally well, no clear winner, and both achieve great enhancements compared with LPI (best). Compared with these bi-LSTM/bi-GRU based models, the evaluation results still demonstrate the superiority of our approach methods, CNN based clustering model, in the most cases. As the results reported by Visin et al. BIBREF33 , despite bi-directional or multi-directional RNN models perform a good non-biased feature extraction, they yet do not outperform state-of-the-art CNN on some tasks..In order to make clear what factors make our proposed method work, we report the bar chart results of ACC and MNI of our proposed methods and the corresponding baseline methods in Figure FIGREF49 and Figure FIGREF53 . It is clear that, although AE and LSA does well or even better than LE and LPI, especially in dataset of both StackOverflow and Biomedical, STC INLINEFORM0 -LE and STC INLINEFORM1 -LPI achieve a much larger performance enhancements than STC INLINEFORM2 -AE and STC INLINEFORM3 -LSA do. The possible reason is that the information the pseudo supervision used to guide the learning of CNN model that make difference. Especially, for AE case, the input features fed into CNN model and the pseudo supervision employed to guide the learning of CNN model are all come from word embeddings. There are no different semantic features to be used into our proposed method, thus the performance enhancements are limited in STC INLINEFORM4 -AE. For LSA case, as we known, LSA is to make matrix factorization to find the best subspace approximation of the original feature space to minimize the global reconstruction error. And as BIBREF24 , BIBREF52 recently point out that word embeddings trained with word2vec or some variances, is essentially to do an operation of matrix factorization. Therefore, the information between input and the pseudo supervision in CNN is not departed very largely from each other, and the performance enhancements of STC INLINEFORM5 -AE is also not quite satisfactory. For LE and LPI case, as we known that LE extracts the manifold structure of the original feature space, and LPI extracts both geometric and discriminating structure of the original feature space BIBREF38 . We guess that our approach STC INLINEFORM6 -LE and STC INLINEFORM7 -LPI achieve enhancements compared with both LE and LPI by a large margin, because both of LE and LPI get useful semantic features, and these features are also different from word embeddings used as input of CNN. From this view, we say that our proposed STC has potential to behave more effective when the pseudo supervision is able to get semantic meaningful features, which is different enough from the input of CNN..Furthermore, from the results of K-means and AE in Table TABREF43 - TABREF44 and Figure FIGREF49 - FIGREF53 , we note that TF-IDF weighting gives a more remarkable improvement for K-means, while TF weighting works better than TF-IDF weighting for Average Embedding. Maybe the reason is that pre-trained word embeddings encode some useful information from external corpus and are able to get even better results without TF-IDF weighting. Meanwhile, we find that LE get quite unusual good performance than LPI, LSA and AE in SearchSnippets dataset, which is not found in the other two datasets. To get clear about this, and also to make a much better demonstration about our proposed approaches and other baselines, we further report 2-dimensional text embeddings on SearchSnippets in Figure FIGREF58 , using t-SNE BIBREF53 to get distributed stochastic neighbor embedding of the feature representations used in the clustering methods. We can see that the results of from AE and LSA seem to be fairly good or even better than the ones from LE and LPI, which is not the same as the results from ACC and NMI in Figure FIGREF49 - FIGREF53 . Meanwhile, RecNN (Ave.) performs better than BoW (both TF and TF-IDF) while RecNN (Top) does not, which is the same as the results from ACC and NMI in Table TABREF43 and Table TABREF44 . Then we guess that both ”the same as” and ”not the same as” above, is just a good example to illustrate that visualization tool, such as t-SNE, get some useful information for measuring results, which is different from the ones of ACC and NMI. Moreover, from this complementary view of t-SNE, we can see that our STC INLINEFORM0 -AE, STC INLINEFORM1 -LSA, STC INLINEFORM2 -LE, and STC INLINEFORM3 -LPI show more clear-cut margins among different semantic topics (that is, tags/labels), compared with AE, LSA, LE and LPI, respectively, as well as compared with both baselines, BoW and RecNN based ones..From all these results, with three measures of ACC, NMI and t-SNE under three datasets, we can get a solid conclusion that our proposed approaches is an effective approaches to get useful semantic features for short text clustering. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "22\n",
      "Question 1: What is one drawback of the clustering methods discussed in the text?\n",
      "\n",
      "Answer 1: One drawback of the clustering methods discussed in the text is that they ignore word order in the text and belong to shallow structures which cannot fully capture accurate semantic similarities.\n",
      "Question : for the text There have been several studies that attempted to overcome the sparseness of short text representation. One way is to expand and enrich the context of data. For example, Banerjee et al. BIBREF3 proposed a method of improving the accuracy of short text clustering by enriching their representation with additional features from Wikipedia, and Fodeh et al. BIBREF4 incorporate semantic knowledge from an ontology into text clustering. However, these works need solid NLP knowledge and still use high-dimensional representation which may result in a waste of both memory and computation time. Another direction is to map the original features into reduced space, such as Latent Semantic Analysis (LSA) BIBREF17 , Laplacian Eigenmaps (LE) BIBREF18 , and Locality Preserving Indexing (LPI) BIBREF19 . Even some researchers explored some sophisticated models to cluster short texts. For example, Yin and Wang BIBREF5 proposed a Dirichlet multinomial mixture model-based approach for short text clustering. Moreover, some studies even focus the above both two streams. For example, Tang et al. BIBREF20 proposed a novel framework which enrich the text features by employing machine translation and reduce the original features simultaneously through matrix factorization techniques..Despite the above clustering methods can alleviate sparseness of short text representation to some extent, most of them ignore word order in the text and belong to shallow structures which can not fully capture accurate semantic similarities. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "23\n",
      "What is Laplacian Eigenmaps (LE) and how does it work in dimensionality reduction?\n",
      "\n",
      "LE is a dimension reduction method that uses the top eigenvectors of the graph Laplacian, defined on the similarity matrix of texts, to discover the manifold structure of the text space. To construct the local similarity matrix, the method uses heat kernel and a tuning parameter. The graph Laplacian is computed by introducing a diagonal matrix and solving an objective function. Finally, the method obtains an optimal real-valued matrix by solving the generalized eigen-problem.\n",
      "Question : for the text As described in Figure FIGREF5 , the dimensionality reduction function is defined as follows: DISPLAYFORM0 .where, INLINEFORM0 are the INLINEFORM1 -dimensional reduced latent space representations. Here, we take four popular dimensionality reduction methods as examples in our framework..Average Embedding (AE): This method directly averages the word embeddings which are respectively weighted with TF and TF-IDF. Huang et al. BIBREF37 used this strategy as the global context in their task, and Socher et al. BIBREF7 and Lai et al. BIBREF9 used this method for text classification. The weighted average of all word vectors in one text can be computed as follows: DISPLAYFORM0 .where INLINEFORM0 can be any weighting function that captures the importance of word INLINEFORM1 in the text INLINEFORM2 ..Latent Semantic Analysis (LSA): LSA BIBREF17 is the most popular global matrix factorization method, which applies a dimension reducing linear projection, Singular Value Decomposition (SVD), of the corresponding term/document matrix. Suppose the rank of INLINEFORM0 is INLINEFORM1 , LSA decompose INLINEFORM2 into the product of three other matrices: DISPLAYFORM0 .where INLINEFORM0 and INLINEFORM1 are the singular values of INLINEFORM2 , INLINEFORM3 is a set of left singular vectors and INLINEFORM4 is a set of right singular vectors. LSA uses the top INLINEFORM5 vectors in INLINEFORM6 as the transformation matrix to embed the original text features into a INLINEFORM7 -dimensional subspace INLINEFORM8 BIBREF17 ..Laplacian Eigenmaps (LE): The top eigenvectors of graph Laplacian, defined on the similarity matrix of texts, are used in the method, which can discover the manifold structure of the text space BIBREF18 . In order to avoid storing the dense similarity matrix, many approximation techniques are proposed to reduce the memory usage and computational complexity for LE. There are two representative approximation methods, sparse similarity matrix and Nystr INLINEFORM0 m approximation. Following previous studies BIBREF38 , BIBREF13 , we select the former technique to construct the INLINEFORM1 local similarity matrix INLINEFORM2 by using heat kernel as follows: DISPLAYFORM0 .where, INLINEFORM0 is a tuning parameter (default is 1) and INLINEFORM1 represents the set of INLINEFORM2 -nearest-neighbors of INLINEFORM3 . By introducing a diagonal INLINEFORM4 matrix INLINEFORM5 whose entries are given by INLINEFORM6 , the graph Laplacian INLINEFORM7 can be computed by ( INLINEFORM8 ). The optimal INLINEFORM9 real-valued matrix INLINEFORM10 can be obtained by solving the following objective function: DISPLAYFORM0 .where INLINEFORM0 is the trace function, INLINEFORM1 requires the different dimensions to be uncorrelated, and INLINEFORM2 requires each dimension to achieve equal probability as positive or negative)..Locality Preserving Indexing (LPI): This method extends LE to deal with unseen texts by approximating the linear function INLINEFORM0 BIBREF13 , and the subspace vectors are obtained by finding the optimal linear approximations to the eigenfunctions of the Laplace Beltrami operator on the Riemannian manifold BIBREF19 . Similar as LE, we first construct the local similarity matrix INLINEFORM1 , then the graph Laplacian INLINEFORM2 can be computed by ( INLINEFORM3 ), where INLINEFORM4 measures the local density around INLINEFORM5 and is equal to INLINEFORM6 . Compute the eigenvectors INLINEFORM7 and eigenvalues INLINEFORM8 of the following generalized eigen-problem: DISPLAYFORM0 .The mapping function INLINEFORM0 can be obtained and applied to the unseen data BIBREF38 ..All of the above methods claim a better performance in capturing semantic similarity between texts in the reduced latent space representation INLINEFORM0 than in the original representation INLINEFORM1 , while the performance of short text clustering can be further enhanced with the help of our framework, self-taught CNN. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "24\n",
      "Question 1: Who funded the research reported in the paper?\n",
      "Answer 1: The research reported in the paper was funded by the National Science Foundation under Grant No. 1659788.\n",
      "Question : for the text The National Science Foundation supports the work reported in this paper under Grant No. 1659788. Any opinions, findings any conclusions or recommendations expressed in this work are those of the author(s) and do not necessarily reflect the views of the National Science Foundation. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "25\n",
      "Question 1: What is the advantage of using Transformers in solving math word problems?\n",
      "\n",
      "Answer 1: Transformers use stacks of attention layers instead of recurrence, which results in more efficient and accurate sequence-to-sequence translation. Applications of Transformers have achieved state-of-the-art performance in many NLP tasks, making it a powerful tool in solving math word problems. Additionally, their easy and efficient training allows for testing of multiple configurations for comprehensive comparison.\n",
      "Question : for the text We view math word problem solving as a sequence-to-sequence translation problem. RNNs have excelled in sequence-to-sequence problems such as translation and question answering. The recent introduction of attention mechanisms has improved the performance of RNN models. Vaswani et al. BIBREF0 introduced the Transformer network, which uses stacks of attention layers instead of recurrence. Applications of Transformers have achieved state-of-the-art performance in many NLP tasks. We use this architecture to produce character sequences that are arithmetic expressions. The models we experiment with are easy and efficient to train, allowing us to test several configurations for a comprehensive comparison. We use several configurations of Transformer networks to learn the prefix, postfix, and infix notations of MWP equations independently..Prefix and postfix representations of equations do not contain parentheses, which has been a source of confusion in some approaches. If the learned target sequences are simple, with fewer characters to generate, it is less likely to make mistakes during generation. Simple targets also may help the learning of the model to be more robust. Experimenting with all three representations for equivalent expressions may help us discover which one works best..We train on standard datasets, which are readily available and commonly used. Our method considers the translation of English text to simple algebraic expressions. After performing experiments by training directly on math word problem corpora, we perform a different set of experiments by pre-training on a general language corpus. The success of pre-trained models such as ELMo BIBREF16, GPT-2 BIBREF17, and BERT BIBREF18 for many natural language tasks, provides reasoning that pre-training is likely to produce better learning by our system. We use pre-training so that the system has some foundational knowledge of English before we train it on the domain-specific text of math word problems. However, the output is not natural language but algebraic expressions, which is likely to limit the effectiveness of such pre-training. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "26\n",
      "Question 1: What is the Illinois dataset and how many 1-step algebra word questions does it contain? \n",
      "Answer 1: The Illinois dataset contains 562 1-step algebra word questions, compiled by the Cognitive Computation Group.\n",
      "Question : for the text We work with four individual datasets. The datasets contain addition, subtraction, multiplication, and division word problems..AI2 BIBREF2. AI2 is a collection of 395 addition and subtraction problems, containing numeric values, where some may not be relevant to the question..CC BIBREF19. The Common Core dataset contains 600 2-step questions. The Cognitive Computation Group at the University of Pennsylvania gathered these questions..IL BIBREF4. The Illinois dataset contains 562 1-step algebra word questions. The Cognitive Computation Group compiled these questions also..MAWPS BIBREF20. MAWPS is a relatively large collection, primarily from other MWP datasets. We use 2,373 of 3,915 MWPs from this set. The problems not used were more complex problems that generate systems of equations. We exclude such problems because generating systems of equations is not our focus..We take a randomly sampled 95% of examples from each dataset for training. From each dataset, MWPs not included in training make up the testing data used when generating our results. Training and testing are repeated three times, and reported results are an average of the three outcomes. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "27\n",
      "Question 1: What approach is used to handle rare numeric values in the input sequence for arithmetic word problems?\n",
      "Answer 1: The input sequence is pre-processed with a number mapping algorithm that replaces each numeric value with a corresponding identifier (e.g., ⟨n1⟩, ⟨n2⟩, etc.), and remembers the necessary mapping. This approach may significantly improve how networks interpret each question.\n",
      "Question : for the text The input sequence is a natural language specification of an arithmetic word problem. The MWP questions and equations have been encoded using the subword text encoder provided by the TensorFlow Datasets library. The output is an expression in prefix, infix, or postfix notation, which then can be manipulated further and solved to obtain a final answer..All examples in the datasets contain numbers, some of which are unique or rare in the corpus. Rare terms are adverse for generalization since the network is unlikely to form good representations for them. As a remedy to this issue, our networks do not consider any relevant numbers during training. Before the networks attempt any translation, we pre-process each question and expression by a number mapping algorithm. This algorithm replaces each numeric value with a corresponding identifier (e.g., $\\langle n1 \\rangle $, $\\langle n2 \\rangle $, etc.), and remembers the necessary mapping. We expect that this approach may significantly improve how networks interpret each question. When translating, the numbers in the original question are tagged and cached. From the encoded English and tags, a predicted sequence resembling an expression presents itself as output. Since each network's learned output resembles an arithmetic expression (e.g., $\\langle n1 \\rangle + \\langle n2 \\rangle * \\langle n3 \\rangle $), we use the cached tag mapping to replace the tags with the corresponding numbers and return a final mathematical expression..Three representation models are trained and tested separately: Prefix-Transformer, Postfix-Transformer, and Infix-Transformer. For each experiment, we use representation-specific Transformer architectures. Each model uses the Adam optimizer with $beta_1=0.95$ and $beta_2=0.99$ with a standard epsilon of $1 \\times e^{-9}$. The learning rate is reduced automatically in each training session as the loss decreases. Throughout the training, each model respects a 10% dropout rate. We employ a batch size of 128 for all training. Each model is trained on MWP data for 300 iterations before testing. The networks are trained on a machine using 1 Nvidia 1080 Ti graphics processing unit (GPU)..We compare medium-sized, small, and minimal networks to show if network size can be reduced to increase training and testing efficiency while retaining high accuracy. Networks over six layers have shown to be non-effective for this task. We tried many configurations of our network models, but report results with only three configurations of Transformers..Transformer Type 1: This network is a small to medium-sized network consisting of 4 Transformer layers. Each layer utilizes 8 attention heads with a depth of 512 and a feed-forward depth of 1024..Transformer Type 2: The second model is small in size, using 2 Transformer layers. The layers utilize 8 attention heads with a depth of 256 and a feed-forward depth of 1024..Transformer Type 3: The third type of model is minimal, using only 1 Transformer layer. This network utilizes 8 attention heads with a depth of 256 and a feed-forward depth of 512. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "28\n",
      "Question 1: What is the purpose of language pre-training in the experiments?\n",
      "\n",
      "Answer 1: The purpose of language pre-training is to introduce a good level of language understanding before training on the MWP data. This training occurs over 30 iterations at the start of the two experiments.\n",
      "Question : for the text We also explore the effect of language pre-training, as discussed earlier. This training occurs over 30 iterations, at the start of the two experiments, to introduce a good level of language understanding before training on the MWP data. The same Transformer architectures are also trained solely on the MWP data. We calculate the reported results as:.where $R$ is the number of test repetitions, which is 3; $N$ is the number of test datasets, which is 4; $P$ is the number of MWPs, and $C$ is the number of correct equation translations. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "29\n",
      "Question 1: What is the purpose of using BLEU-2 scores in this experiment?\n",
      "\n",
      "Answer 1: The purpose of using BLEU-2 scores in this experiment is to spot the differences in representation interpretability and to measure the translation quality of a given model within a window of two adjacent terms. An average BLEU-2 score is calculated per test set to represent the success over that data.\n",
      "Question : for the text Some of the problems encountered by prior approaches seem to be attributable to the use of infix notation. In this experiment, we compare translation BLEU-2 scores to spot the differences in representation interpretability. Traditionally, a BLEU score is a metric of translation quality BIBREF24. Our presented BLEU scores represent an average of scores a given model received over each of the target test sets. We use a standard bi-gram weight to show how accurate translations are within a window of two adjacent terms. After testing translations, we calculate an average BLEU-2 score per test set, which is related to the success over that data. An average of the scores for each dataset become the presented value..where $N$ is the number of test datasets, which is 4. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "30\n",
      "Question 1: How do you count a given test score in this experiment?\n",
      "Answer 1: The test score is counted by a simple \"correct versus incorrect\" method, where the answer to an expression directly ties to all of the translation terms being correct, and partial precision is not considered.\n",
      "Question : for the text This experiment compares our networks to recent previous work. We count a given test score by a simple “correct versus incorrect\" method. The answer to an expression directly ties to all of the translation terms being correct, which is why we do not consider partial precision. We compare average accuracies over 3 test trials on different randomly sampled test sets from each MWP dataset. This calculation more accurately depicts the generalization of our networks. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "31\n",
      "Question 1: What is the metric of performance used for identifying classes from a feature set?\n",
      "\n",
      "Answer 1: The metric of performance used for identifying classes from a feature set is the evaluation between the possible translation classes (all vocabulary subword tokens) and the produced class (predicted token).\n",
      "Question : for the text We calculate the loss in training according to a mean of the sparse categorical cross-entropy formula. Sparse categorical cross-entropy BIBREF23 is used for identifying classes from a feature set, which assumes a large target classification set. Evaluation between the possible translation classes (all vocabulary subword tokens) and the produced class (predicted token) is the metric of performance here. During each evaluation, target terms are masked, predicted, and then compared to the masked (known) value. We adjust the model's loss according to the mean of the translation accuracy after predicting every determined subword in a translation..where $K = |Translation \\; Classes|$, $J = |Translation|$, and $I$ is the number of examples. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "32\n",
      "What was the corpus used for pre-training in this paper?\n",
      "The corpus used for pre-training in this paper was the IMDb Movie Reviews dataset, which contains 314,041 unique sentences.\n",
      "Question : for the text We pre-train half of our networks to endow them with a foundational knowledge of English. Pre-training models on significant-sized language corpora have been a common approach recently. We explore the pre-training approach using a general English corpus because the language of MWPs is regular English, interspersed with numerical values. Ideally, the corpus for pre-training should be a very general and comprehensive corpus like an English Wikipedia dump or many gigabytes of human-generated text scraped from the internet like GPT-2 BIBREF21 used. However, in this paper, we want to perform experiments to see if pre-training with a smaller corpus can help. In particular, for this task, we use the IMDb Movie Reviews dataset BIBREF22. This set contains 314,041 unique sentences. Since movie reviewers wrote this data, it is a reference to natural language not related to arithmetic. Training on a much bigger and general corpus may make the language model stronger, but we leave this for future work..We compare pre-trained models to non-pre-trained models to observe performance differences. Our pre-trained models are trained in an unsupervised fashion to improve the encodings of our fine-tuned solvers. In the pre-training process, we use sentences from the IMDb reviews with a target output of an empty string. We leave the input unlabelled, which focuses the network on adjusting encodings while providing unbiased decoding when we later change from IMDb English text to MWP-Data. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "33\n",
      "Question 1: What approach is taken to convert infix expressions found in MWPs to prefix and postfix expressions?\n",
      "\n",
      "Answer 1: The approach taken is to use two stacks to fill with operators and operands found in the equation. A binary tree structure is formed from these stacks, and traversing the expression tree in pre-order results in a prefix conversion, while post-order traversal gives a postfix expression. Three versions of training and testing data are created to correspond to each type of expression.\n",
      "Question : for the text We take a simple approach to convert infix expressions found in the MWPs to the other two representations. Two stacks are filled by iterating through string characters, one with operators found in the equation and the other with the operands. From these stacks, we form a binary tree structure. Traversing an expression tree in pre-order results in a prefix conversion. Post-order traversal gives us a postfix expression. Three versions of our training and testing data are created to correspond to each type of expression. By training on different representations, we expect our test results to change. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "34\n",
      "Question 1: What type of target expressions performs better in automatic math word problem-solving according to the paper? \n",
      "\n",
      "Answer 1: The use of postfix target expressions performs better than the other two expression formats in automatic math word problem-solving according to the paper.\n",
      "Question : for the text In this paper, we have shown that the use of Transformer networks improves automatic math word problem-solving. We have also shown that the use of postfix target expressions performs better than the other two expression formats. Our improvements are well-motivated but straightforward and easy to use, demonstrating that the well-acclaimed Transformer architecture for language processing can handle MWPs well, obviating the need to build specialized neural architectures for this task..Extensive pre-training over much larger corpora of language has extended the capabilities of many neural approaches. For example, networks like BERT BIBREF18, trained extensively on data from Wikipedia, perform relatively better in many tasks. Pre-training on a much larger corpus remains an extension we would like to try..We want to work with more complex MWP datasets. Our datasets contain basic arithmetic expressions of +, -, * and /, and only up to 3 of them. For example, datasets such as Dolphin18k BIBREF25, consisting of web-answered questions from Yahoo! Answers, require a wider variety of arithmetic operators to be understood by the system..We have noticed that the presence of irrelevant numbers in the sentences for MWPs limits our performance. We can think of such numbers as a sort of adversarial threat to an MWP solver that stress-test it. It may be interesting to explore how to keep a network's performance high, even in such cases..With a hope to further advance this area of research and heighten interests, all of the code and data used is available on GitHub. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "35\n",
      "Question 1: What is one of the challenges of writing programs to solve arithmetic word problems?\n",
      "\n",
      "Answer 1: One of the challenges is correctly extracting the numbers in the problem and generating the appropriate arithmetic expression. Additionally, neural networks sometimes generate seemingly random numbers due to the wide variation of real numbers in the problems.\n",
      "Question : for the text Students are exposed to simple arithmetic word problems starting in elementary school, and most become proficient in solving them at a young age. Automatic solvers of such problems could potentially help educators, as well as become an integral part of general question answering services. However, it has been challenging to write programs to solve even such elementary school level problems well..Solving a math word problem (MWP) starts with one or more sentences describing a transactional situation to be understood. The sentences are processed to produce an arithmetic expression, which is evaluated to provide an answer. Recent neural approaches to solving arithmetic word problems have used various flavors of recurrent neural networks (RNN) as well as reinforcement learning. Such methods have had difficulty achieving a high level of generalization. Often, systems extract the relevant numbers successfully but misplace them in the generated expressions. More problematic, they get the arithmetic operations wrong. The use of infix notation also requires pairs of parentheses to be placed and balanced correctly, bracketing the right numbers. There have been problems with parentheses placement as well..Correctly extracting the numbers in the problem is necessary. Figure FIGREF1 gives examples of some infix representations that a machine learning solver can potentially produce from a simple word problem using the correct numbers. Of the expressions shown, only the first one is correct. After carefully observing expressions that actual problem solvers have generated, we want to explore if the use of infix notation may itself be a part of the problem because it requires the generation of additional characters, the open and close parentheses, which must be balanced and placed correctly..The actual numbers appearing in MWPs vary widely from problem to problem. Real numbers take any conceivable value, making it almost impossible for a neural network to learn representations for them. As a result, trained programs sometimes generate expressions that have seemingly random numbers. For example, in some runs, a trained program could generate a potentially inexplicable expression such as $(25.01 - 4) * 9$ for the problem given in Figure FIGREF1, with one or more numbers not in the problem sentences. We hypothesize that replacing the numbers in the problem statement with generic tags like $\\rm \\langle n1 \\rangle $, $\\rm \\langle n2 \\rangle $, and $\\rm \\langle n3 \\rangle $ and saving their values as a pre-processing step, does not take away from the generality of the solution, but suppresses the problem of fertility in number generation leading to the introduction of numbers not present in the question sentences..Another idea we want to test is whether a neural network which has been pre-trained to acquire language knowledge is better able to “understand\" the problem sentences. Pre-training with a large amount of arithmetic-related text is likely to help develop such knowledge, but due to the lack of large such focused corpora, we want to test whether pre-training with a sufficient general corpus is beneficial..In this paper, we use the Transformer model BIBREF0 to solve arithmetic word problems as a particular case of machine translation from text to the language of arithmetic expressions. Transformers in various configurations have become a staple of NLP in the past two years. Past neural approaches did not treat this problem as pure translation like we do, and additionally, these approaches usually augmented the neural architectures with various external modules such as parse trees or used deep reinforcement learning, which we do not do. In this paper, we demonstrate that Transformers can be used to solve MWPs successfully with the simple adjustments we describe above. We compare performance on four individual datasets. In particular, we show that our translation-based approach outperforms state-of-the-art results reported by BIBREF1, BIBREF2, BIBREF3, BIBREF4, BIBREF5 by a large margin on three of four datasets tested. On average, our best neural architecture outperforms previous results by almost 10%, although our approach is conceptually more straightforward..We organize our paper as follows. The second section presents related work. Then, we discuss our approach. We follow by an analysis of experimental results and compare them to those of other recent approaches. We also discuss our successes and shortcomings. Finally, we share our concluding thoughts and end with our direction for future work. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "36\n",
      "What is the main drawback of past strategies for solving math word problems?\n",
      "\n",
      "The main drawback of past strategies for solving math word problems is that they lack generality and perform poorly when out of domain, as they rely on rules and templates to match sentences to arithmetic expressions.\n",
      "Question : for the text Past strategies have used rules and templates to match sentences to arithmetic expressions. Some such approaches seemed to solve problems impressively within a narrow domain, but performed poorly when out of domain, lacking generality BIBREF6, BIBREF7, BIBREF8, BIBREF9. Kushman et al. BIBREF3 used feature extraction and template-based categorization by representing equations as expression forests and finding a near match. Such methods required human intervention in the form of feature engineering and development of templates and rules, which is not desirable for expandability and adaptability. Hosseini et al. BIBREF2 performed statistical similarity analysis to obtain acceptable results, but did not perform well with texts that were dissimilar to training examples..Existing approaches have used various forms of auxiliary information. Hosseini et al. BIBREF2 used verb categorization to identify important mathematical cues and contexts. Mitra and Baral BIBREF10 used predefined formulas to assist in matching. Koncel-Kedziorski et al. BIBREF11 parsed the input sentences, enumerated all parses, and learned to match, requiring expensive computations. Roy and Roth BIBREF12 performed searches for semantic trees over large spaces..Some recent approaches have transitioned to using neural networks. Semantic parsing takes advantage of RNN architectures to parse MWPs directly into equations or expressions in a math-specific language BIBREF9, BIBREF13. RNNs have shown promising results, but they have had difficulties balancing parenthesis, and also, sometimes incorrectly choose numbers when generating equations. Rehman et al. BIBREF14 used POS tagging and classification of equation templates to produce systems of equations from third-grade level MWPs. Most recently, Sun et al. BIBREF13 used a Bi-Directional LSTM architecture for math word problems. Huang et al. BIBREF15 used a deep reinforcement learning model to achieve character placement in both seen and novel equation templates. Wang et al. BIBREF1 also used deep reinforcement learning. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "37\n",
      "What were the main findings of the experiments?\n",
      "The main findings of the experiments were that both prefix and postfix representations of the target language performed better than the generally used infix notation. Non-pre-trained models performed slightly better than pre-trained models, and small or Type 2 models performed slightly better than minimal-sized and medium-sized Transformer models. The non-pre-trained type 2 prefix Transformer arrangement produced the most consistent translations. Additionally, the AI2 dataset presented challenges due to extraneous or irrelevant numeric values in word descriptions.\n",
      "Question : for the text We now present the results of our various experiments. We compare the three representations of target equations and three architectures of the Transformer model in each test..Results of Experiment 1 are given in Table TABREF21. For clarity, the number in parentheses in front of a row is the Transformer type. By using BLEU scores, we assess the translation capability of each network. This test displays how networks transform different math representations to a character summary level..We compare by average BLEU-2 accuracy among our tests in the Average column of Table TABREF21 to communicate these translation differences. To make it easier to understand the results, Table TABREF22 provides a summary of Table TABREF21..Looking at Tables TABREF21 and TABREF22, we note that both the prefix and postfix representations of our target language perform better than the generally used infix notation. The non-pre-trained models perform slightly better than the pre-trained models, and the small or Type 2 models perform slightly better than the minimal-sized and medium-sized Transformer models. The non-pre-trained type 2 prefix Transformer arrangement produced the most consistent translations..Table TABREF23 provides detailed results of Experiment 2. The numbers are absolute accuracies, i.e., they correspond to cases where the arithmetic expression generated is 100% correct, leading to the correct numeric answer. Results by BIBREF1, BIBREF2, BIBREF4, BIBREF5 are sparse but indicate the scale of success compared to recent past approaches. Prefix, postfix, and infix representations in Table TABREF23 show that network capabilities are changed by how teachable the target data is. The values in the last column of Table TABREF23 are summarized in Table TABREF24. How the models compare with respect to accuracy closely resembles the comparison of BLEU scores, presented earlier. Thus, BLEU scores seem to correlate well with accuracy values in our case..While our networks fell short of BIBREF1 AI2 testing accuracy, we present state-of-the-art results for the remaining three datasets. The AI2 dataset is tricky because it has numeric values in the word descriptions that are extraneous or irrelevant to the actual computation, whereas the other datasets have only relevant numeric values. The type 2 postfix Transformer received the highest testing average of 87.2%..Our attempt at language pre-training fell short of our expectations in all but one tested dataset. We had hoped that more stable language understanding would improve results in general. As previously mentioned, using more general and comprehensive corpora of language could help grow semantic ability. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "38\n",
      "Question 1: Which network configuration provided the most stable network performance? \n",
      "\n",
      "Answer 1: The prefix representation overall provided the most stable network performance.\n",
      "Question : for the text All of the network configurations used were very successful for our task. The prefix representation overall provides the most stable network performance. To display the capability of our most successful model (type 2 postfix Transformer), we present some outputs of the network in Figure FIGREF26..The models respect the syntax of math expressions, even when incorrect. For the majority of questions, our translators were able to determine operators based solely on the context of language..Our pre-training was unsuccessful in improving accuracy, even when applied to networks larger than those reported. We may need to use more inclusive language, or pre-train on very math specific texts to be successful. Our results support our thesis of infix limitation. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "39\n",
      "Question 1: What is the issue with the algorithmic pre-processing of the system's questions and expressions?\n",
      "\n",
      "Answer 1: The issue originates from the algorithm only considering numbers in the problem, which can lead to overlooking number words that should be taken into account. This results in incorrect question tagging and translation, which can be improved by considering all number words and performing their conversion to numerical values.\n",
      "Question : for the text Our system, while performing above standard, could still benefit from some improvements. One issue originates from the algorithmic pre-processing of our questions and expressions. In Figure FIGREF27 we show an example of one such issue. The excerpt comes from a type 3 non-pre-trained Transformer test. The example shows an overlooked identifier, $\\langle n1 \\rangle $. The issue is attributed to the identifier algorithm only considering numbers in the problem. Observe in the question that the word “eight\" is the number we expect to relate to $\\langle n2 \\rangle $. Our identifying algorithm could be improved by considering such number words and performing conversion to a numerical value. If our algorithm performed as expected, the identifier $\\langle n1 \\rangle $ relates with 4 (the first occurring number in the question) and $\\langle n2 \\rangle $ with 8 (the converted number word appearing second in the question). The overall translation was incorrect whether or not our algorithm was successful, but it is essential to analyze problems like these that may result in future improvements. Had all questions been tagged correctly, our performance would have likely improved. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "40\n",
      "Question 1: What feedback signals are used to generate weak labels for the ranking models in the personalized VVA joke experience?\n",
      "\n",
      "Answer 1: Implicit feedback signals are used to generate weak labels for the ranking models in the personalized VVA joke experience.\n",
      "Question : for the text This paper describes systems to personalize a VVA's joke experience using NLP and deep-learning techniques that provide different compromises between accuracy, scalability and robustness. Implicit feedback signals are used to generate weak labels and provide supervision to the ranking models. Results on production data show that models trained on any of the considered labels present a positive real-world impact on user satisfaction, and that the deep learning approaches can potentially improve the joke skill with respect to the other considered methods. In the future, we would like to compare all methods in A/B testing, and to extend the models to other languages. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "41\n",
      "Question 1: What is the focus of the proposed methods to improve the overall user experience with a voice-controlled virtual assistant?\n",
      "\n",
      "Answer 1: The proposed methods aim to personalize the response to each user's request, particularly in providing humorous responses, by recognizing and evaluating humor using traditional NLP techniques and deep learning models such as BERT and an adapted transformer architecture. The models also use binary classifiers to perform point-wise ranking, requiring a labelled dataset generated through implicit user-feedback labelling strategies such as five-minute reuse and one-day return. The system's performance is evaluated and compared using offline data and online A/B testing to optimize user-satisfaction metrics.\n",
      "Question : for the text Voice-controlled virtual assistants (VVA) such as Siri and Alexa have experienced an exponential growth in terms of number of users and provided capabilities. They are used by millions for a variety of tasks including shopping, playing music, and even telling jokes. Arguably, their success is due in part to the emotional and personalized experience they provide. One important aspect of this emotional interaction is humor, a fundamental element of communication. Not only can it create in the user a sense of personality, but also be used as fallback technique for out-of-domain queries BIBREF0. Usually, a VVA's humorous responses are invoked by users with the phrase \"Tell me a joke\". In order to improve the joke experience and overall user satisfaction with a VVA, we propose to personalize the response to each request. To achieve this, a method should be able to recognize and evaluate humor, a challenging task that has been the focus of extensive work. Some authors have applied traditional NLP techniques BIBREF1, while others deep learning models BIBREF2. Moreover, BIBREF3 follows a semantic-based approach, while BIBREF4 and BIBREF5 tackle the challenge from a cognitive and linguistic perspective respectively..To this end, we have developed two methods. The first one is based on traditional NLP techniques. Although relatively simple, it is robust, scalable, and has low latency, a fundamental property for real-time VVA systems. The other approaches combine multi-task learning BIBREF6 and self-attentional networks BIBREF7 to obtain better results, at the cost of added complexity. Both BERT BIBREF8 and an adapted transformer BIBREF7 architecture are considered. This choice of architecture was motivated by the advantages it presents over traditional RNN and CNN models, including better performance BIBREF9, faster training/inference (important for real-time systems), and better sense disambiguation BIBREF10 (an important component of computational humor BIBREF3)..The proposed models use binary classifiers to perform point-wise ranking, and therefore require a labelled dataset. To generate it, we explore two implicit user-feedback labelling strategies: five-minute reuse and one-day return. Online A/B testing is used to determine if these labelling strategies are suited to optimize the desired user-satisfaction metrics, and offline data to evaluated and compared the system's performance. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "42\n",
      "What is the basis of the proposed joke encoder's transformer model?\n",
      "The basis of the joke encoder is a modified transformer, with an additional sub-layer inserted between the two typical transformer sub-layers at certain depths of the network. This sub-layer performs attention over the user's features as studies suggest that humor is subjective and conditioned on the user's context. As a result, the encoder can adapt the representations of the jokes to different user contexts, and the same joke can be encoded differently depending on the user's features.\n",
      "Question : for the text To overcome the LR-model's limitations, we propose the following model (see Figure FIGREF7). In the input layer, features are separated into context, item and user features. Unlike the LR-model, time and text features do not require extensive feature engineering. Instead, simple features (day, month and year) are extracted from the timestamp. After tokenization and stop-word removal, text features are passed through a pre-trained word embeding layer, and later, input into the joke encoder block..The basis of the joke encoder is a modified transformer. Firstly, only the encoder is needed. Moreover, since studies suggest that humor is subjective and conditioned on the user's context BIBREF13, we add an additional sub-layer in the transformer encoder that performs attention over the user's features. This sub-layer, inserted between the two typical transformer sub-layers at certain depths of the network, allows the encoder to adapt the representations of the jokes to different user contexts. Thus, the same joke can be encoded differently depending on the user's features. In practice, this additional sub-layer works like the normal self-attention sub-layer, except it creates its query matrix Q from the sub-layer below, and its K and V matrices from the user features. As an alternative, we also test encoding the jokes using a pre-trained BERT model..Regardless of the used encoder, we average the token representations to obtain a global encoding of the jokes. The same encoder is used to represent the item's (the joke to rank) and the user's (liked and disliked jokes) textual features through weight sharing, and the cosine similarity between both representations are computed. The processed features are then concatenated and passed through a final block of fully connected layers that contains the output layers. Since experiments determined (see Validation section) that both labeling strategies can improve the desired business metrics, instead of optimizing for only one of them, we take a multi-task learning approach. Thus, we have two softmax outputs..Finally, we use a loss function that considers label uncertainty, class imbalance and the different labeling functions. We start from the traditional cross-entropy loss for one labelling function. We then apply uniform label smoothing BIBREF14, which converts the one-hot-encoded label vectors into smoothed label vectors towards $0.5$:.with $\\epsilon $ a hyper-parameter. Label smoothing provides a way of considering the uncertainty on the labels by encouraging the model to be less confident. We have also experimented with other alternatives, including specialized losses such as BIBREF15. However, they did not produce a significant increase in performance in our tests. To further model the possible uncertainty in the feedback, we apply sample weights calculated using an exponential decay function on the time difference between the current and the following training instance of the same customer:.where $w_i$ is the weight of sample $i$, $t_i$ is the time difference between instances $i$ and $i+1$ for the same user, and $a,b$ are hyper-parameters such that $a>0$ and $0<b<1$. The rationale behind these weights is the following. If for example, we consider labeling function 1, and a user asks for consecutive jokes, first within 10 seconds and later within 4.9 minutes, both instances are labeled as positive. However, we hypothesize that there is a lower chance that in the second case the user requested an additional joke because he liked the first one. In addition, class weights are applied to each sample to account for the natural class imbalance of the dataset. Finally, the total loss to be optimized is the weighted sum of the losses for each of the considered labeling functions:.where $w_{l}$ are manually set weights for each label and $\\mathcal {L}_{l}$ are the losses corresponding to each label, which include all the weights mentioned before. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "43\n",
      "Question 1: What are the three categories of raw features that all models have access to?\n",
      "\n",
      "Answer 1: The three categories of raw features that all models have access to are user, item, and contextual features.\n",
      "Question : for the text All models have access to the same raw features, which we conceptually separate into user, item and contextual features. Examples of features in each of these categories are shown in Table TABREF4. Some of these are used directly by the models, while others need to be pre-processed. The manner in which each model consumes them is explained next. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "44\n",
      "Question 1: Why is generating labels for VVA skills challenging?\n",
      "\n",
      "Answer 1: Generating labels for VVA skills is challenging because explicit user feedback is unavailable and asking for feedback may create friction and degrade user experience. Additionally, available humor datasets only include jokes and corresponding labels but lack the personalization features needed for VVA skills. Therefore, implicit feedback labeling strategies are used, which are noisy and only provide weak supervision to the models.\n",
      "Question : for the text Generating labels for this VVA skill is challenging. Label generation through explicit user feedback is unavailable since asking users for feedback creates friction and degrade the user experience. In addition, available humor datasets such as BIBREF3, BIBREF11 only contain jokes and corresponding labels, but not the additional features we need to personalize the jokes..To overcome this difficulty, it is common to resort to implicit feedback. In particular, many VVA applications use interruptions as negative labels, the rationale being that unhappy users will stop the VVA. This strategy, however, is not suitable for our use-case since responses are short and users need to hear the entire joke to decide if it is funny. Instead, we explore two other implicit feedback labelling strategies: five-minute reuse and 1-day return. Five-minute reuse labels an instance positive if it was followed by a new joke request within five-minutes. Conversely, 1-day return marks as positive all joke requests that were followed by a new one within the following 1 to 25-hour interval. Both strategies assume that if a user returns, he is happy with the jokes. This is clearly an approximation, since a returning user might be overall satisfied with the experience, but not with all the jokes. The same is true for the implied negatives; the user might have been satisfied with some or all of the jokes. Therefore, these labels are noisy and only provide weak supervision to the models..Table TABREF2 shows an example of the labels' values for a set of joke requests from one user. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "45\n",
      "Question 1: What is the special treatment given to the Joke Text feature in the logistic regression model for computational humor detection?\n",
      "\n",
      "Answer 1: The Joke Text feature is tokenized and the stop-words are removed before computational humor features are computed on the clean text such as sense combination and ambiguity. In addition, keywords are checked to relate the jokes to specific events, and pre-computed word embeddings with sub-word information are used to represent jokes.\n",
      "Question : for the text To favor simplicity over accuracy, a logistic regression (LR) model is first proposed. Significant effort was put into finding expressive features. Categorical features are one-hot encoded and numerical ones are normalized. The raw Joke Text and Timestamp features require special treatment. The Joke Text is tokenized and the stop-words are removed. We can then compute computational humor features on the clean text such as sense combination BIBREF3 and ambiguity BIBREF12. In addition, since many jokes in our corpus are related to specific events (Christmas, etc), we check for keywords that relate the jokes to them. For example, if \"Santa\" is included, we infer it is a Christmas joke. Finally, pre-computed word embeddings with sub-word information are used to represent jokes by taking the average and maximum vectors over the token representations. Sub-word information is important when encoding jokes since many can contain out-of-vocabulary tokens. The joke's vector representations are also used to compute a summarized view of the user's past liked and disliked jokes. We consider that a user liked a joke when the assigned label is 1, an approximation given the noisy nature of the labels. The user's liked/disliked joke vectors are also combined with the candidate joke vector by taking the cosine similarity between them..For the raw Timestamp feature, we first extract simple time/date features such as month, day and isWeekend. We then compute binary features that mark if the timestamp occurred near one of the special events mentioned before. Some of these events occur the same day every year, while others change (for example, the Super Bowl). In addition, many events are country dependent. The timestamp's event features are combined with the joke's event features to allow the model to capture if an event-related joke occurs at the right time of the year..The LR classifier is trained on the processed features and one of the labels. The model's posterior probability is used to sort the candidates, which are chosen randomly from a pool of unheard jokes. Although useful (see Validation section), this model has several shortcomings. In particular, many of the used features require significant feature engineering and/or are country/language dependent, limiting the extensibility of the model. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "46\n",
      "What was the purpose of the initial A/B testing for the LR model? \n",
      "\n",
      "The initial A/B testing for the LR model was conducted in a production setting to compare the labelling strategies.\n",
      "Question : for the text A two-step validation was conducted for English-speaking customers. An initial A/B testing for the LR model in a production setting was performed to compare the labelling strategies. A second offline comparison of the models was conducted on historical data and a selected labelling strategy. One month of data and a subset of the customers was used (approx. eighty thousand). The sampled dataset presents a fraction of positive labels of approximately 0.5 for reuse and 0.2 for one-day return. Importantly, since this evaluation is done on a subset of users, the dataset characteristic's do not necessarily represent real production traffic. The joke corpus in this dataset contains thousands of unique jokes of different categories (sci-fi, sports, etc) and types (puns, limerick, etc). The dataset was split timewise into training/validation/test sets, and hyperparameters were optimized to maximize the AUC-ROC on the validation set. As a benchmark, we also consider two additional methods: a non-personalized popularity model and one that follows BIBREF16, replacing the transformer joke encoder with a CNN network (the specialized loss and other characteristics of the DL model are kept)..Hyperparameters were optimized using grid-search for the LR-Model. Due to computational constraints, random search was instead used for the DL-Model. In both cases, hyperparameters are selected to optimize the AUC-ROC on the validation set. Table TABREF11 lists some of the considered hyperparameter values and ranges for both models. The actual optimal values are sample specific. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "47\n",
      "What is the main advantage of using DL-based models compared to LR with engineered features?\n",
      "\n",
      "The main advantage of using DL-based models compared to LR with engineered features is that DL-based methods give significant accuracy gain and require no feature engineering, which facilitates the expansion of the joke experience to new markets and languages. However, this comes at the cost of added complexity if deployed in production, and the size of the BERT model can lead to problematic real-time inference due to latency constraints.\n",
      "Question : for the text One-day return was selected for the offline evaluation because models trained on it have a better AUC-ROC, and both labeling strategies were successful in the online validation. All results are expressed as relative change with respect to the popularity model..We start by evaluating the models using AUC-ROC. As seen in Table TABREF14, the transformer-based models, and in particular our custom architecture, outperform all other approaches. Similar conclusions can be reached regarding overall accuracy. However, given the class imbalance, accuracy is not necessarily the best metric to consider. In addition, to better understand the effect to the original transformer architecture, we present the performance of the model with and without the modified loss and special attention sub-layer (see Table TABREF14). Results suggest both modifications have a positive impact on the performance. Finally, to further evaluate the ranking capabilities of the proposed methods, we use top-1 accuracy. Additional positions in the ranking are not considered because only the top ranked joke is presented to the customer. Results show that the DL based models outperform the other systems, with a relative change in top-1 accuracy of 1.4 for DL-BERT and 0.43 for DL-T, compared with 0.14 for the LR method..Results show that the proposed methods provide different compromises between accuracy, scalability and robustness. On one hand, the relatively good performance of the LR model with engineered features provides a strong baseline both in terms of accuracy and training/inference performance, at the cost of being difficult to extend to new countries and languages. On the other hand, DL based methods give a significant accuracy gain and require no feature engineering, which facilitates the expansion of the joke experience to new markets and languages. This comes at a cost of added complexity if deployed in production. In addition, given the size of the BERT model (340M parameters), real-time inference using DL-BERT becomes problematic due to latency constraints. In this regard, the DL-T model could be a good compromise since its complexity can be adapted, and it provides good overall accuracy. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "48\n",
      "Question 1: What are the user-satisfaction metrics that are monitored during the tests?\n",
      "\n",
      "Answer 1: The user-satisfaction metrics that are monitored during the tests include user interruption rate, reuse of this and other VVA skills, and number of active dialogs.\n",
      "Question : for the text Two treatment groups are considered, one per label. Users in the control group are presented jokes at random, without repetition. Several user-satisfaction metrics such as user interruption rate, reuse of this and other VVA skills, and number of active dialogs are monitored during the tests. The relative improvement/decline of these metrics is compared between the treatments and control, and between the treatments themselves. The statistical significance is measured when determining differences between the groups. Results show that the LR-based model consistently outperforms the heuristic method for both labeling strategies, significantly improving retention, dialogs and interruptions. These results suggest that models trained using either label can improve the VVA's joke experience. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "49\n",
      "Question 1: What is the vector space representation of TDs?\n",
      "\n",
      "Answer 1: The vector space representation of TDs (Term Document Matrix) is a common technique used in statistical text analysis. It represents a corpus of documents as a matrix where rows refer to terms and columns refer to documents, and the cells contain the weights associated with each term in each document. This approach allows for the use of mathematical operations and similarity measures to analyze and compare documents.\n",
      "Question : for the text In this section we provide a brief background of vector space representation of TDs and existing similarity measures that have been widely used in statistical text analysis. To begin with, we consider the representation of documents. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "50\n",
      "Question 1: What is TF-IDF and how is it used to weight terms in a document?\n",
      "\n",
      "Answer 1: TF-IDF is a commonly used weight measure that assigns importance to each term within a document based on its frequency within the document (TF) and its uniqueness within a set of documents (IDF). Each term is assigned a TF-IDF value, and the resulting vector of TF-IDF weights is used to represent the document within a vector space model.\n",
      "Question : for the text A document $d$ can be defined as a finite sequence of terms (independent textual entities within a document, for example, words), namely $d=(t_1,t_2,\\dots ,t_n)$. A general idea is to associate weight to each term $t_i$ within $d$, such that.which has proven superior in prior extensive research BIBREF4. The most common weight measure is Term Frequency - Inverse Document Frequency (TF-IDF). TF is the frequency of a term within a single document, and IDF represents the importance, or uniqueness of a term within a set of documents $D=\\lbrace d_1, d_2, \\dots ,d_m\\rbrace $. TF-IDF is defined as follows:.where.such that $f$ is the number of occurrences of $t$ in $d$ and $\\log $ is used to avoid very small values close to zero..Having these measures defined, it becomes obvious that each $w_i$, for $i=1,\\dots ,n$ is assigned the TF-IDF value of the corresponding term. It turns out that each document is represented as a vector of TF-IDF weights within a vector space model (VSM) with its properties BIBREF5. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "1\n",
      "What are the two main approaches in similarity computation?\n",
      "\n",
      "The two main approaches in similarity computation are deterministic and stochastic. Deterministic approaches exploit algebraic properties of vectors and their geometrical interpretation, while stochastic approaches take uncertainty into account.\n",
      "Question : for the text Different ways of computing the similarity of two vector exist. There are two main approaches in similarity computation:.Deterministic - similarity measures exploiting algebraic properties of vectors and their geometrical interpretation. These include, for instance, cosine similarity (CS), Jaccard coefficients (for binary representations), etc..Stochastic - similarity measures in which uncertainty is taken into account. These include, for instance, statistics such as Pearson's Correlation Coefficient (PCC) BIBREF6..Let $\\mathbf {u}$ and $\\mathbf {v}$ be the vector representations of two documents $d_1$ and $d_2$. Cosine similarity simply measures $cos\\theta $, where $\\theta $ is the angle between $\\mathbf {u}$ and $\\mathbf {v}$.(cosine similarity).(PCC).where.All of the above measures are widely used and have proven efficient, but an important aspect is the lack of importance of the order of terms in textual data. It is easy for one to conclude that, two documents containing a single sentence each, but in a reverse order of terms, most deterministic methods fail to express that these are actually very similar. On the other hand, PCC detects only linear correlation, which constraints the diversity present in textual data. In the following section, we study relevant research in solving this problem, and then in Sections SECREF4 and SECREF5 we present our solution and results. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "2\n",
      "What is the goal of the proposed technique for computing similarity between TF-IDF vectors?\n",
      "\n",
      "The goal of the proposed technique is to provide starting information to other researchers in the field of text analysis and to be extensively researched, with an emphasis on implementing machine learning techniques such as clustering and classification of textual data. The technique is intended to reside and be applied in the rapidly emerging area of big data and information retrieval.\n",
      "Question : for the text In this paper we have presented a non-standard technique for computing the similarity between TF-IDF vectors. We have propagated our idea and contributed a portion of new knowledge in this field of text analysis. We have proposed a technique that is widely used in similar fields, and our goal is to provide starting information to other researches in this area. We consider our observations promising and they should be extensively researched..Our experiments have proved that our technique should be a subject for further research. Our future work will concentrate on the implementation of machine learning techniques, such as clustering and subsequent classification of textual data. We expect an information of good quality to be extracted. To summarize, the rapidly emerging area of big data and information retrieval is where our technique should reside and where it should be applied. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "3\n",
      "Question 1: How many TDs were used to conduct the experiments?\n",
      "\n",
      "Answer 1: 14 TDs were used to conduct the experiments.\n",
      "Question : for the text In order to test our proposed approach, we have conducted a series of experiments. In this section, we briefly discuss the outcome and provide a clear view of whether our approach is suitable for knowledge extraction from textual data in a semantic context..We have used a dataset of 14 TDs to conduct our experiments. There are several subjects on which their content is based: (aliens, stories, law, news) BIBREF15. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "4\n",
      "Question 1: What is the main observation about the similarity measures CS, SRCC, and PCC in comparing textual documents?\n",
      "\n",
      "Answer 1: The main observation is that SRCC performs at least equally well as other measures in extracting tiny associations in cases where there exist associations between the documents. CS and PCC may produce unrealistic similarity values, while SRCC provides a more realistic view of the semantic knowledge aggregated in the documents. The values in the table are very small, and SRCC is mostly a few times larger than CS and PCC in cases where there are associations between the documents.\n",
      "Question : for the text In this part, we have compared the similarity values produced by each of the similarity measures CS, SRCC and PCC. We have picked a few notable results and they are summarized in Table TABREF9 below..In Table TABREF9 that SRCC mostly differs from CS and PCC, which also differ in some cases.For instance, $d_1$ refers to leadership in the nineties, while $d_5$ refers to the family and medical lead act of 1993. We have empirically observed that the general topics discussed in these two textual documents are very different. Namely, discusses different frameworks for leadership empowerment, while $d_5$ discusses medical treatment and self-care of employees. We have observed that the term employee is the only connection between $d_1$ and $d_5$. The similarity value of CS of 0.36 is very unreal in this case, while PCC (0.05), and especially SRCC (0.0018) provide a much more realistic view of the semantic knowledge aggregated in these documents. Another example are $d_8$ and $d_9$. The contents of these documents are very straightforward and very similar, because they discuss aliens seen by Boeing-747 pilots and $d_9$ discusses angels that were considered to be aliens. It is obvious that SRCC is able to detect this association as good as CS and PCC which are very good in such straightforward cases..We have observed that SRCC does not perform worse than any other of these similarity measures. It does not always produce the most suitable similarity value, but it indeed does perform at least equally good as other measures. The values in Table TABREF9 are very small, and suggest that SRCC performs well in extracting tiny associations in such cases. It is mostly a few times larger than CS and PCC when there actually exist associations between the documents..These results are visually summarized in Figure FIGREF10. The two above-described examples can be clearly seen as standing out. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "5\n",
      "Question 1: Which TD pairs can SRCC detect the nonlinear association between?\n",
      "\n",
      "Answer 1: SRCC can detect the nonlinear association between TD pairs (d_6,d_10) and (d_7,d_12), according to the presented research. Additionally, it is also suitable for the straightforward case of (d_8,d_9).\n",
      "Question : for the text In this part we will briefly present the nonlinear association between some of the TDs we have used in our experiments. Our purpose is to point out that $(d_6,d_{10})$ and $(d_7,d_{12})$ are the pairs where SRCC is the most appropriate measure for the observed content, and as such, it is able to detect the nonlinear association between them. This can be seen in Figure FIGREF12 below. The straightforward case of $d_8$ and $d_9$ also stands out here (SRCC can also detect it very well)..The obtained results showed that our technique shows good performance on similarity computing, although it is not a perfect measure. But, it sure comes close to convenient and widely used similarity measures such as CS and PCC. The next section provides a conclusion of our research and suggestions for further work. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "6\n",
      "What is clustering in data mining and how is it typically performed?\n",
      "\n",
      "Clustering in data mining is the process of grouping similar data points based on a measure of similarity. It is mostly performed with respect to a measure of similarity between text documents, which must be represented as vectors in a vector space beforehand. This allows for the automatic grouping of similar documents, which can then be used for various purposes, such as news aggregation engines or book recommending systems.\n",
      "Question : for the text Over the past few years, the term big data has become an important key point for research into data mining and information retrieval. Through the years, the quantity of data managed across enterprises has evolved from a simple and imperceptible task to an extent to which it has become the central performance improvement problem. In other words, it evolved to be the next frontier for innovation, competition and productivity BIBREF0. Extracting knowledge from data is now a very competitive environment. Many companies process vast amounts of customer/user data in order to improve the quality of experience (QoE) of their customers. For instance, a typical use-case scenario would be a book seller that performs an automatic extraction of the content of the books a customer has bought, and subsequently extracts knowledge of what customers prefer to read. The knowledge extracted could then be used to recommend other books. Book recommending systems are typical examples where data mining techniques should be considered as the primary tool for making future decisions BIBREF1..KE from TDs is an essential field of research in data mining and it certainly requires techniques that are reliable and accurate in order to neutralize (or even eliminate) uncertainty in future decisions. Grouping TDs based on their content and mutual key information is referred to as clustering. Clustering is mostly performed with respect to a measure of similarity between TDs, which must be represented as vectors in a vector space beforehand BIBREF2. News aggregation engines can be considered as a typical representative where such techniques are extensively applied as a sub-field of natural language processing (NLP)..In this paper we present a new technique for measuring similarity between TDs, represented in a vector space, based on SRCC - \"a statistical measure of association between two things\" BIBREF3, which in this case things refer to TDs. The mathematical properties of SRCC (such as the ability to detect nonlinear correlation) make it compelling to be researched into. Our motivation is to provide a new technique of improving the quality of KE based on the well-known association measure SRCC, as opposed to other well-known TD similarity measures..The paper is organized as follows: Section SECREF2 gives a brief overview of the vector space representation of a TD and the corresponding similarity measures, in Section SECREF3 we address conducted research of the role of SRCC in data mining and trend prediction. Section SECREF4 is a detailed description of the proposed technique, and later, in Section SECREF5 we present clustering and classification experiments conducted on several sets of TDs, while Section SECREF6 summarizes our research and contribution to the broad area of statistical text analysis. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "7\n",
      "What is the main application of similarity measures in textual data? \n",
      "\n",
      "The main application of similarity measures in textual data is considered to be clustering and classification of the content.\n",
      "Question : for the text A significant number of similarity measures have been proposed and this topic has been thoroughly elaborated. Its main application is considered to be clustering and classification of textual data organized in TDs. In this section, we provide an overview of relevant research on this topic, to which we can later compare our proposed technique for computing vector similarity..KE (also referred to as knowledge discovery) techniques are used to extract information from unstructured data, which can be subsequently used for applying supervised or unsupervised learning techniques, such as clustering and classification of the content BIBREF7. Text clustering should address several challenges such as vast amounts of data, very high dimensionality of more than 10,000 terms (dimensions), and most importantly - an understandable description of the clusters BIBREF8, which essentially implies the demand for high quality of extracted information..Regarding high quality KE and information accuracy, much effort has been put into improving similarity measurements. An improvement based on linear algebra, known as Singular Value Decomposition (SVD), is oriented towards word similarity, but instead, its main application is document similarity BIBREF9. Alluring is the fact that this measure takes the advantage of synonym recognition and has been used to achieve human-level scores on multiple-choice synonym questions from the Test of English as a Foreign Language (TOEFL) in a technique known as Latent Semantic Analysis (LSA) BIBREF10 BIBREF5..Other semantic term similarity measures have been also proposed, based on information exclusively derived from large corpora of words, such as Pointwise Mutual Information (PMI), which has been reported to have achieved a large degree of correctness in the synonym questions in the TOEFL and SAT tests BIBREF11..Moreover, normalized knowledge-based measures, such as Leacock & Chodrow BIBREF12, Lesk (\"how to tell a pine cone from an ice-cream cone\" BIBREF13, or measures for the depth of two concepts (preferably vebs) in the Word-Net taxonomy BIBREF14 have experimentally proven to be efficient. Their accuracy converges to approximately 69%, Leacock & Chodrow and Lesk have showed the highest precision, and having them combined turns out to be the approximately optimal solution BIBREF11. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "8\n",
      "What is the main advantage of using SRCC over PCC in detecting similarity between TDs represented in a vector space model?\n",
      "The main advantage of using SRCC over PCC is that unlike PCC, which is only able to detect linear correlation, SRCC's nonlinear ability provides a convenient way of taking different ordering of terms into account.\n",
      "Question : for the text The main idea behind our proposed technique is to introduce uncertainty in the calculations of the similarity between TDs represented in a vector space model, based on the nonlinear properties of SRCC. Unlike PCC, which is only able to detect linear correlation, SRCC's nonlinear ability provides a convenient way of taking different ordering of terms into account. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "9\n",
      "Question 1: How are the ranks of TF-IDF values assigned for the Spearman's Rank Correlation Coefficient?\n",
      "\n",
      "Answer 1: Each TF-IDF value is assigned a rank value based on their sorting criteria, which is determined by sorting the values in ascending order. The largest TF-IDF value is assigned the rank value of n-1, and the least important is assigned a value of 0.\n",
      "Question : for the text The Spreaman's Rank Correlation Coefficient BIBREF3, denoted $\\rho $, has a from which is very similar to PCC. Namely, for $n$ raw scores $U_i, V_i$ for $i=1,\\dots ,n$ denoting TF-IDF values for two document vectors $\\mathbf {U}, \\mathbf {V}$,.where $u_i$ and $v_i$ are the corresponding ranks of $U_i$ and $V_i$, for $i=0,\\dots ,n-1$. A metric to assign the ranks of each of the TF-IDF values has to be determined beforehand. Each $U_i$ is assigned a rank value $u_i$, such that $u_i=0,1,\\dots ,n-1$. It is important to note that the metric by which the TF-IDF values are ranked is essentially their sorting criteria. A convenient way of determining this criteria when dealing with TF-IDF values, which emphasize the importance of a term within a TD set, is to sort these values in an ascending order. Thus, the largest (or most important) TF-IDF value within a TD vector is assigned the rank value of $n-1$, and the least important is assigned a value of 0. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "10\n",
      "What is the advantage of using SRCC over cosine similarity for knowledge extraction in textual data?\n",
      "SRCC is better suited for semantic similarity and can recognize non-linear associations between terms in different sized documents, which cosine similarity cannot. This allows for more accurate knowledge extraction from textual data.\n",
      "Question : for the text Consider two TDs $d_1$ and $d_2$, each containing a single sentence..Document 1: John had asked Mary to marry him before she left..Document 2: Before she left, Mary was asked by John to be his wife..Now consider these sentences lemmatized:.Document 1: John have ask Mary marry before leave..Document 2: Before leave Mary ask John his wife..Let us now represent $d_1$ and $d_2$ as TF-IDF vectors for the vocabulary in our small corpus..The results in Table TABREF7 show that SRCC performs much better in knowledge extraction. The two documents' contents contain the same idea expressed by terms in a different order that John had asked Mary to marry him before she left. It is obvious that cosine similarity cannot recognize this association, but SRCC has successfully recognized it and produced a similarity value of -0.285714..SRCC is essentially conducive to semantic similarity. Rising the importance of a term in a TD will eventually rise its importance in another TD. But if the two TDs are of different size, the terms' importance values will also differ, by which a nonlinear association will emerge. This association will not be recognized by PCC at all (as it only detects linear association), but SRCC will definitely catch this detail and produce the desirable similarity value. The idea is to use SRCC to catch such terms which drive the semantic context of a TD, which will follow a nonlinear and lie on a polynomial curve, and not on the line $x=y$..In our approach, we use a non-standard measure of similarity in textual data with simple and common frequency values, such as TF-IDF, in contrast to the statement that simple frequencies are not enough for high-quality knowledge extraction BIBREF5. In the next section, we will present our experiments and discuss the results we have obtained. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "11\n",
      "Question 1: What organizations provided funding for this work?\n",
      "Answer 1: The work was partly funded by three French National grants from the Agence Nationale de la Recherche and the last author's chair in the PRAIRIE institute. The grant projects were PARSITI (ANR-16-CE33-0021), SoSweet (ANR-15-CE38-0011), and BASNUM (ANR-18-CE38-0003).\n",
      "Question : for the text This work was partly funded by three French National grants from the Agence Nationale de la Recherche, namely projects PARSITI (ANR-16-CE33-0021), SoSweet (ANR-15-CE38-0011) and BASNUM (ANR-18-CE38-0003), as well as by the last author's chair in the PRAIRIE institute. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "12\n",
      "Question 1: Does whole word masking improve the downstream performances of CamemBERT for all tasks? \n",
      "\n",
      "Answer 1: Yes, as reported for English on other downstream tasks, whole word masking improves downstream performances for all tasks except NER.\n",
      "Question : for the text We analyze the addition of whole-word masking on the downstream performance of CamemBERT. As reported for English on other downstream tasks, whole word masking improves downstream performances for all tasks but NER as seen in Table TABREF46. NER is highly sensitive to capitalisation, prefixes, suffixes and other subword features that could be used by a model to correctly identify entity mentions. Thus the added information by learning the masking at a subword level rather than at whole-word level seems to have a detrimental effect on downstream NER results. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "13\n",
      "Question 1: What is the difference between CamemBERT and RoBERTa?\n",
      "\n",
      "Answer 1: CamemBERT differs from RoBERTa mainly with the addition of whole-word masking and the usage of SentencePiece tokenisation.\n",
      "Question : for the text Our approach is based on RoBERTa BIBREF9, which replicates and improves the initial BERT by identifying key hyper-parameters for more robust performance..In this section, we describe the architecture, training objective, optimisation setup and pretraining data that was used for CamemBERT..CamemBERT differs from RoBERTa mainly with the addition of whole-word masking and the usage of SentencePiece tokenisation BIBREF20. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "14\n",
      "Question 1: What is CamemBERT?\n",
      "\n",
      "Answer 1: CamemBERT is a multi-layer bidirectional Transformer, similar to RoBERTa and BERT, with 12 layers, 768 hidden dimensions, 12 attention heads, and 110M parameters, that uses the original BERT $_{\\small \\textsc {BASE}}$ configuration.\n",
      "Question : for the text Similar to RoBERTa and BERT, CamemBERT is a multi-layer bidirectional Transformer BIBREF21. Given the widespread usage of Transformers, we do not describe them in detail here and refer the reader to BIBREF21. CamemBERT uses the original BERT $_{\\small \\textsc {BASE}}$ configuration: 12 layers, 768 hidden dimensions, 12 attention heads, which amounts to 110M parameters. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "15\n",
      "Question 1: What optimization technique was used in the model and for how many steps was it optimized?\n",
      "\n",
      "Answer 1: The model was optimized using Adam with beta1=0.9 and beta2=0.98 for 100k steps.\n",
      "Question : for the text Following BIBREF9, we optimise the model using Adam BIBREF23 ($\\beta _1 = 0.9$, $\\beta _2 = 0.98$) for 100k steps. We use large batch sizes of 8192 sequences. Each sequence contains at most 512 tokens. We enforce each sequence to only contain complete sentences. Additionally, we used the DOC-SENTENCES scenario from BIBREF9, consisting of not mixing multiple documents in the same sequence, which showed slightly better results. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "16\n",
      "Question 1: What is OSCAR and how does it relate to Common Crawl?\n",
      "\n",
      "Answer 1: OSCAR is a set of monolingual corpora extracted from Common Crawl, specifically from the plain text WET format distributed by Common Crawl, which removes all HTML tags and converts all text encodings to UTF-8. OSCAR follows the same approach as BIBREF19 by using a language classification model based on the fastText linear classifier BIBREF26, BIBREF27 pretrained on Wikipedia, Tatoeba and SETimes, which supports 176 different languages. OSCAR performs a deduplication step after language classification and without introducing a specialised filtering scheme, other than only keeping paragraphs containing 100 or more UTF-8 encoded characters, making OSCAR quite close to the original Crawled data.\n",
      "Question : for the text Pretrained language models can be significantly improved by using more data BIBREF9, BIBREF10. Therefore we used French text extracted from Common Crawl, in particular, we use OSCAR BIBREF13 a pre-classified and pre-filtered version of the November 2018 Common Craw snapshot..OSCAR is a set of monolingual corpora extracted from Common Crawl, specifically from the plain text WET format distributed by Common Crawl, which removes all HTML tags and converts all text encodings to UTF-8. OSCAR follows the same approach as BIBREF19 by using a language classification model based on the fastText linear classifier BIBREF26, BIBREF27 pretrained on Wikipedia, Tatoeba and SETimes, which supports 176 different languages..OSCAR performs a deduplication step after language classification and without introducing a specialised filtering scheme, other than only keeping paragraphs containing 100 or more UTF-8 encoded characters, making OSCAR quite close to the original Crawled data..We use the unshuffled version of the French OSCAR corpus, which amounts to 138GB of uncompressed text and 32.7B SentencePiece tokens. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "17\n",
      "Question 1: What is the dynamic masking approach used during training CamemBERT?\n",
      "\n",
      "Answer 1: CamemBERT uses a dynamic masking approach where tokens are masked dynamically instead of fixing them statically for the whole dataset during preprocessing. This approach improves variability and makes the model more robust when training for multiple epochs.\n",
      "Question : for the text We train our model on the Masked Language Modeling (MLM) task. Given an input text sequence composed of $N$ tokens $x_1, ..., x_N$, we select $15\\%$ of tokens for possible replacement. Among those selected tokens, 80% are replaced with the special $<$mask$>$ token, 10% are left unchanged and 10% are replaced by a random token. The model is then trained to predict the initial masked tokens using cross-entropy loss..Following RoBERTa we dynamically mask tokens instead of fixing them statically for the whole dataset during preprocessing. This improves variability and makes the model more robust when training for multiple epochs..Since we segment the input sentence into subwords using SentencePiece, the input tokens to the models can be subwords. An upgraded version of BERT and BIBREF22 have shown that masking whole words instead of individual subwords leads to improved performance. Whole-word masking (WWM) makes the training task more difficult because the model has to predict a whole word instead of predicting only part of the word given the rest. As a result, we used WWM for CamemBERT by first randomly sampling 15% of the words in the sequence and then considering all subword tokens in each of these 15% words for candidate replacement. This amounts to a proportion of selected tokens that is close to the original 15%. These tokens are then either replaced by $<$mask$>$ tokens (80%), left unchanged (10%) or replaced by a random token..Subsequent work has shown that the next sentence prediction task (NSP) originally used in BERT does not improve downstream task performance BIBREF12, BIBREF9, we do not use NSP as a consequence. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "18\n",
      "Question 1: What method is used to segment the input text into subword units?\n",
      "Answer 1: The input text is segmented into subword units using SentencePiece, which is an extension of Byte-Pair encoding (BPE) and WordPiece that does not require pre-tokenisation at the word or token level.\n",
      "Question : for the text We segment the input text into subword units using SentencePiece BIBREF20. SentencePiece is an extension of Byte-Pair encoding (BPE) BIBREF24 and WordPiece BIBREF25 that does not require pre-tokenisation (at the word or token level), thus removing the need for language-specific tokenisers. We use a vocabulary size of 32k subword tokens. These are learned on $10^7$ sentences sampled from the pretraining dataset. We do not use subword regularisation (i.e. sampling from multiple possible segmentations) in our implementation for simplicity. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "19\n",
      "What is the purpose of releasing the CamemBERT model?\n",
      "\n",
      "The purpose of releasing the CamemBERT model is to serve as a strong baseline for future research in French NLP and to improve the performance of downstream tasks in the language. Additionally, the authors hope that their experiments will be reproduced in other languages.\n",
      "Question : for the text CamemBERT improves the state of the art for multiple downstream tasks in French. It is also lighter than other BERT-based approaches such as mBERT or XLM. By releasing our model, we hope that it can serve as a strong baseline for future research in French NLP, and expect our experiments to be reproduced in many other languages. We will publish an updated version in the near future where we will explore and release models trained for longer, with additional downstream tasks, baselines (e.g. XLM) and analysis, we will also train additional models with potentially cleaner corpora such as CCNet BIBREF56 for more accurate performance evaluation and more complete ablation. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "20\n",
      "What types of entities are included in the NER-annotated FTB?\n",
      "\n",
      "The NER-annotated FTB includes seven different types of entities: \"Person\", \"Location\", \"Organisation\", \"Company\", \"Product\", \"POI\" (Point of Interest), and \"Fictional Character\".\n",
      "Question : for the text Named Entity Recognition (NER) is a sequence labeling task that consists in predicting which words refer to real-world objects, such as people, locations, artifacts and organisations. We use the French Treebank (FTB) BIBREF39 in its 2008 version introduced by cc-clustering:09short and with NER annotations by sagot2012annotation. The NER-annotated FTB contains more than 12k sentences and more than 350k tokens extracted from articles of the newspaper Le Monde published between 1989 and 1995. In total, it contains 11,636 entity mentions distributed among 7 different types of entities, namely: 2025 mentions of “Person”, 3761 of “Location”, 2382 of “Organisation”, 3357 of “Company”, 67 of “Product”, 15 of “POI” (Point of Interest) and 29 of “Fictional Character”..A large proportion of the entity mentions in the treebank are multi-word entities. For NER we therefore report the 3 metrics that are commonly used to evaluate models: precision, recall, and F1 score. Here precision measures the percentage of entities found by the system that are correctly tagged, recall measures the percentage of named entities present in the corpus that are found and the F1 score combines both precision and recall measures giving a general idea of a model's performance. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "21\n",
      "Question 1: What is the current state of the art in NER for non-English languages?\n",
      "\n",
      "Answer 1: Recent efforts have settled the state of the art for Spanish and Dutch using the CoNLL 2002 shared task corpus and German using the CoNLL 2003 corpus, with models incorporating contextualized word embeddings. However, limited availability of NER corpora in French has hindered extensive work in this area.\n",
      "Question : for the text Most of the advances in NER haven been achieved on English, particularly focusing on the CoNLL 2003 BIBREF40 and the Ontonotes v5 BIBREF41, BIBREF42 English corpora. NER is a task that was traditionally tackled using Conditional Random Fields (CRF) BIBREF43 which are quite suited for NER; CRFs were later used as decoding layers for Bi-LSTM architectures BIBREF44, BIBREF45 showing considerable improvements over CRFs alone. These Bi-LSTM-CRF architectures were later enhanced with contextualised word embeddings which yet again brought major improvements to the task BIBREF5, BIBREF6. Finally, large pretrained architectures settled the current state of the art showing a small yet important improvement over previous NER-specific architectures BIBREF7, BIBREF46..In non-English NER the CoNLL 2002 shared task included NER corpora for Spanish and Dutch corpora BIBREF47 while the CoNLL 2003 included a German corpus BIBREF40. Here the recent efforts of BIBREF48 settled the state of the art for Spanish and Dutch, while BIBREF6 did it for German..In French, no extensive work has been done due to the limited availability of NER corpora. We compare our model with the strong baselines settled by BIBREF49, who trained both CRF and BiLSTM-CRF architectures on the FTB and enhanced them using heuristics and pretrained word embeddings. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "22\n",
      "Question 1: What is the XNLI dataset and how is it used to evaluate CamemBERT's performance on NLI in French?\n",
      "\n",
      "Answer 1: The XNLI dataset is an extension of the Multi-Genre NLI corpus to 15 languages, including French. The dataset is used to evaluate CamemBERT's performance on the Natural Language Inference (NLI) task in French. Two settings are considered: TRANSLATE-TEST, where the French test set is translated into English and used with an English model, and TRANSLATE-TRAIN, where the French model is fine-tuned on the machine-translated English training set and evaluated on the French test set.\n",
      "Question : for the text We also evaluate our model on the Natural Language Inference (NLI) task, using the French part of the XNLI dataset BIBREF50. NLI consists in predicting whether a hypothesis sentence is entailed, neutral or contradicts a premise sentence..The XNLI dataset is the extension of the Multi-Genre NLI (MultiNLI) corpus BIBREF51 to 15 languages by translating the validation and test sets manually into each of those languages. The English training set is also machine translated for all languages. The dataset is composed of 122k train, 2490 valid and 5010 test examples. As usual, NLI performance is evaluated using accuracy..To evaluate a model on a language other than English (such as French), we consider the two following settings:.TRANSLATE-TEST: The French test set is machine translated into English, and then used with an English classification model. This setting provides a reasonable, although imperfect, way to circumvent the fact that no such data set exists for French, and results in very strong baseline scores..TRANSLATE-TRAIN: The French model is fine-tuned on the machine-translated English training set and then evaluated on the French test set. This is the setting that we used for CamemBERT. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "23\n",
      "Question 1: What was the best model in the original XNLI paper?\n",
      "\n",
      "Answer 1: The best model in the original XNLI paper was BiLSTM-max.\n",
      "Question : for the text For the TRANSLATE-TEST setting, we report results of the English RoBERTa to act as a reference..In the TRANSLATE-TRAIN setting, we report the best scores from previous literature along with ours. BiLSTM-max is the best model in the original XNLI paper, mBERT which has been reported in French in BIBREF52 and XLM (MLM+TLM) is the best-presented model from BIBREF50. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "24\n",
      "Question 1: What are the two downstream tasks that CamemBERT is evaluated on?\n",
      "Answer 1: CamemBERT is evaluated on the two downstream tasks of part-of-speech (POS) tagging and dependency parsing.\n",
      "Question : for the text We fist evaluate CamemBERT on the two downstream tasks of part-of-speech (POS) tagging and dependency parsing. POS tagging is a low-level syntactic task, which consists in assigning to each word its corresponding grammatical category. Dependency parsing consists in predicting the labeled syntactic tree capturing the syntactic relations between words..We run our experiments using the Universal Dependencies (UD) paradigm and its corresponding UD POS tag set BIBREF28 and UD treebank collection version 2.2 BIBREF29, which was used for the CoNLL 2018 shared task. We perform our work on the four freely available French UD treebanks in UD v2.2: GSD, Sequoia, Spoken, and ParTUT..GSD BIBREF30 is the second-largest treebank available for French after the FTB (described in subsection SECREF25), it contains data from blogs, news articles, reviews, and Wikipedia. The Sequoia treebank BIBREF31, BIBREF32 comprises more than 3000 sentences, from the French Europarl, the regional newspaper L’Est Républicain, the French Wikipedia and documents from the European Medicines Agency. Spoken is a corpus converted automatically from the Rhapsodie treebank BIBREF33, BIBREF34 with manual corrections. It consists of 57 sound samples of spoken French with orthographic transcription and phonetic transcription aligned with sound (word boundaries, syllables, and phonemes), syntactic and prosodic annotations. Finally, ParTUT is a conversion of a multilingual parallel treebank developed at the University of Turin, and consisting of a variety of text genres, including talks, legal texts, and Wikipedia articles, among others; ParTUT data is derived from the already-existing parallel treebank Par(allel)TUT BIBREF35 . Table TABREF23 contains a summary comparing the sizes of the treebanks..We evaluate the performance of our models using the standard UPOS accuracy for POS tagging, and Unlabeled Attachment Score (UAS) and Labeled Attachment Score (LAS) for dependency parsing. We assume gold tokenisation and gold word segmentation as provided in the UD treebanks. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "25\n",
      "Question 1: What model did the authors compare CamemBERT to for both POS tagging and dependency parsing in French?\n",
      "\n",
      "Answer 1: The authors compared CamemBERT to UDify, a multitask and multilingual model based on mBERT that is near state-of-the-art on all UD languages including French for both POS tagging and dependency parsing.\n",
      "Question : for the text To demonstrate the value of building a dedicated version of BERT for French, we first compare CamemBERT to the multilingual cased version of BERT (designated as mBERT). We then compare our models to UDify BIBREF36. UDify is a multitask and multilingual model based on mBERT that is near state-of-the-art on all UD languages including French for both POS tagging and dependency parsing..It is relevant to compare CamemBERT to UDify on those tasks because UDify is the work that pushed the furthest the performance in fine-tuning end-to-end a BERT-based model on downstream POS tagging and dependency parsing. Finally, we compare our model to UDPipe Future BIBREF37, a model ranked 3rd in dependency parsing and 6th in POS tagging during the CoNLL 2018 shared task BIBREF38. UDPipe Future provides us a strong baseline that does not make use of any pretrained contextual embedding..We will compare to the more recent cross-lingual language model XLM BIBREF12, as well as the state-of-the-art CoNLL 2018 shared task results with predicted tokenisation and segmentation in an updated version of the paper. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "26\n",
      "Question 1: What are the four tasks that CamemBERT is evaluated on?\n",
      "\n",
      "Answer 1: CamemBERT is evaluated on POS tagging, dependency parsing, NER and NLI tasks.\n",
      "Question : for the text In this section, we measure the performance of CamemBERT by evaluating it on the four aforementioned tasks: POS tagging, dependency parsing, NER and NLI. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "27\n",
      "What evidence supports the hypothesis that pretrained language models can be effectively fine-tuned for various downstream tasks in natural language processing for non-English languages?\n",
      "\n",
      "The evidence supporting this hypothesis is that CamemBERT, a pretrained language model for French, displays improved performance compared to prior work for the four downstream tasks considered. This suggests that the variability in downstream tasks and datasets is handled more efficiently by a general language model than by Wikipedia-pretrained models such as mBERT.\n",
      "Question : for the text CamemBERT displays improved performance compared to prior work for the 4 downstream tasks considered. This confirms the hypothesis that pretrained language models can be effectively fine-tuned for various downstream tasks, as observed for English in previous work. Moreover, our results also show that dedicated monolingual models still outperform multilingual ones. We explain this point in two ways. First, the scale of data is possibly essential to the performance of CamemBERT. Indeed, we use 138GB of uncompressed text vs. 57GB for mBERT. Second, with more data comes more diversity in the pretraining distribution. Reaching state-of-the-art performances on 4 different tasks and 6 different datasets requires robust pretrained models. Our results suggest that the variability in the downstream tasks and datasets considered is handled more efficiently by a general language model than by Wikipedia-pretrained models such as mBERT. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "28\n",
      "What kind of layers are appended on top of CamemBERT's Transformer architecture for different tasks?\n",
      "The relevant predictive layers, such as linear layer or bi-affine graph predictor head, are appended on top of CamemBERT's Transformer architecture for different tasks such as sequence tagging, sequence labeling, and dependency parsing.\n",
      "Question : for the text For each task, we append the relevant predictive layer on top of CamemBERT's Transformer architecture. Following the work done on BERT BIBREF7, for sequence tagging and sequence labeling we append a linear layer respectively to the $<$s$>$ special token and to the first subword token of each word. For dependency parsing, we plug a bi-affine graph predictor head as inspired by BIBREF54 following the work done on multilingual parsing with BERT by BIBREF36. We refer the reader to these two articles for more details on this module..We fine-tune independently CamemBERT for each task and each dataset. We optimise the model using the Adam optimiser BIBREF23 with a fixed learning rate. We run a grid search on a combination of learning rates and batch sizes. We select the best model on the validation set out of the 30 first epochs..Although this might push the performances even further, for all tasks except NLI, we don't apply any regularisation techniques such as weight decay, learning rate warm-up or discriminative fine-tuning. We show that fine-tuning CamemBERT in a straight-forward manner leads to state-of-the-art results on most tasks and outperforms the existing BERT-based models in most cases..The POS tagging, dependency parsing, and NER experiments are run using hugging face's Transformer library extended to support CamemBERT and dependency parsing BIBREF55. The NLI experiments use the fairseq library following the RoBERTa implementation. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "29\n",
      "Question 1: What was the learning rate warmed up to and why?\n",
      "Answer 1: The learning rate was warmed up to a peak value of $0.0007$ instead of the original $0.0001$ due to the large batch size (8192) used in training the RoBERTa implementation in the fairseq library.\n",
      "Question : for the text We use the RoBERTa implementation in the fairseq library BIBREF53. Our learning rate is warmed up for 10k steps up to a peak value of $0.0007$ instead of the original $0.0001$ given our large batch size (8192). The learning rate fades to zero with polynomial decay. We pretrain our model on 256 Nvidia V100 GPUs (32GB each) for 100k steps during 17h. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "30\n",
      "What is the state of the art model for NER on the FTB?\n",
      "\n",
      "The state of the art model for NER on the FTB is CamemBERT. It achieves a slightly better precision and shows a dramatic improvement in finding entity mentions, raising the recall score by 3.5 points, resulting in a 2.36 point increase in the F1 score with respect to the best SEM architecture (BiLSTM-CRF).\n",
      "Question : for the text For named entity recognition, our experiments show that CamemBERT achieves a slightly better precision than the traditional CRF-based SEM architectures described above in Section SECREF25 (CRF and Bi-LSTM+CRF), but shows a dramatic improvement in finding entity mentions, raising the recall score by 3.5 points. Both improvements result in a 2.36 point increase in the F1 score with respect to the best SEM architecture (BiLSTM-CRF), giving CamemBERT the state of the art for NER on the FTB. One other important finding is the results obtained by mBERT. Previous work with this model showed increased performance in NER for German, Dutch and Spanish when mBERT is used as contextualised word embedding for an NER-specific model BIBREF48, but our results suggest that the multilingual setting in which mBERT was trained is simply not enough to use it alone and fine-tune it for French NER, as it shows worse performance than even simple CRF models, suggesting that monolingual models could be better at NER. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "31\n",
      "Question 1: How does CamemBERT compare in performance to multilingual language models on the XNLI benchmark?\n",
      "\n",
      "Answer 1: CamemBERT obtains improved performance over multilingual language models on the TRANSLATE-TRAIN setting with a score of 81.2 compared to XLM's score of 80.2.\n",
      "Question : for the text On the XNLI benchmark, CamemBERT obtains improved performance over multilingual language models on the TRANSLATE-TRAIN setting (81.2 vs. 80.2 for XLM) while using less than half the parameters (110M vs. 250M). However, its performance still lags behind models trained on the original English training set in the TRANSLATE-TEST setting, 81.2 vs. 82.91 for RoBERTa. It should be noted that CamemBERT uses far fewer parameters than RoBERTa (110M vs. 355M parameters). generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "32\n",
      "Question 1: How does CamemBERT compare to other models on POS tagging and dependency parsing tasks?\n",
      "\n",
      "Answer 1: CamemBERT outperforms UDPipe Future and UDify on all available French treebanks for both POS tagging and dependency parsing tasks. It also demonstrates higher performances than mBERT on these tasks. The error reduction is larger for parsing than for tagging, with reductions in LAS of up to 3.33% for Sequoia treebank.\n",
      "Question : for the text For POS tagging and dependency parsing, we compare CamemBERT to three other near state-of-the-art models in Table TABREF32. CamemBERT outperforms UDPipe Future by a large margin for all treebanks and all metrics. Despite a much simpler optimisation process, CamemBERT beats UDify performances on all the available French treebanks..CamemBERT also demonstrates higher performances than mBERT on those tasks. We observe a larger error reduction for parsing than for tagging. For POS tagging, we observe error reductions of respectively 0.71% for GSD, 0.81% for Sequoia, 0.7% for Spoken and 0.28% for ParTUT. For parsing, we observe error reductions in LAS of 2.96% for GSD, 3.33% for Sequoia, 1.70% for Spoken and 1.65% for ParTUT. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "33\n",
      "What is the name of the French language model trained using the RoBERTa architecture?\n",
      "CamemBERT.\n",
      "Question : for the text Pretrained word representations have a long history in Natural Language Processing (NLP), from non-neural methods BIBREF0, BIBREF1, BIBREF2 to neural word embeddings BIBREF3, BIBREF4 and to contextualised representations BIBREF5, BIBREF6. Approaches shifted more recently from using these representations as an input to task-specific architectures to replacing these architectures with large pretrained language models. These models are then fine-tuned to the task at hand with large improvements in performance over a wide range of tasks BIBREF7, BIBREF8, BIBREF9, BIBREF10..These transfer learning methods exhibit clear advantages over more traditional task-specific approaches, probably the most important being that they can be trained in an unsupervised manner. They nevertheless come with implementation challenges, namely the amount of data and computational resources needed for pretraining that can reach hundreds of gigabytes of uncompressed text and require hundreds of GPUs BIBREF11, BIBREF9. The latest transformer architecture has gone uses as much as 750GB of plain text and 1024 TPU v3 for pretraining BIBREF10. This has limited the availability of these state-of-the-art models to the English language, at least in the monolingual setting. Even though multilingual models give remarkable results, they are often larger and their results still lag behind their monolingual counterparts BIBREF12. This is particularly inconvenient as it hinders their practical use in NLP systems as well as the investigation of their language modeling capacity, something that remains to be investigated in the case of, for instance, morphologically rich languages..We take advantage of the newly available multilingual corpus OSCAR BIBREF13 and train a monolingual language model for French using the RoBERTa architecture. We pretrain the model - which we dub CamemBERT- and evaluate it in four different downstream tasks for French: part-of-speech (POS) tagging, dependency parsing, named entity recognition (NER) and natural language inference (NLI). CamemBERT improves the state of the art for most tasks over previous monolingual and multilingual approaches, which confirms the effectiveness of large pretrained language models for French..We summarise our contributions as follows:.We train a monolingual BERT model on the French language using recent large-scale corpora..We evaluate our model on four downstream tasks (POS tagging, dependency parsing, NER and natural language inference (NLI)), achieving state-of-the-art results in most tasks, confirming the effectiveness of large BERT-based models for French..We release our model in a user-friendly format for popular open-source libraries so that it can serve as a strong baseline for future research and be useful for French NLP practitioners. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "34\n",
      "Question 1: Which multilingual model was trained on Wikipedia data for 104 different languages?\n",
      "\n",
      "Answer 1: Multilingual BERT is a single multilingual model for 104 different languages trained on Wikipedia data.\n",
      "Question : for the text Following the success of large pretrained language models, they were extended to the multilingual setting with multilingual BERT , a single multilingual model for 104 different languages trained on Wikipedia data, and later XLM BIBREF12, which greatly improved unsupervised machine translation. A few monolingual models have been released: ELMo models for Japanese, Portuguese, German and Basque and BERT for Simplified and Traditional Chinese and German..However, to the best of our knowledge, no particular effort has been made toward training models for languages other than English, at a scale similar to the latest English models (e.g. RoBERTa trained on more than 100GB of data). generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "35\n",
      "Question 1: What is the difference between non-contextualised word embeddings and contextualised word representations?\n",
      "\n",
      "Answer 1: Non-contextualised word embeddings, such as word2vec, GloVe and fastText, do not take context into account, while contextualised word representations, such as ELMo and Flair, consider the surrounding context. This results in improved performance in downstream tasks, and has paved the way towards larger contextualised models that can replace downstream architectures in most tasks.\n",
      "Question : for the text The first neural word vector representations were non-contextualised word embeddings, most notably word2vec BIBREF3, GloVe BIBREF4 and fastText BIBREF14, which were designed to be used as input to task-specific neural architectures. Contextualised word representations such as ELMo BIBREF5 and flair BIBREF6, improved the expressivity of word embeddings by taking context into account. They improved the performance of downstream tasks when they replaced traditional word representations. This paved the way towards larger contextualised models that replaced downstream architectures in most tasks. These approaches, trained with language modeling objectives, range from LSTM-based architectures such as ULMFiT BIBREF15 to the successful transformer-based architectures such as GPT2 BIBREF8, BERT BIBREF7, RoBERTa BIBREF9 and more recently ALBERT BIBREF16 and T5 BIBREF10. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "36\n",
      "Question 1: What was the dataset used to train fastText word embeddings for 157 languages?\n",
      "Answer 1: The dataset used to train fastText word embeddings for 157 languages was Common Crawl.\n",
      "Question : for the text Since the introduction of word2vec BIBREF3, many attempts have been made to create monolingual models for a wide range of languages. For non-contextual word embeddings, the first two attempts were by BIBREF17 and BIBREF18 who created word embeddings for a large number of languages using Wikipedia. Later BIBREF19 trained fastText word embeddings for 157 languages using Common Crawl and showed that using crawled data significantly increased the performance of the embeddings relatively to those trained only on Wikipedia. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "37\n",
      "Question 1: What is the subject of the text F?\n",
      "\n",
      "Answer 1: The text F does not provide a clear subject or topic.\n",
      "Question : for the text F generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "38\n",
      "What are some of the limitations of the study on measuring controversy through text? \n",
      "\n",
      "One limitation of the study is not being able to account for non-verbal cues and contextual information that may affect the controversial nature of a topic. Additionally, the study focuses on English texts and may not be generalizable to other languages.\n",
      "Question : for the text The task we address in this work is certainly not an easy one, and our study has some limitations, which we discuss in this section. Our work lead us to some conclusions regarding the overall possibility of measuring controversy through text, and what aspects need to be considered to deepen our work. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "39\n",
      "What is the key takeaway from this article?\n",
      "Answer 1: The article introduces a large-scale systematic method for quantifying controversy in social media through content. The method works on various languages and is more robust than state-of-the-art structure-based controversy measures. It could be used in other contexts such as other social networks like Reddit or Facebook. The article also discusses the potential avenues for future research in measuring controversy through text.\n",
      "Question : for the text In this article, we introduced the first large-scale systematic method for quantifying controversy in social media through content. We have shown that this method works on Spanish, English, French and Portuguese, it is context-agnostic and does not require the intervention of a domain expert..We have compared its performance with state-of-the-art structure-based controversy measures showing that they have same performance and it is more robust. We also have shown that more text implies better performance and without significantly increasing computing time, therefore, it could be used in other contexts such as other social networks like Reddit or Facebook and we are going to test it in future works..Training the model is not an expensive task since Fasttext has a good performance at this. However, the best performance for detecting principal communities is obtained by Walktrap. The complexity of that algorithm is O(m$n^2$)BIBREF38, where $m$ and $n$ are the number of edges and vertices respectively. This makes this method rather expensive to compute on big networks. Nevertheless, we have shown that with Louvain the method still obtains a very similar AUC ROC (0.99 with Walktrap and 0.989 with Louvain). With incomplete information its performance gets worse but it is still good (0.96) and better than previous state of the art..This work opens several avenues for future research. One is identifying what words, semantics/concepts or language expressions make differ one community from the other. There are various ways to do this, for instance through the word-embbedings that Fasttext returns after training BIBREF34. Also we could use interpretability techniques on machine learning models BIBREF43. Finally, we could try other techniques for measuring controversy through text, using another NLP model as pre-trained neural network BERT BIBREF44 or, in a completely different approach measuring the dispersion index of the discussions word-embbedings BIBREF25. We are currently starting to follow this direction. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "40\n",
      "Question 1: What is one of the limitations of the language-based approach used in this study?\n",
      "\n",
      "Answer 1: One of the limitations mentioned is that the approach requires a significant amount of text to train an NLP model that can predict tags with a high probability, making it only effective for \"big\" discussions.\n",
      "Question : for the text As our approach to controversy is similar to that of Garimella et al. BIBREF23, we share some of their limitations with respect to several aspects: Evaluation -difficulties to establish ground-truth, Multisided controversies -controversy with more than two sides, Choice of data - manually pick topics, and Overfitting - small set of experiments. Although we have more discussions, it is still small set from statistical point of view. Apart from that, our language-based approach has other limitations which we mention in the following, together with their solutions or mitigation..Data-size. Training an NLP model that can predict tags with a probability greater or equal than 0.9 requires significant amount of text, therefore our method works only for “big\" discussions. Most interesting controversies are those that have consequence at a society level, in general big enough for our method..Multi-language discussions. When multiple languages are participating in a discussion it is common that users tend to retweet more tweets in their own language, creating sub-communities. In this cases our model will tend to predict higher controversy scores. This is the case for example of #germanwings, where users tweet in English, German and Spanish and it has the highest score in no-controversial topics. However, the polarization that we tackle in this work is normally part of a society cell (a nation, a city, etc.), and thus developed in just one language. We think that limiting the effectiveness of our analysis to single-language discussions is not a serious limitation..Twitter only. Our findings are based on datasets coming from Twitter. While this is certainly a limitation, Twitter is one of the main venues for online public discussion, and one of the few for which data is available. Hence, Twitter is a natural choice. However, Twitter's characteristic limit of 280 characters per message (140 till short time ago) is an intrinsic limitation of that network. We think that in other social networks as Facebook or Reddit our method will work even better, as having more text per user could redound on a better NLP model as we verified comparing the results with 140 and 280 characters per post. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "41\n",
      "Question 1: What discussions were used to obtain the results reported in this section?\n",
      "\n",
      "Answer 1: The above proposed method was run over different discussions to obtain the results reported in this section.\n",
      "Question : for the text In this section we report the results obtained by running the above proposed method over different discussions. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "42\n",
      "What is the criteria used to select the discussions for testing the metric?\n",
      "The criteria used to select the discussions for testing the metric include the occurrence of the discussions between March 2015 and June 2019, half with controversy and half without it, in four different languages, occurring in five regions over the world. To determine if the discussions are controversial or not, topics widely covered by mainstream media that have generated ample discussion, both online and offline, are considered. For non-controversy discussions, \"soft news\" and entertainment or impactful/dramatic events that did not generate large controversies were focused on. The presence or absence of controversy is further established by visualizing the corresponding networks through ForceAtlas2 layout. Finally, the split used for testing the measures included half controversial discussions and half non-controversial discussions that refer to the same topics but in different periods of time.\n",
      "Question : for the text In this section we detail the discussions we use to test our metric and how we determine the ground truth (i.e. if the discussion is controversial or not). We use thirty different discussions that took place between March 2015 and June 2019, half of them with controversy and half without it. We considered discussions in four different languages: English, Portuguese, Spanish and French, occurring in five regions over the world: South and North America, Western Europe, Central and Southern Asia. We also studied these discussions taking first 140 characters and then 280 from each tweet to analyze the difference in performance and computing time wrt the length of the posts..To define the amount of data needed to run our method we established that the Fasttext model has to predict at least one user of each community with a probability greater or equal than 0.9 during ten different trainings. If that is not the case, we are not able to use DPC method. This decision made us consider only a subset of the datasets used in BIBREF23, because due to the time elapsed since their work, many tweets had been deleted and consequently the volume of the data was not enough for our framework. To enlarge our experiment base we added new debates, more detailed information about each one is shown in Table TABREF24 in UNKREF6. To select new discussions and to determine if they are controversial or not we looked for topics widely covered by mainstream media, and that have generated ample discussion, both online and offline. For non-controversy discussions we focused on “soft news\" and entertainment, but also to events that, while being impactful and/or dramatic, did not generate large controversies. To validate that intuition, we manually checked a sample of tweets, being unable to identify any clear instance of controversy On the other side, for controversial debates we focused on political events such as elections, corruption cases or justice decisions..To furtherly establish the presence of absence of controversy of our datasets, we visualized the corresponding networks through ForceAtlas2 BIBREF29. Figures FIGREF9 and FIGREF9 show an example of how non-controversial and controversial discussions look like respectively with ForceAtlas2 layout. As we can see in these figures, in a controversial discussion this layout tends to show two well separated groups while in a non-controversial one it tends to be only one big group. More information on the discussions is given in Table TABREF24..To avoid potential overfitting, we use only twelve graphs as testbed during the development of the measures, half of them controversial (netanyahu, ukraine, @mauriciomacri 1-11 Jan, Kavanaugh 3 Oct, @mauriciomacri 11-18 Mar, Bolsonaro 27 Oct) and half non-controversial (sxsw, germanwings, onedirection, ultralive, nepal, mothersday). This procedure resembles a 40/60% train/test split in traditional machine learning applications..Some of the discussions we consider refer to the same topics but in different periods of time. We needed to split them because our computing infrastructure does not allow us to compute such an enormous amount of data. However, being able to estimate controversy with only a subset of the discussion is an advantage, because discussions could take many days or months and we want to identify controversy as soon as possible, without the need of downloading the whole discussion. Moreover, for very long lasting discussions in social networks gathering the whole data would be impractical for any method.. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "43\n",
      "Q: What was the impact of the length of the considered text on the accuracy of the method and its computing time?\n",
      "\n",
      "A: The impact of the length of the considered text on the accuracy of the method was that the overlapping between controversial and non-controversial topics increased, resulting in an AUC of 0.88. However, the computing time of the method was observed to grow almost linearly, ranging from 30 seconds for a set of 111 KB to 84 seconds for a set of 11941 KB. The running times for the whole method over each dataset with 280 characters were between 170 and 2467 seconds, making it a reasonable amount of time in practice.\n",
      "Question : for the text Training a Fasttext model is not a deterministic process, as different runs could yield different results even using the same training set in each one. To analyze if these differences are significant, we decide to compute 20 scores for each discussion. The standard deviations among these 20 scores were low in all cases, with mean 0.01 and maximum 0.05. Consequently, we decided to report in this paper the average between the 20 scores, in practice taking the average between 5 runs would be enough. Figure FIGREF18 reports the scores computed by our measure in each topic for the two cluster methods. The beanplot shows the estimated probability density function for a measure computed on the topics, the individual observations are shown as small white lines in a one-dimensional scatter plot, and the median as a longer black line. The beanplot is divided into two groups, one for controversial topics (left/dark) and one for non-controversial ones (right/light). Hence, the black group shows the score distribution over controversial discussions and the white group over non-controversial ones. A larger separation of the two distributions indicates that the measure is better at capturing the characteristics of controversial topics, because a good separation allows to establish a threshold in the score that separates controversial and non-controversial discussions..As we may see in the figure, the medians are well separated in both cases, with little overlapping. To better quantify this overlap we measure the sensitivity BIBREF42 of these predictions by measuring the area under the ROC curve (AUC ROC), obtaining a value of 0.98 for Walktrap clustering and 0.967 for Louvain (where 1 represents a perfect separation and 0.5 means that they are indistinguishable)..As Garimella et al. BIBREF23 have made their code public , we reproduced their best method Randomwalk on our datasets and measured the AUC ROC, obtaining a score of 0.935. An interesting finding was that their method had a poor performance over their own datasets. This was due to the fact (already explained in Section SECREF4) that it was not possible to retrieve the complete discussions, moreover, in no case could we restore more than 50% of the tweets. So we decided to remove these discussions and measure again the AUC ROC of this method, obtaining a 0.99 value. Our hypothesis is that the performance of that method was seriously hurt by the incompleteness of the data. We also tested our method on these datasets, obtaining a 0.99 AUC ROC with Walktrap and 0.989 with Louvain clustering..We conclude that our method works better, as in practice both approaches show same performances -specially with Walktrap, but in presence of incomplete information our measure is more robust. The performance of Louvain is slightly worse but, as we mentioned in Section SECREF3, this method is much faster. Therefore, we decided to compare the running time of our method with both clustering techniques and also with the Randomwalk algorithm. In figure FIGREF18 we can see the distribution of running times of all techniques through box plots. Both versions of our method are faster than Randomwalk, while Louvain is faster than Walktrap..We now analyze the impact of the length of the considered text in our method. Figure FIGREF18 depicts the results of similar experiment as Figure FIGREF18, but considering only 140 characters per tweet. As we may see, here the overlapping is bigger, having an AUC of 0.88. As for the impact on computing time, we observe that despite of the results of BIBREF34 that reported a complexity of O(h $log_{2}$(k)) at training and test tasks, in practice we observed a linear growth. We measured the running times of the training and predicting phases (the two text-related phases of our method), the resulting times are reported in figure FIGREF18, which shows running time as a function of the text-size. We include also the best estimated function that approximate computing time as a function of text-set size. As it may be seen, time grows almost linearly, ranging from 30 seconds for a set of 111 KB to 84 seconds for a set of 11941 KB. Finally, we measured running times for the whole method over each dataset with 280 characters. Times were between 170 and 2467 seconds with a mean of 842, making it in practice a reasonable amount of time. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "44\n",
      "Question 1: How is a topic defined in the approach used in the study?\n",
      "\n",
      "Answer 1: In the approach used in the study, a topic is defined as a specific hashtag or keyword that is associated with a large enough volume of activity. This approach allows for flexibility in capturing discussions even when they do not have a defined hashtag. For example, during the Brazilian presidential elections, the discussion was captured by the mentions of the candidate's surname, Bolsonaro, which was not a defined hashtag but a widely used keyword.\n",
      "Question : for the text In the literature, a topic is often defined by a single hashtag. However, this might be too restrictive in many cases. In our approach, a topic is operationalized as an specific hashtags or key words. Sometimes a discussion in a particular moment could not have a defined hashtag but it could be around a certain keyword, i.e. a word or expression that is not specifically a hashtag but it is widely used in the topic. For example during the Brazilian presidential elections in 2018 we captured the discussion by the mentions to the word Bolsonaro, that is the principal candidate's surname..Thus, for each topic we retrieve all the tweets that contain one of its hashtags or the keyword and that are generated during the observation window. We also ensure that the selected topic is associated with a large enough volume of activity. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "45\n",
      "What is the main contribution of the proposed method for quantifying controversy on social networks?\n",
      "\n",
      "The main contribution of the proposed method is a new vocabulary-based approach that works in any language and equates the performance of state-of-the-art structure-based methods, allowing for the quantification of controversy and the classification of controversial and non-controversial topics.\n",
      "Question : for the text Controversy is a phenomenom with a high impact at various levels. It has been broadly studied from the perspective of different disciplines, ranging from the seminal analysis of the conflicts within the members of a karate club BIBREF0 to political issues in modern times BIBREF1, BIBREF2. The irruption of digital social networks BIBREF3 gave raise to new ways of intentionally intervening on them for taking some advantage BIBREF4, BIBREF5. Moreover highly contrasting points of view in some groups tend to provoke conflicts that lead to attacks from one community to the other by harassing, “brigading”, or “trolling” it BIBREF6. The existing literature shows different issues that controversy brings up such as splitting of communities, biased information, hateful discussions and attacks between groups, generally proposing ways to solve them. For example, Kumar, Srijan, et al. BIBREF6 analyze many techniques to defend us from attacks in Reddit while Stewart, et al. BIBREF4 insinuate that there was external interference in Twitter during the 2016 US presidential elections to benefit one candidate. Also, as shown in BIBREF7, detecting controversy could provide the basis to improve the “news diet\" of readers, offering the possibility to connect users with different points of views by recommending them new content to read BIBREF8..Moreover, other studies on “bridging echo chambers” BIBREF9 and the positive effects of intergroup dialogue BIBREF10, BIBREF11 suggest that direct engagement could be effective for mitigating such conflicts. Therefore, easily and automatically identifying controversial topics could allow us to quickly implement different strategies for preventing miss-information, fights and bias. Quantifying the controversy is even more powerful, as it allows us to establish controversy levels, and in particular to classify controversial and non-controversial topics by establishing a threshold score that separates the two types of topics. With this aim, we propose in this work a systematic, language-agnostic method to quantify controversy on social networks taking tweet's content as root input. Our main contribution is a new vocabulary-based method that works in any language and equates the performance of state-of-the-art structure-based methods. Finally, controversy quantification through vocabulary analysis opens several research avenues to analyze whether polarization is being created, maintained or augmented by the ways of talking of each community..Having this in mind and if we draw from the premise that when a discussion has a high controversy it is in general due to the presence of two principal communities fighting each other (or, conversely, that when there is no controversy there is just one principal community the members of which share a common point of view), we can measure the controversy by detecting if the discussion has one or two principal jargons in use. Our method is tested on Twitter datasets. This microblogging platform has been widely used to analyze discussions and polarization BIBREF12, BIBREF13, BIBREF14, BIBREF15, BIBREF2. It is a natural choice for these kind of problems, as it represents one of the main fora for public debate in online social media BIBREF15, it is a common destination for affiliative expressions BIBREF16 and is often used to report and read news about current events BIBREF17. An extra advantage of Twitter for this kind of studies is the availability of real-time data generated by millions of users. Other social media platforms offer similar data-sharing services, but few can match the amount of data and the accompanied documentation provided by Twitter. One last asset of Twitter for our work is given by retweets, whom typically indicate endorsement BIBREF18 and hence become a useful concept to model discussions as we can set “who is with who\". However, our method has a general approach and it could be used a priori in any social network. In this work we report excellent result tested on Twitter but in future work we are going to test it in other social networks..Our paper is organized as follows: in Section SECREF2, we review related work. Section SECREF3 contains the detailed explanation of the pipeline we use for quantifying controversy of a topic, and each of its stages. In Section SECREF4 we report the results of an extensive empirical evaluation of the proposed measure of controversy. Finally, Section SECREF5 is devoted to discuss possible improvements and directions for future work, as well as lessons learned. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "46\n",
      "Question 1: What is the rationale behind the controversy measure used in this work?\n",
      "\n",
      "Answer 1: The controversy measure used in this work is based on the notion of dipole moment from physics. The measure calculates the absolute difference in the normalized size of two partitions, and the average value among vertices in each partition. If the two partitions are well-separated, label propagation will assign different extreme values to the two partitions, leading to higher values of the DMC measure. Larger differences in the size of the two partitions lead to smaller values for the measure, which takes values between zero and one.\n",
      "Question : for the text Our approach to measuring controversy is based on a systematic way of characterizing social media activity through its content. We employ a pipeline with five stages, namely graph building, community identification, model training, predicting and controversy measure. The final output of the pipeline is a value that measures how controversial a topic is, with higher values corresponding to higher degrees of controversy. The method is based on analysing posts content through Fasttext BIBREF34, a library for efficient learning of word representations and sentence classification developed by Facebook Research team. In short, our method works as follows: through Fasttext we train a language-agnostic model which can predict the community of many users by their jargon. Then we take there predictions and compute a score based on the physic notion Dipole Moment using a language approach to identify core or characteristic users and set the polarity trough them. We provide a detailed description of each stage in the following..Graph Building.This paragraph provides details about the approach used to build graphs from raw data. As we said in Section SECREF1, we extract our discussions from Twitter. Our purpose is to build a conversation graph that represents activity related to a single topic of discussion -a debate about a specific event..For each topic, we build a graph $G$ where we assign a vertex to each user who contributes to it and we add a directed edge from node $u$ to node $v$ whenever user $u$ retweets a tweet posted by $v$. Retweets typically indicate endorsement BIBREF18: users who retweet signal endorsement of the opinion expressed in the original tweet by propagating it further. Retweets are not constrained to occur only between users who are connected in Twitter's social network, but users are allowed to retweet posts generated by any other user. As many other works in literature BIBREF5, BIBREF35, BIBREF36, BIBREF37, BIBREF4, BIBREF2 we establish that one retweet among a pair of users are needed to define an edge between them..Community Identification.To identify a community's jargon we need to be very accurate at defining its members. If we, in our will of finding two principal communities, force the partition of the graph in that precise number of communities, we may be adding noise in the jargon of the principal communities that are fighting each other. Because of that, we decide to cluster the graph trying two popular algorithms: Walktrap BIBREF38 and Louvain BIBREF39. Both are structure-based algorithms that have very good performance with respect to the Modularity Q measure. These techniques does not detect a fixed number of clusters; their output will depend on the Modularity Q optimization, resulting in less “noisy\" communities. The main differences between the two methods, in what regards our work, are that Louvain is a much faster heuristic algorithm but produces clusters with worse Modularity Q. Therefore, in order to analyze the trade-off between computing time and quality we decide to test both methods. At this step we want to capture the tweets of the principal communities to create the model that could differentiate them. Therefore, we take the two communities identified by the cluster algorithm that have the maximum number of users, and use them for the following step of our method..Model Training.After detecting the principal communities we create our training dataset to feed the model. To do that, we extract the tweets of each cluster, we sanitize and we subject them to some transformations. First, we remove duplicate tweets -e.g. retweets without additional text. Second, we remove from the text of the tweets user names, links, punctuation, tabs, leading and lagging blanks, general spaces and “RT\" - the text that points that a tweet is in fact a retweet..As shown in previous works, emojis are correlated with sentiment BIBREF40. Moreover, as we think that communities will express different sentiment during discussion, it is forseeable that emojis will play an important role as separators of tweets that differentiate between the two sides. Accordingly, we decide to add them to the train-set by translating each emoji into a different word. For example, the emoji :) will be translated into happy and :( into sad. Relations between emojis and words are defined in the R library textclean..Finally, we group tweets by user concatenating them in one string and labeling them with the user's community, namely with tags C1 and C2, corresponding respectively to the biggest and second biggest groups. It is important to note that we take the same number of users of each community to prevent bias in the model. Thus, we use the number of users of the smallest principal community..The train-set built that way is used to feed the model. As we said, we use Fasttext BIBREF34 to do this training. To define the values of the hyper-parameters we use the findings of BIBREF41. In their work they investigate the best hyper-parameters to train word embedding models using Fasttext BIBREF34 and Twitter data. We also change the default value of the hyper-parameter epoch to 20 instead of 5 because we want more convergence preventing as much as possible the variance between different training. These values could change in other context or social networks where we have more text per user or different discussion dynamics..Predicting.The next stage consists of identifying the characteristic users of each side the discussion. These are the users that better represent the jargon of each side. To do that, tweets of the users belonging to the largest connected component of the graph are sanitized and transformed exactly as in the Training step..We decide to restrict to the largest connected component because in all cases it contains more than 90% of the nodes. The remaining 10% of the users don't participate in the discussion from a collective point of view but rather in an isolated way and this kind of intervention does not add interesting information to our approach. Then, we remove from this component users with degree smaller or equal to 2 (i.e. users that were retweeted by another user or retweeted other person less than three times in total). Their participation in the discussion is marginal, consequently they are not relevant wrt controversy as they add more noise than information at measuring time. This step could be adjusted differently in a different social network. We name this result component root-graph..Finally, let's see how we do classification. Considering that Fasttext returns for each classification both the predicted tag and the probability of the prediction, we classify each user of the resulting component by his sanitized tweets with our trained model, and take users that were tagged with a probability greater or equal than 0.9. These are the characteristic users that will be used in next step to compute the controversy measure..Controversy Measure.This section describes the controversy measures used in this work. This computation is inspired in the measure presented by Morales et al. BIBREF2, and is based on the notion of dipole moment that has its origin in physics..First, we assign to the characteristic users the probability returned by the model, negativizing them if the predicted tag was C2. Therefore, these users are assigned values in the set [-1,-0.9] $\\cup $ [0.9,1]. Then, we set values for the rest of the users of the root-graph by label-propagation BIBREF31 - an iterative algorithm to propagate values through a graph by node's neighborhood..Let $n^{+}$ and $n^{-}$ be the number of vertices $V$ with positive and negative values, respectively, and $\\Delta A = \\dfrac{\\mid n^{+} - n^{-}\\mid }{\\mid V \\mid }$ the absolute difference of their normalized size. Moreover, let $gc^{+}$ ($gc^{-}$) be the average value among vertices $n^{+}$ ($n^{-}$) and set $\\tau $ as half their absolute difference, $\\tau = \\dfrac{\\mid gc^{+} - gc^{- }\\mid }{2}$. The dipole moment content controversy measure is defined as: $\\textit {DMC} = (1 -\\Delta A)\\tau $..The rationale for this measure is that if the two sides are well separated, then label propagation will assign different extreme values to the two partitions, where users from one community will have values near to 1 and users from the other to -1, leading to higher values of the DMC measure. Note also that larger differences in the size of the two partitions (reflected in the value of $\\Delta A$) lead to smaller values for the measure, which takes values between zero and one. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "47\n",
      "What is the main focus of the controversy measurement method proposed in the text?\n",
      "\n",
      "The main focus of the controversy measurement method proposed in the text is vocabulary. The method first trains an NLP classifier to estimate the opinion polarity of main users, then runs label-propagation on the endorsement graph to get the polarity of the whole network, and finally computes the controversy score through a computation inspired by Dipole Moment.\n",
      "Question : for the text Many previous works are dedicated to quantifying the polarization observed in online social networks and social media BIBREF1, BIBREF19, BIBREF20, BIBREF21, BIBREF22, BIBREF23. The main characteristic of those works is that the measures proposed are based on the structural characteristics of the underlying graph. Among them, we highlight the work of Garimella et al.BIBREF23 that presents an extensive comparison of controversy measures, different graph-building approaches, and data sources, achieving the best performance of all. In their research they propose different metrics to measure polarization on Twitter. Their techniques based on the structure of the endorsement graph can successfully detect whether a discussion (represented by a set of tweets), is controversial or not regardless of the context and most importantly, without the need of any domain expertise. They also consider two different methods to measure controversy based on the analysis of the posts contents, but both fail when used to create a measure of controversy..Matakos et al. BIBREF24 develop a polarization index. Their measure captures the tendency of opinions to concentrate in network communities, creating echo-chambers. They obtain a good performance at identifying controversy by taking into account both the network structure and the existing opinions of users. However, they model opinions as positive or negative with a real number between -1 and 1. Their performance is good, but although it is an opinion-based method it is not a text-related one.Other recent works BIBREF25, BIBREF26, BIBREF27 have shown that communities may express themselves with different terms or ways of speaking, use different jargon, which in turn can be detected with the use of text-related techniques..In his thesis BIBREF28, Jang explains controversy via generating a summary of two conflicting stances that make up the controversy. This work shows that a specific sub-set of tweets could represent the two opposite positions in a polarized debate..A good tool to see how communities interact is ForceAtlas2 BIBREF29, a force-directed layout widely used for visualization. This layout has been recently found to be very useful at visualizing community interactions BIBREF30, as this algorithm will draw groups with little communication between them in different areas, whereas, if they have many interactions they will be drawn closer to each other. Therefore, whenever there is controversy the layout will show two well separated groups and will tend to show only one big community otherwise..The method we propose to measure the controversy equates in accuracy the one developed by Garimella et al.BIBREF23 and improves considerably computing time and robustness wrt the amount of data needed to effectively apply it. Our method is also based on a graph approach but it has its main focus on the vocabulary. We first train an NLP classifier that estimates opinion polarity of main users, then we run label-propagation BIBREF31 on the endorsement graph to get polarity of the whole network. Finally we compute the controversy score through a computation inspired in Dipole Moment, a measure used in physics to estimate electric polarity on a system. In our experiments we use the same data-sets from other works BIBREF32, BIBREF23, BIBREF33 as well as other datasets that we collected by us using a similar criterion (described in Section SECREF4). generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "48\n",
      "Question 1: What is the purpose of a multi-classifier system in sentiment analysis and opinion mining? \n",
      "\n",
      "Answer 1: The purpose of a multi-classifier system is to improve the accuracy of sentiment analysis and opinion mining by using multiple algorithms to classify text data into different sentiment categories. This approach allows for a more comprehensive analysis of user-generated content, and can help to identify nuanced expressions of sentiment that may be missed by a single algorithm.\n",
      "Question : for the text Microblog Sentiment Analysis 100590.Multi-classifier System for Sentiment Analysis and Opinion Mining 351.Sentiment Analysis in Social Media 120.Sentiment Analysis of Microblogging Data 110168.Sentiment Analysis of Reviews 110169.Sentiment Analysis, Basics of 110159.Sentiment Quantification of User-Generated Content 110170.Social Media Analysis for Monitoring Political Sentiment 110172.Twitter Microblog Sentiment Analysis 265.User Sentiment and Opinion Analysis 192 generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "49\n",
      "Question 1: What is the main goal of sentiment analysis on Twitter? \n",
      "Answer 1: The main goal of sentiment analysis on Twitter is to identify and categorize opinions expressed in a tweet, in order to determine the author's attitude toward a particular topic or in general.\n",
      "Question : for the text Sentiment analysis on Twitter is the use of natural language processing techniques to identify and categorize opinions expressed in a tweet, in order to determine the author's attitude toward a particular topic or in general. Typically, discrete labels such as positive, negative, neutral, and objective are used for this purpose, but it is also possible to use labels on an ordinal scale, or even continuous numerical values. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "50\n",
      "What are some Twitter-specific techniques used in text pre-processing for sentiment analysis?\n",
      "\n",
      "Some Twitter-specific techniques used in text pre-processing for sentiment analysis include substitution/removal of URLs, user mentions, hashtags, and emoticons, spelling correction, elongation normalization, abbreviation lookup, punctuation removal, detection of amplifiers and diminishers, negation scope detection, and the use of Twitter-specific NLP tools such as part-of-speech and named entity taggers, and syntactic parsers.\n",
      "Question : for the text Pre-processing. Tweets are subject to standard preprocessing steps for text such as tokenization, stemming, lemmatization, stop-word removal, and part-of-speech tagging. Moreover, due to their noisy nature, they are also processed using some Twitter-specific techniques such as substitution/removal of URLs, of user mentions, of hashtags, and of emoticons, spelling correction, elongation normalization, abbreviation lookup, punctuation removal, detection of amplifiers and diminishers, negation scope detection, etc. For this, one typically uses Twitter-specific NLP tools such as part-of-speech and named entity taggers, syntactic parsers, etc. BIBREF47 , BIBREF48 , BIBREF49 ..Negation handling. Special handling is also done for negation. The most popular approach to negation handling is to transform any word that appeared in a negation context by adding a suffix _NEG to it, e.g., good would become good_NEG BIBREF50 , BIBREF10 . A negated context is typically defined as a text span between a negation word, e.g., no, not, shouldn't, and a punctuation mark or the end of the message. Alternatively, one could flip the polarity of sentiment words, e.g., the positive word good would become negative when negated. It has also been argued BIBREF51 that negation affects different words differently, and thus it was also proposed to build and use special sentiment polarity lexicons for words in negation contexts BIBREF52 ..Features. Traditionally, systems for Sentiment Analysis on Twitter have relied on handcrafted features derived from word-level (e.g., great, freshly roasted coffee, becoming president) and character-level INLINEFORM0 -grams (e.g., bec, beco, comin, oming), stems (e.g., becom), lemmata (e.g., become, roast), punctuation (e.g., exclamation and question marks), part-of-speech tags (e.g., adjectives, adverbs, verbs, nouns), word clusters (e.g., probably, probly, and maybe could be collapsed to the same word cluster), and Twitter-specific encodings such as emoticons (e.g., ;), :D), hashtags (#Brexit), user tags (e.g., @allenai_org), abbreviations (e.g., RT, BTW, F2F, OMG), elongated words (e.g., soooo, yaayyy), use of capitalization (e.g., proportion of ALL CAPS words), URLs, etc. Finally, the most important features are those based on the presence of words and phrases in sentiment polarity lexicons with positive/negative scores; examples of such features include number of positive terms, number of negative terms, ratio of the number of positive terms to the number of positive+negative terms, ratio of the number of negative terms to the number of positive+negative terms, sum of all positive scores, sum of all negative scores, sum of all scores, etc..Supervised learning. Traditionally, the above features were fed into classifiers such as Maximum Entropy (MaxEnt) and Support Vector Machines (SVM) with various kernels. However, observation over the SemEval Twitter sentiment task in recent years shows growing interest in, and by now clear dominance of methods based on deep learning. In particular, the best-performing systems at SemEval-2015 and SemEval-2016 used deep convolutional networks BIBREF53 , BIBREF54 . Conversely, kernel machines seem to be less frequently used than in the past, and the use of learning methods other than the ones mentioned above is at this point scarce. All these models are examples of supervised learning as they need labeled training data..Semi-supervised learning. We should note two things about the use of deep neural networks. First they can often do quite well without the need for explicit feature modeling, as they can learn the relevant features in their hidden layers starting from the raw text. Second, they have too many parameters, and thus they require a lot of training data, orders of magnitude more than it is realistic to have manually annotated. A popular way to solve this latter problem is to use self training, a form of semi-supervised learning, where first a system is trained on the available training data only, then this system is applied to make predictions on a large unannotated set of tweets, and finally it is trained for a few more iterations on its own predictions. This works because parts of the network, e.g., with convolution or with LSTMs BIBREF55 , BIBREF54 , BIBREF56 , need to learn something like a language model, i.e., which word is likely to follow which one. Training these parts needs no labels. While these parts can be also pre-trained, it is easier, and often better, to use self training..Distantly-supervised learning. Another way to make use of large unannotated datasets is to rely on distant supervision BIBREF41 . For example, one can annotate tweets for sentiment polarity based on whether they contain a positive or a negative emoticon. This results in noisy labels, which can be used to train a system BIBREF54 , to induce sentiment-specific word embeddings BIBREF57 , sentiment-polarity lexicons BIBREF25 , etc..Unsupervised learning. Fully unsupervised learning is not a popular method for addressing sentiment analysis tasks. Yet, some features used in sentiment analysis have been learned in an unsupervised way, e.g., Brown clusters to generalize over words BIBREF58 . Similarly, word embeddings are typically trained from raw tweets that have no annotation for sentiment (even though there is also work on sentiment-specific word embeddings BIBREF57 , which uses distant supervision). generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "1\n",
      "Question 1: What methods do the authors believe will be the main focus of future sentiment analysis research?\n",
      "\n",
      "Answer 1: The authors believe that deep learning, semi-supervised and distantly-supervised methods, together with more attention paid to linguistic structure and sentiment compositionality, will be the main focus of future sentiment analysis research.\n",
      "Question : for the text We expect the quest for more interesting formulations of the general sentiment analysis task to continue. We see competitions such as those at SemEval as the engine of this innovation, as they not only perform head-to-head comparisons, but also create databases and tools that enable follow-up research for many years afterward..In terms of methods, we believe that deep learning BIBREF55 , BIBREF54 , BIBREF56 , together with semi-supervised and distantly-supervised methods BIBREF67 , BIBREF57 , will be the main focus of future research. We also expect more attention to be paid to linguistic structure and sentiment compositionality BIBREF68 , BIBREF69 . Moreover, we forecast more interest for languages other than English, and for cross-lingual methods BIBREF40 , BIBREF70 , BIBREF71 , which will allow leveraging on the rich resources that are already available for English. Last, but not least, the increase in opinion spam on Twitter will make it important to study astroturfing BIBREF72 and troll detection BIBREF73 , BIBREF74 , BIBREF75 . generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "2\n",
      "Question 1: What is the goal of sentiment analysis? \n",
      "Answer 1: The goal of sentiment analysis is to determine the attitude of a speaker or writer towards a topic, or the overall polarity of a piece of text.\n",
      "Question : for the text Sentiment Analysis: This is text analysis aiming to determine the attitude of a speaker or a writer with respect to some topic or the overall contextual polarity of a piece of text. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "3\n",
      "Question 1: What was the limitation of sentiment analysis before the rise of social media?\n",
      "\n",
      "Answer 1: Before the rise of social media, sentiment analysis was limited to learning about the language of sentiment in general, and primarily focused on newswire texts and customer reviews. It lacked resources and lexicons, especially for short social media messages, which hindered research in sentiment analysis on Twitter.\n",
      "Question : for the text Sentiment analysis emerged as a popular research direction in the early 2000s. Initially, it was regarded as standard document classification into topics such as business, sport, and politics BIBREF10 . However, researchers soon realized that it was quite different from standard document classification BIBREF11 , and that it crucially needed external knowledge in the form of sentiment polarity lexicons..Around the same time, other researchers realized the importance of external sentiment lexicons, e.g., Turney BIBREF12 proposed an unsupervised approach to learn the sentiment orientation of words/phrases: positive vs. negative. Later work studied the linguistic aspects of expressing opinions, evaluations, and speculations BIBREF13 , the role of context in determining the sentiment orientation BIBREF14 , of deeper linguistic processing such as negation handling BIBREF15 , of finer-grained sentiment distinctions BIBREF16 , of positional information BIBREF17 , etc. Moreover, it was recognized that in many cases, it is crucial to know not just the polarity of the sentiment but also the topic toward which this sentiment is expressed BIBREF18 ..Until the rise of social media, research on opinion mining and sentiment analysis had focused primarily on learning about the language of sentiment in general, meaning that it was either genre-agnostic BIBREF19 or focused on newswire texts BIBREF20 and customer reviews (e.g., from web forums), most notably about movies BIBREF10 and restaurants BIBREF21 but also about hotels, digital cameras, cell phones, MP3 and DVD players BIBREF22 , laptops BIBREF21 , etc. This has given rise to several resources, mostly word and phrase polarity lexicons, which have proven to be very valuable for their respective domains and types of texts, but less useful for short social media messages..Later, with the emergence of social media, sentiment analysis in Twitter became a hot research topic. Unfortunately, research in that direction was hindered by the unavailability of suitable datasets and lexicons for system training, development, and testing. While some Twitter-specific resources were developed, initially they were either small and proprietary, such as the i-sieve corpus BIBREF6 , were created only for Spanish like the TASS corpus BIBREF23 , or relied on noisy labels obtained automatically, e.g., based on emoticons and hashtags BIBREF24 , BIBREF25 , BIBREF10 ..This situation changed with the shared task on Sentiment Analysis on Twitter, which was organized at SemEval, the International Workshop on Semantic Evaluation, a semantic evaluation forum previously known as SensEval. The task ran in 2013, 2014, 2015, and 2016, attracting over 40 participating teams in all four editions. While the focus was on general tweets, the task also featured out-of-domain testing on SMS messages, LiveJournal messages, as well as on sarcastic tweets..SemEval-2013 Task 2 BIBREF26 and SemEval-2014 Task 9 BIBREF27 focused on expression-level and message-level polarity. SemEval-2015 Task 10 BIBREF28 , BIBREF29 featured topic-based message polarity classification on detecting trends toward a topic and on determining the out-of-context (a priori) strength of association of Twitter terms with positive sentiment. SemEval-2016 Task 4 BIBREF30 introduced a 5-point scale, which is used for human review ratings on popular websites such as Amazon, TripAdvisor, Yelp, etc.; from a research perspective, this meant moving from classification to ordinal regression. Moreover, it focused on quantification, i.e., determining what proportion of a set of tweets on a given topic are positive/negative about it. It also featured a 5-point scale ordinal quantification subtask BIBREF31 ..Other related tasks have explored aspect-based sentiment analysis BIBREF32 , BIBREF33 , BIBREF21 , sentiment analysis of figurative language on Twitter BIBREF34 , implicit event polarity BIBREF35 , stance in tweets BIBREF36 , out-of-context sentiment intensity of phrases BIBREF37 , and emotion detection BIBREF38 . Some of these tasks featured languages other than English. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "4\n",
      "What is sentiment analysis and why is it important in social media?\n",
      "Sentiment analysis is the process of analyzing and determining the attitudes, emotions, and opinions expressed in social media content. It is important in social media because it allows us to study public opinion at a scale that was previously impossible, helping various fields such as marketing, political science, and social studies to answer important questions about products, policies, and public figures. However, the informal language and style of social media has presented new challenges for natural language processing applications, which had previously relied on NLP tools tuned for formal text genres such as newswire.\n",
      "Question : for the text Internet and the proliferation of smart mobile devices have changed the way information is created, shared, and spreads, e.g., microblogs such as Twitter, weblogs such as LiveJournal, social networks such as Facebook, and instant messengers such as Skype and WhatsApp are now commonly used to share thoughts and opinions about anything in the surrounding world. This has resulted in the proliferation of social media content, thus creating new opportunities to study public opinion at a scale that was never possible before..Naturally, this abundance of data has quickly attracted business and research interest from various fields including marketing, political science, and social studies, among many others, which are interested in questions like these: Do people like the new Apple Watch? What do they hate about iPhone6? Do Americans support ObamaCare? What do Europeans think of Pope's visit to Palestine? How do we recognize the emergence of health problems such as depression? Do Germans like how Angela Merkel is handling the refugee crisis in Europe? What do republican voters in USA like/hate about Donald Trump? How do Scottish feel about the Brexit?.Answering these questions requires studying the sentiment of opinions people express in social media, which has given rise to the fast growth of the field of sentiment analysis in social media, with Twitter being especially popular for research due to its scale, representativeness, variety of topics discussed, as well as ease of public access to its messages BIBREF0 , BIBREF1 ..Despite all these opportunities, the rise of social media has also presented new challenges for natural language processing (NLP) applications, which had largely relied on NLP tools tuned for formal text genres such as newswire, and thus were not readily applicable to the informal language and style of social media. That language proved to be quite challenging with its use of creative spelling and punctuation, misspellings, slang, new words, URLs, and genre-specific terminology and abbreviations, e.g., RT for re-tweet and #hashtags. In addition to the genre difference, there is also a difference in length: social media messages are generally short, often length-limited by design as in Twitter, i.e., a sentence or a headline rather than a full document. How to handle such challenges has only recently been the subject of thorough research BIBREF2 , BIBREF3 , BIBREF4 , BIBREF5 , BIBREF6 , BIBREF7 , BIBREF8 , BIBREF9 . generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "5\n",
      "What are some of the areas where sentiment analysis on Twitter can be applied?\n",
      "Sentiment analysis on Twitter can be applied in a number of areas, including political science, economics, social science, market research, studying company reputation online, measuring customer satisfaction, identifying detractors and promoters, forecasting market growth, predicting the future income from newly-released movies, forecasting the outcome of upcoming elections, and studying political polarization.\n",
      "Question : for the text Sentiment analysis on Twitter has applications in a number of areas, including political science BIBREF39 , BIBREF40 , BIBREF41 , economics BIBREF42 , BIBREF7 , social science BIBREF43 , and market research BIBREF44 , BIBREF45 . It is used to study company reputation online BIBREF45 , to measure customer satisfaction, to identify detractors and promoters, to forecast market growth BIBREF42 , to predict the future income from newly-released movies, to forecast the outcome of upcoming elections BIBREF41 , BIBREF7 , to study political polarization BIBREF39 , BIBREF9 , etc. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "6\n",
      "What is SemEval and what is its significance in sentiment analysis on Twitter?\n",
      "\n",
      "SemEval is the International Workshop on Semantic Evaluation, which has created benchmark datasets and enabled direct comparison between different systems and approaches for sentiment analysis on Twitter. Its significance lies in promoting research and development of sentiment analysis methods, leading to advancements in the field.\n",
      "Question : for the text Sentiment analysis has a wide number of applications in areas such as market research, political and social sciences, and for studying public opinion in general, and Twitter is one of the most commonly-used platforms for this. This is due to its streaming nature, which allows for real-time analysis, to its social aspect, which encourages people to share opinions, and to the short size of the tweets, which simplifies linguistic analysis..There are several formulations of the task of Sentiment Analysis on Twitter that look at different sizes of the target (e.g., at the level of words vs. phrases vs. tweets vs. sets of tweets), at different types of semantic targets (e.g., aspect vs. topic vs. overall tweet), at the explicitness of the target (e.g., sentiment vs. stance detection), at the scale of the expected label (2-point vs. 3-point vs. ordinal), etc. All these are explored at SemEval, the International Workshop on Semantic Evaluation, which has created a number of benchmark datasets and has enabled direct comparison between different systems and approaches, both as part of the competition and beyond..Traditionally, the task has been addressed using supervised and semi-supervised methods, as well as using distant supervision, with the most important resource being sentiment polarity lexicons, and with feature-rich approaches as the dominant research direction for years. With the recent rise of deep learning, which in many cases eliminates the need for any explicit feature modeling, the importance of both lexicons and features diminishes, while at the same time attention is shifting towards learning from large unlabeled data, which is needed to train the high number of parameters of such complex models. Finally, as methods for sentiment analysis mature, more attention is also being paid to linguistic structure and to multi-linguality and cross-linguality. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "7\n",
      "Question 1: What surveys are recommended for general research on sentiment analysis?\n",
      "\n",
      "Answer 1: The surveys recommended for general research on sentiment analysis are BIBREF76 and BIBREF15.\n",
      "Question : for the text For general research on sentiment analysis, we recommend the following surveys: BIBREF76 and BIBREF15 . For sentiment analysis on Twitter, we recommend the overview article on Sentiment Analysis on Twitter about the SemEval task BIBREF28 as well as the task description papers for different editions of the task BIBREF30 , BIBREF26 , BIBREF29 , BIBREF27 . generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "8\n",
      "Question 1: What is the dominant approach for building large sentiment polarity lexicons? \n",
      "Answer 1: The dominant approach is based on pointwise mutual information and bootstrapping, as proposed by Turney. This approach starts with a small set of seed positive and negative words and induces sentiment polarity orientation for new words in a large unannotated set of texts, using co-occurrence patterns with positive and negative seeds to estimate sentiment strength. This approach has been used to create automatic lexicons for sentiment analysis in Twitter, including the Hashtag Sentiment Lexicon and the Sentiment140 lexicon.\n",
      "Question : for the text Despite the wide variety of knowledge sources explored so far in the literature, sentiment polarity lexicons remain the most commonly used resource for the task of sentiment analysis..Until recently, such sentiment polarity lexicons were manually crafted and were thus of small to moderate size, e.g., LIWC BIBREF59 has 2,300 words, the General Inquirer BIBREF60 contains 4,206 words, Bing Liu's lexicon BIBREF22 includes 6,786 words, and MPQA BIBREF14 has about 8,000 words..Early efforts toward building sentiment polarity lexicons automatically yielded lexicons of moderate sizes such as the SentiWordNet BIBREF19 , BIBREF61 . However, recent results have shown that automatically extracted large-scale lexicons (e.g., up to a million words and phrases) offer important performance advantages, as confirmed at shared tasks on Sentiment Analysis on Twitter at SemEval 2013-2016 BIBREF30 , BIBREF26 , BIBREF29 , BIBREF27 . Using such large-scale lexicons was crucial for the performance of the top-ranked systems. Similar observations were made in the related Aspect-Based Sentiment Analysis task at SemEval 2014 BIBREF21 . In both tasks, the winning systems benefitted from building and using massive sentiment polarity lexicons BIBREF25 , BIBREF62 ..The two most popular large-scale lexicons were the Hashtag Sentiment Lexicon and the Sentiment140 lexicon, which were developed by the team of NRC Canada for their participation in the SemEval-2013 shared task on sentiment analysis on Twitter. Similar automatically induced lexicons proved useful for other SemEval tasks, e.g., for SemEval-2016 Task 3 on Community Question Answering BIBREF63 , BIBREF30 ..The importance of building sentiment polarity lexicons has resulted in a special subtask BIBREF29 at SemEval-2015 (part of Task 4) and an entire task BIBREF37 at SemEval-2016 (namely, Task 7), on predicting the out-of-context sentiment intensity of words and phrases. Yet, we should note though that the utility of using sentiment polarity lexicons for sentiment analysis probably needs to be revisited, as the best system at SemEval-2016 Task 4 could win without using any lexicons BIBREF53 ; it relied on semi-supervised learning using a deep neural network..Various approaches have been proposed in the literature for bootstrapping sentiment polarity lexicons starting from a small set of seeds: positive and negative terms (words and phrases). The dominant approach is that of Turney BIBREF12 , who uses pointwise mutual information and bootstrapping to build a large lexicon and to estimate the semantic orientation of each word in that lexicon. He starts with a small set of seed positive (e.g., excellent) and negative words (e.g., bad), and then uses these words to induce sentiment polarity orientation for new words in a large unannotated set of texts (in his case, product reviews). The idea is that words that co-occur in the same text with positive seed words are likely to be positive, while those that tend to co-occur with negative words are likely to be negative. To quantify this intuition, Turney defines the notion of sentiment orientation (SO) for a term INLINEFORM0 as follows:. INLINEFORM0 .where PMI is the pointwise mutual information, INLINEFORM0 and INLINEFORM1 are placeholders standing for any of the seed positive and negative terms, respectively, and INLINEFORM2 is a target word/phrase from the large unannotated set of texts (here tweets)..A positive/negative value for INLINEFORM0 indicates positive/negative polarity for the word INLINEFORM1 , and its magnitude shows the corresponding sentiment strength. In turn, INLINEFORM2 , where INLINEFORM3 is the probability to see INLINEFORM4 with any of the seed positive words in the same tweet, INLINEFORM5 is the probability to see INLINEFORM6 in any tweet, and INLINEFORM7 is the probability to see any of the seed positive words in a tweet; INLINEFORM8 is defined similarly..The pointwise mutual information is a notion from information theory: given two random variables INLINEFORM0 and INLINEFORM1 , the mutual information of INLINEFORM2 and INLINEFORM3 is the “amount of information” (in units such as bits) obtained about the random variable INLINEFORM4 , through the random variable INLINEFORM5 BIBREF64 ..Let INLINEFORM0 and INLINEFORM1 be two values from the sample space of INLINEFORM2 and INLINEFORM3 , respectively. The pointwise mutual information between INLINEFORM4 and INLINEFORM5 is defined as follows: DISPLAYFORM0 . INLINEFORM0 takes values between INLINEFORM1 , which happens when INLINEFORM2 = 0, and INLINEFORM3 if INLINEFORM4 ..In his experiments, Turney BIBREF12 used five positive and five negative words as seeds. His PMI-based approach further served as the basis for the creation of the two above-mentioned large-scale automatic lexicons for sentiment analysis in Twitter for English, initially developed by NRC for their participation in SemEval-2013 BIBREF25 . The Hashtag Sentiment Lexicon uses as seeds hashtags containing 32 positive and 36 negative words, e.g., #happy and #sad. Similarly, the Sentiment140 lexicon uses smileys as seed indicators for positive and negative sentiment, e.g., :), :-), and :)) as positive seeds, and :( and :-( as negative ones..An alternative approach to lexicon induction has been proposed BIBREF65 , which, instead of using PMI, assigns positive/negative labels to the unlabeled tweets (based on the seeds), and then trains an SVM classifier on them, using word INLINEFORM0 -grams as features. These INLINEFORM1 -grams are then used as lexicon entries (words and phrases) with the learned classifier weights as polarity scores. Finally, it has been shown that sizable further performance gains can be obtained by starting with mid-sized seeds, i.e., hundreds of words and phrases BIBREF66 . generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "9\n",
      "Question 1: What is microblog sentiment analysis and Twitter opinion mining?\n",
      "Answer 1: Microblog sentiment analysis and Twitter opinion mining refer to the use of natural language processing and machine learning algorithms to analyze and understand the sentiment behind short messages or posts on social media platforms, primarily Twitter. This analysis helps in understanding the opinions, emotions, and attitudes of people towards a particular topic or event.\n",
      "Question : for the text Microblog sentiment analysis; Twitter opinion mining generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "10\n",
      "What is aspect-based sentiment analysis? \n",
      "\n",
      "Aspect-based sentiment analysis is a task where the sentiment is not about the overall subject of a piece of text, but rather about specific aspects or features of the subject. For example, in a tweet about a phone, the sentiment may be focused on the size or camera quality of the phone, rather than the phone as a whole. It is an active research area that aims to better understand and analyze the nuances of sentiment expressed in text.\n",
      "Question : for the text Tweet-level sentiment. The simplest and also the most popular task of sentiment analysis on Twitter is to determine the overall sentiment expressed by the author of a tweet BIBREF30 , BIBREF28 , BIBREF26 , BIBREF29 , BIBREF27 . Typically, this means choosing one of the following three classes to describe the sentiment: Positive, Negative, and Neutral. Here are some examples:.Positive: @nokia lumia620 cute and small and pocket-size, and available in the brigh test colours of day! #lumiacaption.Negative: I hate tweeting on my iPhone 5 it's so small :(.Neutral: If you work as a security in a samsung store...Does that make you guardian of the galaxy??.Sentiment polarity lexicons. Naturally, the overall sentiment in a tweet can be determined based on the sentiment-bearing words and phrases it contains as well as based on emoticons such as ;) and:(. For this purpose, researchers have been using lexicons of sentiment-bearing words. For example, cute is a positive word, while hate is a negative one, and the occurrence of these words in (1) and (2) can help determine the overall polarity of the respective tweet. We will discuss these lexicons in more detail below..Prior sentiment polarity of multi-word phrases. Unfortunately, many sentiment-bearing words are not universally good or universally bad. For example, the polarity of an adjective could depend on the noun it modifies, e.g., hot coffee and unpredictable story express positive sentiment, while hot beer and unpredictable steering are negative. Thus, determining the out-of-context (a priori) strength of association of Twitter terms, especially multi-word terms, with positive/negative sentiment is an active research direction BIBREF28 , BIBREF29 ..Phrase-level polarity in context. Even when the target noun is the same, the polarity of the modifying adjective could be different in different tweets, e.g., small is positive in (1) but negative in (2), even though they both refer to a phone. Thus, there has been research in determining the sentiment polarity of a term in the context of a tweet BIBREF26 , BIBREF29 , BIBREF27 ..Sarcasm. Going back to tweet-level sentiment analysis, we should mention sarcastic tweets, which are particularly challenging as the sentiment they express is often the opposite of what the words they contain suggest BIBREF4 , BIBREF29 , BIBREF27 . For example, (4) and (5) express a negative sentiment even though they contain positive words and phrases such as thanks, love, and boosts my morale..Negative: Thanks manager for putting me on the schedule for Sunday.Negative: I just love missing my train every single day. Really boosts my morale..Sentiment toward a topic. Even though tweets are short, as they are limited to 140 characters by design (even though this was relaxed a bit as of September 19, 2016, and now media attachments such as images, videos, polls, etc., and quoted tweets no longer reduce the character count), they are still long enough to allow the tweet's author to mention several topics and to express potentially different sentiment toward each of them. A topic can be anything that people express opinions about, e.g., a product (e.g., iPhone6), a political candidate (e.g., Donald Trump), a policy (e.g., Obamacare), an event (e.g., Brexit), etc. For example, in (6) the author is positive about Donald Trump but negative about Hillary Clinton. A political analyzer would not be interested so much in the overall sentiment expressed in the tweet (even though one could argue that here it is positive overall), but rather in the sentiment with respect to a topic of his/her interest of study..As a democrat I couldnt ethically support Hillary no matter who was running against her. Just so glad that its Trump, just love the guy!.(topic: Hillary INLINEFORM0 Negative).(topic: Trump INLINEFORM0 Positive).Aspect-based sentiment analysis. Looking again at (1) and (2), we can say that the sentiment is not about the phone (lumia620 and iPhone 5, respectively), but rather about some specific aspect thereof, namely, size. Similarly, in (7) instead of sentiment toward the topic lasagna, we can see sentiment toward two aspects thereof: quality (Positive sentiment) and quantity (Negative sentiment). Aspect-based sentiment analysis is an active research area BIBREF32 , BIBREF33 , BIBREF21 ..The lasagna is delicious but do not come here on an empty stomach..Stance detection. A task related to, but arguably different in some respect from sentiment analysis, is that of stance detection. The goal here is to determine whether the author of a piece of text is in favor of, against, or neutral toward a proposition or a target BIBREF36 . For example, in (8) the author has a negative stance toward the proposition w​omen have the right to abortion, even though the target is not mentioned at all. Similarly, in (9§) the author expresses a negative sentiment toward Mitt Romney, from which one can imply that s/he has a positive stance toward the target ​Barack Obama..A​ foetus has rights too! Make your voice heard..(Target: w​omen have the right to abortion INLINEFORM0 Against).A​ll Mitt Romney cares about is making money for the rich..(Target: ​Barack Obama INLINEFORM0 InFavor).Ordinal regression. The above tasks were offered in different granularities, e.g., 2-way (Positive, Negative), 3-way (Positive, Neutral, Negative), 4-way (Positive, Neutral, Negative, Objective), 5-way (HighlyPositive, Positive, Neutral, Negative, HighlyNegative), and sometimes even 11-way BIBREF34 . It is important to note that the 5-way and the 11-way scales are ordinal, i.e., the classes can be associated with numbers, e.g., INLINEFORM0 2, INLINEFORM1 1, 0, 1, and 2 for the 5-point scale. This changes the machine learning task as not all mistakes are equal anymore BIBREF16 . For example, misclassifying a HighlyNegative example as HighlyPositive is a bigger mistake than misclassifying it as Negative or as Neutral. From a machine learning perspective, this means moving from classification to ordinal regression. This also requires different evaluation measures BIBREF30 ..Quantification. Practical applications are hardly ever interested in the sentiment expressed in a specific tweet. Rather, they look at estimating the prevalence of positive and negative tweets about a given topic in a set of tweets from some time interval. Most (if not all) tweet sentiment classification studies conducted within political science BIBREF39 , BIBREF40 , BIBREF41 , economics BIBREF42 , BIBREF7 , social science BIBREF43 , and market research BIBREF44 , BIBREF45 use Twitter with an interest in aggregate data and not in individual classifications. Thus, some tasks, such as SemEval-2016 Task 4 BIBREF30 , replace classification with class prevalence estimation, which is also known as quantification in data mining and related fields. Note that quantification is not a mere byproduct of classification, since a good classifier is not necessarily a good quantifier, and vice versa BIBREF46 . Finally, in case of multiple labels on an ordinal scale, we have yet another machine learning problem: ordinal quantification. Both versions of quantification require specific evaluation measures and machine learning algorithms. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "11\n",
      "Question 1: What was the purpose of the first round of annotation?\n",
      "Answer 1: The first round of annotation aimed to collect original and uncommon sentence change suggestions.\n",
      "Question : for the text We acquired the data in two rounds of annotation. In the first one, we were looking for original and uncommon sentence change suggestions. In the second one, we collected sentence alternations using ideas from the first round. The first and second rounds of annotation could be broadly called as collecting ideas and collecting data, respectively. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "12\n",
      "What was the most common type of modification made by annotators in the first round of annotation?\n",
      "\n",
      "The most common type of modification made by annotators in the first round of annotation was to transform the sentence into an interrogative/imperative sentence or to change the word order. However, examples provided to annotators in this round heavily influenced their decisions, resulting in almost two thirds of all modifications being based on those examples.\n",
      "Question : for the text We manually selected 15 newspaper headlines. Eleven annotators were asked to modify each headline up to 20 times and describe the modification with a short name. They were given an example sentence and several of its possible alternations, see tab:firstroundexamples..Unfortunately, these examples turned out to be highly influential on the annotators' decisions and they correspond to almost two thirds of all of modifications gathered in the first round. Other very common transformations include change of a word order or transformation into a interrogative/imperative sentence..Other interesting modification were also proposed such as change into a fairy-tale style, excessive use of diminutives/vulgarisms or dadaism—a swap of roles in the sentence so that the resulting sentence is grammatically correct but nonsensical in our world. Of these suggestions, we selected only the dadaistic swap of roles for the current exploration (see nonsense in Table TABREF7)..In total, we collected 984 sentences with 269 described unique changes. We use them as an inspiration for second round of annotation. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "13\n",
      "What were the criteria for selecting source sentences for annotations?\n",
      "\n",
      "The source sentences for annotations were selected based on two sources, Global Voices and OpenSubtitles, to have different styles of seed sentences. The sentences selected had to have more than 5 and less than 15 words, and were manually chosen to exclude sentences that were unreal, incomplete, vague or overly dependent on context. Sentences that could not be modified were marked as IMPOSSIBLE. Most sentences were assigned to several annotators to explore possible diversity in sentence paraphrasing.\n",
      "Question : for the text The source sentences for annotations were selected from Czech data of Global Voices BIBREF24 and OpenSubtitles BIBREF25. We used two sources in order to have different styles of seed sentences, both journalistic and common spoken language. We considered only sentences with more than 5 and less than 15 words and we manually selected 150 of them for further annotation. This step was necessary to remove sentences that are:.too unreal, out of this world, such as:.Jedno fotonový torpédo a je z tebe vesmírná topinka..“One photon torpedo and you're a space toast.”.photo captions (i.e. incomplete sentences), e.g.:.Zvláštní ekvádorský případ Correa vs. Crudo.“Specific Ecuadorian case Correa vs. Crudo”.too vague, overly dependent on the context:.Běž tam a mluv na ni..“Go there and speak to her.”.Many of the intended sentence transformations would be impossible to apply to such sentences and annotators' time would be wasted. Even after such filtering, it was still quite possible that a desired sentence modification could not be achieved for a sentence. For such a case, we gave the annotators the option to enter the keyword IMPOSSIBLE instead of the particular (impossible) modification..This option allowed to explicitly state that no such transformation is possible. At the same time most of the transformations are likely to lead to a large number possible outcomes. As documented in scratching2013, Czech sentence might have hundreds of thousand of paraphrases. To support some minimal exploration of this possible diversity, most of sentences were assigned to several annotators. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "14\n",
      "Question 1: How many modification types were selected for COSTRA 1.0 and why were two paraphrases requested for each sentence?\n",
      "\n",
      "Answer 1: 15 modification types were selected for COSTRA 1.0 and two paraphrases were requested for each sentence to determine if a good sentence embedding would put paraphrases close together in vector space.\n",
      "Question : for the text We selected 15 modifications types to collect COSTRA 1.0. They are presented in annotationinstructions..We asked for two distinct paraphrases of each sentence because we believe that a good sentence embedding should put paraphrases close together in vector space..Several modification types were specifically selected to constitute a thorough test of embeddings. In different meaning, the annotators should create a sentence with some other meaning using the same words as the original sentence. Other transformations which should be difficult for embeddings include minimal change, in which the sentence meaning should be significantly changed by using only very small modification, or nonsense, in which words of the source sentence should be shuffled so that it is grammatically correct, but without any sense. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "15\n",
      "Question 1: What measures were taken to reduce the impact of typos on the performance of embedding methods during the annotation process?\n",
      "\n",
      "Answer 1: The statistical spellchecker and grammar checker Korektor was applied to the collected sentence variations, and after manual inspection of 519 identified errors, 129 were fixed to minimize the influence of typos on the performance of embedding methods.\n",
      "Question : for the text The annotation is a challenging task and the annotators naturally make mistakes. Unfortunately, a single typo can significantly influence the resulting embedding BIBREF26. After collecting all the sentence variations, we applied the statistical spellchecker and grammar checker Korektor BIBREF27 in order to minimize influence of typos to performance of embedding methods. We manually inspected 519 errors identified by Korektor and fixed 129, which were identified correctly. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "16\n",
      "What are some methods of converting a sequence of words into a vector?\n",
      "Answer 1: Some methods include BiLSTM with max-pooling, masked language modeling, max-pooling last states of neural machine translation, and the encoder final state in attentionless neural machine translation.\n",
      "Question : for the text As hinted above, there are many methods of converting a sequence of words into a vector in a highly dimensional space. To name a few: BiLSTM with the max-pooling trained for natural language inference BIBREF13, masked language modeling and next sentence prediction using bidirectional Transformer BIBREF14, max-pooling last states of neural machine translation among many languages BIBREF15 or the encoder final state in attentionless neural machine translation BIBREF16..The most common way of evaluating methods of sentence embeddings is extrinsic, using so called `transfer tasks', i.e. comparing embeddings via the performance in downstream tasks such as paraphrasing, entailment, sentence sentiment analysis, natural language inference and other assignments. However, even simple bag-of-words (BOW) approaches achieve often competitive results on such tasks BIBREF17..Adi16 introduce intrinsic evaluation by measuring the ability of models to encode basic linguistic properties of a sentence such as its length, word order, and word occurrences. These so called `probing tasks' are further extended by a depth of the syntactic tree, top constituent or verb tense by DBLP:journals/corr/abs-1805-01070..Both transfer and probing tasks are integrated in SentEval BIBREF18 framework for sentence vector representations. Later, Perone2018 applied SentEval to eleven different encoding methods revealing that there is no consistently well performing method across all tasks. SentEval was further criticized for pitfalls such as comparing different embedding sizes or correlation between tasks BIBREF19, BIBREF20..shi-etal-2016-string show that NMT encoder is able to capture syntactic information about the source sentence. DBLP:journals/corr/BelinkovDDSG17 examine the ability of NMT to learn morphology through POS and morphological tagging..Still, very little is known about semantic properties of sentence embeddings. Interestingly, cifka:bojar:meanings:2018 observe that the better self-attention embeddings serve in NMT, the worse they perform in most of SentEval tasks..zhu-etal-2018-exploring generate automatically sentence variations such as:.Original sentence: A rooster pecked grain..Synonym Substitution: A cock pecked grain..Not-Negation: A rooster didn't peck grain..Quantifier-Negation: There was no rooster pecking grain..and compare their triplets by examining distances between their embeddings, i.e. distance between (1) and (2) should be smaller than distances between (1) and (3), (2) and (3), similarly, (3) and (4) should be closer together than (1)–(3) or (1)–(4)..In our previous study BIBREF21, we examined the effect of small sentence alternations in sentence vector spaces. We used sentence pairs automatically extracted from datasets for natural language inference BIBREF22, BIBREF23 and observed, that the simple vector difference, familiar from word embeddings, serves reasonably well also in sentence embedding spaces. The examined relations were however very simple: a change of gender, number, addition of an adjective, etc. The structure of the sentence and its wording remained almost identical..We would like to move to more interesting non-trivial sentence comparison, beyond those in zhu-etal-2018-exploring or BaBo2019, such as change of style of a sentence, the introduction of a small modification that drastically changes the meaning of a sentence or reshuffling of words in a sentence that alters its meaning..Unfortunately, such a dataset cannot be generated automatically and it is not available to our best knowledge. We try to start filling this gap with COSTRA 1.0. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "17\n",
      "Question 1: What is the purpose of the COSTRA 1.0 corpus?\n",
      "\n",
      "Answer 1: The purpose of the COSTRA 1.0 corpus is to analyze a wide spectrum sentence embeddings methods and see how well they reflect semantic relations between sentences in the corpus. It is also freely available for researchers to evaluate their sentence embeddings in terms of \"relatability.\"\n",
      "Question : for the text We presented COSTRA 1.0, a small corpus of complex transformations of Czech sentences..We plan to use this corpus to analyze a wide spectrum sentence embeddings methods to see to what extent the continuous space they induce reflects semantic relations between sentences in our corpus. The very first analysis using LASER embeddings indicates lack of “meaning relatability”, i.e. the ability to move along a trajectory in the space in order to reach desired sentence transformations. Actually, not even paraphrases are found in close neighbourhoods of embedded sentences. More “semantic” sentence embeddings methods are thus to be sought for..The corpus is freely available at the following link:.http://hdl.handle.net/11234/1-3123.Aside from extending the corpus in Czech and adding other language variants, we are also considering to wrap COSTRA 1.0 into an API such as SentEval, so that it is very easy for researchers to evaluate their sentence embeddings in terms of “relatability”. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "18\n",
      "What was the most repeated transformation among the annotators? \n",
      "\n",
      "The most repeated transformations were past, future, and ban.\n",
      "Question : for the text In the second round, we collected 293 annotations from 12 annotators. After Korektor, there are 4262 unique sentences (including 150 seed sentences) that form the COSTRA 1.0 dataset. Statistics of individual annotators are available in tab:statistics..The time needed to carry out one piece of annotation (i.e. to provide one seed sentence with all 15 transformations) was on average almost 20 minutes but some annotators easily needed even half an hour. Out of the 4262 distinct sentences, only 188 was recorded more than once. In other words, the chance of two annotators producing the same output string is quite low. The most repeated transformations are by far past, future and ban. The least repeated is paraphrase with only single one repeated..multiple-annots documents this in another way. The 293 annotations are split into groups depending on how many annotators saw the same input sentence: 30 annotations were annotated by one person only, 30 annotations by two different persons etc. The last column shows the number of unique outputs obtained in that group. Across all cases, 96.8% of produced strings were unique..In line with instructions, the annotators were using the IMPOSSIBLE option scarcely (95 times, i.e. only 2%). It was also a case of 7 annotators only; the remaining 5 annotators were capable of producing all requested transformations. The top three transformations considered unfeasible were different meaning (using the same set of words), past (esp. for sentences already in the past tense) and simple sentence. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "19\n",
      "What method was used to embed COSTRA sentences?\n",
      "LASER BIBREF15 was used to embed COSTRA sentences, the same method that performed well in revealing linear relations in BaBo2019.\n",
      "Question : for the text We embedded COSTRA sentences with LASER BIBREF15, the method that performed very well in revealing linear relations in BaBo2019. Having browsed a number of 2D visualizations (PCA and t-SNE) of the space, we have to conclude that visually, LASER space does not seem to exhibit any of the desired topological properties discussed above, see fig:pca for one example..The lack of semantic relations in the LASER space is also reflected in vector similarities, summarized in similarities. The minimal change operation substantially changed the meaning of the sentence, and yet the embedding of the transformation lies very closely to the original sentence (average similarity of 0.930). Tense changes and some form of negation or banning also keep the vectors very similar..The lowest average similarity was observed for generalization (0.739) and simplification (0.781), which is not any bad sign. However the fact that paraphrases have much smaller similarity (0.826) than opposite meaning (0.902) documents that the vector space lacks in terms of “relatability”. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "20\n",
      "What is the goal of the COSTRA dataset?\n",
      "The goal of the COSTRA dataset is to support studies of semantic and syntactic relations between sentences in the continuous space, aiming to explore sentence meaning relatability and search for an embedding method which exhibits meaningful operations or changes in the set of sentences of a language (or more languages at once). The dataset could serve for training or provide a test set, and could provide a “skeleton” to the continuous space of sentence embeddings.\n",
      "Question : for the text Vector representations are becoming truly essential in majority of natural language processing tasks. Word embeddings became widely popular with the introduction of word2vec BIBREF0 and GloVe BIBREF1 and their properties have been analyzed in length from various aspects..Studies of word embeddings range from word similarity BIBREF2, BIBREF3, over the ability to capture derivational relations BIBREF4, linear superposition of multiple senses BIBREF5, the ability to predict semantic hierarchies BIBREF6 or POS tags BIBREF7 up to data efficiency BIBREF8..Several studies BIBREF9, BIBREF10, BIBREF11, BIBREF12 show that word vector representations are capable of capturing meaningful syntactic and semantic regularities. These include, for example, male/female relation demonstrated by the pairs “man:woman”, “king:queen” and the country/capital relation (“Russia:Moscow”, “Japan:Tokyo”). These regularities correspond to simple arithmetic operations in the vector space..Sentence embeddings are becoming equally ubiquitous in NLP, with novel representations appearing almost every other week. With an overwhelming number of methods to compute sentence vector representations, the study of their general properties becomes difficult. Furthermore, it is not so clear in which way the embeddings should be evaluated..In an attempt to bring together more traditional representations of sentence meanings and the emerging vector representations, bojar:etal:jnle:representations:2019 introduce a number of aspects or desirable properties of sentence embeddings. One of them is denoted as “relatability”, which highlights the correspondence between meaningful differences between sentences and geometrical relations between their respective embeddings in the highly dimensional continuous vector space. If such a correspondence could be found, we could use geometrical operations in the space to induce meaningful changes in sentences..In this work, we present COSTRA, a new dataset of COmplex Sentence TRAnsformations. In its first version, the dataset is limited to sample sentences in Czech. The goal is to support studies of semantic and syntactic relations between sentences in the continuous space. Our dataset is the prerequisite for one of possible ways of exploring sentence meaning relatability: we envision that the continuous space of sentences induced by an ideal embedding method would exhibit topological similarity to the graph of sentence variations. For instance, one could argue that a subset of sentences could be organized along a linear scale reflecting the formalness of the language used. Another set of sentences could form a partially ordered set of gradually less and less concrete statements. And yet another set, intersecting both of the previous ones in multiple sentences could be partially or linearly ordered according to the strength of the speakers confidence in the claim..Our long term goal is to search for an embedding method which exhibits this behaviour, i.e. that the topological map of the embedding space corresponds to meaningful operations or changes in the set of sentences of a language (or more languages at once). We prefer this behaviour to emerge, as it happened for word vector operations, but regardless if the behaviour is emergent or trained, we need a dataset of sentences illustrating these patterns. If large enough, such a dataset could serve for training. If it will be smaller, it will provide a test set. In either case, these sentences could provide a “skeleton” to the continuous space of sentence embeddings..The paper is structured as follows: related summarizes existing methods of sentence embeddings evaluation and related work. annotation describes our methodology for constructing our dataset. data details the obtained dataset and some first observations. We conclude and provide the link to the dataset in conclusion generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "21\n",
      "Question 1: Who is being thanked for proofreading and providing advice?\n",
      "Answer 1: Michaela Benk is being thanked for proofreading and providing helpful advice.\n",
      "Question : for the text We thank Michaela Benk for proofreading and helpful advice. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "22\n",
      "Question 1: What is SemSentSum and what kind of results does it achieve?\n",
      "\n",
      "Answer 1: SemSentSum is a fully data-driven model that combines universal embeddings and domain-specific embeddings using a graph convolutional network to achieve competitive results in multi-document summarization without the need for hand-crafted features or additional annotated data. It performs well on both short and long summaries (665 bytes and 100 words).\n",
      "Question : for the text In this work, we propose a method to combine two types of sentence embeddings : 1) universal embeddings, pre-trained on a large corpus such as Wikipedia and incorporating general semantic structures across sentences and 2) domain-specific embeddings, learned during training. We merge them together by using a graph convolutional network that eliminates the need of hand-crafted features or additional annotated data..We introduce a fully data-driven model SemSentSum that achieves competitive results for multi-document summarization on both kind of summary length (665 bytes and 100 words summaries), without requiring hand-crafted features or additional annotated data..As SemSentSum is domain-independent, we believe that our sentence semantic relation graph and model can be used for other tasks including detecting information cascades, query-focused summarization, keyphrase extraction and information retrieval. In addition, we plan to leave the weights of the sentence semantic relation graph dynamic during training, and to integrate an attention mechanism directly into the graph. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "23\n",
      "What are the two main components of SemSentSum?\n",
      "The two main components of SemSentSum are the sentence encoder (Sent) and the graph convolutional neural network (GCN).\n",
      "Question : for the text In order to quantify the contribution of the different components of SemSentSum, we try variations of our model by removing different modules one at a time. Our two main elements are the sentence encoder (Sent) and the graph convolutional neural network (GCN). When we omit Sent, we substitute it with the pre-trained sentence embeddings used to build our sentence semantic relation graph. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "24\n",
      "Question 1: What datasets were used for the experiments on multi-document summarization?\n",
      "\n",
      "Answer 1: The experiments were conducted on the most commonly used datasets from the Document Understanding Conferences (DUC), specifically DUC 2001, 2002, 2003 and 2004.\n",
      "Question : for the text We conduct experiments on the most commonly used datasets for multi-document summarization from the Document Understanding Conferences (DUC). We use DUC 2001, 2002, 2003 and 2004 as the tasks of generic multi-document summarization, because they have been carried out during these years. We use DUC 2001, 2002, 2003 and 2004 for generic multi-document summarization, where DUC 2001/2002 are used for training, DUC 2003 for validation and finally, DUC 2004 for testing, following the common practice. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "25\n",
      "What is the main metric used for comparison between produced summaries and golden ones? \n",
      "\n",
      "Answer 1: The main metrics used for comparison between produced summaries and golden ones are ROUGE-1 and ROUGE-2 recall scores as proposed by BIBREF11.\n",
      "Question : for the text For the evaluation, we use ROUGE BIBREF10 with the official parameters of the DUC tasks and also truncate the summaries to 100 words for DUC 2001/2002/2003 and to 665 bytes for DUC 2004. Notably, we take ROUGE-1 and ROUGE-2 recall scores as the main metrics for comparison between produced summaries and golden ones as proposed by BIBREF11. The goal of the ROUGE-N metric is to compute the ratio of the number of N-grams from the generated summary matching these of the human reference summaries. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "26\n",
      "Question 1: What is the similarity threshold used in the summary generation process?\n",
      "\n",
      "Answer 1: The similarity threshold used in the summary generation process is $0.8$ which was tuned on the validation set.\n",
      "Question : for the text To define the edge weights of our sentence semantic relation graph, we employ the 600-dimensional pre-trained unigram model of BIBREF6, using English Wikipedia as source corpus. We keep only edges having a weight larger than $t_{sim}^g = 0.5$ (tuned on the validation set). For word embeddings, the 300-dimensional pre-trained GloVe embeddings BIBREF12 are used and fixed during training. The output dimension of the sentence embeddings produced by the sentence encoder is the same as that of the word embeddings, i.e. 300. For the graph convolutional network, the number of hidden units is 128 and the size of the generated hidden feature vectors is also 300. We use a batch size of 1, a learning rate of $0.0075$ using Adam optimizer BIBREF13 with $\\beta _1=0.9, \\beta _2=0.999$ and $\\epsilon =10^{-8}$. In order to make SemSentSum generalize better, we use dropout BIBREF14 of $0.2$, batch normalization BIBREF15, clip the gradient norm at $1.0$ if higher, add L2-norm regularizer with a regularization factor of $10^{-12}$ and train using early stopping with a patience of 10 iterations. Finally, the similarity threshold $t_{sim}^s$ in the summary generation process is $0.8$ (tuned on the validation set). generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "27\n",
      "What are the three dimensions used to evaluate SemSentSum?\n",
      "\n",
      "The three dimensions used to evaluate SemSentSum are 1) summarization performance, 2) impact of sentence semantic relation graph generation using various methods and different thresholds, and 3) an ablation study to analyze the importance of each component of SemSentSum.\n",
      "Question : for the text Three dimensions are used to evaluate our model SemSentSum : 1) the summarization performance, to assess its capability 2) the impact of the sentence semantic relation graph generation using various methods and different thresholds $t_{sim}^g$ 3) an ablation study to analyze the importance of each component of SemSentSum. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "28\n",
      "What is the importance of the sentence semantic relation graph in SemSentSum?\n",
      "The sentence semantic relation graph is important in SemSentSum as it helps to capture semantic attributes well and analyze the semantic structures to capture sentence similarities. Additionally, the fine-tuned sentence embeddings obtained via the encoder boosts the performance, making these methods complementary.\n",
      "Question : for the text We quantify the contribution of each module of SemSentSum in Table TABREF36 for 665 bytes summaries (we obtain similar results for 100 words). Removing the sentence encoder produces slightly lower results. This shows that the sentence semantic relation graph captures semantic attributes well, while the fine-tuned sentence embeddings obtained via the encoder help boost the performance, making these methods complementary. By disabling only the graph convolutional layer, a drastic drop in terms of performance is observed, which emphasizes that the relationship among sentences is indeed important and not present in the data itself. Therefore, our sentence semantic relation graph is able to capture sentence similarities by analyzing the semantic structures. Interestingly, if we remove the sentence encoder in addition to the graph convolutional layer, similar results are achieved. This confirms that alone, the sentence encoder is not able to compute an efficient representation of sentences for the task of multi-document summarization, probably due to the poor size of the DUC datasets. Finally, we can observe that the use of sentence embeddings only results in similar performance to the baselines, which rely on sentence or document embeddings BIBREF18, BIBREF19. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "29\n",
      "What is the reason for the inverted U-shaped curve in the ROUGE scores for cosine similarity? \n",
      "\n",
      "The reason for the inverted U-shaped curve in the ROUGE scores for cosine similarity is because its distribution is similar to a normal distribution as it relies on the semantics of sentences instead of individual words, while the other methods such as PADG, Textrank, and Tf-idf are more skewed towards zero.\n",
      "Question : for the text Table TABREF34 shows the results of different methods to create the sentence semantic relation graph with various thresholds $t_{sim}^g$ for 665 bytes summaries (we obtain similar results for 100 words). A first observation is that using cosine similarity with sentence embeddings significantly outperforms all other methods for ROUGE-1 and ROUGE-2 scores, mainly because it relies on the semantic of sentences instead of their individual words. A second is that different methods evolve similarly : PADG, Textrank, Tf-idf behave similarly to an U-shaped curve for both ROUGE scores while Cosine is the only one having an inverted U-shaped curve. The reason for this behavior is a consequence of its distribution being similar to a normal distribution because it relies on the semantic instead of words, while the others are more skewed towards zero. This confirms our hypothesis that 1) having a complete graph does not allow the model to leverage much the semantic 2) a sparse graph might not contain enough information to exploit similarities. Finally, Lexrank and ADG have different trends between both ROUGE scores. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "30\n",
      "What type of models did SemSentSum outperform in the category relying on sentence or document embeddings? \n",
      "\n",
      "SemSentSum outperformed MMR, PV-DBOW+BS, and PG-MMR in the category relying on sentence or document embeddings.\n",
      "Question : for the text We compare the results of SemSentSum for both settings : 665 bytes and 100 words summaries. We only include models using the same parameters to compute the ROUGE-1/ROUGE-2 score and recall as metrics..The results for 665 bytes summaries are reported in Table TABREF28. We compare SemSentSum with three types of model relying on either 1) sentence or document embeddings 2) various hand-crafted features or 3) additional data..For the first category, we significantly outperform MMR BIBREF18, PV-DBOW+BS BIBREF19 and PG-MMR BIBREF20. Although their methods are based on embeddings to represent the meaning, it shows that using only various distance metrics or encoder-decoder architecture on these is not efficient for the task of multi-document summarization (as also shown in the Ablation Study). We hypothesize that SemSentSum performs better by leveraging pre-trained sentence embeddings and hence lowering the effects of data scarcity..Systems based on hand-crafted features include a widely-used learning-based summarization method, built on support vector regression SVR BIBREF21 ; a graph-based method based on approximating discourse graph G-Flow BIBREF2 ; Peer 65 which is the best peer systems participating in DUC evaluations ; and the recursive neural network R2N2 of BIBREF1 that learns automatically combinations of hand-crafted features. As can be seen, among these models completely dependent on hand-crafted features, SemSentSum achieves highest performance on both ROUGE scores. This denotes that using different linguistic and word-based features might not be enough to capture the semantic structures, in addition to being cumbersome to craft..The last type of model is shown in TCSum BIBREF4 and uses transfer learning from a text classifier model, based on a domain-related dataset of $30\\,000$ documents from New York Times (sharing the same topics of the DUC datasets). In terms of ROUGE-1, SemSentSum significantly outperforms TCSum and performs similarly on ROUGE-2 score. This demonstrates that collecting more manually annotated data and training two models is unnecessary, in addition to being difficult to use in other domains, whereas SemSentSum is fully data driven, domain-independent and usable in realistic scenarios..Table TABREF32 depicts models producing 100 words summaries, all depending on hand-crafted features. We use as baselines FreqSum BIBREF22 ; TsSum BIBREF23 ; traditional graph-based approaches such as Cont. LexRank BIBREF9 ; Centroid BIBREF24 ; CLASSY04 BIBREF25 ; its improved version CLASSY11 BIBREF26 and the greedy model GreedyKL BIBREF27. All of these models are significantly underperforming compared to SemSentSum. In addition, we include state-of-the-art models : RegSum BIBREF0 and GCN+PADG BIBREF3. We outperform both in terms of ROUGE-1. For ROUGE-2 scores we achieve better results than GCN+PADG but without any use of domain-specific hand-crafted features and a much smaller and simpler model. Finally, RegSum achieves a similar ROUGE-2 score but computes sentence saliences based on word scores, incorporating a rich set of word-level and domain-specific features. Nonetheless, our model is competitive and does not depend on hand-crafted features due to its full data-driven nature and thus, it is not limited to a single domain..Consequently, the experiments show that achieving good performance for multi-document summarization without hand-crafted features or additional data is clearly feasible and SemSentSum produces competitive results without depending on these, is domain independent, fast to train and thus usable in real scenarios. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "31\n",
      "Question 1: What is the TextRank method for building a sentence semantic relation graph?\n",
      "\n",
      "Answer 1: The TextRank method is a weighted graph created where nodes are sentences and edges defined by a similarity measure based on word overlap. An algorithm similar to PageRank is used to compute sentence importance and refine edge weights.\n",
      "Question : for the text We investigate different methods to build our sentence semantic relation graph and vary the value of $t_{sim}^g$ from $0.0$ to $0.75$ to study the impact of the threshold cut-off. Among these are :.Cosine : Using cosine similarity ;.Tf-idf : Considering a node as the query and another as document. The weight corresponds to the cosine similarity between the query and the document ;.TextRank BIBREF16 : A weighted graph is created where nodes are sentences and edges defined by a similarity measure based on word overlap. Afterward, an algorithm similar to PageRank BIBREF17 is used to compute sentence importance and refined edge weights ;.LexRank BIBREF9 : An unsupervised multi-document summarizer based on the concept of eigenvector centrality in a graph of sentences to set up the edge weights ;.Approximate Discourse Graph (ADG) BIBREF2 : Approximation of a discourse graph where nodes are sentences and edges $(S_u,S_v)$ indicates sentence $S_v$ can be placed after $S_u$ in a coherent summary ;.Personalized ADG (PADG) BIBREF3 : Normalized version of ADG where sentence nodes are normalized over all edges. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "32\n",
      "Question 1: What is the official summary length in the DUC competition and how does SemSentSum compare to other models in this aspect?\n",
      "\n",
      "Answer 1: The official summary length in the DUC competition is 665 bytes. SemSentSum was trained on DUC 2001/2002, tuned on DUC 2003, and assessed on DUC 2004 with summaries truncated to 665 bytes to fairly compare it with other models in the literature. Additionally, SemSentSum was also tested with a length constraint of 100 words, which to the author's knowledge, makes it the first model to conduct experiments on both summary lengths and compare its performance with other models that produce either 100 words or 665 bytes summaries.\n",
      "Question : for the text We train our model SemSentSum on DUC 2001/2002, tune it on DUC 2003 and assess the performance on DUC 2004. In order to fairly compare SemSentSum with other models available in the literature, experiments are conducted with summaries truncated to 665 bytes (official summary length in the DUC competition), but also with summaries with a length constraint of 100 words. To the best of our knowledge, we are the first to conduct experiments on both summary lengths and compare our model with other systems producing either 100 words or 665 bytes summaries. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "33\n",
      "What are the two types of sentence embeddings used in SemSentSum and how are they complementary to each other?\n",
      "\n",
      "SemSentSum utilizes two types of sentence embeddings: general embeddings pre-trained on a large corpus and domain-specific embeddings learned during training. The general embeddings capture common semantic structures from a variety of texts, while the domain-specific embeddings capture aspects related to the domain of the input documents. These two types of embeddings are complementary, as they share different properties and enable the system to be domain-independent while also being able to capture domain-specific information.\n",
      "Question : for the text Today's increasing flood of information on the web creates a need for automated multi-document summarization systems that produce high quality summaries. However, producing summaries in a multi-document setting is difficult, as the language used to display the same information in a sentence can vary significantly, making it difficult for summarization models to capture. Given the complexity of the task and the lack of datasets, most researchers use extractive summarization, where the final summary is composed of existing sentences in the input documents. More specifically, extractive summarization systems output summaries in two steps : via sentence ranking, where an importance score is assigned to each sentence, and via the subsequent sentence selection, where the most appropriate sentence is chosen, by considering 1) their importance and 2) their frequency among all documents. Due to data sparcity, models heavily rely on well-designed features at the word level BIBREF0, BIBREF1, BIBREF2, BIBREF3 or take advantage of other large, manually annotated datasets and then apply transfer learning BIBREF4. Additionally, most of the time, all sentences in the same collection of documents are processed independently and therefore, their relationships are lost..In realistic scenarios, features are hard to craft, gathering additional annotated data is costly, and the large variety in expressing the same fact cannot be handled by the use of word-based features only, as is often the case. In this paper, we address these obstacles by proposing to simultaneously leverage two types of sentence embeddings, namely embeddings pre-trained on a large corpus that capture a variety of meanings and domain-specific embeddings learned during training. The former is typically trained on an unrelated corpus composed of high quality texts, allowing to cover additional contexts for each encountered word and sentence. Hereby, we build on the assumption that sentence embeddings capture both the syntactic and semantic content of sentences. We hypothesize that using two types of sentence embeddings, general and domain-specific, is beneficial for the task of multi-document summarization, as the former captures the most common semantic structures from a large, general corpus, while the latter captures the aspects related to the domain..We present SemSentSum (Figure FIGREF3), a fully data-driven summarization system, which does not depend on hand-crafted features, nor additional data, and is thus domain-independent. It first makes use of general sentence embedding knowledge to build a sentenc semantic relation graph that captures sentence similarities (Section SECREF4). In a second step, it trains genre-specific sentence embeddings related to the domains of the collection of documents, by utilizing a sentence encoder (Section SECREF5). Both representations are afterwards merged, by using a graph convolutional network BIBREF5 (Section SECREF6). Then, it employs a linear layer to project high-level hidden features for individual sentences to salience scores (Section SECREF8). Finally, it greedily produces relevant and non-redundant summaries by using sentence embeddings to detect similarities between candidate sentences and the current summary (Section SECREF11)..The main contributions of this work are as follows :.We aggregate two types of sentences embeddings using a graph representation. They share different properties and are consequently complementary. The first one is trained on a large unrelated corpus to model general semantics among sentences, whereas the second is domain-specific to the dataset and learned during training. Together, they enable a model to be domain-independent as it can be applied easily on other domains. Moreover, it could be used for other tasks including detecting information cascades, query-focused summarization, keyphrase extraction and information retrieval..We devise a competitive multi-document summarization system, which does not need hand-crafted features nor additional annotated data. Moreover, the results are competitive for 665-byte and 100-word summaries. Usually, models are compared in one of the two settings but not both and thus lack comparability. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "34\n",
      "Question 1: What is the goal of SemSentSum in relation to the input documents?\n",
      "\n",
      "Answer 1: The goal of SemSentSum is to produce a summary using a subset of related documents in the input, ordered in a specific way. The summary is created by selecting a subset of sentences from the documents, which are assigned salience scores based on semantic relation graphs and recurrent neural networks. A greedy method is then applied to select the most important sentences for the summary until reaching the length limit.\n",
      "Question : for the text Let $C$ denote a collection of related documents composed of a set of documents $\\lbrace D_i|i \\in [1,N]\\rbrace $ where $N$ is the number of documents. Moreover, each document $D_i$ consists of a set of sentences $\\lbrace S_{i,j}|j \\in [1,M]\\rbrace $, $M$ being the number of sentences in $D_i$. Given a collection of related documents $C$, our goal is to produce a summary $Sum$ using a subset of these in the input documents ordered in some way, such that $Sum = (S_{i_1,j_1},S_{i_2,j_2},...,S_{i_n,j_m})$..In this section, we describe how SemSentSum estimates the salience score of each sentence and how it selects a subset of these to create the final summary. The architecture of SemSentSum is depicted in Figure FIGREF3..In order to perform sentence selection, we first build our sentence semantic relation graph, where each vertex is a sentence and edges capture the semantic similarity among them. At the same time, each sentence is fed into a recurrent neural network, as a sentence encoder, to generate sentence embeddings using the last hidden states. A single-layer graph convolutional neural network is then applied on top, where the sentence semantic relation graph is the adjacency matrix and the sentence embeddings are the node features. Afterward, a linear layer is used to project high-level hidden features for individual sentences to salience scores, representing how salient a sentence is with respect to the final summary. Finally, based on this, we devise an innovative greedy method that leverages sentence embeddings to detect redundant sentences and select sentences until reaching the summary length limit. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "35\n",
      "Question 1: What is the purpose of running a graph convolution on the first order neighborhood in the sentence semantic relation graph?\n",
      "\n",
      "Answer 1: The purpose of running a graph convolution on the first order neighborhood in the sentence semantic relation graph is to capture high-level hidden features for each sentence, encapsulating both sentence information and the graph structure. It allows us to leverage the information present in the sentence semantic relation graph, which is not present in the data via universal embeddings.\n",
      "Question : for the text After having computed all sentence embeddings and the sentence semantic relation graph, we apply a single-layer Graph Convolutional Network (GCN) from BIBREF5, in order to capture high-level hidden features for each sentence, encapsulating sentence information as well as the graph structure..We believe that our sentence semantic relation graph contains information not present in the data (via universal embeddings) and thus, we leverage this information by running a graph convolution on the first order neighborhood..The GCN model takes as input the node features matrix $X$ and a squared adjacency matrix $A$. The former contains all sentence embeddings of the collection of documents, while the latter is our underlying sentence semantic relation graph. It outputs hidden representations for each node that encode both local graph structure and nodes's features. In order to take into account the sentences themselves during the information propagation, we add self-connections (i.e. the identity matrix) to $A$ such that $\\tilde{A} = A + I$..Subsequently, we obtain our sentence hidden features by using Equation DISPLAY_FORM7..where $W_i$ is the weight matrix of the $i$'th graph convolution layer and $b_i$ the bias vector. We choose the Exponential Linear Unit (ELU) activation function from BIBREF8 due to its ability to handle the vanishing gradient problem, by pushing the mean unit activations close to zero and consequently facilitating the backpropagation. By using only one hidden layer, as we only have one input-to-hidden layer and one hidden-to-output layer, we limit the information propagation to the first order neighborhood. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "36\n",
      "Question 1: How do we estimate a salience score for each sentence?\n",
      "Answer 1: We use a simple linear layer to estimate a salience score for each sentence and then normalize the scores via softmax to obtain our estimated salience score $S^s_{i,j}$.\n",
      "Question : for the text We use a simple linear layer to estimate a salience score for each sentence and then normalize the scores via softmax and obtain our estimated salience score $S^s_{i,j}$. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "37\n",
      "Question 1: What type of neural network is used for the sentence encoder in this study?\n",
      "\n",
      "Answer 1: The sentence encoder used in this study is a single-layer forward recurrent neural network with Long Short-Term Memory (LSTM).\n",
      "Question : for the text Given a list of documents $C$, we encode each document's sentence $S_{i,j}$, where each has at most $L$ words $(w_{i,j,1}, w_{i,j,2}, ..., w_{i,j,L})$. In our experiments, all words are kept and converted into word embeddings, which are then fed to the sentence encoder in order to compute specialized sentence embeddings $S^{\\prime }_{i,j}$. We employ a single-layer forward recurrent neural network, using Long Short-Term Memory (LSTM) of BIBREF7 as sentence encoder, where the sentence embeddings are extracted from the last hidden states. We then concatenate all sentence embeddings into a matrix $X$ which constitutes the input node features that will be used by the graph convolutional network. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "38\n",
      "Question 1: What is the purpose of the edge-removal-method mentioned in the text?\n",
      "\n",
      "Answer 1: The edge-removal-method is used to remove edges below a certain threshold of similarity in order to emphasize high sentence similarity and overcome the problem of having a complete graph without exploiting semantic structure.\n",
      "Question : for the text We model the semantic relationship among sentences using a graph representation. In this graph, each vertex is a sentence $S_{i,j}$ ($j$'th sentence of document $D_i$) from the collection documents $C$ and an undirected edge between $S_{i_u,j_u}$ and $S_{i_v,j_v}$ indicates their degree of similarity. In order to compute the semantic similarity, we use the model of BIBREF6 trained on the English Wikipedia corpus. In this manner, we incorporate general knowledge (i.e. not domain-specific) that will complete the specialized sentence embeddings obtained during training (see Section SECREF5). We process sentences by their model and compute the cosine similarity between every sentence pair, resulting in a complete graph. However, having a complete graph alone does not allow the model to leverage the semantic structure across sentences significantly, as every sentence pair is connected, and likewise, a sparse graph does not contain enough information to exploit semantic similarities. Furthermore, all edges have a weight above zero, since it is very unlikely that two sentence embeddings are completely orthogonal. To overcome this problem, we introduce an edge-removal-method, where every edge below a certain threshold $t_{sim}^g$ is removed in order to emphasize high sentence similarity. Nonetheless, $t_{sim}^g$ should not be too large, as we otherwise found the model to be prone to overfitting. After removing edges below $t_{sim}^g$, our sentence semantic relation graph is used as the adjacency matrix $A$. The impact of $t_{sim}^g$ with different values is shown in Section SECREF26..Based on our aforementioned hypothesis that a combination of general and genre-specific sentence embeddings is beneficial for the task of multi-document summarization, we further incorporate general sentence embeddings, pre-trained on Wikipedia entries, into edges between sentences. Additionally, we compute specialised sentence embeddings, which are related to the domains of the documents (see Section SECREF35)..Note that 1) the pre-trained sentence embeddings are only used to compute the weights of the edges and are not used by the summarization model (as others are produced by the sentence encoder) and 2) the edge weights are static and do not change during training. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "39\n",
      "Question 1: How does SemSentSum construct an informative and non-redundant summary?\n",
      "\n",
      "Answer 1: SemSentSum uses a greedy strategy to construct a summary by discarding sentences with less than 9 words and then sorting them based on their estimated salience scores in descending order. It iteratively dequeues the sentence with the highest score and appends it to the current summary if it is non-redundant. Similarity between a candidate sentence and the current summary is determined using cosine similarity between their sentence embeddings, with a certain threshold for dissimilarity. The approach focuses on semantic sentence structures and similarity between sentence meanings rather than just word similarities.\n",
      "Question : for the text While our model SemSentSum provides estimated saliency scores, we use a greedy strategy to construct an informative and non-redundant summary $Sum$. We first discard sentences having less than 9 words, as in BIBREF9, and then sort them in descending order of their estimated salience scores. We iteratively dequeue the sentence having the highest score and append it to the current summary $Sum$ if it is non-redundant with respect to the current content of $Sum$. We iterate until reaching the summary length limit..To determine the similarity of a candidate sentence with the current summary, a sentence is considered as dissimilar if and only if the cosine similarity between its sentence embeddings and the embeddings of the current summary is below a certain threshold $t_{sim}^s$. We use the pre-trained model of BIBREF6 to compute sentence as well as summary embeddings, similarly to the sentence semantic relation graph construction. Our approach is novel, since it focuses on the semantic sentence structures and captures similarity between sentence meanings, instead of focusing on word similarities only, like previous TF-IDF approaches ( BIBREF0, BIBREF1, BIBREF3, BIBREF4). generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "40\n",
      "Question 1: What is the loss function used in training SemSentSum model?\n",
      "\n",
      "Answer 1: SemSentSum model is trained using cross-entropy loss function which minimizes the difference between the predicted salience score and the ROUGE-1 F1 score for each sentence.\n",
      "Question : for the text Our model SemSentSum is trained in an end-to-end manner and minimizes the cross-entropy loss of Equation DISPLAY_FORM10 between the salience score prediction and the ROUGE-1 $F_1$ score for each sentence..$F_1(S)$ is computed as the ROUGE-1 $F_1$ score, unlike the common practice in the area of single and multi-document summarization as recall favors longer sentences whereas $F_1$ prevents this tendency. The scores are normalized via softmax. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "41\n",
      "What is the main difference between SemSentSum and the approach taken in BIBREF3? \n",
      "\n",
      "The main difference between SemSentSum and the approach taken in BIBREF3 is that SemSentSum builds the sentence semantic relation graph using pre-trained sentence embeddings with cosine similarity, while BIBREF3 creates a normalized version of the approximate discourse graph based on hand-crafted features.\n",
      "Question : for the text The idea of using multiple embeddings has been employed at the word level. BIBREF28 use an attention mechanism to combine the embeddings for each word for the task of natural language inference. BIBREF29, BIBREF30 concatenate the embeddings of each word into a vector before feeding a neural network for the tasks of aspect extraction and sentiment analysis. To our knowledge, we are the first to combine multiple types of sentence embeddings..Extractive multi-document summarization has been addressed by a large range of approaches. Several of them employ graph-based methods. BIBREF31 introduced a cross-document structure theory, as a basis for multi-document summarization. BIBREF9 proposed LexRank, an unsupervised multi-document summarizer based on the concept of eigenvector centrality in a graph of sentences. Other works exploit shallow or deep features from the graph's topology BIBREF32, BIBREF33. BIBREF34 pairs graph-based methods (e.g. random walk) with clustering. BIBREF35 improved results by using a reinforced random walk model to rank sentences and keep non-redundant ones. The system by BIBREF2 does sentence selection, while balancing coherence and salience and by building a graph that approximates discourse relations across sentences BIBREF36..Besides graph-based methods, other viable approaches include Maximum Marginal Relevance BIBREF37, which uses a greedy approach to select sentences and considers the tradeoff between relevance and redundancy ; support vector regression BIBREF21 ; conditional random field BIBREF38 ; or hidden markov model BIBREF25. Yet other approaches rely on n-grams regression as in BIBREF39. More recently, BIBREF1 built a recursive neural network, which tries to automatically detect combination of hand-crafted features. BIBREF4 employ a neural model for text classification on a large manually annotated dataset and apply transfer learning for multi-document summarization afterward..The work most closely related to ours is BIBREF3. They create a normalized version of the approximate discourse graph BIBREF2, based on hand-crafted features, where sentence nodes are normalized over all the incoming edges. They then employ a deep neural network, composed of a sentence encoder, three graph convolutional layers, one document encoder and an attention mechanism. Afterward, they greedily select sentences using TF-IDF similarity to detect redundant sentences. Our model differs in four ways : 1) we build our sentence semantic relation graph by using pre-trained sentence embeddings with cosine similarity, where neither heavy preprocessing, nor hand-crafted features are necessary. Thus, our model is fully data-driven and domain-independent unlike other systems. In addition, the sentence semantic relation graph could be used for other tasks than multi-document summarization, such as detecting information cascades, query-focused summarization, keyphrase extraction or information retrieval, as it is not composed of hand-crafted features. 2) SemSentSum is much smaller and consequently has fewer parameters as it only uses a sentence encoder and a single convolutional layer. 3) The loss function is based on ROUGE-1 $F_1$ score instead of recall to prevent the tendency of choosing longer sentences. 4) Our method for summary generation is also different and novel as we leverage sentence embeddings to compute the similarity between a candidate sentence and the current summary instead of TF-IDF based approaches. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "42\n",
      "What is the filter window size used for the second wide convolution layer in all datasets?\n",
      "\n",
      "Answer 1: The filter window size used for the second wide convolution layer in all datasets is 5 with 14 feature maps each.\n",
      "Question : for the text For the Stanford Twitter Sentiment Corpus, we use the number of samples as BIBREF5 . The training data is selected 80K tweets for a training data and 16K tweets for the development set randomly from the training data of BIBREF0 . We conduct a binary prediction for STS Corpus..For Sander dataset, we use standard 10-fold cross validation as BIBREF14 . We construct the development set by selecting 10% randomly from 9-fold training data..In Health Care Reform Corpus, we also select 10% randomly for the development set in a training set and construct as BIBREF14 for comparison. We describe the summary of datasets in Table III..for all datasets, the filter window size ( INLINEFORM0 ) is 7 with 6 feature maps each for the first wide convolution layer, the second wide convolution layer has a filter window size of 5 with 14 feature maps each. Dropout rate ( INLINEFORM1 ) is 0.5, INLINEFORM2 constraint, learning rate is 0.1 and momentum of 0.9. Mini-batch size for STS Corpus is 100 and others are 4. In addition, training is done through stochastic gradient descent over shuffled mini-batches with Adadelta update rule BIBREF19 ..we use the publicly available Word2Vec trained from 100 billion words from Google and TwitterGlove of Stanford is performed on aggregated global word-word co-occurrence statistics from a corpus. Word2Vec has dimensionality of 300 and Twitter Glove have dimensionality of 200. Words that do not present in the set of pre-train words are initialized randomly. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "43\n",
      "Question 1: What is the advantage of using Semantic Rules in the model?\n",
      "\n",
      "Answer 1: Semantic Rules are effective in increasing classification accuracy, as seen in the evaluation of the model in Table V of the full paper. The use of SR allows the model to capture important features and combine them for maximum benefit.\n",
      "Question : for the text As can be seen, the models with SR outperforms the model with no SR. Semantic rules is effective in order to increase classification accuracy. We evaluate the efficiency of SR for the model in Table V of our full paper . We also conduct two experiments on two separate models: DeepCNN and Bi-LSTM in order to show the effectiveness of combination of DeepCNN and Bi-LSTM. In addition, the model using TwitterGlove outperform the model using GoogleW2V because TwitterGlove captures more information in Twitter than GoogleW2V. These results show that the character-level information and SR have a great impact on Twitter Data. The pre-train word vectors are good, universal feature extractors. The difference between our model and other approaches is the ability of our model to capture important features by using SR and combine these features at high benefit. The use of DeepCNN can learn a representation of words in higher abstract level. The combination of global character fixed-sized feature vectors and a word embedding helps the model to find important detectors for particles such as 'not' that negate sentiment and potentiate sentiment such as 'too', 'so' standing beside expected features. The model not only learns to recognize single n-grams, but also patterns in n-grams lead to form a structure significance of a sentence. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "44\n",
      "What is the proposed model composed of? \n",
      "The proposed model consists of a deep learning classifier and a tweet processor with a combination of DeepCNN and Bi-LSTM. The tweet processor standardizes tweets and applies semantic rules on datasets. The deep learning classifier and the tweet processor are treated as two distinct components in a framework.\n",
      "Question : for the text Our proposed model consists of a deep learning classifier and a tweet processor. The deep learning classifier is a combination of DeepCNN and Bi-LSTM. The tweet processor standardizes tweets and then applies semantic rules on datasets. We construct a framework that treats the deep learning classifier and the tweet processor as two distinct components. We believe that standardizing data is an important step to achieve high accuracy. To formulate our problem in increasing the accuracy of the classifier, we illustrate our model in Figure. FIGREF4 as follows:.Tweets are firstly considered via a processor based on preprocessing steps BIBREF0 and the semantic rules-based method BIBREF11 in order to standardize tweets and capture only important information containing the main sentiment of a tweet..We use DeepCNN with Wide convolution for character-level embeddings. A wide convolution can learn to recognize specific n-grams at every position in a word that allows features to be extracted independently of these positions in the word. These features maintain the order and relative positions of characters. A DeepCNN is constructed by two wide convolution layers and the need of multiple wide convolution layers is widely accepted that a model constructing by multiple processing layers have the ability to learn representations of data with higher levels of abstraction BIBREF12 . Therefore, we use DeepCNN for character-level embeddings to support morphological and shape information for a word. The DeepCNN produces INLINEFORM0 global fixed-sized feature vectors for INLINEFORM1 words..A combination of the global fixed-size feature vectors and word-level embedding is fed into Bi-LSTM. The Bi-LSTM produces a sentence-level representation by maintaining the order of words..Our work is philosophically similar to BIBREF5 . However, our model is distinguished with their approaches in two aspects:.Using DeepCNN with two wide convolution layers to increase representation with multiple levels of abstraction..Integrating global character fixed-sized feature vectors with word-level embedding to extract a sentence-wide feature set via Bi-LSTM. This deals with three main problems: (i) Sentences have any different size; (ii) The semantic and the syntactic of words in a sentence are captured in order to increase information for a word; (iii) Important information of characters that can appear at any position in a word are extracted..In sub-section B, we introduce various kinds of dataset. The modules of our model are constructed in other sub-sections. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "45\n",
      "Question 1: What is the contribution of semantic rules in improving the classification accuracy of Tweet sentiment analysis? \n",
      "\n",
      "Answer 1: The use of semantic rules can help in handling non-essential sub-tweets and therefore improve the classification accuracy of Tweet sentiment analysis along with the use of character embeddings through DeepCNN.\n",
      "Question : for the text In the present work, we have pointed out that the use of character embeddings through a DeepCNN to enhance information for word embeddings built on top of Word2Vec or TwitterGlove improves classification accuracy in Tweet sentiment classification. Our results add to the well-establish evidence that character vectors are an important ingredient for word-level in deep learning for NLP. In addition, semantic rules contribute handling non-essential sub-tweets in order to improve classification accuracy. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "46\n",
      "Question 1: What is the STS Corpus and how was the test set constructed? \n",
      "\n",
      "Answer 1: The STS Corpus is a dataset containing 1,600K training tweets collected by a crawler. The test set was manually constructed by BIBREF0 and includes 177 negative and 182 positive tweets. Despite its small size, the Stanford test set has been widely used in different evaluation tasks.\n",
      "Question : for the text Stanford - Twitter Sentiment Corpus (STS Corpus): STS Corpus contains 1,600K training tweets collected by a crawler from BIBREF0 . BIBREF0 constructed a test set manually with 177 negative and 182 positive tweets. The Stanford test set is small. However, it has been widely used in different evaluation tasks BIBREF0 BIBREF5 BIBREF13 ..Sanders - Twitter Sentiment Corpus: This dataset consists of hand-classified tweets collected by using search terms: INLINEFORM0 , #google, #microsoft and #twitter. We construct the dataset as BIBREF14 for binary classification..Health Care Reform (HCR): This dataset was constructed by crawling tweets containing the hashtag #hcr BIBREF15 . Task is to predict positive/negative tweets BIBREF14 . generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "47\n",
      "What is the purpose of the first layer in the DeepCNN model for sentiment analysis?\n",
      "\n",
      "The purpose of the first layer in the DeepCNN model for sentiment analysis is to extract local features around each character window of the given word and use max pooling over character windows to produce a global fixed-sized feature vector for the word. This allows for the retrieval of important context characters and the transformation of representation at the previous level into a representation at a higher abstract level.\n",
      "Question : for the text DeepCNN in the deep learning module is illustrated in Figure. FIGREF22 . The DeepCNN has two wide convolution layers. The first layer extract local features around each character windows of the given word and using a max pooling over character windows to produce a global fixed-sized feature vector for the word. The second layer retrieves important context characters and transforms the representation at previous level into a representation at higher abstract level. We have INLINEFORM0 global character fixed-sized feature vectors for INLINEFORM1 words..In the next step of Figure. FIGREF4 , we construct the vector INLINEFORM0 by concatenating the word-level embedding with the global character fixed-size feature vectors. The input of Bi-LSTM is a sequence of embeddings INLINEFORM1 . The use of the global character fixed-size feature vectors increases the relationship of words in the word-level embedding. The purpose of this Bi-LSTM is to capture the context of words in a sentence and maintain the order of words toward to extract sentence-level representation. The top of the model is a softmax function to predict sentiment label. We describe in detail the kinds of CNN and LSTM that we use in next sub-part 1 and 2..The one-dimensional convolution called time-delay neural net has a filter vector INLINEFORM0 and take the dot product of filter INLINEFORM1 with each m-grams in the sequence of characters INLINEFORM2 of a word in order to obtain a sequence INLINEFORM3 : DISPLAYFORM0 .Based on Equation 1, we have two types of convolutions that depend on the range of the index INLINEFORM0 . The narrow type of convolution requires that INLINEFORM1 and produce a sequence INLINEFORM2 . The wide type of convolution does not require on INLINEFORM3 or INLINEFORM4 and produce a sequence INLINEFORM5 . Out-of-range input values INLINEFORM6 where INLINEFORM7 or INLINEFORM8 are taken to be zero. We use wide convolution for our model..Given a word INLINEFORM0 composed of INLINEFORM1 characters INLINEFORM2 , we take a character embedding INLINEFORM3 for each character INLINEFORM4 and construct a character matrix INLINEFORM5 as following Equation. 2: DISPLAYFORM0 .The values of the embeddings INLINEFORM0 are parameters that are optimized during training. The trained weights in the filter INLINEFORM1 correspond to a feature detector which learns to recognize a specific class of n-grams. The n-grams have size INLINEFORM2 . The use of a wide convolution has some advantages more than a narrow convolution because a wide convolution ensures that all weights of filter reach the whole characters of a word at the margins. The resulting matrix has dimension INLINEFORM3 ..Long Short-Term Memory networks usually called LSTMs are a improved version of RNN. The core idea behind LSTMs is the cell state which can maintain its state over time, and non-linear gating units which regulate the information flow into and out of the cell. The LSTM architecture that we used in our proposed model is described in BIBREF9 . A single LSTM memory cell is implemented by the following composite function: DISPLAYFORM0 DISPLAYFORM1 .where INLINEFORM0 is the logistic sigmoid function, INLINEFORM1 and INLINEFORM2 are the input gate, forget gate, output gate, cell and cell input activation vectors respectively. All of them have a same size as the hidden vector INLINEFORM3 . INLINEFORM4 is the hidden-input gate matrix, INLINEFORM5 is the input-output gate matrix. The bias terms which are added to INLINEFORM6 and INLINEFORM7 have been omitted for clarity. In addition, we also use the full gradient for calculating with full backpropagation through time (BPTT) described in BIBREF10 . A LSTM gradients using finite differences could be checked and making practical implementations more reliable. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "48\n",
      "Question 1: How does the accuracy of the model presented in Table IV compare to the state-of-the-art model on the STS Corpus?\n",
      "\n",
      "Answer 1: The accuracy of the model presented in Table IV is 86.63, which is the best prediction accuracy so far for the STS Corpus. This accuracy surpasses the state-of-the-art model presented in BIBREF5, which uses a CharSCNN.\n",
      "Question : for the text Table IV shows the result of our model for sentiment classification against other models. We compare our model performance with the approaches of BIBREF0 BIBREF5 on STS Corpus. BIBREF0 reported the results of Maximum Entropy (MaxEnt), NB, SVM on STS Corpus having good performance in previous time. The model of BIBREF5 is a state-of-the-art so far by using a CharSCNN. As can be seen, 86.63 is the best prediction accuracy of our model so far for the STS Corpus..For Sanders and HCR datasets, we compare results with the model of BIBREF14 that used a ensemble of multiple base classifiers (ENS) such as NB, Random Forest (RF), SVM and Logistic Regression (LR). The ENS model is combined with bag-of-words (BoW), feature hashing (FH) and lexicons. The model of BIBREF14 is a state-of-the-art on Sanders and HCR datasets. Our models outperform the model of BIBREF14 for the Sanders dataset and HCR dataset. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "49\n",
      "What is the goal of this research?\n",
      "\n",
      "The goal of this research is to use a Deep Convolutional Neural Network (DeepCNN) to exploit the information of characters of words in order to support word-level embedding and a Bi-LSTM to produce a sentence-wide feature representation based on these embeddings for Twitter sentiment classification.\n",
      "Question : for the text Twitter sentiment classification have intensively researched in recent years BIBREF0 BIBREF1 . Different approaches were developed for Twitter sentiment classification by using machine learning such as Support Vector Machine (SVM) with rule-based features BIBREF2 and the combination of SVMs and Naive Bayes (NB) BIBREF3 . In addition, hybrid approaches combining lexicon-based and machine learning methods also achieved high performance described in BIBREF4 . However, a problem of traditional machine learning is how to define a feature extractor for a specific domain in order to extract important features..Deep learning models are different from traditional machine learning methods in that a deep learning model does not depend on feature extractors because features are extracted during training progress. The use of deep learning methods becomes to achieve remarkable results for sentiment analysis BIBREF5 BIBREF6 BIBREF7 . Some researchers used Convolutional Neural Network (CNN) for sentiment classification. CNN models have been shown to be effective for NLP. For example, BIBREF6 proposed various kinds of CNN to learn sentiment-bearing sentence vectors, BIBREF5 adopted two CNNs in character-level to sentence-level representation for sentiment analysis. BIBREF7 constructs experiments on a character-level CNN for several large-scale datasets. In addition, Long Short-Term Memory (LSTM) is another state-of-the-art semantic composition model for sentiment classification with many variants described in BIBREF8 . The studies reveal that using a CNN is useful in extracting information and finding feature detectors from texts. In addition, a LSTM can be good in maintaining word order and the context of words. However, in some important aspects, the use of CNN or LSTM separately may not capture enough information..Inspired by the models above, the goal of this research is using a Deep Convolutional Neural Network (DeepCNN) to exploit the information of characters of words in order to support word-level embedding. A Bi-LSTM produces a sentence-wide feature representation based on these embeddings. The Bi-LSTM is a version of BIBREF9 with Full Gradient described in BIBREF10 . In addition, the rules-based approach also effects classification accuracy by focusing on important sub-sentences expressing the main sentiment of a tweet while removing unnecessary parts of a tweet. The paper makes the following contributions:.The organization of the present paper is as follows: In section 2, we describe the model architecture which introduces the structure of the model. We explain the basic idea of model and the way of constructing the model. Section 3 show results and analysis and section 4 summarize this paper. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "50\n",
      "Question 1: Why were emoticons removed from the training dataset in a previous study?\n",
      "\n",
      "Answer 1: Emoticons were removed from the training dataset in a previous study because they were considered noisy labels and could negatively impact the accuracy of classifiers. The study believed that classifiers would learn better from non-emoticon features, such as unigrams and bi-grams. However, this also posed a limitation if the test set contained emoticons as they would not be recognized by the classifiers.\n",
      "Question : for the text We firstly take unique properties of Twitter in order to reduce the feature space such as Username, Usage of links, None, URLs and Repeated Letters. We then process retweets, stop words, links, URLs, mentions, punctuation and accentuation. For emoticons, BIBREF0 revealed that the training process makes the use of emoticons as noisy labels and they stripped the emoticons out from their training dataset because BIBREF0 believed that if we consider the emoticons, there is a negative impact on the accuracies of classifiers. In addition, removing emoticons makes the classifiers learns from other features (e.g. unigrams and bi-grams) presented in tweets and the classifiers only use these non-emoticon features to predict the sentiment of tweets. However, there is a problem is that if the test set contains emoticons, they do not influence the classifiers because emoticon features do not contain in its training data. This is a limitation of BIBREF0 , because the emoticon features would be useful when classifying test data. Therefore, we keep emoticon features in the datasets because deep learning models can capture more information from emoticon features for increasing classification accuracy. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "1\n",
      "Question 1: What is the purpose of using regularization in machine learning?\n",
      "Answer 1: The purpose of using regularization in machine learning is to restrict the values of the weight vectors, which are represented by INLINEFORM0 and prevent them from becoming too large, thereby preventing overfitting. BIBREF18 is used to place a constraint on the weight vectors.\n",
      "Question : for the text For regularization, we use a constraint on INLINEFORM0 of the weight vectors BIBREF18 . generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "2\n",
      "Question 1: What is the purpose of using a fixed-sized word vocabulary and a fixed-sized character vocabulary in constructing embedding inputs for the model?\n",
      "\n",
      "Answer 1: The purpose of using a fixed-sized word vocabulary and a fixed-sized character vocabulary is to encode character-level embeddings and word-level embeddings, respectively, to construct embedding inputs for the model. The pre-trained word-level embeddings capture the syntactic and semantic information of words, while the character fixed-size feature vectors correspond to word-level embeddings in a sentence.\n",
      "Question : for the text To construct embedding inputs for our model, we use a fixed-sized word vocabulary INLINEFORM0 and a fixed-sized character vocabulary INLINEFORM1 . Given a word INLINEFORM2 is composed from characters INLINEFORM3 , the character-level embeddings are encoded by column vectors INLINEFORM4 in the embedding matrix INLINEFORM5 , where INLINEFORM6 is the size of the character vocabulary. For word-level embedding INLINEFORM7 , we use a pre-trained word-level embedding with dimension 200 or 300. A pre-trained word-level embedding can capture the syntactic and semantic information of words BIBREF17 . We build every word INLINEFORM8 into an embedding INLINEFORM9 which is constructed by two sub-vectors: the word-level embedding INLINEFORM10 and the character fixed-size feature vector INLINEFORM11 of INLINEFORM12 where INLINEFORM13 is the length of the filter of wide convolutions. We have INLINEFORM14 character fixed-size feature vectors corresponding to word-level embedding in a sentence. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "3\n",
      "Q: What is the purpose of the rule-based approach mentioned in the text?\n",
      "\n",
      "A: The purpose of the rule-based approach mentioned in the text is to assist in handling negation and dealing with specific PoS particles in order to effectively affect the final output of classification for sentiment analysis of tweets. The approach uses ten semantic rules in a hybrid approach, but the proposed method only utilizes five of these rules. The rules are displayed in Table TABREF15, which includes examples from STS Corpus and output after using the rules. The approach is used to remove unessential parts in a tweet and focus on the main sentiment expressed in sub-sentences following specific PoS particles.\n",
      "Question : for the text In Twitter social networking, people express their opinions containing sub-sentences. These sub-sentences using specific PoS particles (Conjunction and Conjunctive adverbs), like \"but, while, however, despite, however\" have different polarities. However, the overall sentiment of tweets often focus on certain sub-sentences. For example:.@lonedog bwahahah...you are amazing! However, it was quite the letdown..@kirstiealley my dentist is great but she's expensive...=(.In two tweets above, the overall sentiment is negative. However, the main sentiment is only in the sub-sentences following but and however. This inspires a processing step to remove unessential parts in a tweet. Rule-based approach can assists these problems in handling negation and dealing with specific PoS particles led to effectively affect the final output of classification BIBREF11 BIBREF16 . BIBREF11 summarized a full presentation of their semantic rules approach and devised ten semantic rules in their hybrid approach based on the presentation of BIBREF16 . We use five rules in the semantic rules set because other five rules are only used to compute polarity of words after POS tagging or Parsing steps. We follow the same naming convention for rules utilized by BIBREF11 to represent the rules utilized in our proposed method. The rules utilized in the proposed method are displayed in Table TABREF15 in which is included examples from STS Corpus and output after using the rules. Table TABREF16 illustrates the number of processed sentences on each dataset. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "4\n",
      "Q: What is the source of funding for this work?\n",
      "\n",
      "A: The work is supported by the National Natural Science Foundation of China, with grant numbers 61472453, U1401256, U1501252, U1611264, U1711261, and U1711262.\n",
      "Question : for the text We thank the three anonymous authors for their constructive comments. This work is supported by the National Natural Science Foundation of China (61472453, U1401256, U1501252, U1611264, U1711261, U1711262). generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "5\n",
      "What is observed about the weights attributed to neighbor entities in LAN's approach?\n",
      "\n",
      "The observations from the Subject-10 testing set show that LAN attributes higher weights to neighbors with more relevant relations and neighbor entities that are more informative. For instance, when the query relation is origin, LAN attributes higher weights to neighbors involved in place_lived and breed_origin, which are helpful to imply origin. Additionally, neighbors with the relation gender gain the lowest weights as they do not indicate anything about the query relation. Similarly, when the query relation is profession, LAN attributes higher weights to neighbor entities like Aristotle, Metaphysics, and Aesthetics, which are all related to the answer Philosopher.\n",
      "Question : for the text In order to visualize how LAN specifies weights to neighbors, we sample some cases from the Subject-10 testing set. From Table FIGREF50 , we have the following observations. First, with the query relation, LAN could attribute higher weights to neighbors with more relevant relations. In the first case, when the query is origin, the top two neighbors are involved by place_lived and breed_origin, which are helpful to imply origin. In addition, in all three cases, neighbors with relation gender gain the lowest weights since they imply nothing about the query relation. Second, LAN could attribute higher weights to neighbor entities that are more informative. When the query relation is profession, the neighbors Aristotle, Metaphysics and Aesthetics are all relevant to the answer Philosopher. In the third case, we also observe similar situations. Here, the neighbor with the highest weight is (institution, University_of_Calgary) since the query relation place_lived helps the aggregator to focus on the neighboring relation institution, then the neighbor entity University_of_Calgary assists in locating the answer Calgary. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "6\n",
      "What is the proposed model for effective neighborhood aggregation in KG embedding and how does it attribute different weights to an entity's neighbors?\n",
      "The proposed model is called LAN (Logic Attention Network), which attributes different weights to an entity's neighbors in a permutation invariant manner considering both the redundancy of neighbors and the query relation. The weights are estimated from data with logic rules at a coarse relation level and neural attention network at a fine neighbor level.\n",
      "Question : for the text In this paper, we address inductive KG embedding, which helps embed emerging entities efficiently. We formulate three characteristics required for effective neighborhood aggregators. To meet the three characteristics, we propose LAN, which attributes different weights to an entity's neighbors in a permutation invariant manner, considering both the redundancy of neighbors and the query relation. The weights are estimated from data with logic rules at a coarse relation level, and neural attention network at a fine neighbor level. Experiments show that LAN outperforms baseline models significantly on two typical KG completion tasks. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "7\n",
      "What is the purpose of filtering and splitting data sets in constructing the required datasets for the inductive scenario in triplet classification?\n",
      "\n",
      "The purpose of filtering and splitting data sets is to ensure that unseen entities would not appear in final training set or validation set. This is important in the inductive scenario to test the model's ability to classify new entities that were unseen during training.\n",
      "Question : for the text In both tasks, we need datasets whose test sets contain new entities unseen during training. For the task of triplet classification, we directly use the datasets released by BIBREF9 ijcai2017-250 which are based on WordNet11 BIBREF29 . Since they do not conduct experiments on the link prediction task, we construct the required datasets based on FB15K BIBREF11 following a similar protocol used in BIBREF9 ijcai2017-250 as follows..Sampling unseen entities. Firstly, we randomly sample INLINEFORM0 of the original testing triplets to form a new test set INLINEFORM1 for our inductive scenario ( BIBREF9 ijcai2017-250 samples INLINEFORM2 testing triplets). Then two different strategies are used to construct the candidate unseen entities INLINEFORM6 . One is called Subject, where only entities appearing as the subjects in INLINEFORM7 are added to INLINEFORM8 . Another is called Object, where only objects in INLINEFORM9 are added to INLINEFORM10 . For an entity INLINEFORM11 , if it does not have any neighbor in the original training set, such an entity is filtered out, yielding the final unseen entity set INLINEFORM12 . For a triplet INLINEFORM13 , if INLINEFORM14 or INLINEFORM15 , it is removed from INLINEFORM16 ..Filtering and splitting data sets. The second step is to ensure that unseen entities would not appear in final training set or validation set. We split the original training set into two data sets, the new training set and auxiliary set. For a triplet INLINEFORM0 in original training set, if INLINEFORM1 , it is added to the new training set. If INLINEFORM2 or INLINEFORM3 , it is added to the auxiliary set, which serves as existing neighbors for unseen entities in INLINEFORM4 ..Finally, for a triplet INLINEFORM0 in the original validation set, if INLINEFORM1 or INLINEFORM2 , it is removed from the validation set..The statistics for the resulting INLINEFORM0 datasets using Subject and Object strategies are in Table TABREF34 . generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "8\n",
      "Q1: Which model consistently achieves the best results on all datasets in the KBC task?\n",
      "\n",
      "A1: The LAN model consistently achieves the best results on all datasets, demonstrating the effectiveness of LAN on this KBC task.\n",
      "Question : for the text The results are reported in Table TABREF42 . Since we did not achieve the same results for MEAN as reported in BIBREF9 ijcai2017-250 with either our implementation or their released source code, the best results from their original paper are reported. From the table, we observe that, on one hand, LSTM results in poorer performance compared with MEAN, which involves fewer parameters though. This demonstrates the necessity of the permutation invariance for designing neighborhood aggregators for KGs. On the other hand, our LAN model consistently achieves the best results on all datasets, demonstrating the effectiveness of LAN on this KBC task. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "9\n",
      "Question 1: What are the two knowledge graph completion tasks evaluated in the text and how is the effectiveness of the LAN model compared with the baseline aggregators?\n",
      "\n",
      "Answer 1: The two knowledge graph completion tasks evaluated in the text are link prediction and triplet classification. The effectiveness of the LAN model is compared with two baseline aggregators, MEAN and LSTM, as described in the Encoder section. MEAN is used as a pooling function since it leads to the best performance in BIBREF9 ijcai2017-250, while LSTM is used due to its large expressive capability BIBREF26.\n",
      "Question : for the text We evaluate the effectiveness of our LAN model on two typical knowledge graph completion tasks, i.e., link prediction and triplet classification. We compare our LAN with two baseline aggregators, MEAN and LSTM, as described in the Encoder section. MEAN is used on behalf of pooling functions since it leads to the best performance in BIBREF9 ijcai2017-250. LSTM is used due to its large expressive capability BIBREF26 . generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "10\n",
      "What is the main advantage of the LAN model over other baselines in the link prediction task?\n",
      "\n",
      "The LAN model outperforms all other baselines significantly, especially on the Hit@k metrics, demonstrating its superiority in treating the neighbors differently in a permutation invariant way. Additionally, it shows consistent improvements in the MRR metric, which is proposed to address the flaws of the MR metric, further validating its effectiveness in link prediction.\n",
      "Question : for the text The results on Subject-10 and Object-10 are reported in Table TABREF43 . The results on other datasets are similar and we summarize them later in Figure FIGREF50 . From Table TABREF43 , we still observe consistent results for all the models as in the triplet classification task. Firstly, LSTM results in the poorest performance on all datasets. Secondly, our LAN model outperforms all the other baselines significantly, especially on the Hit@k metrics. The improvement on the MR metric of LAN might not be considerable. This is due to the flaw of the MR metric since it is more sensitive to lower positions of the ranking, which is actually of less importance. The MRR metric is proposed for this reason, where we could observe consistent improvements brought by LAN. The effectiveness of LAN on link prediction validates LAN's superiority to other aggregators and the necessities to treat the neighbors differently in a permutation invariant way. To analyze whether LAN outperforms the others for expected reasons and generalizes to other configurations, we conduct the following studies..In this experiment, we would like to confirm that it's necessary for the aggregator to be aware of the query relation. Specifically, we investigate the attention neural network and design two degenerated baselines. One is referred to as Query-Attention and is simply an attention network as in LAN except that the logic rule mechanism is removed. The other is referred to as Global-Attention, which is also an attention network except that the query relation embedding INLINEFORM0 in Eq. ( EQREF28 ) is masked by a zero vector. The results are reported in Table TABREF46 . We observe that although superior to MEAN, Global-Attention is outperformed by Query-Attention, demonstrating the necessity of query relation awareness. The superiority of Global-Attention over MEAN could be attributed to the fact that the attention mechanism is effective to identify the neighbors which are globally important regardless of the query..We find that the logic rules greatly help to improve the attention network in LAN. We confirm this point by conducting further experiments where the logic rule mechanism is isolated as a single model (referred to as Logic Rules Only). The results are also demonstrated in Table TABREF46 , from which we find that Query-Attention outperforms MEAN by a limited margin. Meanwhile, Logic Rules Only outperforms both MEAN and Query-Attention by significant margins. These results demonstrate the effectiveness of logic rules in assigning meaningful weights to the neighbors. Specifically, in order to generate representations for unseen entities, it is crucial to incorporate the logic rules to train the aggregator, instead of depending solely on neural networks to learn from the data. By combining the logic rules and neural networks, LAN takes a step further in outperforming all the other models..To find out whether the superiority of LAN to the baselines can generalize to other scoring functions, we replace the scoring function in Eq. ( EQREF20 ) and Eq. ( EQREF36 ) by three typical scoring functions mentioned in Related Works. We omit the results of LSTM, for it is still inferior to MEAN. The results are listed in Table TABREF48 , from which we observe that with different scoring functions, LAN outperforms MEAN consistently by a large margin on all the evaluation metrics. Note that TransE leads to the best results on MEAN and LAN..It's reasonable to suppose that when the ratio of the unseen entities over the training entities increases (namely the observed knowledge graph becomes sparser), all models' performance would deteriorate. To figure out whether our LAN could suffer less on sparse knowledge graphs, we conduct link prediction on datasets with different sample rates INLINEFORM0 as described in Step 1 of the Data Construction section. The results are displayed in Figure FIGREF50 . We observe that the increasing proportion of unseen entities certainly has a negative impact on all models. However, the performance of LAN does not decrease as drastically as that of MEAN and LSTM, indicating that LAN is more robust on sparse KGs. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "11\n",
      "Question 1: What hyper-parameters were searched to find the optimal configurations for all datasets?\n",
      "\n",
      "Answer 1: The hyper-parameters searched to find the optimal configurations for all datasets were learning rate (INLINEFORM0 in INLINEFORM1), embedding dimension for neighbors (INLINEFORM2 in INLINEFORM3), and margin (INLINEFORM4 in INLINEFORM5). The optimal configurations found were INLINEFORM6.\n",
      "Question : for the text Since this task is also conducted in BIBREF9 ijcai2017-250, we use the same configurations with learning rate INLINEFORM0 , embedding dimension INLINEFORM1 , and margin INLINEFORM2 for all datasets. We randomly sample 64 neighbors for each entity. Zero padding is used when the number of neighbors is less than 64. L2-regularization is applied on the parameters of LAN. The regularization rate is INLINEFORM3 ..We search the best hyper-parameters of all models according to the performance on validation set. In detail, we search learning rate INLINEFORM0 in INLINEFORM1 , embedding dimension for neighbors INLINEFORM2 in INLINEFORM3 , and margin INLINEFORM4 in INLINEFORM5 . The optimal configurations are INLINEFORM6 for all the datasets. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "12\n",
      "Question 1: What is the scoring function used to rank candidate triplets?\n",
      "\n",
      "Answer 1: The scoring function used to rank candidate triplets is defined as INLINEFORM5 in Eq. (EQREF20).\n",
      "Question : for the text Link prediction in the inductive setting aims at reasoning the missing part “?” in a triplet when given INLINEFORM0 or INLINEFORM1 with emerging entities INLINEFORM2 or INLINEFORM3 respectively. To tackle the task, we firstly hide the object (subject) of each testing triplet in Subject-R (Object-R) to produce a missing part. Then we replace the missing part with all entities in the entity set INLINEFORM4 to construct candidate triplets. We compute the scoring function INLINEFORM5 defined in Eq. ( EQREF20 ) for all candidate triplets, and rank them in descending order. Finally, we evaluate whether the ground-truth entities are ranked ahead of other entities. We use traditional evaluation metrics as in the KG completion literature, i.e., Mean Rank (MR), Mean Reciprocal Rank (MRR), and the proportion of ground truth entities ranked top-k (Hits@k, INLINEFORM6 ). Since certain candidate triplets might also be true, we follow previous works and filter out these fake negatives before ranking. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "13\n",
      "Question 1: What is the purpose of triplet classification?\n",
      "\n",
      "Answer 1: The purpose of triplet classification is to classify a fact triplet as true or false.\n",
      "Question : for the text Triplet classification aims at classifying a fact triplet INLINEFORM0 as true or false. In the dataset of BIBREF9 ijcai2017-250, triplets in the validation and testing sets are labeled as true or false, while triplets in the training set are all true ones..To tackle this task, we preset a threshold INLINEFORM0 for each relation r. If INLINEFORM1 , the triplet is classified as positive, otherwise it is negative. We determine the optimal INLINEFORM2 by maximizing classification accuracy on the validation set. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "14\n",
      "What is the purpose of the encoder-decoder framework illustrated in Figure FIGREF12?\n",
      "\n",
      "The purpose of the encoder-decoder framework illustrated in Figure FIGREF12 is to obtain a neighborhood aggregator.\n",
      "Question : for the text To obtain such a neighborhood aggregator INLINEFORM0 , we adopt an encoder-decoder framework as illustrated by Figure FIGREF12 . Given a training triplet, the encoder INLINEFORM1 encodes INLINEFORM2 and INLINEFORM3 into two embeddings with INLINEFORM4 . The decoder measures the plausibility of the triplet, and provides feedbacks to the encoder to adjust the parameters of INLINEFORM5 . In the remainder of this section, we describe general configurations of the two components..As specified in Figure FIGREF12 , for an entity INLINEFORM0 on focus, the encoder works on a collection of input neighbor embeddings, and output INLINEFORM1 's embedding. To differentiate between input and output embeddings, we use superscripts INLINEFORM2 and INLINEFORM3 on the respective vectors. Let INLINEFORM4 , which is obtained from an embedding matrix INLINEFORM5 , be the embedding of a neighbor INLINEFORM6 , where INLINEFORM7 . To reflect the impact of relation INLINEFORM8 on INLINEFORM9 , we apply a relation-specific transforming function INLINEFORM10 on INLINEFORM11 as follows, DISPLAYFORM0 .where INLINEFORM0 is the transforming vector for relation INLINEFORM1 and is restricted as a unit vector. We adopt this transformation from BIBREF27 wang2014knowledge since it does not involve matrix product operations and is of low computation complexity..After neighbor embeddings are transformed, these transformed embeddings are fed to the aggregator INLINEFORM0 to output an embedding INLINEFORM1 for the target entity INLINEFORM2 , i.e., DISPLAYFORM0 .By definition, an aggregator INLINEFORM0 essentially takes as input a collection of vectors INLINEFORM1 ( INLINEFORM2 ) and maps them to a single vector. With this observation, the following two types of functions seem to be natural choices for neighborhood aggregators, and have been adopted previously:.Pooling Functions. A typical pooling function is mean-pooling, which is defined by INLINEFORM0 . Besides mean-pooling, other previously adopted choices include sum- and max-pooling BIBREF9 . Due to their simple forms, pooling functions are permutation-invariant, but consider the neighbors equally. It is aware of neither potential redundancy in the neighborhood nor the query relations..Recurrent Neural Networks (RNNs). In various natural language processing tasks, RNNs prove effective in modeling sequential dependencies. In BIBREF26 , the authors adopt an RNN variant LSTM BIBREF28 as neighborhood aggregator, i.e., INLINEFORM0 . To train and apply the LSTM-based aggregator, they have to randomly permute the neighbors, which violates the permutation variance property..Given the subject and object embeddings INLINEFORM0 and INLINEFORM1 output by the encoder, the decoder is required to measure the plausibility of the training triplet. To avoid potential mixture with relations INLINEFORM2 in the neighborhood, we refer to the relation in the training triplet by query relation, and denote it by INLINEFORM3 instead. After looking up INLINEFORM4 's representation INLINEFORM5 from an embedding matrix INLINEFORM6 , the decoder scores the training triplet INLINEFORM7 with a scoring function INLINEFORM8 . Following BIBREF9 ijcai2017-250, we mainly investigate a scoring function based on TransE BIBREF11 defined by DISPLAYFORM0 .where INLINEFORM0 denotes the L1 norm. To test whether the studied aggregators generalize among different scoring function, we will also consider several alternatives in experiments. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "15\n",
      "What is the proposed approach for improving traditional neighborhood aggregators and why is it necessary?\n",
      "\n",
      "Answer 1: The proposed approach is to generalize the aggregators from only depending on transformed embeddings to also consider other useful information in the neighborhood and the query relation. This is necessary because traditional aggregators neglect potentially important information in the neighborhood and the query relation, which may facilitate more effective aggregation of transformed embeddings.\n",
      "Question : for the text Traditional neighborhood aggregators only depend on collections of transformed embeddings. They neglect other useful information in the neighborhood INLINEFORM0 and the query relation INLINEFORM1 , which may facilitate more effective aggregation of the transformed embeddings. To this end, we propose generalizing the aggregators from INLINEFORM2 to INLINEFORM3 ..Specifically, for an entity INLINEFORM0 , its neighbors INLINEFORM1 should contribute differently to INLINEFORM2 according to its importance in representing INLINEFORM3 . To consider the different contribution while preserving the permutation invariance property, we employ a weighted or attention-based aggregating approach on the transformed embeddings. The additional information in INLINEFORM4 and INLINEFORM5 is then exploited when estimating the attention weights. Formally, we obtain INLINEFORM6 by DISPLAYFORM0 .Here INLINEFORM0 is the attention weight specified for each neighbor INLINEFORM1 given INLINEFORM2 and the query relation INLINEFORM3 ..To assign larger weights INLINEFORM0 to more important neighbors, from the perspective of INLINEFORM1 , we ask ourselves two questions at progressive levels: 1) What types of neighboring relations may lead us to potentially important neighbors? 2) Following those relations, which specific neighbor (in transformed embedding) may contain important information? Inspired by the two questions, we adopt the following two mechanisms to estimate INLINEFORM2 ..Relations in a KG are simply not independent of each other. For an entity INLINEFORM0 , one neighboring relation INLINEFORM1 may imply the existence of another neighboring relation INLINEFORM2 , though they may not necessarily connect INLINEFORM3 to the same neighbor. For example, a neighboring relation play_for may suggest the home city, i.e., live_in, of the current athlete entity. Following notations in logics, we denote potential dependency between INLINEFORM4 and INLINEFORM5 by a “logic rule” INLINEFORM6 . To measure the extent of such dependency, we define the confidence of a logic rule INLINEFORM7 as follows: DISPLAYFORM0 .Here the function INLINEFORM0 equals 1 when INLINEFORM1 is true and 0 otherwise. As an empirical statistic over the entire KG, INLINEFORM2 is larger if more entities with neighboring relation INLINEFORM3 also have INLINEFORM4 as a neighboring relation..With the confidence scores INLINEFORM0 between all relation pairs at hand, we are ready to characterize neighboring relations INLINEFORM1 that lead to important neighbors. On one hand, such a relation INLINEFORM2 should have a large INLINEFORM3 , i.e., it is statistically relevant to INLINEFORM4 . Following the above example, play_for should be consulted to if the query relation is live_in. On the other hand, INLINEFORM5 should not be implied by other relations in the neighborhood. For example, no matter whether the query relation is live_in or not, the neighboring relation work_as should not be assigned too much weight, because sufficient information is already provided by play_for..Following the above intuitions, we implement the logic rule mechanism of measuring neighboring relations' usefulness as follow: DISPLAYFORM0 .We note that INLINEFORM0 promotes relations INLINEFORM1 strongly implying INLINEFORM2 (the numerator) and demotes those implied by some other relation in the same neighborhood (the denominator). In this manner, our logic rule mechanism addresses both query relation awareness and neighborhood redundancy awareness..With global statistics about relations, the logic rule mechanism guides the attention weight to be distributed at a coarse granularity of relations. However, it may be insufficient not to consult finer-grained information hidden in the transformed neighbor embeddings to determine which neighbor is important indeed. To take the transformed embeddings into consideration, we adopt an attention network BIBREF10 ..Specifically, given a query relation INLINEFORM0 , the importance of an entity INLINEFORM1 's neighbor INLINEFORM2 is measured by DISPLAYFORM0 .Here the unnormalized attention weight INLINEFORM0 is given by an attention neural network as DISPLAYFORM0 .In this equation, INLINEFORM0 and INLINEFORM1 are global attention parameters, while INLINEFORM2 is a relation-specific attention parameter for the query relation INLINEFORM3 . All those attention parameters are regarded as parameters of the encoder, and learned directly from the data..Note that, unlike the logic rule mechanism at relation level, the computation of INLINEFORM0 concentrates more on the neighbor INLINEFORM1 itself. This is useful when the neighbor entity INLINEFORM2 is also helpful to explain the current training triplet. For example, in Figure FIGREF12 , the neighbor Chicago_Bulls could help to imply the object of live_in since there are other athletes playing for Chicago_Bulls while living in Chicago. Although working at the neighbor level, the dependency on transformed neighbor embeddings INLINEFORM3 and the relation-specific parameter INLINEFORM4 make INLINEFORM5 aware of both neighborhood redundancy and the query relation..Finally, to incorporate these two weighting mechanisms together in measuring the importance of neighbors, we employ a double-view attention and reformulate Eq. ( EQREF22 ) as DISPLAYFORM0  generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "16\n",
      "Question 1: What is one limitation of inductive KG embedding models that use description text or images as inputs?\n",
      "\n",
      "Answer 1: One limitation of inductive KG embedding models that use description text or images as inputs is that it is not clear whether the embeddings generated are powerful enough to infer implicit or new facts beyond those expressed in the text/image. Additionally, these approaches may not be helpful when domain experts introduce new entities via partial facts rather than text or images.\n",
      "Question : for the text To relieve the issue of emerging entities, several inductive KG embedding models are proposed, including BIBREF16 xie2016representation, BIBREF6 shi2018open and BIBREF17 xie2016image which use description text or images as inputs. Although the resultant embeddings may be utilized for KG completion, it is not clear whether the embeddings are powerful enough to infer implicit or new facts beyond those expressed in the text/image. Moreover, when domain experts are recruited to introduce new entities via partial facts rather than text or images, those approaches may not help much..In light of the above scenario, existing neighbors of an emerging entity are considered as another type of input for inductive models. In BIBREF9 ijcai2017-250, the authors propose applying Graph Neural Network (GNN) on the KG, which generates the embedding of a new entity by aggregating all its known neighbors. However, their model aggregates the neighbors via simple pooling functions, which neglects the difference among the neighbors. Other works like BIBREF18 fu2017hin2vec and BIBREF19 tang2015pte aim at embedding nodes for node classification given the entire graph and thus are inapplicable for inductive KG-specific tasks. BIBREF20 schlichtkrull2017modeling and BIBREF21 xiong2018one also rely on neighborhood structures to embed entities, but they either work transductively or focus on emerging relations..Finally, we note another related line of studies on node representation learning for homogeneous graphs. Similar to text- or image-based inductive models for KGs, BIBREF22 duran2017learning, BIBREF23 yang2016revisiting, BIBREF24 velivckovic2017graph and BIBREF25 rossi2018deep exploit additional node attributes to embed unseen nodes. Another work more related to ours is BIBREF26 hamilton2017inductive. They tackle inductive node embedding by the neighborhood aggregation scheme. Their aggregators either trivially treat neighbors equally or unnecessarily require them to be ordered. Moreover, like all embedding models for homogeneous graphs, their model cannot be directly applied to KGs with multi-relational edges. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "17\n",
      "What are the three desired properties that neighborhood aggregators for KGs should possess?\n",
      " \n",
      "The desired properties for neighborhood aggregators for KGs are permutation invariance, capturing neighborhood redundancy, and modeling query relation at both relation- and neighbor-levels in a coarse-to-fine manner.\n",
      "Question : for the text Knowledge graphs (KGs) such as Freebase BIBREF0 , DBpedia BIBREF1 , and YAGO BIBREF2 play a critical role in various NLP tasks, including question answering BIBREF3 , information retrieval BIBREF4 , and personalized recommendation BIBREF5 . A typical KG consists of numerous facts about a predefined set of entities. Each fact is in the form of a triplet INLINEFORM0 (or INLINEFORM1 for short), where INLINEFORM2 and INLINEFORM3 are two entities and INLINEFORM4 is a relation the fact describes. Due to the discrete and incomplete natures of KGs, various KG embedding models are proposed to facilitate KG completion tasks, e.g., link prediction and triplet classification. After vectorizing entities and relations in a low-dimensional space, those models predict missing facts by manipulating the involved entity and relation embeddings..Although proving successful in previous studies, traditional KG embedding models simply ignore the evolving nature of KGs. They require all entities to be present when training the embeddings. However, BIBREF6 shi2018open suggest that, on DBpedia, 200 new entities emerge on a daily basis between late 2015 and early 2016. Given the infeasibility of retraining embeddings from scratch whenever new entities come, missing facts about emerging entities are, unfortunately, not guaranteed to be inferred in time..By transforming realistic networks, e.g., citation graphs, social networks, and protein interaction graphs, to simple graphs with single-typed and undirected edges, recent explorations BIBREF7 shed light on the evolution issue for homogeneous graphs. While learning embeddings for existing nodes, they inductively learn a neighborhood aggregator that represents a node by aggregating its neighbors' embeddings. The embeddings of unseen nodes can then be obtained by applying the aggregator on their existing neighbors..It is well received that KGs differ from homogeneous graphs by their multi-relational structure BIBREF8 . Despite the difference, it seems promising to generalize the neighborhood aggregating scheme to embed emerging KG entities in an inductive manner. For example, in Figure FIGREF1 , a news article may describe an emerging entity (marked gray) as well as some facts involving existing entities. By generalizing structural information in the underlying KG, e.g., other entities residing in a similar neighborhood or involving similar relations, to the current entity's neighborhood, we can infer that it may probably live in Chicago..Inspired by the above example, the inductive KG embedding problem boils down to designing a KG-specific neighborhood aggregator to capture essential neighborhood information. Intuitively, an ideal aggregator should have the following desired properties:.This paper concentrates on KG-specific neighborhood aggregators, which is of practical importance but only received limited focus BIBREF9 . To the best of our knowledge, neither conventional aggregators for homogeneous graphs nor those for KGs satisfy all the above three properties. In this regard, we employ the attention mechanism BIBREF10 and propose an aggregator called Logic Attention Network (LAN). Aggregating neighbors by a weighted combination of their transformed embeddings, LAN is inherently permutation invariant. To estimate the attention weights in LAN, we adopt two mechanisms to model relation- and neighbor-level information in a coarse-to-fine manner, At both levels, LAN is made aware of both neighborhood redundancy and query relation..To summarize, our contributions are: (1) We propose three desired properties that decent neighborhood aggregators for KGs should possess. (2) We propose a novel aggregator, i.e., Logic Attention Network, to facilitate inductive KG embedding. (3) We conduct extensive comparisons with conventional aggregators on two KG completions tasks. The results validate the superiority of LAN w.r.t. the three properties. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "18\n",
      "Question 1: What is the name of the novel aggregator discussed in the section?\n",
      "Answer 1: The novel aggregator discussed in the section is called Logic Attention Network (LAN).\n",
      "Question : for the text As discussed above, traditional neighborhood aggregators do not preserve all desired properties. In this section, we describe a novel aggregator, namely Logic Attention Network (LAN), which addresses all three properties. We also provide details in training the LAN aggregator. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "19\n",
      "What is the purpose of the neighborhood aggregator in a knowledge graph?\n",
      "The purpose of the neighborhood aggregator is to embed entities in a low-dimensional vector based on their neighborhood relations in the knowledge graph. This can be used to infer new facts about entities and to measure the plausibility of unknown triplets.\n",
      "Question : for the text Let INLINEFORM0 and INLINEFORM1 be two sets of entities and relations of size INLINEFORM2 and INLINEFORM3 , respectively. A knowledge graph is composed of a set of triplet facts, namely DISPLAYFORM0 .For each INLINEFORM0 , we denote the reverse of INLINEFORM1 by INLINEFORM2 , and add an additional triplet INLINEFORM3 to INLINEFORM4 ..For an entity INLINEFORM0 , we denote by INLINEFORM1 its neighborhood in INLINEFORM2 , i.e., all related entities with the involved relations. Formally, DISPLAYFORM0 .We denote the projection of INLINEFORM0 on INLINEFORM1 and INLINEFORM2 by INLINEFORM3 and INLINEFORM4 , respectively. Here INLINEFORM5 are neighbors and INLINEFORM6 are neighboring relations. When the context is clear, we simplify the INLINEFORM7 -th entity INLINEFORM8 by its subscript INLINEFORM9 . We denote vectors by bold lower letters, and matrices or sets of vectors by bold upper letters..Given a knowledge graph INLINEFORM0 , we would like to learn a neighborhood aggregator INLINEFORM1 that acts as follows:.For an entity INLINEFORM0 on INLINEFORM1 , INLINEFORM2 depends on INLINEFORM3 's neighborhood INLINEFORM4 to embed INLINEFORM5 as a low-dimensional vector INLINEFORM6 ;.For an unknown triplet INLINEFORM0 , the embeddings of INLINEFORM1 and INLINEFORM2 output by INLINEFORM3 suggest the plausibility of the triplet..When a new entity emerges with some triplets involving INLINEFORM0 and INLINEFORM1 , we could apply such an aggregator INLINEFORM2 on its newly established neighborhood, and use the output embedding to infer new facts about it. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "20\n",
      "Question 1: What is the purpose of using positive and negative triplets in training the entire model in Figure FIGREF12?\n",
      "\n",
      "Answer 1: The purpose of using positive and negative triplets in training the entire model in Figure FIGREF12 is to encourage the decoder to give high scores for positive triplets and low scores for negative ones. Positive triplets are naturally taken from the knowledge graph, while negative triplets are generated by randomly corrupting the object or subject of each positive triplet. The training objective is defined by applying a margin-based ranking loss on each triplet to optimize the output entity embeddings. To make the input embeddings and the aggregation more meaningful, a subtask is set up to define a second scoring function using input embeddings from the knowledge graph, and a margin-based ranking loss is defined for the subtask. Finally, the overall training objective is formulated by combining the main task and the subtask.\n",
      "Question : for the text To train the entire model in Figure FIGREF12 , we need both positive triplets and negative ones. All triplets INLINEFORM0 from the knowledge graph naturally serve as positive triplets, which we denote by INLINEFORM1 . To make up for the absence of negative triplets, for each INLINEFORM2 , we randomly corrupt the object or subject (but not both) by another entity in INLINEFORM3 , and denote the corresponding negative triplets by INLINEFORM4 . Formally, DISPLAYFORM0 .To encourage the decoder to give high scores for positive triplets and low scores for negative ones, we apply a margin-based ranking loss on each triplet INLINEFORM0 , i.e., DISPLAYFORM0 .Here INLINEFORM0 denotes the positive part of x, and INLINEFORM1 is a hyper-parameter for the margin. Finally, the training objective is defined by DISPLAYFORM0 .The above training objective only optimizes the output of the aggregator, i.e., the output entity embeddings INLINEFORM0 . The input entity embeddings INLINEFORM1 , however, are not directly aware of the structure of the entire KG. To make the input embeddings and thus the aggregation more meaningful, we set up a subtask for LAN..First, we define a second scoring function, which is similar to Eq. ( EQREF20 ) except that input embeddings INLINEFORM0 from INLINEFORM1 are used to represent the subject and object, i.e., DISPLAYFORM0 .The embedding of query relation INLINEFORM0 is obtained from the same embedding matrix INLINEFORM1 as in the first scoring function. Then a similar margin-based ranking loss INLINEFORM2 as Eq. ( EQREF32 ) is defined for the subtask. Finally, we combine the subtask with the main task, and reformulate the overall training objective of LAN as DISPLAYFORM0  generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "21\n",
      "What are some typical KG embedding models?\n",
      "\n",
      "Some typical KG embedding models include TransE, Distmult, Complex, and Analogy, among others.\n",
      "Question : for the text In recent years, representation learning problems on KGs have received much attention due to the wide applications of the resultant entity and relation embeddings. Typical KG embedding models include TransE BIBREF11 , Distmult BIBREF12 , Complex BIBREF13 , Analogy BIBREF14 , to name a few. For more explorations, we refer readers to an extensive survey BIBREF15 . However, conventional approaches on KG embedding work in a transductive manner. They require that all entities should be seen during training. Such limitation hinders them from efficiently generalizing to emerging entities. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "22\n",
      "Question 1: Who partially supported the work of Hao Wang and Yan Yang?\n",
      "\n",
      "Answer 1: The work of Hao Wang and Yan Yang was partially supported by a grant from the National Natural Science Foundation of China (No. 61572407).\n",
      "Question : for the text Hao Wang and Yan Yang's work was partially supported by a grant from the National Natural Science Foundation of China (No. 61572407). generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "23\n",
      "Question 1: How does the proposed CNN based model handle noisy labels during training?\n",
      "\n",
      "Answer 1: The proposed model learns to handle noisy labels during training by training two networks alternately and using the learned noisy transition matrices to tackle noisy labels.\n",
      "Question : for the text This paper proposed a novel CNN based model for sentence-level sentiment classification learning for data with noisy labels. The proposed model learns to handle noisy labels during training by training two networks alternately. The learned noisy transition matrices are used to tackle noisy labels. Experimental results showed that the proposed model outperforms a wide range of baselines markedly. We believe that learning with noisy labels is a promising direction as it is often easy to collect noisy-labeled training data. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "24\n",
      "What are the two types of experiments conducted to evaluate the performance of the proposed NetAb model?\n",
      "The two types of experiments conducted to evaluate the performance of the proposed NetAb model are: (1) corrupting clean-labeled datasets to produce noisy-labeled datasets to show the impact of noises on sentiment classification accuracy; (2) collecting real noisy data and using them to train models to evaluate the performance of NetAb.\n",
      "Question : for the text In this section, we evaluate the performance of the proposed NetAb model. we conduct two types of experiments. (1) We corrupt clean-labeled datasets to produce noisy-labeled datasets to show the impact of noises on sentiment classification accuracy. (2) We collect some real noisy data and use them to train models to evaluate the performance of NetAb..Clean-labeled Datasets. We use three clean labeled datasets. The first one is the movie sentence polarity dataset from BIBREF19. The other two datasets are laptop and restaurant datasets collected from SemEval-2016 . The former consists of laptop review sentences and the latter consists of restaurant review sentences. The original datasets (i.e., Laptop and Restaurant) were annotated with aspect polarity in each sentence. We used all sentences with only one polarity (positive or negative) for their aspects. That is, we only used sentences with aspects having the same sentiment label in each sentence. Thus, the sentiment of each aspect gives the ground-truth as the sentiments of all aspects are the same..For each clean-labeled dataset, the sentences are randomly partitioned into training set and test set with $80\\%$ and $20\\%$, respectively. Following BIBREF25, We also randomly select $10\\%$ of the test data for validation to check the model during training. Summary statistics of the training, validation, and test data are shown in Table TABREF9..Noisy-labeled Training Datasets. For the above three domains (movie, laptop, and restaurant), we collected 2,000 reviews for each domain from the same review source. We extracted sentences from each review and assigned review's label to its sentences. Like previous work, we treat 4 or 5 stars as positive and 1 or 2 stars as negative. The data is noisy because a positive (negative) review can contain negative (positive) sentences, and there are also neutral sentences. This gives us three noisy-labeled training datasets. We still use the same test sets as those for the clean-labeled datasets. Summary statistics of all the datasets are shown in Table TABREF9..Experiment 1: Here we use the clean-labeled data (i.e., the last three columns in Table TABREF9). We corrupt the clean training data by switching the labels of some random instances based on a noise rate parameter. Then we use the corrupted data to train NetAb and CNN BIBREF25..The test accuracy curves with the noise rates [0, $0.1$, $0.2$, $0.3$, $0.4$, $0.5$] are shown in Figure FIGREF13. From the figure, we can see that the test accuracy drops from around 0.8 to 0.5 when the noise rate increases from 0 to 0.5, but our NetAb outperforms CNN. The results clearly show that the performance of the CNN drops quite a lot with the noise rate increasing..Experiment 2: Here we use the real noisy-labeled training data to train our model and the baselines, and then test on the test data in Table TABREF9. Our goal is two fold. First, we want to evaluate NetAb using real noisy data. Second, we want to see whether sentences with review level labels can be used to build effective SSC models..Baselines. We use one strong non-DNN baseline, NBSVM (with unigrams or bigrams features) BIBREF23 and six DNN baselines. The first DNN baseline is CNN BIBREF25, which does not handle noisy labels. The other five were designed to handle noisy labels..The comparison results are shown in Table TABREF12. From the results, we can make the following observations. (1) Our NetAb model achieves the best ACC and F1 on all datasets except for F1 of negative class on Laptop. The results demonstrate the superiority of NetAb. (2) NetAb outperforms the baselines designed for learning with noisy labels. These baselines are inferior to ours as they were tailored for image classification. Note that we found no existing method to deal with noisy labels for SSC. Training Details. We use the publicly available pre-trained embedding GloVe.840B BIBREF48 to initialize the word vectors and the embedding dimension is 300..For each baseline, we obtain the system from its author and use its default parameters. As the DNN baselines (except CNN) were proposed for image classification, we change the input channels from 3 to 1. For our NetAb, we follow BIBREF25 to use window sizes of 3, 4 and 5 words with 100 feature maps per window size, resulting in 300-dimensional encoding vectors. The input length of sentence is set to 40. The network parameters are updated using the Adam optimizer BIBREF49 with a learning rate of 0.001. The learning rate is clipped gradually using a norm of 0.96 in performing the Adam optimization. The dropout rate is 0.5 in the input layer. The number of epochs is 200 and batch size is 50. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "25\n",
      "What is the problem that this paper aims to address?\n",
      "\n",
      "The paper aims to address the problem of learning accurate sentence-level sentiment classifiers even when the training data is noisy-labeled due to subjective sentiment annotation or disagreements among annotators.\n",
      "Question : for the text It is well known that sentiment annotation or labeling is subjective BIBREF0. Annotators often have many disagreements. This is especially so for crowd-workers who are not well trained. That is why one always feels that there are many errors in an annotated dataset. In this paper, we study whether it is possible to build accurate sentiment classifiers even with noisy-labeled training data. Sentiment classification aims to classify a piece of text according to the polarity of the sentiment expressed in the text, e.g., positive or negative BIBREF1, BIBREF0, BIBREF2. In this work, we focus on sentence-level sentiment classification (SSC) with labeling errors..As we will see in the experiment section, noisy labels in the training data can be highly damaging, especially for DNNs because they easily fit the training data and memorize their labels even when training data are corrupted with noisy labels BIBREF3. Collecting datasets annotated with clean labels is costly and time-consuming as DNN based models usually require a large number of training examples. Researchers and practitioners typically have to resort to crowdsourcing. However, as mentioned above, the crowdsourced annotations can be quite noisy. Research on learning with noisy labels dates back to 1980s BIBREF4. It is still vibrant today BIBREF5, BIBREF6, BIBREF7, BIBREF8, BIBREF9, BIBREF10, BIBREF11, BIBREF12 as it is highly challenging. We will discuss the related work in the next section..This paper studies the problem of learning with noisy labels for SSC. Formally, we study the following problem..Problem Definition: Given noisy labeled training sentences $S=\\lbrace (x_1,y_1),...,(x_n,y_n)\\rbrace $, where $x_i|_{i=1}^n$ is the $i$-th sentence and $y_i\\in \\lbrace 1,...,c\\rbrace $ is the sentiment label of this sentence, the noisy labeled sentences are used to train a DNN model for a SSC task. The trained model is then used to classify sentences with clean labels to one of the $c$ sentiment labels..In this paper, we propose a convolutional neural Network with Ab-networks (NetAb) to deal with noisy labels during training, as shown in Figure FIGREF2. We will introduce the details in the subsequent sections. Basically, NetAb consists of two convolutional neural networks (CNNs) (see Figure FIGREF2), one for learning sentiment scores to predict `clean' labels and the other for learning a noise transition matrix to handle input noisy labels. We call the two CNNs A-network and Ab-network, respectively. The fundamental here is that (1) DNNs memorize easy instances first and gradually adapt to hard instances as training epochs increase BIBREF3, BIBREF13; and (2) noisy labels are theoretically flipped from the clean/true labels by a noise transition matrix BIBREF14, BIBREF15, BIBREF16, BIBREF17. We motivate and propose a CNN model with a transition layer to estimate the noise transition matrix for the input noisy labels, while exploiting another CNN to predict `clean' labels for the input training (and test) sentences. In training, we pre-train A-network in early epochs and then train Ab-network and A-network with their own loss functions in an alternating manner. To our knowledge, this is the first work that addresses the noisy label problem in sentence-level sentiment analysis. Our experimental results show that the proposed model outperforms the state-of-the-art methods. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "26\n",
      "Question 1: What is the key idea of the proposed model NetAb?\n",
      "\n",
      "Answer 1: The key idea of the proposed model NetAb is to train two CNNs alternately, one for addressing the input noisy labels and the other for predicting `clean' labels. The CNN performing noisy label estimation also estimates the noisy transition matrix to deal with noisy labels, and both CNNs share most of their parameters except for the Gate unit and the clean loss. The A-network predicts `clean' labels, and the Ab-network handles the input noisy labels.\n",
      "Question : for the text Our model builds on CNN BIBREF25. The key idea is to train two CNNs alternately, one for addressing the input noisy labels and the other for predicting `clean' labels. The overall architecture of the proposed model is given in Figure FIGREF2. Before going further, we first introduce a proposition, a property, and an assumption below..Proposition 1 Noisy labels are flipped from clean labels by an unknown noise transition matrix..Proposition UNKREF3 is reformulated from BIBREF16 and has been investigated in BIBREF14, BIBREF15, BIBREF41. This proposition shows that if we know the noise transition matrix, we can use it to recover the clean labels. In other words, we can put noise transition matrix on clean labels to deal with noisy labels. Given these, we ask the following question: How to estimate such an unknown noise transition matrix?.Below we give a solution to this question based on the following property of DNNs..Property 1 DNNs tend to prioritize memorization of simple instances first and then gradually memorize hard instances BIBREF3..BIBREF13 further investigated this property of DNNs. Our setting is that simple instances are sentences of clean labels and hard instances are those with noisy labels. We also have the following assumption..Assumption 1 The noise rate of the training data is less than $50\\%$..This assumption is usually satisfied in practice because without it, it is hard to tackle the input noisy labels during training..Based on the above preliminaries, we need to estimate the noisy transition matrix $Q\\in \\mathbb {R}^{c\\times c}$ ($c=2$ in our case, i.e., positive and negative), and train two classifiers $\\ddot{y}\\sim P(\\ddot{y}|x,\\theta )$ and $\\widehat{y}\\sim \\ P(\\widehat{y}|x,\\vartheta )$, where $x$ is an input sentence, $\\ddot{y}$ is its noisy label, $\\widehat{y}$ is its `clean' label, $\\theta $ and $\\vartheta $ are the parameters of two classifiers. Note that both $\\ddot{y}$ and $\\widehat{y}$ here are the prediction results from our model, not the input labels. We propose to formulate the probability of the sentence $x$ labeled as $j$ with.where $P(\\ddot{y}=j|\\widehat{y}=i)$ is an item (the $ji$-th item) in the noisy transition matrix $Q$. We can see that the noisy transition matrix $Q$ is exploited on the `clean' scores $P(\\widehat{y}|x,\\vartheta )$ to tackle noisy labels..We now present our model NetAb and introduce how NetAb performs Eq. (DISPLAY_FORM6). As shown in Figure FIGREF2, NetAb consists of two CNNs. The intuition here is that we use one CNN to perform $P(\\widehat{y}=i|x,\\vartheta )$ and use another CNN to perform $P(\\ddot{y}=j|x,\\theta )$. Meanwhile, the CNN performing $P(\\ddot{y}=j|x,\\theta )$ estimates the noise transition matrix $Q$ to deal with noisy labels. Thus we add a transition layer into this CNN..More precisely, in Figure FIGREF2, the CNN with a clean loss performs $P(\\widehat{y}=i|x,\\vartheta )$. We call this CNN the A-network. The other CNN with a noisy loss performs $P(\\ddot{y}=j|x,\\theta )$. We call this CNN the Ab-network. Ab-network shares all the parameters of A-network except the parameters from the Gate unit and the clean loss. In addition, Ab-network has a transition layer to estimate the noisy transition matrix $Q$. In such a way, A-network predict `clean' labels, and Ab-network handles the input noisy labels..We use cross-entropy with the predicted labels $\\ddot{y}$ and the input labels $y$ (given in the dataset) to compute the noisy loss, formulated as below.where $\\mathbb {I}$ is the indicator function (if $y\\!==\\!i$, $\\mathbb {I}\\!=\\!1$; otherwise, $\\mathbb {I}\\!=\\!0$), and $|\\ddot{S}|$ is the number of sentences to train Ab-network in each batch..Similarly, we use cross-entropy with the predicted labels $\\widehat{y}$ and the input labels $y$ to compute the clean loss, formulated as.where $|\\widehat{S}|$ is the number of sentences to train A-network in each batch..Next we introduce how our model learns the parameters ($\\vartheta $, $\\theta $ and $Q$). An embedding matrix $v$ is produced for each sentence $x$ by looking up a pre-trained word embedding database (e.g., GloVe.840B BIBREF48). Then an encoding vector $h\\!=\\!CNN(v)$ (and $u\\!=\\!CNN(v)$) is produced for each embedding matrix $v$ in A-network (and Ab-network). A sofmax classifier gives us $P(\\hat{y}\\!=\\!i|x,\\vartheta )$ (i.e., `clean' sentiment scores) on the learned encoding vector $h$. As the noise transition matrix $Q$ indicates the transition values from clean labels to noisy labels, we compute $Q$ as follows.where $W_i$ is a trainable parameter matrix, $b_i$ and $f_i$ are two trainable parameter vectors. They are trained in the Ab-network. Finally, $P(\\ddot{y}=j|x,\\theta )$ is computed by Eq. (DISPLAY_FORM6)..In training, NetAb is trained end-to-end. Based on Proposition UNKREF3 and Property UNKREF4, we pre-train A-network in early epochs (e.g., 5 epochs). Then we train Ab-network and A-network in an alternating manner. The two networks are trained using their respective cross-entropy loss. Given a batch of sentences, we first train Ab-network. Then we use the scores predicted from A-network to select some possibly clean sentences from this batch and train A-network on the selected sentences. Specifically speaking, we use the predicted scores to compute sentiment labels by $\\arg \\max _i \\lbrace \\ddot{y}=i|\\ddot{y}\\sim P(\\ddot{y}|x,\\theta )\\rbrace $. Then we select the sentences whose resulting sentiment label equals to the input label. The selection process is marked by a Gate unit in Figure FIGREF2. When testing a sentence, we use A-network to produce the final classification result. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "27\n",
      "What are the three main directions explored by DNNs based approaches to learning with noisy labels?\n",
      "\n",
      "The three main directions explored by DNNs based approaches to learning with noisy labels are (1) training DNNs on selected samples, (2) modifying the loss function of DNNs with regularization biases, and (3) plugging an extra layer into DNNs.\n",
      "Question : for the text Our work is related to sentence sentiment classification (SSC). SSC has been studied extensively BIBREF18, BIBREF19, BIBREF20, BIBREF21, BIBREF22, BIBREF23, BIBREF24, BIBREF25, BIBREF26, BIBREF27, BIBREF28. None of them can handle noisy labels. Since many social media datasets are noisy, researchers have tried to build robust models BIBREF29, BIBREF30, BIBREF31. However, they treat noisy data as additional information and don't specifically handle noisy labels. A noise-aware classification model in BIBREF12 trains using data annotated with multiple labels. BIBREF32 exploited the connection of users and noisy labels of sentiments in social networks. Since the two works use multiple-labeled data or users' information (we only use single-labeled data, and we do not use any additional information), they have different settings than ours..Our work is closely related to DNNs based approaches to learning with noisy labels. DNNs based approaches explored three main directions: (1) training DNNs on selected samples BIBREF33, BIBREF34, BIBREF35, BIBREF17, (2) modifying the loss function of DNNs with regularization biases BIBREF5, BIBREF36, BIBREF37, BIBREF38, BIBREF39, BIBREF40, and (3) plugging an extra layer into DNNs BIBREF14, BIBREF41, BIBREF15, BIBREF16. All these approaches were proposed for image classification where training images were corrupted with noisy labels. Some of them require noise rate to be known a priori in order to tune their models during training BIBREF37, BIBREF17. Our approach combines direction (1) and direction (3), and trains two networks jointly without knowing the noise rate. We have used five latest existing methods in our experiments for SSC. The experimental results show that they are inferior to our proposed method. In addition, BIBREF42, BIBREF43, BIBREF44, BIBREF45, BIBREF46, and BIBREF47 studied weakly-supervised DNNs or semi-supervised DNNs. But they still need some clean-labeled training data. We use no clean-labeled data. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "28\n",
      "Question 1: Who provided financial support for the study?\n",
      "Answer 1: Huawei Technologies Co., Ltd. provided financial support for the study.\n",
      "Question : for the text We thank our anonymous reviewers, study participants, and Huawei Technologies Co., Ltd. for financial support. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "29\n",
      "What model performs the best overall in Table TABREF50?\n",
      "\n",
      "The SMERTI-Transformer model performs the best overall in Table TABREF50, with SMERTI-RNN as the second-best performing model.\n",
      "Question : for the text As seen in Table TABREF50, the SMERTI variations outperform all baseline models overall, particularly in RE Match. SMERTI-Transformer performs the best, with SMERTI-RNN second. The WordNet models achieve high Sentiment Preservation, but much lower on RE Match. W2V-STEM achieves comparably high RE Match, but lowest Fluency..These results correspond well with our automatic evaluation results in Table TABREF38. We look at the Pearson correlation values between RE Match, Fluency, and Sentiment Preservation with CSS, SLOR, and SPA, respectively. These are 0.9952, 0.9327, and 0.8768, respectively, demonstrating that our automatic metrics are highly effective and correspond well with human ratings. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "30\n",
      "Question 1: Which model performs the worst overall when it comes to STE, and why?\n",
      "\n",
      "Answer 1: The WordNet models perform the worst overall when it comes to STE. This is likely because they modify little of the text and only verbs and nouns, leading to grammatically incorrect text that flows poorly. Additionally, they do not account for context, and many words (such as proper nouns) do not exist in WordNet.\n",
      "Question : for the text As seen in Table TABREF38, both SMERTI variations achieve higher STES and outperform the other models overall, with the WordNet models performing the worst. SMERTI excels especially on fluency and content similarity. The transformer variation achieves slightly higher SLOR, while the RNN variation achieves slightly higher CSS. The WordNet models perform strongest in sentiment preservation (SPA), likely because they modify little of the text and only verbs and nouns. They achieve by far the lowest CSS, likely in part due to this limited text replacement. They also do not account for context, and many words (e.g. proper nouns) do not exist in WordNet. Overall, the WordNet models are not very effective at STE..W2V-STEM achieves the lowest SLOR, especially for higher RRT, as supported by the example in Table TABREF41 (see also Appendix F). W2V-STEM and WordNet models output grammatically incorrect text that flows poorly. In many cases, words are repeated multiple times. We analyze the average Type Token Ratio (TTR) values of each model's outputs, which is the ratio of unique divided by total words. As shown in Table TABREF52, the SMERTI variations achieve the highest TTR, while W2V-STEM and NWN-STEM the lowest..Note that while W2V-STEM achieves lower CSS than SMERTI, it performs comparably in this aspect. This is likely due to its vector arithmetic operations algorithm, which replaces each word with one more similar to the RE. This is also supported by the lower TTR, as W2V-STEM frequently outputs the same words multiple times. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "31\n",
      "Question 1: Why does the CSS value for news headlines appear to be slightly higher in SMERTI's analysis?\n",
      "\n",
      "Answer 1: According to the text, the CSS value for news headlines appears to be slightly higher in SMERTI's analysis because news headlines are typically shorter and designed to be attention grabbers, carrying more semantic meaning.\n",
      "Question : for the text As seen in Table TABREF58, SMERTI's SPA is lowest for news headlines. Amazon and Yelp reviews naturally carry stronger sentiment, likely making it easier to generate text with similar sentiment..Both SMERTI's and the input text's SLOR appear to be lower for Yelp reviews. This may be due to many reasons, such as more typos and emojis within the original reviews, and so forth..SMERTI's CSS values are slightly higher for news headlines. This may be due to them typically being shorter and carrying more semantic meaning as they are designed to be attention grabbers..Overall, it seems that using datasets which inherently carry more sentiment will lead to better sentiment preservation. Further, the quality of the dataset's original text, unsurprisingly, influences the ability of SMERTI to generate fluent text. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "32\n",
      "What is the relationship between MRT/RRT and SMERTI's SPA, SLOR, and CSS?\n",
      "\n",
      "As MRT/RRT increases, SMERTI's SPA and SLOR decrease while CSS increases. These relationships are supported by strong Pearson correlation values of -0.9972, -0.9183, and 0.9078, respectively.\n",
      "Question : for the text From Table TABREF60, it can be seen that as MRT/RRT increases, SMERTI's SPA and SLOR decrease while CSS increases. These relationships are very strong as supported by the Pearson correlation values of -0.9972, -0.9183, and 0.9078, respectively. When SMERTI can alter more text, it has the opportunity to replace more related to sentiment while producing more of semantic similarity to the $RE$..Further, SMERTI generates more of the text itself, becoming less similar to the human-written input, resulting in lower fluency. To further demonstrate this, we look at average SMERTI BLEU BIBREF36 scores against MRT/RRT, shown in Table TABREF60. BLEU generally indicates how close two pieces of text are in content and structure, with higher values indicating greater similarity. We report our final BLEU scores as the average scores of 1 to 4-grams. As expected, BLEU decreases as MRT/RRT increases, and this relationship is very strong as supported by the Pearson correlation value of -0.9960..It is clear that MRT/RRT represents a trade-off between CSS against SPA and SLOR. It is thus an adjustable parameter that can be used to control the generated text, and balance semantic exchange against fluency and sentiment preservation. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "33\n",
      "What is SMERTI's SPA value for adjectives? \n",
      "\n",
      "Answer 1: SMERTI's SPA value is lowest for adjectives, likely because they typically carry the most sentiment.\n",
      "Question : for the text As seen from Table TABREF55 , SMERTI's SPA values are highest for nouns, likely because they typically carry little sentiment, and lowest for adjectives, likely because they typically carry the most..SLOR is lowest for adjectives and highest for phrases and nouns. Adjectives typically carry less semantic meaning and SMERTI likely has more trouble figuring out how best to infill the text. In contrast, nouns typically carry more, and phrases the most (since they consist of multiple words)..SMERTI's CSS is highest for phrases then nouns, likely due to phrases and nouns carrying more semantic meaning, making it easier to generate semantically similar text. Both SMERTI's and the input text's CSS are lowest for adjectives, likely because they carry little semantic meaning..Overall, SMERTI appears to be more effective on nouns and phrases than verbs and adjectives. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "34\n",
      "Question 1: What are some potential directions for future work in semantic text exchange (STE)?\n",
      "\n",
      "Answer 1: Some potential directions for future work in STE include adding specific methods to control sentiment, fine-tuning SMERTI for preservation of persona or personality, and experimenting with other text infilling models such as fine-tuning BERT. Additionally, a larger and more diverse participant pool is needed for human evaluation.\n",
      "Question : for the text We introduced the task of semantic text exchange (STE), demonstrated that our pipeline SMERTI performs well on STE, and proposed an STES metric for evaluating overall STE performance. SMERTI outperformed other models and was the most balanced overall. We also showed a trade-off between semantic exchange against fluency and sentiment preservation, which can be controlled by the masking (replacement) rate threshold..Potential directions for future work include adding specific methods to control sentiment, and fine-tuning SMERTI for preservation of persona or personality. Experimenting with other text infilling models (e.g. fine-tuning BERT BIBREF8) is also an area of exploration. Lastly, our human evaluation is limited in size and a larger and more diverse participant pool is needed..We conclude by addressing potential ethical misuses of STE, including assisting in the generation of spam and fake-reviews/news. These risks come with any intelligent chatbot work, but we feel that the benefits, including usage in the detection of misuse such as fake-news, greatly outweigh the risks and help progress NLP and AI research. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "35\n",
      "Question 1: According to Table TABREF41, which model is able to generate high quality output text similar to the $RE$ while flowing better than other models' outputs?\n",
      "\n",
      "Answer 1: SMERTI is the model that is able to generate high quality output text similar to the $RE$ while flowing better than other models' outputs, as observed from Table TABREF41.\n",
      "Question : for the text Table TABREF38 shows overall average results by model. Table TABREF41 shows outputs for a Yelp example..As observed from Table TABREF41 (see also Appendix F), SMERTI is able to generate high quality output text similar to the $RE$ while flowing better than other models' outputs. It can replace entire phrases and sentences due to its variable length infilling. Note that for nouns, the outputs from GWN-STEM and NWN-STEM are equivalent. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "36\n",
      "What is the reasoning behind filtering the verbs and adjectives through a list of sentiment words?\n",
      "\n",
      "Answer 1: The verbs and adjectives are filtered through a list of sentiment words to ensure that evaluation REs are not chosen that would significantly alter the text's sentiment. This is done by using a sentiment word list to filter out any REs that could be biased towards a positive or negative sentiment.\n",
      "Question : for the text We manually select 10 nouns, 10 verbs, 10 adjectives, and 5 phrases from the top 10% most frequent words/phrases in each test set as our evaluation $RE$s. We filter the verbs and adjectives through a list of sentiment words BIBREF30 to ensure we do not choose $RE$s that would obviously significantly alter the text's sentiment..For each evaluation $RE$, we choose one-hundred lines from the corresponding test set that does not already contain $RE$. We choose lines with at least five words, as many with less carry little semantic meaning (e.g. `Great!', `It is okay'). For Amazon and Yelp, we choose 50 positive and 50 negative lines per $RE$. We repeat this process three times, resulting in three sets of 1000 lines per dataset per POS (excluding phrases), and three sets of 500 lines per dataset for phrases. Our final results are averaged metrics over these three sets..For SMERTI-Transformer, SMERTI-RNN, and W2V-STEM, we generate four outputs per text for MRT/RRT of 20%, 40%, 60%, and 80%, which represent upper-bounds on the percentage of the input that can be masked and/or replaced. Note that NWN-STEM and GWN-STEM can only evaluate on limited POS and their maximum replacement rates are limited. We select MINsim values of 0.075 and 0 for nouns and 0.1 and 0 for verbs, as these result in replacement rates approximately equal to the actual MR/RR of the other models' outputs for 20% and 40% MRT/RRT, respectively. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "37\n",
      "Question 1: How is Sentiment Preservation calculated for each model?\n",
      "\n",
      "Answer 1: Sentiment Preservation is calculated by comparing the average Sentiment rating for each model's output text to the Sentiment rating of the input text, and both should fall under the negative range of less than 2.5, neutral range of 2.5 to 3.5 inclusive or positive range of greater than 3.5. If this condition is met, it is counted as a valid case of Sentiment Preservation, and this is repeated for every evaluation line to calculate the final values per model.\n",
      "Question : for the text Average human evaluation scores are displayed in Table TABREF50. Sentiment Preservation (between 0 and 1) is calculated by comparing the average Sentiment rating for each model's output text to the Sentiment rating of the input text, and if both are less than 2.5 (negative), between 2.5 and 3.5 inclusive (neutral), or greater than 3.5 (positive), this is counted as a valid case of Sentiment Preservation. We repeat this for every evaluation line to calculate the final values per model. Harmonic means of all three metrics (using rescaled 0-1 values of RE Match and Fluency) are also displayed. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "38\n",
      "Question 1: How many participants were involved in the human evaluation and what was their affiliation?\n",
      "\n",
      "Answer 1: There were eight participants involved in the human evaluation, six males and two females, who were affiliated project researchers aged 20-39 at the University of Waterloo.\n",
      "Question : for the text We conduct a human evaluation with eight participants, 6 males and 2 females, that are affiliated project researchers aged 20-39 at the University of Waterloo. We randomly choose one evaluation line for a randomly selected word or phrase for each POS per dataset. The input text and each model's output (for 40% MRT/RRT - chosen as a good middle ground) for each line is presented to participants, resulting in a total of 54 pieces of text, and rated on the following criteria from 1-5:.RE Match: “How related is the entire text to the concept of [X]\", where [X] is a word or phrase (1 - not at all related, 3 - somewhat related, 5 - very related). Note here that [X] is a given $RE$..Fluency: “Does the text make sense and flow well?\" (1 - not at all, 3 - somewhat, 5 - very).Sentiment: “How do you think the author of the text was feeling?\" (1 - very negative, 3 - neutral, 5 - very positive).Each participant evaluates every piece of text. They are presented with a single piece of text at a time, with the order of models, POS, and datasets completely randomized. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "39\n",
      "What does SLOR measure?\n",
      "SLOR measures aspects of text fluency such as grammaticality. Higher values represent higher fluency.\n",
      "Question : for the text Fluency (SLOR) We use syntactic log-odds ratio (SLOR) BIBREF31 for sentence level fluency and modify from their word-level formula to character-level ($SLOR_{c}$). We use Flair perplexity values from a language model trained on the One Billion Words corpus BIBREF32:.where $|S|$ and $|w|$ are the character lengths of the input text $S$ and the word $w$, respectively, $p_M(S)$ and $p_M(w)$ are the probabilities of $S$ and $w$ under the language model $M$, respectively, and $PPL_S$ and $PPL_w$ are the character-level perplexities of $S$ and $w$, respectively. SLOR (from hereon we refer to character-level SLOR as simply SLOR) measures aspects of text fluency such as grammaticality. Higher values represent higher fluency..We rescale resulting SLOR values to the interval [0,1] by first fitting and normalizing a Gaussian distribution. We then truncate normalized data points outside [-3,3], which shifts approximately 0.69% of total data. Finally, we divide each data point by six and add 0.5 to each result..Sentiment Preservation Accuracy (SPA) is defined as the percentage of outputs that carry the same sentiment as the input. We use VADER BIBREF33 to evaluate sentiment as positive, negative, or neutral. It handles typos, emojis, and other aspects of online text. Content Similarity Score (CSS) ranges from 0 to 1 and indicates the semantic similarity between generated text and the $RE$. A value closer to 1 indicates stronger semantic exchange, as the output is closer in semantic content to the $RE$. We also use the USE for this due to its design and strong performance as previously mentioned. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "40\n",
      "Question 1: What is the Semantic Text Exchange Score (STES)?\n",
      "\n",
      "Answer 1: The Semantic Text Exchange Score (STES) is a single score that evaluates the overall performance of a model on STE by combining the key evaluation metrics. It uses the harmonic mean, similar to the F1 score, and ranges between 0 and 1. STES penalizes models that perform poorly in one or more metrics and favors balanced models achieving strong results in all three metrics.\n",
      "Question : for the text We come up with a single score to evaluate overall performance of a model on STE that combines the key evaluation metrics. It uses the harmonic mean, similar to the F1 score (or F-score) BIBREF34, BIBREF35, and we call it the Semantic Text Exchange Score (STES):.where $A$ is SPA, $B$ is SLOR, and $C$ is CSS. STES ranges between 0 and 1, with scores closer to 1 representing higher overall performance. Like the F1 score, STES penalizes models which perform very poorly in one or more metrics, and favors balanced models achieving strong results in all three. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "41\n",
      "Question 1: What is the difference between NWN-STEM and GWN-STEM?\n",
      "\n",
      "Answer 1: NWN-STEM replaces only similar nouns in the text with nouns extracted from the associated reference review set, whereas GWN-STEM replaces nouns, verbs, and adjectives in the text similar to the $RE$ with those extracted from the associated reference review set. In other words, GWN-STEM works for all three types of words (nouns, verbs, and adjectives), while NWN-STEM works only for nouns.\n",
      "Question : for the text We implement three models to benchmark against. First is NWN-STEM (Algorithm 2 from BIBREF20). We use the training sets as the “reference review sets\" to extract similar nouns to the $RE$ (using MINsim = 0.1). We then replace nouns in the text similar to the $RE$ with nouns extracted from the associated reference review set..Secondly, we modify NWN-STEM to work for verbs and adjectives, and call this GWN-STEM. From the reference review sets, we extract similar nouns, verbs, and adjectives to the $RE$ (using MINsim = 0.1), where the $RE$ is now not restricted to being a noun. We replace nouns, verbs, and adjectives in the text similar to the $RE$ with those extracted from the associated reference review set..Lastly, we implement W2V-STEM using Gensim BIBREF29. We train uni-gram Word2Vec models for single word $RE$s, and four-gram models for phrases. Models are trained on the training sets. We use cosine similarity to determine the most similar word/phrase in the input text to $RE$, which is the replaced $OE$. For all other words/phrases, we calculate $w_{i}^{\\prime } = w_{i} - w_{OE} + w_{RE}$, where $w_{i}$ is the original word/phrase's embedding vector, $w_{OE}$ is the $OE$'s, $w_{RE}$ is the $RE$'s, and $w_{i}^{\\prime }$ is the resulting embedding vector. The replacement word/phrase is $w_{i}^{\\prime }$'s nearest neighbour. We use similarity thresholds to adjust replacement rates (RR) and produce text under various replacement rate thresholds (RRT). generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "42\n",
      "Question 1: What is the criteria used to classify reviews as positive, neutral or negative in the Amazon and Yelp datasets?\n",
      "\n",
      "Answer 1: Reviews greater than three stars are treated as containing positive sentiment, equal to three stars as neutral, and less than three stars as negative in the Amazon and Yelp datasets.\n",
      "Question : for the text We train our two TIMs on the three datasets. The Amazon dataset BIBREF1 contains over 83 million user reviews on products, with duplicate reviews removed. The Yelp dataset includes over six million user reviews on businesses. The news headlines dataset from Kaggle contains approximately $200,000$ news headlines from 2012 to 2018 obtained from HuffPost BIBREF2..We filter the text to obtain reviews and headlines which are English, do not contain hyperlinks and other obvious noise, and are less than 20 words long. We found that many longer than twenty words ramble on and are too verbose for our purposes. Rather than filtering by individual sentences we keep each text in its entirety so SMERTI can learn to generate multiple sentences at once. We preprocess the text by lowercasing and removing rare/duplicate punctuation and space..For Amazon and Yelp, we treat reviews greater than three stars as containing positive sentiment, equal to three stars as neutral, and less than three stars as negative. For each training and testing set, we include an equal number of randomly selected positive and negative reviews, and half as many neutral reviews. This is because neutral reviews only occupy one out of five stars compared to positive and negative which occupy two each. Our dataset statistics can be found in Appendix B. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "43\n",
      "Question 1: How does the masked text approach work in the training of TIM models for text infilling?\n",
      "\n",
      "Answer 1: In the masked text approach, a tiered masking approach is used where 15%, 30%, and 45% of the words are randomly masked in one-third of the lines of each dataset. These masked texts serve as inputs, while the original texts serve as ground-truth. This allows the TIM models to learn relationships between masked and unmasked words. The bidirectional RNN decoder fills in the blanks one by one, and the transformer uses scaled dot-product attention with the same hyperparameters as BIBREF27 to minimize the cross entropy loss between its output and the ground-truth.\n",
      "Question : for the text To set up our training and testing data for text infilling, we mask the text. We use a tiered masking approach: for each dataset, we randomly mask 15% of the words in one-third of the lines, 30% of the words in another one-third, and 45% in the remaining one-third. These masked texts serve as the inputs, while the original texts serve as the ground-truth. This allows our TIM models to learn relationships between masked words and relationships between masked and unmasked words..The bidirectional RNN decoder fills in blanks one by one, with the objective of minimizing the cross entropy loss between its output and the ground-truth. We use a hidden size of 500, two layers for the encoder and decoder, teacher-forcing ratio of 1.0, learning rate of 0.0001, dropout of 0.1, batch size of 64, and train for up to 40 epochs..For the transformer, we use scaled dot-product attention and the same hyperparameters as BIBREF27. We use the Adam optimizer BIBREF28 with $\\beta _1 = 0.9, \\beta _2 = 0.98$, and $\\epsilon = 10^{-9}$. As in BIBREF27, we increase the $learning\\_rate$ linearly for the first $warmup\\_steps$ training steps, and then decrease the $learning\\_rate$ proportionally to the inverse square root of the step number. We set $factor=1$ and use $warmup\\_steps = 2000$. We use a batch size of 4096, and we train for up to 40 epochs. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "44\n",
      "What is the proposed pipeline for semantic text exchange? \n",
      "\n",
      "The proposed pipeline for semantic text exchange is called SMERTI (pronounced `smarty') and it combines entity replacement (ER), similarity masking (SM), and text infilling (TI) to modify the semantic content of text.\n",
      "Question : for the text There has been significant research on style transfer, with the goal of changing the style of text while preserving its semantic content. The alternative where semantics are adjusted while keeping style intact, which we call semantic text exchange (STE), has not been investigated to the best of our knowledge. Consider the following example, where the replacement entity defines the new semantic context:.Original Text: It is sunny outside! Ugh, that means I must wear sunscreen. I hate being sweaty and sticky all over. Replacement Entity: weather = rainy Desired Text: It is rainy outside! Ugh, that means I must bring an umbrella. I hate being wet and having to carry it around..The weather within the original text is sunny, whereas the actual weather may be rainy. Not only is the word sunny replaced with rainy, but the rest of the text's content is changed while preserving its negative sentiment and fluency. With the rise of natural language processing (NLP) has come an increased demand for massive amounts of text data. Manually collecting and scraping data requires a significant amount of time and effort, and data augmentation techniques for NLP are limited compared to fields such as computer vision. STE can be used for text data augmentation by producing various modifications of a piece of text that differ in semantic content..Another use of STE is in building emotionally aligned chatbots and virtual assistants. This is useful for reasons such as marketing, overall enjoyment of interaction, and mental health therapy. However, due to limited data with emotional content in specific semantic contexts, the generated text may contain incorrect semantic content. STE can adjust text semantics (e.g. to align with reality or a specific task) while preserving emotions..One specific example is the development of virtual assistants with adjustable socio-emotional personalities in the effort to construct assistive technologies for persons with cognitive disabilities. Adjusting the emotional delivery of text in subtle ways can have a strong effect on the adoption of the technologies BIBREF0. It is challenging to transfer style this subtly due to lack of datasets on specific topics with consistent emotions. Instead, large datasets of emotionally consistent interactions not confined to specific topics exist. Hence, it is effective to generate text with a particular emotion and then adjust its semantics..We propose a pipeline called SMERTI (pronounced `smarty') for STE. Combining entity replacement (ER), similarity masking (SM), and text infilling (TI), SMERTI can modify the semantic content of text. We define a metric called the Semantic Text Exchange Score (STES) that evaluates the overall ability of a model to perform STE, and an adjustable parameter masking (replacement) rate threshold (MRT/RRT) that can be used to control the amount of semantic change..We evaluate on three datasets: Yelp and Amazon reviews BIBREF1, and Kaggle news headlines BIBREF2. We implement three baseline models for comparison: Noun WordNet Semantic Text Exchange Model (NWN-STEM), General WordNet Semantic Text Exchange Model (GWN-STEM), and Word2Vec Semantic Text Exchange Model (W2V-STEM)..We illustrate the STE performance of two SMERTI variations on the datasets, demonstrating outperformance of the baselines and pipeline stability. We also run a human evaluation supporting our results. We analyze the results in detail and investigate relationships between the semantic change, fluency, sentiment, and MRT/RRT. Our major contributions can be summarized as:.We define a new task called semantic text exchange (STE) with increasing importance in NLP applications that modifies text semantics while preserving other aspects such as sentiment..We propose a pipeline SMERTI capable of multi-word entity replacement and text infilling, and demonstrate its outperformance of baselines..We define an evaluation metric for overall performance on semantic text exchange called the Semantic Text Exchange Score (STES). generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "45\n",
      "Question 1: What is the difference between BIBREF16 and BIBREF17-BIBREF19 in terms of generating reviews?\n",
      "\n",
      "Answer 1: BIBREF16 generates reviews from scratch using language models, while BIBREF17-BIBREF19 generate reviews using auxiliary information such as item category and star rating. BIBREF20 also generates reviews from scratch, but includes a review customization component that modifies the generated review to fit a new topic or context.\n",
      "Question : for the text BIBREF16 generates fake reviews from scratch using language models. BIBREF17, BIBREF18, BIBREF19 generate reviews from scratch given auxiliary information (e.g. the item category and star rating). BIBREF20 generates reviews using RNNs with two components: generation from scratch and review customization (Algorithm 2 in BIBREF20). They define review customization as modifying the generated review to fit a new topic or context, such as from a Japanese restaurant to an Italian one. They condition on a keyword identifying the desired context, and replace similar nouns with others using WordNet BIBREF21. They require a “reference dataset\" (required to be “on topic\"; easy enough for restaurant reviews, but less so for arbitrary conversational agents). As noted by BIBREF19, the method of BIBREF20 may also replace words independently of context. We implement their review customization algorithm (NWN-STEM) and a modified version (GWN-STEM) as baseline models. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "46\n",
      "Question 1: What is the goal of style/sentiment transfer models?\n",
      "\n",
      "Answer 1: The goal of style/sentiment transfer models is to learn latent representations of various text aspects such as context and attributes, separate style from content, and encode them into hidden representations to generate a new sentence with a targeted sentiment attribute using an RNN decoder. Notable works in this area include BIBREF12, BIBREF13, BIBREF14, and BIBREF15.\n",
      "Question : for the text Notable works in style/sentiment transfer include BIBREF12, BIBREF13, BIBREF14, BIBREF15. They attempt to learn latent representations of various text aspects such as its context and attributes, or separate style from content and encode them into hidden representations. They then use an RNN decoder to generate a new sentence given a targeted sentiment attribute. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "47\n",
      "Question 1: What is the main difference between MaskGAN and SMERTI in terms of infilling capability? \n",
      "\n",
      "Answer 1: The main difference between MaskGAN and SMERTI is that MaskGAN is restricted to filling in a single word per mask token, while SMERTI is capable of variable length infilling for more flexible output, guided by semantic similarity resulting in more natural infilling and fulfillment of the STE task.\n",
      "Question : for the text Text infilling is the task of filling in missing parts of sentences called masks. MaskGAN BIBREF10 is restricted to a single word per mask token, while SMERTI is capable of variable length infilling for more flexible output. BIBREF11 uses a transformer-based architecture. They fill in random masks, while SMERTI fills in masks guided by semantic similarity, resulting in more natural infilling and fulfillment of the STE task. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "48\n",
      "Question 1: What is the SMERTI model and why was the Universal Sentence Encoder chosen for it?\n",
      "\n",
      "Answer 1: SMERTI is a model for text similarity tasks that uses the Universal Sentence Encoder (USE) as its underlying method. USE was chosen for SMERTI because it is designed for transfer learning and shows higher performance on textual similarity tasks compared to other models.\n",
      "Question : for the text Word2Vec BIBREF3, BIBREF4 allows for analogy representation through vector arithmetic. We implement a baseline (W2V-STEM) using this technique. The Universal Sentence Encoder (USE) BIBREF5 encodes sentences and is trained on a variety of web sources and the Stanford Natural Language Inference corpus BIBREF6. Flair embeddings BIBREF7 are based on architectures such as BERT BIBREF8. We use USE for SMERTI as it is designed for transfer learning and shows higher performance on textual similarity tasks compared to other models BIBREF9. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "49\n",
      "Question 1: What tools are used for entity replacement? \n",
      "Answer 1: The combination of Universal Sentence Encoder and Stanford Parser is used for entity replacement.\n",
      "Question : for the text For entity replacement, we use a combination of the Universal Sentence Encoder BIBREF5 and Stanford Parser BIBREF22. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "50\n",
      "Question 1: What is the Stanford Parser used for?\n",
      "\n",
      "Answer 1: The Stanford Parser is used for determining the grammatical structure of sentences, including phrases and part-of-speech (POS) labeling.\n",
      "Question : for the text The Stanford Parser is a constituency parser that determines the grammatical structure of sentences, including phrases and part-of-speech (POS) labelling. By feeding our $RE$ through the parser, we are able to determine its parse-tree. Iterating through the parse-tree and its sub-trees, we can obtain a list of constituent tags for the $RE$. We then feed our input text $S$ through the parser, and through a similar process, we can obtain a list of leaves (where leaves under a single label are concatenated) that are equal or similar to any of the $RE$ constituent tags. This generates a list of entities having the same (or similar) grammatical structure as the $RE$, and are likely candidates for the $OE$. We then feed these entities along with the $RE$ into the Universal Sentence Encoder (USE). generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "1\n",
      "What is being replaced in the text?\n",
      "The most similar entity or word to $OE$ is replaced with $RE$. \n",
      "\n",
      "Question 2:\n",
      "What model is used to compute the semantic similarity between two embeddings? \n",
      "The angular (cosine) distance with the deep averaging network (DAN) and transformer model is used to compute the semantic similarity between two embeddings. \n",
      "\n",
      "Answer 2:\n",
      "The angular (cosine) distance with the deep averaging network (DAN) and transformer model is used to compute the semantic similarity between two embeddings.\n",
      "Question : for the text The USE is a sentence-level embedding model that comes with a deep averaging network (DAN) and transformer model BIBREF5. We choose the transformer model as these embeddings take context into account, and the exact same word/phrase will have a different embedding depending on its context and surrounding words..We compute the semantic similarity between two embeddings $u$ and $v$: $sim(u,v)$, using the angular (cosine) distance, defined as: $\\cos (\\theta _{u,v}) = (u\\cdot v)/(||u|| ||v||)$, such that $sim(u,v) = 1-\\frac{1}{\\pi }arccos(\\cos (\\theta _{u,v}))$. Results are in $[0,1]$, with higher values representing greater similarity..Using USE and the above equation, we can identify words/phrases within the input text $S$ which are most similar to $RE$. To assist with this, we use the Stanford Parser as described above to obtain a list of candidate entities. In the rare case that this list is empty, we feed in each word of $S$ into USE, and identify which word is the most similar to $RE$. We then replace the most similar entity or word ($OE$) with the $RE$ and generate $S^{\\prime }$..An example of this entity replacement process is in Figure FIGREF18. Two parse-trees are shown: for $RE$ (a) and $S$ (b) and (c). Figure FIGREF18(d) is a semantic similarity heat-map generated from the USE embeddings of the candidate $OE$s and $RE$, where values are similarity scores in the range $[0,1]$..As seen in Figure FIGREF18(d), we calculate semantic similarities between $RE$ and entities within $S$ which have noun constituency tags. Looking at the row for our $RE$ restaurant, the most similar entity (excluding itself) is hotel. We can then generate:.$S^{\\prime }$ = i love this restaurant ! the beds are comfortable and the service is great ! generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "2\n",
      "Question 1: What is the purpose of the Text Infilling Module (TIM) in SMERTI?\n",
      "Answer 1: The Text Infilling Module (TIM) is used to fill in [mask] tokens in the modified text with words that better suit the replacement entity, which modifies the semantics in the rest of the text. The final output text is called $\\hat{S}$.\n",
      "Question : for the text The task is to transform a corpus $C$ of lines of text $S_i$ and associated replacement entities $RE_i:C = \\lbrace (S_1,RE_1),(S_2,RE_2),\\ldots , (S_n, RE_n)\\rbrace $ to a modified corpus $\\hat{C} = \\lbrace \\hat{S}_1,\\hat{S}_2,\\ldots ,\\hat{S}_n\\rbrace $, where $\\hat{S}_i$ are the original text lines $S_i$ replaced with $RE_i$ and overall semantics adjusted. SMERTI consists of the following modules, shown in Figure FIGREF15:.Entity Replacement Module (ERM): Identify which word(s) within the original text are best replaced with the $RE$, which we call the Original Entity ($OE$). We replace $OE$ in $S$ with $RE$. We call this modified text $S^{\\prime }$..Similarity Masking Module (SMM): Identify words/phrases in $S^{\\prime }$ similar to $OE$ and replace them with a [mask]. Group adjacent [mask]s into a single one so we can fill a variable length of text into each. We call this masked text $S^{\\prime \\prime }$..Text Infilling Module (TIM): Fill in [mask] tokens with text that better suits the $RE$. This will modify semantics in the rest of the text. This final output text is called $\\hat{S}$. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "3\n",
      "Question 1: What is the purpose of the similarity threshold (ST) in the masking process?\n",
      "\n",
      "Answer 1: The purpose of the similarity threshold (ST) is to select a subset of words to mask. It is increased in intervals of 0.05 until the actual masking rate falls below the masking rate threshold (MRT), which is defined by the user. This allows for the generation of text that can differ semantically in subtle ways and assists with text data augmentation.\n",
      "Question : for the text Next, we mask words similar to $OE$ to generate $S^{\\prime \\prime }$ using USE. We look at semantic similarities between every word in $S$ and $OE$, along with semantic similarities between $OE$ and the candidate entities determined in the previous ERM step to broaden the range of phrases our module can mask. We ignore $RE$, $OE$, and any entities or phrases containing $OE$ (for example, `this hotel')..After determining words similar to the $OE$ (discussed below), we replace each of them with a [mask] token. Next, we replace [mask] tokens adjacent to each other with a single [mask]..We set a base similarity threshold (ST) that selects a subset of words to mask. We compare the actual fraction of masked words to the masking rate threshold (MRT), as defined by the user, and increase ST in intervals of $0.05$ until the actual masking rate falls below the MRT. Some sample masked outputs ($S^{\\prime \\prime }$) using various MRT-ST combinations for the previous example are shown in Table TABREF21 (more examples in Appendix A)..The MRT is similar to the temperature parameter used to control the “novelty” of generated text in works such as BIBREF20. A high MRT means the user wants to generate text very semantically dissimilar to the original, and may be desired in cases such as creating a lively chatbot or correcting text that is heavily incorrect semantically. A low MRT means the user wants to generate text semantically similar to the original, and may be desired in cases such as text recovery, grammar correction, or correcting a minor semantic error in text. By varying the MRT, various pieces of text that differ semantically in subtle ways can be generated, assisting greatly with text data augmentation. The MRT also affects sentiment and fluency, as we show in Section SECREF59. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "4\n",
      "Question 1: What are the two seq2seq models used for TIM and their names?\n",
      "Answer 1: The two seq2seq models used for TIM are an RNN model called SMERTI-RNN and a transformer model called SMERTI-Transformer.\n",
      "Question : for the text We use two seq2seq models for our TIM: an RNN (recurrent neural network) model BIBREF23 (called SMERTI-RNN), and a transformer model (called SMERTI-Transformer). generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "5\n",
      "Question 1: What mechanism is used to combat information loss in the decoder?\n",
      "\n",
      "Answer 1: The attention mechanism is used to combat information loss in the decoder. A Luong attention layer is implemented which uses global attention, where all the encoder's hidden states are considered and the decoder's current time-step hidden state is used to calculate attention weights. The dot score function is used for attention.\n",
      "Question : for the text We use a bidirectional variant of the GRU BIBREF24, and hence two RNNs for the encoder: one reads the input sequence in standard sequential order, and the other is fed this sequence in reverse. The outputs are summed at each time step, giving us the ability to encode information from both past and future context..The decoder generates the output in a sequential token-by-token manner. To combat information loss, we implement the attention mechanism BIBREF25. We use a Luong attention layer BIBREF26 which uses global attention, where all the encoder's hidden states are considered, and use the decoder's current time-step hidden state to calculate attention weights. We use the dot score function for attention, where $h_t$ is the current target decoder state and $\\bar{h}_s$ is all encoder states: $score(h_t,\\bar{h}_s)=h_t^T\\bar{h}_s$. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "6\n",
      "Question 1: What is the purpose of the multi-head self-attention token decoder in the transformer architecture?\n",
      "\n",
      "Answer 1: The purpose of the multi-head self-attention token decoder in the transformer architecture is to allow the model to jointly attend to information from different positions and to make use of both local and global semantic information while filling in each [mask].\n",
      "Question : for the text Our second model makes use of the transformer architecture, and our implementation replicates BIBREF27. We use an encoder-decoder structure with a multi-head self-attention token decoder to condition on information from both past and future context. It maps a query and set of key-value pairs to an output. The queries and keys are of dimension $d_k$, and values of dimension $d_v$. To compute the attention, we pack a set of queries, keys, and values into matrices $Q$, $K$, and $V$, respectively. The matrix of outputs is computed as:..Multi-head attention allows the model to jointly attend to information from different positions. The decoder can make use of both local and global semantic information while filling in each [mask]. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "7\n",
      "Question 1: How does CN-Celeb compare to VoxCeleb in terms of speaker recognition difficulty?\n",
      "\n",
      "Answer 1: Experimental results showed that CN-Celeb is significantly more challenging for speaker recognition research than VoxCeleb, suggesting that the performance of current speaker recognition techniques may be worse than previously thought in unconstrained conditions.\n",
      "Question : for the text We introduced a free dataset CN-Celeb for speaker recognition research. The dataset contains more than $130k$ utterances from $1,000$ Chinese celebrities, and covers 11 different genres in real world. We compared CN-Celeb and VoxCeleb, a widely used dataset in speaker recognition, by setting up a series of experiments based on two state-of-the-art speaker recognition models. Experimental results demonstrated that CN-Celeb is significantly different from VoxCeleb, and it is more challenging for speaker recognition research. The EER performance we obtained in this paper suggests that in unconstrained conditions, the performance of the current speaker recognition techniques might be much worse than it was thought. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "8\n",
      "Question 1: What were the two datasets used in the experiments on speaker recognition?\n",
      "Answer 1: The two datasets used in the experiments on speaker recognition were VoxCeleb and CN-Celeb.\n",
      "Question : for the text In this section, we present a series of experiments on speaker recognition using VoxCeleb and CN-Celeb, to compare the complexity of the two datasets. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "9\n",
      "Question 1: What is the difference between the averaged length of utterances in SITW and CN-Celeb(E)?\n",
      "\n",
      "Answer 1: The averaged length of utterances in SITW is more than 80 seconds, while it is about 8 seconds for CN-Celeb(E).\n",
      "Question : for the text We first present the basic results evaluated on SITW and CN-Celeb(E). Both the front-end (i-vector or x-vector models) and back-end (LDA-PLDA) models were trained with the VoxCeleb training set. Note that for SITW, the averaged length of the utterances is more than 80 seconds, while this number is about 8 seconds for CN-Celeb(E). For a better comparison, we resegmented the data of SITW and created a new dataset denoted by SITW(S), where the averaged lengths of the enrollment and test utterances are 28 and 8 seconds, respectively. These numbers are similar to the statistics of CN-Celeb(E)..The results in terms of the equal error rate (EER) are reported in Table TABREF24. It can be observed that for both the i-vector system and the x-vector system, the performance on CN-Celeb(E) is much worse than the performance on SITW and SITW(S). This indicates that there is big difference between these two datasets. From another perspective, it demonstrates that the model trained with VoxCeleb does not generalize well, although it has achieved reasonable performance on data from a similar source (SITW). generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "10\n",
      "Question 1: How many speakers are included in the VoxCeleb training set?\n",
      "\n",
      "Answer 1: The VoxCeleb training set includes $7,185$ speakers.\n",
      "Question : for the text VoxCeleb: The entire dataset involves two parts: VoxCeleb1 and VoxCeleb2. We used SITW BIBREF21, a subset of VoxCeleb1 as the evaluation set. The rest of VoxCeleb1 was merged with VoxCeleb2 to form the training set (simply denoted by VoxCeleb). The training set involves $1,236,567$ utterances from $7,185$ speakers, and the evaluation set involves $6,445$ utterances from 299 speakers (precisely, this is the Eval. Core set within SITW)..CN-Celeb: The entire dataset was split into two parts: the first part CN-Celeb(T) involves $111,260$ utterances from 800 speakers and was used as the training set; the second part CN-Celeb(E) involves $18,849$ utterances from 200 speakers and was used as the evaluation set. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "11\n",
      "What dataset was created for a fair comparison between CN-Celeb and VoxCeleb?\n",
      "A new dataset called VoxCeleb(L) was created by randomly sampling 800 speakers from VoxCeleb to make its size comparable to CN-Celeb(T). This dataset was used for back-end (LDA-PLDA) training to compare the performance of systems based on each dataset.\n",
      "Question : for the text To further compare CN-Celeb and VoxCeleb in a quantitative way, we built systems based on CN-Celeb and VoxCeleb, respectively. For a fair comparison, we randomly sampled 800 speakers from VoxCeleb and built a new dataset VoxCeleb(L) whose size is comparable to CN-Celeb(T). This data set was used for back-end (LDA-PLDA) training..The experimental results are shown in Table TABREF26. Note that the performance of all the comparative experiments show the same trend with the i-vector system and the x-vector system, we therefore only analyze the i-vector results..Firstly, it can be seen that the system trained purely on VoxCeleb obtained good performance on SITW(S) (1st row). This is understandable as VoxCeleb and SITW(S) were collected from the same source. For the pure CN-Celeb system (2nd row), although CN-Celeb(T) and CN-Celeb(E) are from the same source, the performance is still poor (14.24%). More importantly, with re-training the back-end model with VoxCeleb(L) (4th row), the performance on SITW becomes better than the same-source result on CN-Celeb(E) (11.34% vs 14.24%). All these results reconfirmed the significant difference between the two datasets, and indicates that CN-Celeb is more challenging than VoxCeleb. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "12\n",
      "What was the dimensionality of the i-vector space used in the i-vector system?\n",
      "The dimensionality of the i-vector space used in the i-vector system was 400.\n",
      "Question : for the text Two state-of-the-art baseline systems were built following the Kaldi SITW recipe BIBREF22: an i-vector system BIBREF3 and an x-vector system BIBREF10..For the i-vector system, the acoustic feature involved 24-dimensional MFCCs plus the log energy, augmented by the first- and second-order derivatives. We also applied the cepstral mean normalization (CMN) and the energy-based voice active detection (VAD). The universal background model (UBM) consisted of $2,048$ Gaussian components, and the dimensionality of the i-vector space was 400. LDA was applied to reduce the dimensionality of the i-vectors to 150. The PLDA model was used for scoring BIBREF4..For the x-vector system, the feature-learning component was a 5-layer time-delay neural network (TDNN). The slicing parameters for the five time-delay layers were: {$t$-2, $t$-1, $t$, $t$+1, $t$+2}, {$t$-2, $t$, $t$+2}, {$t$-3, $t$, $t$+3}, {$t$}, {$t$}. The statistic pooling layer computed the mean and standard deviation of the frame-level features from a speech segment. The size of the output layer was consistent with the number of speakers in the training set. Once trained, the activations of the penultimate hidden layer were read out as x-vectors. In our experiments, the dimension of the x-vectors trained on VoxCeleb was set to 512, while for CN-Celeb, it was set to 256, considering the less number of speakers in the training set. Afterwards, the x-vectors were projected to 150-dimensional vectors by LDA, and finally the PLDA model was employed to score the trials. Refer to BIBREF10 for more details. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "13\n",
      "Question 1: What is the difference between CN-Celeb and VoxCeleb?\n",
      "\n",
      "Answer 1: CN-Celeb is a large-scale speaker dataset that specially focuses on Chinese celebrities and covers more genres of speech compared to VoxCeleb. Additionally, CN-Celeb involves human check for more complex genres, while VoxCeleb is fully automated.\n",
      "Question : for the text Speaker recognition including identification and verification, aims to recognize claimed identities of speakers. After decades of research, performance of speaker recognition systems has been vastly improved, and the technique has been deployed to a wide range of practical applications. Nevertheless, the present speaker recognition approaches are still far from reliable in unconstrained conditions where uncertainties within the speech recordings could be arbitrary. These uncertainties might be caused by multiple factors, including free text, multiple channels, environmental noises, speaking styles, and physiological status. These uncertainties make the speaker recognition task highly challenging BIBREF0, BIBREF1..Researchers have devoted much effort to address the difficulties in unconstrained conditions. Early methods are based on probabilistic models that treat these uncertainties as an additive Gaussian noise. JFA BIBREF2, BIBREF3 and PLDA BIBREF4 are the most famous among such models. These models, however, are shallow and linear, and therefore cannot deal with the complexity of real-life applications. Recent advance in deep learning methods offers a new opportunity BIBREF5, BIBREF6, BIBREF7, BIBREF8. Resorting to the power of deep neural networks (DNNs) in representation learning, these methods can remove unwanted uncertainties by propagating speech signals through the DNN layer by layer and retain speaker-relevant features only BIBREF9. Significant improvement in robustness has been achieved by the DNN-based approach BIBREF10, which makes it more suitable for applications in unconstrained conditions..The success of DNN-based methods, however, largely relies on a large amount of data, in particular data that involve the true complexity in unconstrained conditions. Unfortunately, most existing datasets for speaker recognition are collected in constrained conditions, where the acoustic environment, channel and speaking style do not change significantly for each speaker BIBREF11, BIBREF12, BIBREF13. These datasets tend to deliver over optimistic performance and do not meet the request of research on speaker recognition in unconstrained conditions..To address this shortage in datasets, researchers have started to collect data `in the wild'. The most successful `wild' dataset may be VoxCeleb BIBREF14, BIBREF15, which contains millions of utterances from over thousands of speakers. The utterances were collected from open-source media using a fully automated pipeline based on computer vision techniques, in particular face detection, tracking and recognition, plus video-audio synchronization. The automated pipeline is almost costless, and thus greatly improves the efficiency of data collection..In this paper, we re-implement the automated pipeline of VoxCeleb and collect a new large-scale speaker dataset, named CN-Celeb. Compared with VoxCeleb, CN-Celeb has three distinct features:.CN-Celeb specially focuses on Chinese celebrities, and contains more than $130,000$ utterances from $1,000$ persons..CN-Celeb covers more genres of speech. We intentionally collected data from 11 genres, including entertainment, interview, singing, play, movie, vlog, live broadcast, speech, drama, recitation and advertisement. The speech of a particular speaker may be in more than 5 genres. As a comparison, most of the utterances in VoxCeleb were extracted from interview videos. The diversity in genres makes our database more representative for the true scenarios in unconstrained conditions, but also more challenging..CN-Celeb is not fully automated, but involves human check. We found that more complex the genre is, more errors the automated pipeline tends to produce. Ironically, the error-pron segments could be highly valuable as they tend to be boundary samples. We therefore choose a two-stage strategy that employs the automated pipeline to perform pre-selection, and then perform human check..The rest of the paper is organized as follows. Section SECREF2 presents a detailed description for CN-Celeb, and Section SECREF3 presents more quantitative comparisons between CN-Celeb and VoxCeleb on the speaker recognition task. Section SECREF4 concludes the entire paper. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "14\n",
      "Question 1: How does the variation in speaking styles of the speakers in CN-Celeb challenge speaker recognition research?\n",
      "Answer 1: The varying speaking styles of the speakers in CN-Celeb result in significant variation in their utterances. This makes it more challenging for speaker recognition research as it requires models to be trained on a larger and more diverse set of data in order to accurately recognize and identify a speaker across all their different speaking styles.\n",
      "Question : for the text Table TABREF13 summarizes the main difference between CN-Celeb and VoxCeleb. Compared to VoxCeleb, CN-Celeb is a more complex dataset and more challenging for speaker recognition research. More details of these challenges are as follows..Most of the utterances involve real-world noise, including ambient noise, background babbling, music, cheers and laugh..A certain amount of utterances involve strong and overlapped background speakers, especially in the dram and movie genres..Most of speakers have different genres of utterances, which results in significant variation in speaking styles..The utterances of the same speaker may be recorded at different time and with different devices, leading to serious cross-time and cross-channel problems..Most of the utterances are short, which meets the scenarios of most real applications but leads to unreliable decision. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "15\n",
      "What was the purpose of applying a human check to the automated pipeline used to collect CN-Celeb?\n",
      "\n",
      "The purpose of applying a human check to the automated pipeline used to collect CN-Celeb was to remove incorrect segments and reduce errors caused by a purely automated process. This process is much faster than purely human-based segmentation.\n",
      "Question : for the text CN-Celeb was collected following a two-stage strategy: firstly we used an automated pipeline to extract potential segments of the Person of Interest (POI), and then applied a human check to remove incorrect segments. This process is much faster than purely human-based segmentation, and reduces errors caused by a purely automated process..Briefly, the automated pipeline we used is similar to the one used to collect VoxCeleb1 BIBREF14 and VoxCeleb2 BIBREF15, though we made some modification to increase efficiency and precision. Especially, we introduced a new face-speaker double check step that fused the information from both the image and speech signals to increase the recall rate while maintaining the precision..The detailed steps of the collection process are summarized as follows..STEP 1. POI list design. We manually selected $1,000$ Chinese celebrities as our target speakers. These speakers were mostly from the entertainment sector, such as singers, drama actors/actrees, news reporters, interviewers. Region diversity was also taken into account so that variation in accent was covered..STEP 2. Pictures and videos download. Pictures and videos of the $1,000$ POIs were downloaded from the data source (https://www.bilibili.com/) by searching for the names of the persons. In order to specify that we were searching for POI names, the word `human' was added in the search queries. The downloaded videos were manually examined and were categorized into the 11 genres..STEP 3. Face detection and tracking. For each POI, we first obtained the portrait of the person. This was achieved by detecting and clipping the face images from all pictures of that person. The RetinaFace algorithm was used to perform the detection and clipping BIBREF16. Afterwards, video segments that contain the target person were extracted. This was achieved by three steps: (1) For each frame, detect all the faces appearing in the frame using RetinaFace; (2) Determine if the target person appears by comparing the POI portrait and the faces detected in the frame. We used the ArcFace face recognition system BIBREF17 to perform the comparison; (3) Apply the MOSSE face tracking system BIBREF18 to produce face streams..STEP 4. Active speaker verification. As in BIBREF14, an active speaker verification system was employed to verify if the speech was really spoken by the target person. This is necessary as it is possible that the target person appears in the video but the speech is from other persons. We used the SyncNet model BIBREF19 as in BIBREF14 to perform the task. This model was trained to detect if a stream of mouth movement and a stream of speech are synchronized. In our implementation, the stream of mouth movement was derived from the face stream produced by the MOSSE system..STEP 5. Double check by speaker recognition..Although SyncNet worked well for videos in simple genres, it failed for videos of complex genres such as movie and vlog. A possible reason is that the video content of these genres may change dramatically in time, which leads to unreliable estimation for the stream of the mouth movement, hence unreliable synchronization detection. In order to improve the robustness of the active speaker verification in complex genres, we introduced a double check procedure based on speaker recognition. The idea is simple: whenever the speaker recognition system states a very low confidence for the target speaker, the segment will be discarded even if the confidence from SyncNet is high; vice versa, if the speaker recognition system states a very high confidence, the segment will be retained. We used an off-the-shelf speaker recognition system BIBREF20 to perform this double check. In our study, this double check improved the recall rate by 30% absolutely..STEP 6. Human check..The segments produced by the above automated pipeline were finally checked by human. According to our experience, this human check is rather efficient: one could check 1 hour of speech in 1 hour. As a comparison, if we do not apply the automated pre-selection, checking 1 hour of speech requires 4 hours. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "16\n",
      "Question 1: What is the purpose of the CN-Celeb dataset?\n",
      "\n",
      "Answer 1: The original purpose of the CN-Celeb dataset is to investigate the true difficulties of speaker recognition techniques in unconstrained conditions, and to provide a resource for researchers to build prototype systems and evaluate the performance.\n",
      "Question : for the text The original purpose of the CN-Celeb dataset is to investigate the true difficulties of speaker recognition techniques in unconstrained conditions, and provide a resource for researchers to build prototype systems and evaluate the performance. Ideally, it can be used as a standalone data source, and can be also used with other datasets together, in particular VoxCeleb which is free and large. For this reason, CN-Celeb tries to be distinguished from but also complementary to VoxCeleb from the beginning of the design. This leads to three features that we have discussed in the previous section: Chinese focused, complex genres, and quality guarantee by human check..In summary, CN-Celeb contains over $130,000$ utterances from $1,000$ Chinese celebrities. It covers 11 genres and the total amount of speech waveforms is 274 hours. Table TABREF5 gives the data distribution over the genres, and Table TABREF6 presents the data distribution over the length of utterances. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "17\n",
      "What is the main contribution of this paper?\n",
      "The main contribution of this paper is the introduction of a novel conditional MLM task to fine-tune BERT into a conditional BERT model, which can be used for data augmentation in sentence classification tasks and style transfer tasks. The experiment results show that the model outperforms several baseline methods.\n",
      "Question : for the text In this paper, we fine-tune BERT to conditional BERT by introducing a novel conditional MLM task. After being well trained, the conditional BERT can be applied to data augmentation for sentence classification tasks. Experiment results show that our model outperforms several baseline methods obviously. Furthermore, we demonstrate that our conditional BERT can also be applied to style transfer task. In the future, (1)We will explore how to perform text data augmentation on imbalanced datasets with pre-trained language model, (2) we believe the idea of conditional BERT contextual augmentation is universal and will be applied to paragraph or document level data augmentation. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "18\n",
      "What is the difference between the input embeddings of BERT and conditional BERT?\n",
      "\n",
      "The input embeddings of BERT are the sum of the token embeddings, segmentation embeddings, and position embeddings, while the input embeddings of conditional BERT have label embeddings instead of segmentation embeddings. The conditional BERT model is trained using a new task called conditional masked language model, which considers both the context and label information to predict the masked word.\n",
      "Question : for the text As shown in Fig 1 , our conditional BERT shares the same model architecture with the original BERT. The differences are the input representation and training procedure..The input embeddings of BERT are the sum of the token embeddings, the segmentation embeddings and the position embeddings. For the segmentation embeddings in BERT, a learned sentence A embedding is added to every token of the first sentence, and if a second sentence exists, a sentence B embedding will be added to every token of the second sentence. However, the segmentation embeddings has no connection to the actual annotated labels of a sentence, like sense, sentiment or subjectivity, so predicted word is not always compatible with annotated labels. For example, given a positive movie remark “this actor is good\", we have the word “good\" masked. Through the Masked Language Model task by BERT, the predicted word in the masked position has potential to be negative word likes \"bad\" or \"boring\". Such new generated sentences by substituting masked words are implausible with respect to their original labels, which will be harmful if added to the corpus to apply augmentation. In order to address this issue, we propose a new task: “conditional masked language model\"..The conditional masked language model randomly masks some of the tokens from the labeled sentence, and the objective is to predict the original vocabulary index of the masked word based on both its context and its label. Given a masked token ${t_i}$ , the context ${\\textbf {\\textit {S}}\\backslash \\lbrace t_i \\rbrace }$ and label ${y}$ are both considered, aiming to calculate ${p(\\cdot |y,\\textbf {\\textit {S}}\\backslash \\lbrace t_i \\rbrace )}$ , instead of calculating ${p(\\cdot |\\textbf {\\textit {S}}\\backslash \\lbrace t_i \\rbrace )}$ . Unlike MLM pre-training, the conditional MLM objective allows the representation to fuse the context information and the label information, which allows us to further train a label-conditional deep bidirectional representations..To perform conditional MLM task, we fine-tune on pre-trained BERT. We alter the segmentation embeddings to label embeddings, which are learned corresponding to their annotated labels on labeled datasets. Note that the BERT are designed with segmentation embedding being embedding A or embedding B, so when a downstream task dataset with more than two labels, we have to adapt the size of embedding to label size compatible. We train conditional BERT using conditional MLM task on labeled dataset. After the model has converged, it is expected to be able to predict words in masked position both considering the context and the label. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "19\n",
      "Question 1: What is the purpose of the Conditional BERT contextual augmentation algorithm?\n",
      "\n",
      "Answer 1: The purpose of the Conditional BERT contextual augmentation algorithm is to utilize a trained conditional BERT model to augment sentences in a labeled dataset. The algorithm masks a few words in a given sentence, predicts label-compatible words through the conditional BERT model, and generates a new sentence that shares similar context and label with the original sentence. The new sentences are added to the original dataset to augment it. The augmented dataset can be used for downstream tasks.\n",
      "Question : for the text After the conditional BERT is well-trained, we utilize it to augment sentences. Given a labeled sentence from the corpus, we randomly mask a few words in the sentence. Through conditional BERT, various words compatibly with the label of the sentence are predicted by conditional BERT. After substituting the masked words with predicted words, a new sentences is generated, which shares similar context and same label with original sentence. Then new sentences are added to original corpus. We elaborate the entire process in algorithm \"Conditional BERT Contextual Augmentation\" ..Conditional BERT contextual augmentation algorithm. Fine-tuning on the pre-trained BERT , we retrofit BERT to conditional BERT using conditional MLM task on labeled dataset. After the model converged, we utilize it to augment sentences. New sentences are added into dataset to augment the dataset. [1] Alter the segmentation embeddings to label embeddings Fine-tune the pre-trained BERT using conditional MLM task on labeled dataset D until convergence each iteration i=1,2,...,M Sample a sentence $s$ from D Randomly mask $k$ words Using fine-tuned conditional BERT to predict label-compatible words on masked positions to generate a new sentence $S^{\\prime }$ Add new sentences into dataset $D$ to get augmented dataset $D^{\\prime }$ Perform downstream task on augmented dataset $D^{\\prime }$  generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "20\n",
      "Question 1: What is style transfer and how is it related to conditional BERT?\n",
      "\n",
      "Answer 1: Style transfer is the task of changing the stylistic properties of a given text without altering its intent or meaning. It is related to conditional BERT because both tasks involve changing specific words in a sentence while preserving its context. Conditional BERT can be used to predict new substitutes for masked style-relevant words in a sentence to achieve style transfer.\n",
      "Question : for the text In this section, we further deep into the connection to style transfer and apply our well trained conditional BERT to style transfer task. Style transfer is defined as the task of rephrasing the text to contain specific stylistic properties without changing the intent or affect within the context BIBREF32 . Our conditional MLM task changes words in the text condition on given label without changing the context. View from this point, the two tasks are very close. So in order to apply conditional BERT to style transfer task, given a specific stylistic sentence, we break it into two steps: first, we find the words relevant to the style; second, we mask the style-relevant words, then use conditional BERT to predict new substitutes with sentence context and target style property. In order to find style-relevant words in a sentence, we refer to Xu BIBREF33 , which proposed an attention-based method to extract the contribution of each word to the sentence sentimental label. For example, given a positive movie remark “This movie is funny and interesting\", we filter out the words contributes largely to the label and mask them. Then through our conditional BERT contextual augmentation method, we fill in the masked position by predicting words conditioning on opposite label and sentence context, resulting in “This movie is boring and dull\". The words “boring\" and “dull\" contribute to the new sentence being labeled as negative style. We sample some sentences from dataset SST2, transferring them to the opposite label, as listed in table 4 . generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "21\n",
      "Question 1: What is the SST dataset used for?\n",
      "\n",
      "Answer 1: The SST (Stanford Sentiment Treebank) dataset is used for sentiment classification on movie reviews, which are annotated with five labels (SST5: very positive, positive, neutral, negative, or very negative) or two labels (SST2: positive or negative).\n",
      "Question : for the text Six benchmark classification datasets are listed in table 1 . Following Kim BIBREF24 , for a dataset without validation data, we use 10% of its training set for the validation set. Summary statistics of six classification datasets are shown in table 1..SST BIBREF25 SST (Stanford Sentiment Treebank) is a dataset for sentiment classification on movie reviews, which are annotated with five labels (SST5: very positive, positive, neutral, negative, or very negative) or two labels (SST2: positive or negative)..Subj BIBREF26 Subj (Subjectivity dataset) is annotated with whether a sentence is subjective or objective..MPQA BIBREF27 MPQA Opinion Corpus is an opinion polarity detection dataset of short phrases rather than sentences, which contains news articles from a wide variety of news sources manually annotated for opinions and other private states (i.e., beliefs, emotions, sentiments, speculations, etc.)..RT BIBREF28 RT is another movie review sentiment dataset contains a collection of short review excerpts from Rotten Tomatoes collected by Bo Pang and Lillian Lee..TREC BIBREF29 TREC is a dataset for classification of the six question types (whether the question is about person, location, numeric information, etc.). generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "22\n",
      "What is the pre-trained BERT model used in the experiment?\n",
      "\n",
      "The pre-trained BERT model used in the experiment is BERT$_{BASE}$, with 12 layers (i.e., Transformer blocks), a hidden size of 768, and the number of self-attention heads of 12. It has a total of 110M parameters.\n",
      "Question : for the text In this section, we present conditional BERT parameter settings and, following Kobayashi BIBREF6 , we apply different augmentation methods on two types of neural models through six text classification tasks. The pre-trained BERT model we used in our experiment is BERT $_{BASE}$ , with number of layers (i.e., Transformer blocks) $L = 12$ , the hidden size $ H = 768$ , and the number of self-attention heads $A = 12$ , total parameters $= 110M$ . Detailed pre-train parameters setting can be found in original paper BIBREF11 . For each task, we perform the following steps independently. First, we evaluate the augmentation ability of original BERT model pre-trained on MLM task. We use pre-trained BERT to augment dataset, by predicted masked words only condition on context for each sentence. Second, we fine-tune the original BERT model to a conditional BERT. Well-trained conditional BERT augments each sentence in dataset by predicted masked words condition on both context and label. Third, we compare the performance of the two methods with Kobayashi's BIBREF6 contextual augmentation results. Note that the original BERT’s segmentation embeddings layer is compatible with two-label dataset. When the task-specific dataset is with more than two different labels, we should re-train a label size compatible label embeddings layer instead of directly fine-tuning the pre-trained one. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "23\n",
      "Question 1: What is the key technique proposed by ULMFiT for fine-tuning a language model?\n",
      "\n",
      "Answer 1: ULMFiT proposes the key technique of fine-tuning a language model for transfer learning by using discriminative fine-tuning and gradual unfreezing of layers.\n",
      "Question : for the text Language model pre-training has attracted wide attention and fine-tuning on pre-trained language model has shown to be effective for improving many downstream natural language processing tasks. Dai BIBREF7 pre-trained unlabeled data to improve Sequence Learning with recurrent networks. Howard BIBREF8 proposed a general transfer learning method, Universal Language Model Fine-tuning (ULMFiT), with the key techniques for fine-tuning a language model. Radford BIBREF9 proposed that by generative pre-training of a language model on a diverse corpus of unlabeled text, large gains on a diverse range of tasks could be realized. Radford BIBREF9 achieved large improvements on many sentence-level tasks from the GLUE benchmark BIBREF10 . BERT BIBREF11 obtained new state-of-the-art results on a broad range of diverse tasks. BERT pre-trained deep bidirectional representations which jointly conditioned on both left and right context in all layers, following by discriminative fine-tuning on each specific task. Unlike previous works fine-tuning pre-trained language model to perform discriminative tasks, we aim to apply pre-trained BERT on generative tasks by perform the masked language model(MLM) task. To generate sentences that are compatible with given labels, we retrofit BERT to conditional BERT, by introducing a conditional masked language model task and fine-tuning BERT on the task. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "24\n",
      "What is the proposed data augmentation method in this paper? \n",
      "The proposed data augmentation method in this paper is called conditional BERT contextual augmentation, which uses a fine-tuned BERT model to offer a wider range of substitute words predicted by the masked language model task, and introduces a new fine-tuning objective called the \"conditional masked language model\" to predict label-compatible words based on both context and sentence label. This method improves performance of deep neural network models on text classification tasks.\n",
      "Question : for the text Deep neural network-based models are easy to overfit and result in losing their generalization due to limited size of training data. In order to address the issue, data augmentation methods are often applied to generate more training samples. Recent years have witnessed great success in applying data augmentation in the field of speech area BIBREF0 , BIBREF1 and computer vision BIBREF2 , BIBREF3 , BIBREF4 . Data augmentation in these areas can be easily performed by transformations like resizing, mirroring, random cropping, and color shifting. However, applying these universal transformations to texts is largely randomized and uncontrollable, which makes it impossible to ensure the semantic invariance and label correctness. For example, given a movie review “The actors is good\", by mirroring we get “doog si srotca ehT\", or by random cropping we get “actors is\", both of which are meaningless..Existing data augmentation methods for text are often loss of generality, which are developed with handcrafted rules or pipelines for specific domains. A general approach for text data augmentation is replacement-based method, which generates new sentences by replacing the words in the sentences with relevant words (e.g. synonyms). However, words with synonyms from a handcrafted lexical database likes WordNet BIBREF5 are very limited , and the replacement-based augmentation with synonyms can only produce limited diverse patterns from the original texts. To address the limitation of replacement-based methods, Kobayashi BIBREF6 proposed contextual augmentation for labeled sentences by offering a wide range of substitute words, which are predicted by a label-conditional bidirectional language model according to the context. But contextual augmentation suffers from two shortages: the bidirectional language model is simply shallow concatenation of a forward and backward model, and the usage of LSTM models restricts their prediction ability to a short range..BERT, which stands for Bidirectional Encoder Representations from Transformers, pre-trained deep bidirectional representations by jointly conditioning on both left and right context in all layers. BERT addressed the unidirectional constraint by proposing a “masked language model\" (MLM) objective by masking some percentage of the input tokens at random, and predicting the masked words based on its context. This is very similar to how contextual augmentation predict the replacement words. But BERT was proposed to pre-train text representations, so MLM task is performed in an unsupervised way, taking no label variance into consideration..This paper focuses on the replacement-based methods, by proposing a novel data augmentation method called conditional BERT contextual augmentation. The method applies contextual augmentation by conditional BERT, which is fine-tuned on BERT. We adopt BERT as our pre-trained language model with two reasons. First, BERT is based on Transformer. Transformer provides us with a more structured memory for handling long-term dependencies in text. Second, BERT, as a deep bidirectional model, is strictly more powerful than the shallow concatenation of a left-to-right and right-to left model. So we apply BERT to contextual augmentation for labeled sentences, by offering a wider range of substitute words predicted by the masked language model task. However, the masked language model predicts the masked word based only on its context, so the predicted word maybe incompatible with the annotated labels of the original sentences. In order to address this issue, we introduce a new fine-tuning objective: the \"conditional masked language model\"(C-MLM). The conditional masked language model randomly masks some of the tokens from an input, and the objective is to predict a label-compatible word based on both its context and sentence label. Unlike Kobayashi's work, the C-MLM objective allows a deep bidirectional representations by jointly conditioning on both left and right context in all layers. In order to evaluate how well our augmentation method improves performance of deep neural network models, following Kobayashi BIBREF6 , we experiment it on two most common neural network structures, LSTM-RNN and CNN, on text classification tasks. Through the experiments on six various different text classification tasks, we demonstrate that the proposed conditional BERT model augments sentence better than baselines, and conditional BERT contextual augmentation method can be easily applied to both convolutional or recurrent neural networks classifier. We further explore our conditional MLM task’s connection with style transfer task and demonstrate that our conditional BERT can also be applied to style transfer too..Our contributions are concluded as follows:.To our best knowledge, this is the first attempt to alter BERT to a conditional BERT or apply BERT on text generation tasks. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "25\n",
      "Question 1: What is the Masked Language Model (MLM) task proposed by BERT?\n",
      "\n",
      "Answer 1: The MLM task is a method proposed by BERT to train a deep bidirectional language model. It randomly masks some percentage of input tokens and predicts only those masked tokens according to their context in the cloze sentence. The final hidden vectors corresponding to the mask tokens are fed into an output softmax over the vocabulary to produce words with a probability distribution. MLM task only predicts the masked words rather than reconstructing the entire input, which suggests that more pre-training steps are required for the model to converge.\n",
      "Question : for the text In general, the language model(LM) models the probability of generating natural language sentences or documents. Given a sequence $\\textbf {\\textit {S}}$ of N tokens, $<t_1,t_2,...,t_N>$ , a forward language model allows us to predict the probability of the sequence as: .$$p(t_1,t_2,...,t_N) = \\prod _{i=1}^{N}p(t_i|t_1, t_2,..., t_{i-1}).$$   (Eq. 8) .Similarly, a backward language model allows us to predict the probability of the sentence as: .$$p(t_1,t_2,...,t_N) = \\prod _{i=1}^{N}p(t_i|t_{i+1}, t_{i+2},..., t_N).$$   (Eq. 9) .Traditionally, a bidirectional language model a shallow concatenation of independently trained forward and backward LMs..In order to train a deep bidirectional language model, BERT proposed Masked Language Model (MLM) task, which was also referred to Cloze Task BIBREF23 . MLM task randomly masks some percentage of the input tokens, and then predicts only those masked tokens according to their context. Given a masked token ${t_i}$ , the context is the tokens surrounding token ${t_i}$ in the sequence $\\textbf {\\textit {S}}$ , i.e. cloze sentence ${\\textbf {\\textit {S}}\\backslash \\lbrace t_i \\rbrace }$ . The final hidden vectors corresponding to the mask tokens are fed into an output softmax over the vocabulary to produce words with a probability distribution ${p(\\cdot |\\textbf {\\textit {S}}\\backslash \\lbrace t_i \\rbrace )}$ . MLM task only predicts the masked words rather than reconstructing the entire input, which suggests that more pre-training steps are required for the model to converge. Pre-trained BERT can augment sentences through MLM task, by predicting new words in masked positions according to their context. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "26\n",
      "What is the advantage of using a deep bidirectional language model for text data augmentation?\n",
      "The advantage is that the attention mechanism within the model allows for a more structured memory for handling long-term dependencies in text, resulting in more general and robust improvement on various downstream tasks.\n",
      "Question : for the text Text data augmentation has been extensively studied in natural language processing. Sample-based methods includes downsampling from the majority classes and oversampling from the minority class, both of which perform weakly in practice. Generation-based methods employ deep generative models such as GANs BIBREF12 or VAEs BIBREF13 , BIBREF14 , trying to generate sentences from a continuous space with desired attributes of sentiment and tense. However, sentences generated in these methods are very hard to guarantee the quality both in label compatibility and sentence readability. In some specific areas BIBREF15 , BIBREF16 , BIBREF17 . word replacement augmentation was applied. Wang BIBREF18 proposed the use of neighboring words in continuous representations to create new instances for every word in a tweet to augment the training dataset. Zhang BIBREF19 extracted all replaceable words from the given text and randomly choose $r$ of them to be replaced, then substituted the replaceable words with synonyms from WordNet BIBREF5 . Kolomiyets BIBREF20 replaced only the headwords under a task-specific assumption that temporal trigger words usually occur as headwords. Kolomiyets BIBREF20 selected substitute words with top- $K$ scores given by the Latent Words LM BIBREF21 , which is a LM based on fixed length contexts. Fadaee BIBREF22 focused on the rare word problem in machine translation, replacing words in a source sentence with only rare words. A word in the translated sentence is also replaced using a word alignment method and a rightward LM. The work most similar to our research is Kobayashi BIBREF6 . Kobayashi used a fill-in-the-blank context for data augmentation by replacing every words in the sentence with language model. In order to prevent the generated words from reversing the information related to the labels of the sentences, Kobayashi BIBREF6 introduced a conditional constraint to control the replacement of words. Unlike previous works, we adopt a deep bidirectional language model to apply replacement, and the attention mechanism within our model allows a more structured memory for handling long-term dependencies in text, which resulting in more general and robust improvement on various downstream tasks. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "27\n",
      "What baseline methods did the authors compare their proposed method to in their evaluation?\n",
      "\n",
      "The authors compared their proposed conditional BERT contextual augmentation method to three baseline methods: synonym replacement, context augmentation, and label-conditional language model architecture.\n",
      "Question : for the text We evaluate the performance improvement brought by conditional BERT contextual augmentation on sentence classification tasks, so we need to prepare two common sentence classifiers beforehand. For comparison, following Kobayashi BIBREF6 , we adopt two typical classifier architectures: CNN or LSTM-RNN. The CNN-based classifier BIBREF24 has convolutional filters of size 3, 4, 5 and word embeddings. All outputs of each filter are concatenated before applied with a max-pooling over time, then fed into a two-layer feed-forward network with ReLU, followed by the softmax function. An RNN-based classifier has a single layer LSTM and word embeddings, whose output is fed into an output affine layer with the softmax function. For both the architectures, dropout BIBREF30 and Adam optimization BIBREF31 are applied during training. The train process is finish by early stopping with validation at each epoch..Sentence classifier hyper-parameters including learning rate, embedding dimension, unit or filter size, and dropout ratio, are selected using grid-search for each task-specific dataset. We refer to Kobayashi's implementation in the released code. For BERT, all hyper-parameters are kept the same as Devlin BIBREF11 , codes in Tensorflow and PyTorch are all available on github and pre-trained BERT model can also be downloaded. The number of conditional BERT training epochs ranges in [1-50] and number of masked words ranges in [1-2]..We compare the performance improvements obtained by our proposed method with the following baseline methods, “w/\" means “with\":.w/synonym: Words are randomly replaced with synonyms from WordNet BIBREF5 ..w/context: Proposed by Kobayashi BIBREF6 , which used a bidirectional language model to apply contextual augmentation, each word was replaced with a probability..w/context+label: Kobayashi’s contextual augmentation method BIBREF6 in a label-conditional LM architecture..Table 2 lists the accuracies of the all methods on two classifier architectures. The results show that, for various datasets on different classifier architectures, our conditional BERT contextual augmentation improves the model performances most. BERT can also augments sentences to some extent, but not as much as conditional BERT does. For we masked words randomly, the masked words may be label-sensitive or label-insensitive. If label-insensitive words are masked, words predicted through BERT may not be compatible with original labels. The improvement over all benchmark datasets also shows that conditional BERT is a general augmentation method for multi-labels sentence classification tasks..We also explore the effect of number of training steps to the performance of conditional BERT data augmentation. The fine-tuning epoch setting ranges in [1-50], we list the fine-tuning epoch of conditional BERT to outperform BERT for various benchmarks in table 3 . The results show that our conditional BERT contextual augmentation can achieve obvious performance improvement after only a few fine-tuning epochs, which is very convenient to apply to downstream tasks. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "28\n",
      "Question 1: Can QG systems automatically identify question-worthy parts within a given input passage?\n",
      "\n",
      "Answer 1: Yes, QG systems can automatically identify question-worthy parts within a given input passage. This task is synonymous with content selection in traditional QG, and some works have implemented it using neural networks such as sentence selection and keyphrase extraction. However, learning what aspect to ask about can be challenging when the question requires reasoning over multiple pieces of information within the passage.\n",
      "Question : for the text The aforementioned models require the target answer as an input, in which the answer essentially serves as the focus of asking. However, in the case that only the input passage is given, a QG system should automatically identify question-worthy parts within the passage. This task is synonymous with content selection in traditional QG. To date, only two works BIBREF58 , BIBREF59 have worked in this setting. They both follow the traditional decomposition of QG into content selection and question construction but implement each task using neural networks. For content selection, BIBREF58 learn a sentence selection task to identify question-worthy sentences from the input paragraph using a neural sequence tagging model. BIBREF59 train a neural keyphrase extractor to predict keyphrases of the passage. For question construction, they both employed the Seq2Seq model, for which the input is either the selected sentence or the input passage with keyphrases as target answer..However, learning what aspect to ask about is quite challenging when the question requires reasoning over multiple pieces of information within the passage; cf the Gollum question from the introduction. Beyond retrieving question-worthy information, we believe that studying how different reasoning patterns (e.g., inductive, deductive, causal and analogical) affects the generation process will be an aspect for future study. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "29\n",
      "Question 1: How does Bloom's taxonomy categorize cognitive levels involved in question asking?\n",
      "Answer 1: Bloom's taxonomy categorizes the cognitive levels involved in question asking into six categories: Remembering, Understanding, Applying, Analyzing, Evaluating and Creating.\n",
      "Question : for the text Finally, we consider the required cognitive process behind question asking, a distinguishing factor for questions BIBREF32 . A typical framework that attempts to categorize the cognitive levels involved in question asking comes from Bloom's taxonomy BIBREF33 , which has undergone several revisions and currently has six cognitive levels: Remembering, Understanding, Applying, Analyzing, Evaluating and Creating BIBREF32 ..Traditional QG focuses on shallow levels of Bloom's taxonomy: typical QG research is on generating sentence-based factoid questions (e.g., Who, What, Where questions), whose answers are simple constituents in the input sentence BIBREF2 , BIBREF13 . However, a QG system achieving human cognitive level should be able to generate meaningful questions that cater to higher levels of Bloom's taxonomy BIBREF34 , such as Why, What-if, and How questions. Traditionally, those “deep” questions are generated through shallow methods such as handcrafted templates BIBREF20 , BIBREF21 ; however, these methods lack a real understanding and reasoning over the input..Although asking deep questions is complex, NQG's ability to generalize over voluminous data has enabled recent research to explore the comprehension and reasoning aspects of QG BIBREF35 , BIBREF1 , BIBREF8 , BIBREF34 . We investigate this trend in Section \"Generation of Deep Questions\" , examining the limitations of current Seq2Seq model in generating deep questions, and the efforts made by existing works, indicating further directions ahead..The rest of this paper provides a systematic survey of NQG, covering corpus and evaluation metrics before examining specific neural models. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "30\n",
      "Question 1: What are some emerging trends in NQG?\n",
      "\n",
      "Answer 1: Three emerging trends in NQG are multi-task learning, wider input modalities, and deep question generation.\n",
      "Question : for the text We have presented a comprehensive survey of NQG, categorizing current NQG models based on different QG-specific and common technical variations, and summarizing three emerging trends in NQG: multi-task learning, wider input modalities, and deep question generation..What's next for NGQ? We end with future potential directions by applying past insights to current NQG models; the “unknown unknown\", promising directions yet explored..When to Ask: Besides learning what and how to ask, in many real-world applications that question plays an important role, such as automated tutoring and conversational systems, learning when to ask become an important issue. In contrast to general dialog management BIBREF85 , no research has explored when machine should ask an engaging question in dialog. Modeling question asking as an interactive and dynamic process may become an interesting topic ahead..Personalized QG: Question asking is quite personalized: people with different characters and knowledge background ask different questions. However, integrating QG with user modeling in dialog management or recommendation system has not yet been explored. Explicitly modeling user state and awareness leads us towards personalized QG, which dovetails deep, end-to-end QG with deep user modeling and pairs the dual of generation–comprehension much in the same vein as in the vision–image generation area. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "31\n",
      "Question 1: What is the required cognitive level to answer the question?\n",
      "\n",
      "Answer 1: The required cognitive level to answer the question is one of the corpus-related factors that affect the difficulty of question generation.\n",
      "Question : for the text As QG can be regarded as a dual task of QA, in principle any QA dataset can be used for QG as well. However, there are at least two corpus-related factors that affect the difficulty of question generation. The first is the required cognitive level to answer the question, as we discussed in the previous section. Current NQG has achieved promising results on datasets consisting mainly of shallow factoid questions, such as SQuAD BIBREF36 and MS MARCO BIBREF38 . However, the performance drops significantly on deep question datasets, such as LearningQ BIBREF8 , shown in Section \"Generation of Deep Questions\" . The second factor is the answer type, i.e., the expected form of the answer, typically having four settings: (1) the answer is a text span in the passage, which is usually the case for factoid questions, (2) human-generated, abstractive answer that may not appear in the passage, usually the case for deep questions, (3) multiple choice question where question and its distractors should be jointly generated, and (4) no given answer, which requires the model to automatically learn what is worthy to ask. The design of NQG system differs accordingly..Table 1 presents a listing of the NQG corpora grouped by their cognitive level and answer type, along with their statistics. Among them, SQuAD was used by most groups as the benchmark to evaluate their NQG models. This provides a fair comparison between different techniques. However, it raises the issue that most NQG models work on factoid questions with answer as text span, leaving other types of QG problems less investigated, such as generating deep multi-choice questions. To overcome this, a wider variety of corpora should be benchmarked against in future NQG research. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "32\n",
      "Question 1: What are the three trends that practitioners should pay attention to as NQG becomes more prominent in QG? \n",
      "Answer 1: The three trends that practitioners should be aware of as NQG evolves are Multi-task Learning, Wider Input Modalities, and Deep Question Generation.\n",
      "Question : for the text We discuss three trends that we wish to call practitioners' attention to as NQG evolves to take the center stage in QG: Multi-task Learning, Wider Input Modalities and Deep Question Generation. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "33\n",
      "Question 1: How do NQG systems consider the target answer when generating questions?\n",
      "\n",
      "Answer 1: NQG systems consider the target answer as an additional input to guide the model in deciding which information to focus on when generating questions. This is typically done by either treating the answer's position as an extra input feature or by encoding the answer with a separate RNN.\n",
      "Question : for the text The most commonly considered factor by current NQG systems is the target answer, which is typically taken as an additional input to guide the model in deciding which information to focus on when generating; otherwise, the NQG model tend to generate questions without specific target (e.g., “What is mentioned?\"). Models have solved this by either treating the answer's position as an extra input feature BIBREF48 , BIBREF51 , or by encoding the answer with a separate RNN BIBREF49 , BIBREF52 ..The first type of method augments each input word vector with an extra answer indicator feature, indicating whether this word is within the answer span. BIBREF48 implement this feature using the BIO tagging scheme, while BIBREF50 directly use a binary indicator. In addition to the target answer, BIBREF53 argued that the context words closer to the answer also deserve more attention from the model, since they are usually more relevant. To this end, they incorporate trainable position embeddings $(d_{p_1}, d_{p_2}, \\cdots , d_{p_n})$ into the computation of attention distribution, where $p_i$ is the relative distance between the $i$ -th word and the answer, and $d_{p_i}$ is the embedding of $p_i$ . This achieved an extra BLEU-4 gain of $0.89$ on SQuAD..To generate answer-related questions, extra answer indicators explicitly emphasize the importance of answer; however, it also increases the tendency that generated questions include words from the answer, resulting in useless questions, as observed by BIBREF52 . For example, given the input “John Francis O’Hara was elected president of Notre Dame in 1934.\", an improperly generated question would be “Who was elected John Francis?\", which exposes some words in the answer. To address this, they propose to replace the answer into a special token for passage encoding, and a separate RNN is used to encode the answer. The outputs from two encoders are concatenated as inputs to the decoder. BIBREF54 adopted a similar idea that separately encodes passage and answer, but they instead use the multi-perspective matching between two encodings as an extra input to the decoder..We forecast treating the passage and the target answer separately as a future trend, as it results in a more flexible model, which generalizes to the abstractive case when the answer is not a text span in the input passage. However, this inevitably increases the model complexity and difficulty in training. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "34\n",
      "Question 1: Can automatic evaluation metrics for NLG accurately measure fluency, adequacy, and coherence of generated questions?\n",
      "\n",
      "Answer 1: Some studies have shown that automatic evaluation metrics such as BLEU, METEOR, and ROUGE do not correlate well with fluency, adequacy, and coherence of generated questions as they primarily compute the n-gram similarity between the source sentence and the generated question. Therefore, improved evaluation schemes are required to specifically investigate the mechanism of question asking.\n",
      "Question : for the text Although the datasets are commonly shared between QG and QA, it is not the case for evaluation: it is challenging to define a gold standard of proper questions to ask. Meaningful, syntactically correct, semantically sound and natural are all useful criteria, yet they are hard to quantify. Most QG systems involve human evaluation, commonly by randomly sampling a few hundred generated questions, and asking human annotators to rate them on a 5-point Likert scale. The average rank or the percentage of best-ranked questions are reported and used for quality marks..As human evaluation is time-consuming, common automatic evaluation metrics for NLG, such as BLEU BIBREF41 , METEOR BIBREF42 , and ROUGE BIBREF43 , are also widely used. However, some studies BIBREF44 , BIBREF45 have shown that these metrics do not correlate well with fluency, adequacy, coherence, as they essentially compute the $n$ -gram similarity between the source sentence and the generated question. To overcome this, BIBREF46 proposed a new metric to evaluate the “answerability” of a question by calculating the scores for several question-specific factors, including question type, content words, function words, and named entities. However, as it is newly proposed, it has not been applied to evaluate any NQG system yet..To accurately measure what makes a good question, especially deep questions, improved evaluation schemes are required to specifically investigate the mechanism of question asking. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "35\n",
      "What are the three aspects that provide a conceptual framework for understanding QG?\n",
      "The three aspects are (1) the learning paradigm, (2) the input modalities, and (3) the cognitive level it involves.\n",
      "Question : for the text For the sake of clean exposition, we first provide a broad overview of QG by conceptualizing the problem from the perspective of the three introduced aspects: (1) its learning paradigm, (2) its input modalities, and (3) the cognitive level it involves. This combines past research with recent trends, providing insights on how NQG connects to traditional QG research. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "36\n",
      "Question 1: What is the difficulty in generating high-quality deep questions, according to BIBREF81 and BIBREF82?\n",
      "\n",
      "Answer 1: BIBREF81 pointed out that asking high-quality deep questions is difficult, even for humans. Citing the study from BIBREF82, it was shown that students in college asked only about 6 deep-reasoning questions per hour in a question-encouraging tutoring session. These deep questions require reasoning over events, evaluation, opinions, syntheses or reasons, corresponding to higher-order cognitive levels.\n",
      "Question : for the text Endowing a QG system with the ability to ask deep questions will help us build curious machines that can interact with humans in a better manner. However, BIBREF81 pointed out that asking high-quality deep questions is difficult, even for humans. Citing the study from BIBREF82 to show that students in college asked only about 6 deep-reasoning questions per hour in a question–encouraging tutoring session. These deep questions are often about events, evaluation, opinions, syntheses or reasons, corresponding to higher-order cognitive levels..To verify the effectiveness of existing NQG models in generating deep questions, BIBREF8 conducted an empirical study that applies the attention Seq2Seq model on LearningQ, a deep-question centric dataset containing over 60 $\\%$ questions that require reasoning over multiple sentences or external knowledge to answer. However, the results were poor; the model achieved miniscule BLEU-4 scores of $< 4$ and METEOR scores of $< 9$ , compared with $> 12$ (BLEU-4) and $> 16$ (METEOR) on SQuAD. Despite further in-depth analysis are needed to explore the reasons behind, we believe there are two plausible explanations: (1) Seq2Seq models handle long inputs ineffectively, and (2) Seq2Seq models lack the ability to reason over multiple pieces of information..Despite still having a long way to go, some works have set out a path forward. A few early QG works attempted to solve this through building deep semantic representations of the entire text, using concept maps over keywords BIBREF83 or minimal recursion semantics BIBREF84 to reason over concepts in the text. BIBREF35 proposed a crowdsourcing-based workflow that involves building an intermediate ontology for the input text, soliciting question templates through crowdsourcing, and generating deep questions based on template retrieval and ranking. Although this process is semi-automatic, it provides a practical and efficient way towards deep QG. In a separate line of work, BIBREF1 proposed a framework that simulates how people ask deep questions by treating questions as formal programs that execute on the state of the world, outputting an answer..Based on our survey, we believe the roadmap towards deep NGQ points towards research that will (1) enhance the NGQ model with the ability to consider relationships among multiple source sentences, (2) explicitly model typical reasoning patterns, and (3) understand and simulate the mechanism behind human question asking. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "37\n",
      "Question 1: What are some examples of input modalities used in traditional QG?\n",
      "Answer 1: Traditional QG mainly focused on textual inputs, specifically declarative sentences, due to their application domains in question answering and education.\n",
      "Question : for the text Question generation is an NLG task for which the input has a wealth of possibilities depending on applications. While a host of input modalities have been considered in other NLG tasks, such as text summarization BIBREF24 , image captioning BIBREF25 and table-to-text generation BIBREF26 , traditional QG mainly focused on textual inputs, especially declarative sentences, explained by the original application domains of question answering and education, which also typically featured textual inputs..Recently, with the growth of various QA applications such as Knowledge Base Question Answering (KBQA) BIBREF27 and Visual Question Answering (VQA) BIBREF28 , NQG research has also widened the spectrum of sources to include knowledge bases BIBREF29 and images BIBREF10 . This trend is also spurred by the remarkable success of neural models in feature representation, especially on image features BIBREF30 and knowledge representations BIBREF31 . We discuss adapting NQG models to other input modalities in Section \"Wider Input Modalities\" . generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "38\n",
      "Question 1: What is Question Generation (QG)?\n",
      "\n",
      "Answer 1: QG is the task of automatically generating questions from various inputs such as raw text, database, or semantic representation. It involves generating grammatically and semantically correct questions and is of practical importance in education, dialog systems, and reading comprehension. Recent developments in deep learning have brought in three emergent trends in QG: (1) the change of learning paradigm, (2) the broadening of the input spectrum, and (3) the generation of deep questions.\n",
      "Question : for the text Question Generation (QG) concerns the task of “automatically generating questions from various inputs such as raw text, database, or semantic representation\" BIBREF0 . People have the ability to ask rich, creative, and revealing questions BIBREF1 ; e.g., asking Why did Gollum betray his master Frodo Baggins? after reading the fantasy novel The Lord of the Rings. How can machines be endowed with the ability to ask relevant and to-the-point questions, given various inputs? This is a challenging, complementary task to Question Answering (QA). Both QA and QG require an in-depth understanding of the input source and the ability to reason over relevant contexts. But beyond understanding, QG additionally integrates the challenges of Natural Language Generation (NLG), i.e., generating grammatically and semantically correct questions..QG is of practical importance: in education, forming good questions are crucial for evaluating students’ knowledge and stimulating self-learning. QG can generate assessments for course materials BIBREF2 or be used as a component in adaptive, intelligent tutoring systems BIBREF3 . In dialog systems, fluent QG is an important skill for chatbots, e.g., in initiating conversations or obtaining specific information from human users. QA and reading comprehension also benefit from QG, by reducing the needed human labor for creating large-scale datasets. We can say that traditional QG mainly focused on generating factoid questions from a single sentence or a paragraph, spurred by a series of workshops during 2008–2012 BIBREF4 , BIBREF5 , BIBREF6 , BIBREF7 ..Recently, driven by advances in deep learning, QG research has also begun to utilize “neural” techniques, to develop end-to-end neural models to generate deeper questions BIBREF8 and to pursue broader applications BIBREF9 , BIBREF10 ..While there have been considerable advances made in NQG, the area lacks a comprehensive survey. This paper fills this gap by presenting a systematic survey on recent development of NQG, focusing on three emergent trends that deep learning has brought in QG: (1) the change of learning paradigm, (2) the broadening of the input spectrum, and (3) the generation of deep questions. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "39\n",
      "What are the two fundamental aspects in QG research?\n",
      "The two fundamental aspects in QG research are \"what to ask\" and \"how to ask\".\n",
      "Question : for the text QG research traditionally considers two fundamental aspects in question asking: “What to ask” and “How to ask”. A typical QG task considers the identification of the important aspects to ask about (“what to ask”), and learning to realize such identified aspects as natural language (“how to ask”). Deciding what to ask is a form of machine understanding: a machine needs to capture important information dependent on the target application, akin to automatic summarization. Learning how to ask, however, focuses on aspects of the language quality such as grammatical correctness, semantically preciseness and language flexibility..Past research took a reductionist approach, separately considering these two problems of “what” and “how” via content selection and question construction. Given a sentence or a paragraph as input, content selection selects a particular salient topic worthwhile to ask about and determines the question type (What, When, Who, etc.). Approaches either take a syntactic BIBREF11 , BIBREF12 , BIBREF13 or semantic BIBREF14 , BIBREF3 , BIBREF15 , BIBREF16 tack, both starting by applying syntactic or semantic parsing, respectively, to obtain intermediate symbolic representations. Question construction then converts intermediate representations to a natural language question, taking either a tranformation- or template-based approach. The former BIBREF17 , BIBREF18 , BIBREF13 rearranges the surface form of the input sentence to produce the question; the latter BIBREF19 , BIBREF20 , BIBREF21 generates questions from pre-defined question templates. Unfortunately, such QG architectures are limiting, as their representation is confined to the variety of intermediate representations, transformation rules or templates..In contrast, neural models motivate an end-to-end architectures. Deep learned frameworks contrast with the reductionist approach, admitting approaches that jointly optimize for both the “what” and “how” in an unified framework. The majority of current NQG models follow the sequence-to-sequence (Seq2Seq) framework that use a unified representation and joint learning of content selection (via the encoder) and question construction (via the decoder). In this framework, traditional parsing-based content selection has been replaced by more flexible approaches such as attention BIBREF22 and copying mechanism BIBREF23 . Question construction has become completely data-driven, requiring far less labor compared to transformation rules, enabling better language flexibility compared to question templates..However, unlike other Seq2Seq learning NLG tasks, such as Machine Translation, Image Captioning, and Abstractive Summarization, which can be loosely regarded as learning a one-to-one mapping, generated questions can differ significantly when the intent of asking differs (e.g., the target answer, the target aspect to ask about, and the question's depth). In Section \"Methodology\" , we summarize different NQG methodologies based on Seq2Seq framework, investigating how some of these QG-specific factors are integrated with neural models, and discussing what could be further explored. The change of learning paradigm in NQG era is also represented by multi-task learning with other NLP tasks, for which we discuss in Section \"Multi-task Learning\" . generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "40\n",
      "Question 1: What is the goal of an NQG model under the Seq2Seq architecture? \n",
      "Answer 1: The goal of an NQG model under the Seq2Seq architecture is to generate a question asking about the target answer in the passage, which is defined as finding the best question that maximizes the conditional likelihood given the passage and the answer.\n",
      "Question : for the text Many current NQG models follow the Seq2Seq architecture. Under this framework, given a passage (usually a sentence) $X = (x_1, \\cdots , x_n)$ and (possibly) a target answer $A$ (a text span in the passage) as input, an NQG model aims to generate a question $Y = (y_1, \\cdots , y_m)$ asking about the target answer $A$ in the passage $X$ , which is defined as finding the best question $\\bar{Y}$ that maximizes the conditional likelihood given the passage $X$ and the answer $A$ :.$$\\bar{Y} & = \\arg \\max _Y P(Y \\vert X, A) \\\\\n",
      "\\vspace{-14.22636pt}\n",
      "& = \\arg \\max _Y \\sum _{t=1}^m P(y_t \\vert X, A, y_{< t})$$   (Eq. 5) . BIBREF47 pioneered the first NQG model using an attention Seq2Seq model BIBREF22 , which feeds a sentence into an RNN-based encoder, and generate a question about the sentence through a decoder. The attention mechanism is applied to help decoder pay attention to the most relevant parts of the input sentence while generating a question. Note that this base model does not take the target answer as input. Subsequently, neural models have adopted attention mechanism as a default BIBREF48 , BIBREF49 , BIBREF50 ..Although these NQG models all share the Seq2Seq framework, they differ in the consideration of — (1) QG-specific factors (e.g., answer encoding, question word generation, and paragraph-level contexts), and (2) common NLG techniques (e.g., copying mechanism, linguistic features, and reinforcement learning) — discussed next. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "41\n",
      "Question 1: How has QG been applied to improve semantic parsing and QA tasks?\n",
      "Answer 1: QG has been used to generate pseudo-labeled data for an enlarged training set in semantic parsing, and to create more questions for self-training and iterative retraining in QA. QG has also been used in multi-task learning for text summarization and to generate answer-related questions for candidate answers in QA. Joint training of QG and QA has also been explored, but further research is needed to improve QG performance.\n",
      "Question : for the text As QG has become more mature, work has started to investigate how QG can assist in other NLP tasks, and vice versa. Some NLP tasks benefit from enriching training samples by QG to alleviate the data shortage problem. This idea has been successfully applied to semantic parsing BIBREF66 and QA BIBREF67 . In the semantic parsing task that maps a natural language question to a SQL query, BIBREF66 achieved a 3 $\\%$ performance gain with an enlarged training set that contains pseudo-labeled $(SQL, question)$ pairs generated by a Seq2Seq QG model. In QA, BIBREF67 employed the idea of self-training BIBREF68 to jointly learn QA and QG. The QA and QG models are first trained on a labeled corpus. Then, the QG model is used to create more questions from an unlabeled text corpus and the QA model is used to answer these newly-created questions. The newly-generated question–answer pairs form an enlarged dataset to iteratively retrain the two models. The process is repeated while performance of both models improve..Investigating the core aspect of QG, we say that a well-trained QG system should have the ability to: (1) find the most salient information in the passage to ask questions about, and (2) given this salient information as target answer, to generate an answer related question. BIBREF69 leveraged the first characteristic to improve text summarization by performing multi-task learning of summarization with QG, as both these two tasks require the ability to search for salient information in the passage. BIBREF49 applied the second characteristic to improve QA. For an input question $q$ and a candidate answer $\\hat{a}$ , they generate a question $\\hat{q}$ for $\\hat{a}$ by way of QG system. Since the generated question $\\hat{q}$ is closely related to $\\hat{a}$ , the similarity between $q$ and $\\hat{q}$ helps to evaluate whether $\\hat{a}$ is the correct answer..Other works focus on jointly training to combine QG and QA. BIBREF70 simultaneously train the QG and QA models in the same Seq2Seq model by alternating input data between QA and QG examples. BIBREF71 proposed a training algorithm that generalizes Generative Adversarial Network (GANs) BIBREF72 under the question answering scenario. The model improves QG by incorporating an additional QA-specific loss, and improving QA performance by adding artificially generated training instances from QG. However, while joint training has shown some effectiveness, due to the mixed objectives, its performance on QG are lower than the state-of-the-art results, which leaves room for future exploration. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "42\n",
      "Question 1: How did BIBREF51 address the challenge of effectively utilizing relevant contexts in Seq2Seq models as input texts get longer?\n",
      "\n",
      "Answer 1: BIBREF51 proposed a gated self-attention encoder to refine the encoded context by fusing important information with the context's self-representation properly, which has achieved state-of-the-art results on SQuAD. The encoded representation is fed through a gated self-matching network to embed intra-passage dependencies and a feature fusion gate chooses relevant information between the original and self-matching enhanced representations.\n",
      "Question : for the text Leveraging rich paragraph-level contexts around the input text is another natural consideration to produce better questions. According to BIBREF47 , around 20% of questions in SQuAD require paragraph-level information to be answered. However, as input texts get longer, Seq2Seq models have a tougher time effectively utilizing relevant contexts, while avoiding irrelevant information..To address this challenge, BIBREF51 proposed a gated self-attention encoder to refine the encoded context by fusing important information with the context's self-representation properly, which has achieved state-of-the-art results on SQuAD. The long passage consisting of input texts and its context is first embedded via LSTM with answer position as an extra feature. The encoded representation is then fed through a gated self-matching network BIBREF55 to aggregate information from the entire passage and embed intra-passage dependencies. Finally, a feature fusion gate BIBREF56 chooses relevant information between the original and self-matching enhanced representations..Instead of leveraging the whole context, BIBREF57 performed a pre-filtering by running a coreference resolution system on the context passage to obtain coreference clusters for both the input sentence and the answer. The co-referred sentences are then fed into a gating network, from which the outputs serve as extra features to be concatenated with the original input vectors. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "43\n",
      "Question 1: Why is it important to consider question word generation in QG?\n",
      "\n",
      "Answer 1: Question words play a vital role in QG as they determine the type and intention of the question being generated. However, current NQG systems often produce mismatched question words with the answer type, making it crucial to explore and improve question word generation in QG.\n",
      "Question : for the text Question words (e.g., “when”, “how”, and “why”) also play a vital role in QG; BIBREF53 observed that the mismatch between generated question words and answer type is common for current NQG systems. For example, a when-question should be triggered for answer “the end of the Mexican War\" while a why-question is generated by the model. A few works BIBREF49 , BIBREF53 considered question word generation separately in model design.. BIBREF49 proposed to first generate a question template that contains question word (e.g., “how to #\", where # is the placeholder), before generating the rest of the question. To this end, they train two Seq2Seq models; the former learns to generate question templates for a given text , while the latter learns to fill the blank of template to form a complete question. Instead of a two-stage framework, BIBREF53 proposed a more flexible model by introducing an additional decoding mode that generates the question word. When entering this mode, the decoder produces a question word distribution based on a restricted set of vocabulary using the answer embedding, the decoder state, and the context vector. The switch between different modes is controlled by a discrete variable produced by a learnable module of the model in each decoding step..Determining the appropriate question word harks back to question type identification, which is correlated with the question intention, as different intents may yield different questions, even when presented with the same (passage, answer) input pair. This points to the direction of exploring question pragmatics, where external contextual information (such as intent) can inform and influence how questions should optimally be generated. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "44\n",
      "Question 1: What technique do most NQG models employ for copying words from the source sentence to the question during decoding?\n",
      "\n",
      "Answer 1: Most NQG models employ the copying mechanism, which directly copies relevant words from the source sentence to the question during decoding. This technique is widely accepted as it is common to refer back to phrases and entities appearing in the text when formulating factoid questions.\n",
      "Question : for the text Common techniques of NLG have also been considered in NQG model, summarized as 3 tactics:.1. Copying Mechanism. Most NQG models BIBREF48 , BIBREF60 , BIBREF61 , BIBREF50 , BIBREF62 employ the copying mechanism of BIBREF23 , which directly copies relevant words from the source sentence to the question during decoding. This idea is widely accepted as it is common to refer back to phrases and entities appearing in the text when formulating factoid questions, and difficult for a RNN decoder to generate such rare words on its own..2. Linguistic Features. Approaches also seek to leverage additional linguistic features that complements word embeddings, including word case, POS and NER tags BIBREF48 , BIBREF61 as well as coreference BIBREF50 and dependency information BIBREF62 . These categorical features are vectorized and concatenated with word embeddings. The feature vectors can be either one-hot or trainable and serve as input to the encoder..3. Policy Gradient. Optimizing for just ground-truth log likelihood ignores the many equivalent ways of asking a question. Relevant QG work BIBREF60 , BIBREF63 have adopted policy gradient methods to add task-specific rewards (such as BLEU or ROUGE) to the original objective. This helps to diversify the questions generated, as the model learns to distribute probability mass among equivalent expressions rather than the single ground truth question. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "45\n",
      "Question 1: What is the state-of-the-art model for NQG on SQuAD, according to Table 2?\n",
      "\n",
      "Answer 1: According to Table 2, as of the writing of the text, BIBREF51 is the state-of-the-art model for NQG on SQuAD.\n",
      "Question : for the text In Table 2 , we summarize existing NQG models with their employed techniques and their best-reported performance on SQuAD. These methods achieve comparable results; as of this writing, BIBREF51 is the state-of-the-art..Two points deserve mention. First, while the copying mechanism has shown marked improvements, there exist shortcomings. BIBREF52 observed many invalid answer-revealing questions attributed to the use of the copying mechanism; cf the John Francis example in Section \"Emerging Trends\" . They abandoned copying but still achieved a performance rivaling other systems. In parallel application areas such as machine translation, the copy mechanism has been to a large extent replaced with self-attention BIBREF64 or transformer BIBREF65 . The future prospect of the copying mechanism requires further investigation. Second, recent approaches that employ paragraph-level contexts have shown promising results: not only boosting performance, but also constituting a step towards deep question generation, which requires reasoning over rich contexts. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "46\n",
      "Question 1: Who was born in New York?\n",
      "\n",
      "Answer 1: Donald Trump was born in New York.\n",
      "Question : for the text QG work now has incorporated input from knowledge bases (KBQG) and images (VQG)..Inspired by the use of SQuAD as a question benchmark, BIBREF9 created a 30M large-scale dataset of (KB triple, question) pairs to spur KBQG work. They baselined an attention seq2seq model to generate the target factoid question. Due to KB sparsity, many entities and predicates are unseen or rarely seen at training time. BIBREF73 address these few-/zero-shot issues by applying the copying mechanism and incorporating textual contexts to enrich the information for rare entities and relations. Since a single KB triple provides only limited information, KB-generated questions also overgeneralize — a model asks “Who was born in New York?\" when given the triple (Donald_Trump, Place_of_birth, New_York). To solve this, BIBREF29 enrich the input with a sequence of keywords collected from its related triples..Visual Question Generation (VQG) is another emerging topic which aims to ask questions given an image. We categorize VQG into grounded- and open-ended VQG by the level of cognition. Grounded VQG generates visually grounded questions, i.e., all relevant information for the answer can be found in the input image BIBREF74 . A key purpose of grounded VQG is to support the dataset construction for VQA. To ensure the questions are grounded, existing systems rely on image captions to varying degrees. BIBREF75 and BIBREF76 simply convert image captions into questions using rule-based methods with textual patterns. BIBREF74 proposed a neural model that can generate questions with diverse types for a single image, using separate networks to construct dense image captions and to select question types..In contrast to grounded QG, humans ask higher cognitive level questions about what can be inferred rather than what can be seen from an image. Motivated by this, BIBREF10 proposed open-ended VQG that aims to generate natural and engaging questions about an image. These are deep questions that require high cognition such as analyzing and creation. With significant progress in deep generative models, marked by variational auto-encoders (VAEs) and GANs, such models are also used in open-ended VQG to bring “creativity” into generated questions BIBREF77 , BIBREF78 , showing promising results. This also brings hope to address deep QG from text, as applied in NLG: e.g., SeqGAN BIBREF79 and LeakGAN BIBREF80 . generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "47\n",
      "What is the purpose of constructing a transformation matrix and learning a transforming function between two languages for named entity (NE) distribution modeling?\n",
      "Answer 1: The purpose is to obtain the NE distribution of a target language without an NE dictionary from a source language with known NE distributions. This is achieved by constructing a transformation matrix and learning a transforming function between the two languages, using a set of parallel word pairs and their embeddings.\n",
      "Question : for the text As the isomorphism characteristic exists between languages BIBREF3 , BIBREF15 , we can apply the distributional modeling for every languages in the same way. For a target language without an NE dictionary, its NE distribution can be obtained from a source language with known NE distributions by learning the transforming function between these two languages. We construct the transformation matrix $W$ via a set of parallel word pairs (the set will be referred to seed pairs hereafter) and their word embeddings $\\lbrace X^{(i)}, Z^{(i)}\\rbrace _{i=1}^m$ BIBREF3 , $\\lbrace X^{(i)}\\rbrace _{i=1}^m$ , $\\lbrace Z^{(i)}\\rbrace _{i=1}^m$ are the source and target word embeddings respectively. $W$ can be learned by solving the matrix equation $XW = Z$ . Then, given the source center vector ${ O_1}$ , the mapping center vector ${O_2}$ can be expressed as: .$${ O_2} = W^T{O_1}$$   (Eq. 11) .Actually, the isomorphism (mapping) between embedding spaces is the type of affine isomorphism by furthermore considering embedding in continuous space. The invariant characteristics of relative position BIBREF16 , BIBREF17 , BIBREF18 , BIBREF19 in affine transformation is applied to correct transformation matrix errors caused by limited amount of parallel word pairs (the set will be referred to seed pairs hereafter). As shown in Figure 2, the ratio of the line segments keep constant when the distance is linearly enlarged or shortened. Recall that point $Q$ is an affine combination of two other noncoincident points $Q_1$ and $Q_2$ on the line: $Q = (1-t)Q_1 + tQ_2 $ ..We apply the affine mapping $f$ and get: $f(Q) = f((1-t)Q_1 + tQ_2) = (1-t)f(Q_1) + tf(Q_2)$ Obviously, the constant ratio $t$ is not affected by the affine transformation $f$ . That is, $Q$ has the same relative distances between it and $Q_1$ and $Q_2$ during the process of transformation. Based on the above characteristic, for any point $X^{(i)}$ in the source space and its mapping point $Z^{(i)}$ , $X^{(i)}$ and $f(Q) = f((1-t)Q_1 + tQ_2) = (1-t)f(Q_1) + tf(Q_2)$0 cut off radiuses with the same ratio, namely, the ratio of the distance of these two points to their centers and their radiuses remains unchanged. .$$\\frac{E( O_1, X^{(i)})}{r_1} = \\frac{E( O_2, Z^{(i)})}{r_2}$$   (Eq. 15) .where $E$ represents the adopted Euclidean distance, ${O_1, O_2, r_1, r_2}$ are the centers and radii of hyperspheres. We convert the equation and learn the optimized mapping center ${O_2}$ and ratio $K$ via the seed pairs: .$${K = \\frac{r_2}{r_1} = \\frac{E( O_2, Z^{(i)})}{E( O_1, X^{(i)})}}$$   (Eq. 16) .$$\\begin{aligned}\n",
      "E( O_2, Z^{(i)}) &= K * E( O_1, X^{(i)}) \\quad r_2 &= K * r_1 \\\\\n",
      "\\end{aligned}$$   (Eq. 17) .Given the seed pairs $\\lbrace X^{(i)}, Z^{(i)}\\rbrace _{i=1}^m$ , the initialized center $O_2$ in Equation (3), the center $ O_1 $ and radius $ r_1 $ of the hypersphere in source language space, we may work out the optimized ratio $K$ , the mapping center $ O_2 $ and radius $ r_2 $ in target language space by solving the linear equation group (5). generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "48\n",
      "What is the evaluation method used to judge the hypersphere mapping between English and Chinese?\n",
      "\n",
      "The evaluation method used to judge the hypersphere mapping between English and Chinese includes precision (P), recall (R) and F-1 score (F1), which are calculated based on the volumes of the target, mapping, and intersection hyperspheres using Monte Carlo methods.\n",
      "Question : for the text The following preparations were made for the mapping: $(i)$ A large enough NE dictionary in source (resource-rich) corpus; $(ii)$ A small amount of annotated seed pairs. We use $s$ to represent the number of seed pairs and $d$ to represent the number of unknown variables. With seed pair size $s < d$ , the matrix can be solved with much loose constraints, and $F_1$ score remarkably increases with more seed pairs. Once $s > d$ , the linear equation group will be always determined by strong enough constraints, which leads to a stable solution. Based on the characteristics, we only take two dozen of seed pairs on each type in following experiments. We combine human translation and online translation together for double verification for this small set of seed pairs. In this part, we utilize English and Chinese as the corpus of known NEs in turn, and predict the NE distribution of the other language..Evaluation In order to quantitatively represent the mapping effect, we present a new evaluation method to judge the hypersphere mapping between English and Chinese: .$$\\begin{aligned}\n",
      "P = \\frac{V_i}{V_m} \\quad R = \\frac{V_i}{V_t} \\quad F_1 = \\frac{2 * P * R}{P + R}\n",
      "\\end{aligned}$$   (Eq. 29) .where ${V_t, V_m, V_i}$ represent the volumes of the target, mapping and intersection hyperspheres. Due to the difficulty of calculating the volume of hyperspheres in high dimensions, we adopt Monte Carlo methods to simulate the volume BIBREF25 . we generate a great quantity of points in the embedding spaces, and take the amount of the points falling in each hypersphere as its volume..Mapping between English and Chinese Table 4 shows the comparisons of cross-lingual named entity extraction performance. We use the unsupervised method proposed in BIBREF26 to generate cross-lingual embeddings. $k$ -NN and SVM are the same as monolingual cases in Table 3 except for the training set. $k$ -NN $_{150}$ and SVM $_{150}$ use 20% of the NEs in source language and 150 NEs (50 LOC, PER and ORG) in target language for training, while $k$ -NN $_{2500}$ and SVM $_{2500}$ use 20% of the NEs in source language and 2500 NEs (1000 LOC and PER, 500 ORG) in target language. $k$ -NN and SVM depend much on the annotated training set, requiring more than $1K$ training samples to provide a performance as our model offers. Due to the instability of ORG type in length, taking the average of each word embedding may disobey the syntactic and semantic regularities of ORG NEs, thereby undermines the multilingual isomorphism characteristics, which causes the inferior performance of our model on this type of NEs. This suggests that build better representations NEs for multi-word NEs may contribute to a better performance in our model..Mapping to truly Low-resource Language We build named entity dataset for a truly resource-poor language, Indonesian, and manually examine the nearest words to the hypersphere center for 'gold-standard' evaluation. We take English as the source language, the settings of the dimension $D$ and the number of seed pairs $s$ are the same as the above experiments between Chinese and English. From the results listed in Table 5, we can see that even the precision of the top-100 NEs are 0.350 $F_1$ /0.440 $F_1$ /0.310 $F_1$ , respectively, which proves the this distribution can indeed serves as a candidate NE dictionary for Indonesian..[9] The authors of BIBREF24 publish an updated results (92.98) on CoNLL-2003 dataset in https://github.com/zalandoresearch/flair/issues/206 on their 0.3.2 version, and this is the best result at our most try. [10] This is the reported state-of-the-art result in their github. [11]We use the same parameters as the authors release in https://github.com/zalandoresearch/flair/issues/173 and obtain the result of 89.45 on ONTONOTES 5.0 dataset. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "49\n",
      "What is the proposed solution for representing named entities through an open definition?\n",
      "\n",
      "The proposed solution is to model the embedding distributions of named entities with least parameters and visualize these distributions in a hypersphere. This allows for a mapping between the distribution of named entities in embedding spaces, which can aid in cross-lingual NE recognition and building NE dictionaries for low-resource languages. Additionally, the hypersphere features can be used to enhance off-the-shelf NER systems.\n",
      "Question : for the text Named entities being an open set which keeps expanding are difficult to represent through a closed NE dictionary. This work mitigates significant defects in previous closed NE definitions and proposes a new open definition for NEs by modeling their embedding distributions with least parameters. We visualize NE distributions in monolingual case and perform an effective isomorphism spaces mapping in cross-lingual case. According to our work, we demonstrate that common named entity types (PER, LOC, ORG) tend to be densely distributed in a hypersphere and it is possible to build a mapping between the NE distributions in embedding spaces to help cross-lingual NE recognition. Experimental results show that the distribution of named entities via mapping can be used as a good enough replacement for the original distribution. Then the discovery is used to build an NE dictionary for Indonesian being a truly low-resource language, which also gives satisfactory precision. Finally, our simple hypersphere features being the representation of NE likelihood can be used for enhancing off-the-shelf NER systems by concatenating with word embeddings and the output of BiLSTM in the input layer and encode layer, respectively, and we achieve a new state-of-the-art $F_1$ score of 89.75 on ONTONOTES 5.0 benchmark. In this work, we also give a better solution for unregistered NEs. For any newly emerged NE together with its embedding, in case we obtain the hypersphere of each named entity, the corresponding named entity category can be determined by calculating the distance between its word embedding and the center of each hypersphere. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "50\n",
      "Question 1: What models were used to evaluate the hypersphere model?\n",
      "Answer 1: The three models used to evaluate the hypersphere model were open monolingual NE modeling, embedding distribution mapping, and refinement NE recognition.\n",
      "Question : for the text In this section, we evaluate the hypersphere model based on the three models introduced above: open monolingual NE modeling, embedding distribution mapping and refinement NE recognition. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "1\n",
      "Question 1: What is the Euclidean distance referred to as in the text?\n",
      "\n",
      "Answer 1: The Euclidean distance is represented as a 3- $D$ vector and referred to as HS vector in the text.\n",
      "Question : for the text The Euclidean distance between word and hypersphere centers can be pre-computed as its NE likelihood, which may provide informative clues for NE recognition. We only consider three entity types in our experiment, and the Euclidean distance which is represented as a 3- $D$ vector and referred to HS vector hereafter) is added to four representative off-the-shelf NER systems to verify its effectiveness. We feed HS vector into different layers of the neural network: (1) input layer $[x_k; c_k; HS]$ ; (2) output layer of LSTM $[h_k; HS]$ , where $x_k$ , $w_k$ and $h_k$ represent word embeddings, char embeddings and the output of LSTM, respectively. All of these models are based on classical BiLSTM-CRF architecture BIBREF20 , except that BIBREF21 replaces CRF layer with softmax. These four baseline systems are introduced as follows.. BIBREF22 concatenates ELMo with word embeddings as the input of LSTM to enhance word representations as it carries both syntactic and semantic information.. BIBREF21 uses distant supervision for NER task and propose a new Tie or Break tagging scheme, where entity spans and entity types are encoded into two folds. They first build a binary classifier to distinguish Break from Tie, and then learn the entity types according to their occurrence and frequency in NE dictionary. The authors conduct their experiments on biomedical datasets rather than standard benchmark, so we extract the NEs in training data as the domain-specific dictionary. This work creates a promising prospect for using dictionary to replace the role of training data.. BIBREF23 takes advantage of the power of the 120 entity types from annotated data in Wikipedia. Cosine similarity between the word embedding and the embedding of each entity type is concatenated as the 120- $D$ feature vector (which is called LS vector in their paper) and then fed into the input layer of LSTM. Lexical feature has been shown a key factor to NE recognition.. BIBREF24 passes sentences as sequences of characters into a character-level language model to produce a novel type of word embedding, contextual string embeddings, where one word may have different embeddings as the embeddings are computed both on the characters of a word and its surrounding context. Such embeddings are then fed into the input layer of LSTM. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "2\n",
      "What is the main focus of this study on named entity recognition?\n",
      "\n",
      "The main focus of this study is to improve named entity recognition for resource-poor languages using word embeddings and a hypersphere mapping between embedding spaces, with the aim of learning from limited annotated linguistic resources.\n",
      "Question : for the text Named Entity Recognition is a major natural language processing task that recognizes the proper labels such as LOC (Location), PER (Person), ORG (Organization), etc. Like words or phrase, being a sort of language constituent, named entities also benefit from better representation for better processing. Continuous word representations, known as word embeddings, well capture semantic and syntactic regularities of words BIBREF0 and perform well in monolingual NE recognition BIBREF1 , BIBREF2 . Word embeddings also exhibit isomorphism structure across languages BIBREF3 . On account of these characteristics above, we attempt to utilize word embeddings to improve NE recognition for resource-poor languages with the help of richer ones. The state-of-the-art cross-lingual NE recognition methods are mainly based on annotation projection methods according to parallel corpora, translations BIBREF4 , BIBREF5 , BIBREF6 , BIBREF7 and Wikipedia methods BIBREF8 , BIBREF9 , BIBREF10 , BIBREF11 ..Most annotated corpus based NE recognition tasks can benefit a great deal from a known NE dictionary, as NEs are those words which carry common sense knowledge quite differ from the rest ones in any language vocabulary. This work will focus on the NE recognition from plain text instead of corpus based NE recognition. For a purpose of learning from limited annotated linguistic resources, our preliminary discovery shows that it is possible to build a geometric space projection between embedding spaces to help cross-lingual NE recognition. Our study contains two main steps: First, we explore the NE distribution in monolingual case. Next, we learn a hypersphere mapping between embedding spaces of languages with minimal supervision..Despite the simplicity of our model, we make the following contributions. First, for word embeddings generated by different dimensions and objective functions, all common NE types (PER, LOC, ORG) tend to be densely distributed in a hypersphere, which gives a better solution to characterize the general NE distribution rather than existing closed dictionary definition for NE. Second, with the help of the hypersphere mapping, it is possible to capture the NE distribution of resource-poor languages with only a small amount of annotated data. Third, our method is highly friendly to unregistered NEs, as the distance to each hypersphere center is the only factor needed to determine their NE categories. Finally, by adding hypersphere features we can significantly improve the performance of off-the-shelf named entity recognition (NER) systems. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "3\n",
      "Question: What is the goal of building a model to describe the distribution of word embeddings of named entities?\n",
      "\n",
      "Answer: The goal is to use these parameters as a decoder for any word to directly determine whether it belongs to a certain type of entity and improve the performance of state-of-the-art NE recognition systems.\n",
      "Question : for the text Encouraged by the verification of nearest neighbors of NEs still being NEs, we attempt to build a model which can represent this property with least parameters. Namely, given an NE dictionary on a monolingual, we build a model to describe the distribution of the word embeddings of these entities, then we can easily use these parameters as a decoder for any word to directly determine whether it belongs to a certain type of entity. In this section, we first introduce the open modeling from embedding distribution in monolingual cases, and then put forward the mapping of the distribution model between languages, and then use the mapping to build named entity dataset for resource-poor languages. Finally, we use the proposed named entity model to improve the performance of state-of-the-art NE recognition systems. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "4\n",
      "Q: What is the most suitable dimension for ORG, PER, and LOC NE types? \n",
      "A: The most suitable dimension for ORG, PER, and LOC NE types are 16- ${D}$ /16- ${D}$ /24- ${D}$, respectively.\n",
      "Question : for the text The NE distribution is closely correlated to the dimension of the embedding space, we train the word embeddings from 2- $D$ to 300- $D$ and search for the most suitable dimension for each NE type. For each dimension, we carefully tune the center and radius of the hypersphere using the method introduced in section 3.1 for maximize $F_1$ score, and select the dimension with maximize $F_1$ score. The most suitable dimension for ORG, PER, LOC are 16- ${D}$ /16- ${D}$ /24- ${D}$ (these dimensions will be used as parameters in the following experiments), respectively . We discover that in low-dimensional space, the distributions of NEs are better. In high dimensions, the curse of dimension could be the main reason to limit the performance..Table 3 lists the final maximum $F_1$ score of three NE types. The results of the three types of NE are almost 50%, and PER type performs best. The main factor may be that PER NEs are represented as single-word in our dictionary, and word embeddings can better represents their meanings. The result also states that better representations for multi-word NEs which are not covered by the dictionary instead of the average of each word may help bring better results. Besides, the incompleteness of NE dictionaries and noises during pre-processing may cause a decrease on the performance. Overall, hypersphere model has shown been effectively used as the open modeling for NEs. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "5\n",
      "Question 1: How do the hypersphere features improve the performance of NER systems, and what is the new state-of-the-art F1 score achieved using them?\n",
      "\n",
      "Answer 1: The hypersphere features contribute to nearly all three types of entities, and they enhance strong state-of-the-art baselines by 0.33/0.72/0.23 F1 point on CoNLL-2003 and ONTONOTES 5.0 datasets. The HS feature is comparable to the more complicated LS feature and surpasses its baseline by 0.58/0.78 F1 point. The new state-of-the-art F1 score achieved using HS features is 89.75 on ONTONOTES 5.0, while matching the state-of-the-art F1 score of 92.95 on the CoNLL-2003 dataset.\n",
      "Question : for the text To evaluate the influence of our hypersphere feature for off-the-shelf NER systems, we perform the NE recognition on two standard NER benchmark datasets, CoNLL2003 and ONTONOTES 5.0. Our results in Table 6 and Table 7 demonstrate the power of hypersphere features, which contribute to nearly all of the three types of entities as shown in Table 6, except for a slight drop in the PER type of BIBREF22 on a strong baseline. HS features stably enhance all strong state-of-the-art baselines, BIBREF22 , BIBREF21 and BIBREF23 by 0.33/0.72/0.23 $F_1$ point and 0.13/0.3/0.1 $F_1$ point on both benchmark datasets, CoNLL-2003 and ONTONOTES 5.0. We show that our HS feature is also comparable with previous much more complicated LS feature, and our model surpasses their baseline (without LS feature) by 0.58/0.78 $F_1$ point with only HS features. We establish a new state-of-the-art $F_1$ score of 89.75 on ONTONOTES 5.0, while matching state-of-the-art performance with a $F_1$ score of 92.95 on CoNLL-2003 dataset. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "6\n",
      "What is the mathematical intuition behind using a hypersphere for entity representation and classification?\n",
      "\n",
      "The mathematical intuition behind using a hypersphere for entity representation and classification is similar to that of support vector machine (SVM), which uses a kernel to obtain the optimal margin in very high dimensional spaces through linear hyperplane separation in Descartes coordination. However, in the case of entity representation and classification, a closed surface is needed instead of an open hyperplane, and the hypersphere is such a smooth, closed boundary with the least parameters as a counterpart of hyperplane in Descartes coordinates. The hypersphere is a suitable boundary shape to model the separation of entity distributions.\n",
      "Question : for the text As illustrated is Figure 1, the embedding distribution of NEs is aggregated, and there exists a certain boundary between different types of NEs. We construct an open representation for each type of NEs – hypersphere, the NE type of any entity can be easily judged by checking whether it is inside a hypersphere, which makes a difference from the defining way of any limited and insufficient NE dictionary. The hypersphere can be expressed as follows: .$$E( X, O) \\le r$$   (Eq. 9) .where E represents the adopted Euclidean distance, X is referred to any point in the hypersphere, $ O $ and $ r $ are the center vector and radius. For each entity type, we attempt to construct a hypersphere which encompass as many congeneric NEs as possible, and as few as possible inhomogeneous NEs, we use $F_1$ score as a trade-off between these two concerns. We carefully tune the center and radius of the hypersphere to maximize its $F_1$ score: we first fix the center as the average of all NE embeddings from known NE dictionaries, and search the best radius in $[minDist, maxDist]$ , where $minDist/maxDist$ refers to the distance between the center and its nearest/farthest neighbors; Then, we kick NEs which are far from the center with the distance threshold $q$ (much larger than the radius) to generate a new center; Finally, we tune the threshold $q$ and repeat the above steps to find the most suitable center and radius..The mathematical intuition for using a hypersphere can be interpreted in a manner similar to support vector machine (SVM) BIBREF14 , which uses the kernel to obtain the optimal margin in very high dimensional spaces through linear hyperplane separation in Descartes coordination. We transfer the idea to the separation of NE distributions. The only difference is about boundary shape, what we need is a closed surface instead of an open hyperplane, and hypersphere is such a smooth, closed boundary (with least parameters as well) in polar coordinates as counterpart of hyperplane in Descartes coordinates. Using the least principle to model the mathematical objective also follows the Occam razor principle..Figure 1 also reveals that the distribution of PER NEs is compact, while ORG NE distribution is relatively sparse. Syntactically, PER NEs are more stable in terms of position and length in sentences compared to ORG NEs, so that they have a more accurate embedding representation with strong strong syntax and semantics, making the corresponding word embeddings closer to central region of the hypersphere. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "7\n",
      "Question 1: What is the advantage of using word embeddings in NE recognition?\n",
      "\n",
      "Answer 1: Word embeddings can reveal linguistic features in morphological, syntactic, and semantic perspectives, making it easier to recognize named entities. They can enhance the performance of NE recognition by clustering embeddings and using multiple cluster granularities, as well as considering that syntactically and semantically similar words are likely to be neighbors in the embedding space.\n",
      "Question : for the text In recent years, word embeddings have also been used as a feature to enhance the NE recognition, with the revealing of linguistic features in morphological, syntactic and semantic perspective. BIBREF1 clustered the word embeddings and combined multiple cluster granularities to improve the NE recognition performance. Our work likewise use word embeddings to help NE recognition, we make use of the characteristic that syntactically and semantically s are more likely to be neighbors in embedding spaces and construct a hypersphere model to encompass NEs..Cross-lingual knowledge transfer is a highly promising work for resource-poor languages, annotation projection and representation projection are widely used in NE recognition BIBREF27 , BIBREF5 , BIBREF4 , BIBREF28 , BIBREF29 , BIBREF30 . These works put forward inconvenient requirements for parallel or comparable corpora, a large amount of annotated or translation data or bilingual lexicon. Different from any existing work to the best of our knowledge, this is the first work that merely uses isomorphic mappings in low-dimensional embedding spaces to recognize NEs, and we introduce a mathematically simple model to describe NE embedding distribution from visualization results, showing it works for both monolingual and cross-lingual situations. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "8\n",
      "Question 1: What toolkits are used for pre-processing in the experiment?\n",
      "\n",
      "Answer 1: NLTK and LANGID toolkits are used for pre-processing in English, while OpenCC and THULAC are used for pre-processing in Chinese in the experiment.\n",
      "Question : for the text In this experiment, we adopt pre-trained word embeddings from Wikipedia corpus. Our preliminary experiments will be conducted on English and Chinese. For the former, we use NLTK toolkit and LANGID toolkit to perform the pre-processing. For the latter, we first use OpenCC to simplify characters, and then use THULAC to perform word segmentation..In order to make the experimental results more accurate and credible, we manually annotate two large enough Chinese and English NE dictionaries for training and test. Table 2 lists the statistics of Wikipedia corpus and the annotated data. Our dictionary contains many multi-word NEs in LOC and ORG types as accounted in the second column for each language in Table 2, while we only include single-word PER NEs in our dictionary, since the English first name and last name are separated, and Chinese word segmentation cuts most of the PER entities together. We pre-train quality multi-word and single-word embeddings and aim to maximize the coverage of the NEs in the dictionary. The pre-trained word embeddings cover 82.3% / 82.51% of LOC NEs and 70.2% / 63.61% of ORG NEs in English and Chinese, respectively. For other multi-word NEs, we simply calculate the average vector of each word embedding as their representations. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "9\n",
      "Question 1: What method was used to calculate the nearest neighbors of words in the study?\n",
      "\n",
      "Answer 1: The nearest neighbors of words were calculated using the Euclidean distance metric, by comparing the embedding of each word to the embeddings of all other words in the vocabulary.\n",
      "Question : for the text Seok BIBREF2 proposed that similar words are more likely to occupy close spatial positions, since their word embeddings carries syntactical and semantical informative clues. For an intuitive understanding, they listed the nearest neighbors of words included in the PER and ORG tags under cosine similarity metric. To empirically verify this observation and explore the performance of this property in Euclidean space , we list Top-5 nearest neighbors under Euclidean distance metric in Table 1 and illustrate a standard t-SNE BIBREF12 2- $D$ projection of the embeddings of three entity types with a sample of 500 words for each type..Nearest neighbors are calculated by comparing the Euclidean distance between the embedding of each word (such as Fohnsdorf, Belgian, and Ltd.) and the embeddings of all other words in the vocabulary. We pre-train word embeddings using the continuous skip-gram model BIBREF13 with the tool, and obtain multi-word and single-word phrases with a maximum length of 8, and a minimum word frequency cutoff of 3. The examples in Table 1 and visualization in Figure 1 demonstrate that the above observation suits well under Euclidean distance metric for NE recognition either for monolingual or multilingual situations. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "10\n",
      "Question 1: What classifiers were used in the baseline model for the experiments?\n",
      "\n",
      "Answer 1: The baseline model for the experiments used Naive Bayes, SVM, and the Maximum Entropy classifiers.\n",
      "Question : for the text The baseline model for our experiments is explained in the paper by Alec Go [1]. The model uses the Naive Bayes, SVM, and the Maximum Entropy classifiers for their experiment. Their feature vector is either composed of Unigrams, Bigrams, Unigrams + Bigrams, or Unigrams + POS tags..This work achieved the following maximum accuracies:.a) 82.2 for the Unigram feature vector, using the SVM classifier,.b) 83.0 for the Unigram + Bigram feature vector, using the MaxEnt classifier, and 82.7 using the Naive Bayes classifier..c) 81.9 for the Unigram + POS feature vector, using the SVM classifier..These baseline accuracies were on a training dataset of 1.6 million tweets, and a test dataset of 500 tweets. We are using the same training dataset for our experiments. We later present the baseline accuracies on a training set of 200K tweets, and a test dataset of 5000 tweets; we compare our model's accuracy with these baseline accuracy values on the same test data of 5000 tweets. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "11\n",
      "Question 1: How can a higher accuracy be achieved in sentiment classification of Twitter messages despite the constraint on computation power?\n",
      "\n",
      "Answer 1: A higher accuracy can be obtained by training on a smaller dataset and with a much faster computation time. This can be achieved using a subjectivity threshold to selectively filter the training data, incorporating a more complex preprocessing stage, and using an additional heuristic for sentiment classification, along with the conventional machine learning techniques. Our subjectivity filtering process can achieve a better generalised model for sentiment classification.\n",
      "Question : for the text We show that a higher accuracy can be obtained in sentiment classification of Twitter messages training on a smaller dataset and with a much faster computation time, and hence the issue of constraint on computation power is resolved to a certain extent. This can be achieved using a subjectivity threshold to selectively filter the training data, incorporating a more complex preprocessing stage, and using an additional heuristic for sentiment classification, along with the conventional machine learning techniques. As Twitter data is abundant, our subjectivity filtering process can achieve a better generalised model for sentiment classification. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "12\n",
      "Question 1: How many tweets are in the training dataset? \n",
      "Answer 1: There are 1.6 million tweets in the training dataset.\n",
      "Question : for the text Our training dataset has 1.6 million tweets, and 5000 tweets in the test dataset. Since the test dataset provided comprised only 500 tweets, we have taken part of the training data (exactly 5000 tweets, distinct from the training dataset) as the test dataset. We remove emoticons from our training and test data. The table below shows some sample tweets.. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "13\n",
      "Question 1: What is the heuristic used to obtain labels for test data in the study?\n",
      "\n",
      "Answer 1: The heuristic used to obtain labels for test data in the study involves maintaining a dictionary of frequently used words and their polarity scores, including all synonyms of a word (from WordNet) present in a tweet, and assigning them the same score as the dictionary word. The Effective Word Scores of a tweet are then calculated based on the number of words in the tweet with polarity scores. The heuristic for obtaining the label of a tweet is defined as (EFWS(5) or EFWS(4)) and (EFWS(2)) for positive sentiment, and (EFWS(5) or EFWS(4)) and (EFWS(2)) for negative sentiment. This heuristic was found to have an accuracy of around 85% for a training dataset of 100K and a test dataset of 5K.\n",
      "Question : for the text We have described our baseline model above. So the feature vectors we collate results for, are Unigram, Unigram + Bigram, and Unigram + POS. We have already made two major changes before the training starts on our dataset as compared to our baseline model. Firstly, our training dataset will be filtered according to the subjectivity threshold. And secondly, our preprocessing is much more robust as compared to their work..Now let us look at an additional heuristic we use to obtain labels for our test data. Along with dictionaries for stop words and acronyms, we also maintain a dictionary of a list of frequently used words and their polarity scores. This dictionary has around 2500 words and their polarity score ranging from -5 to 5. At runtime, we also use all synonyms of a word (from WordNet) present in a tweet and also the dictionary, and assign them the same score as the dictionary word. There is a reasonable assumption here, that the synonyms aren't very extremal in nature, that is, a word with a polarity score of 2 cannot have a synonym which has a polarity score of 5. Now, we calculate the Effective Word Scores of a tweet..We define the Effective Word Score of score x as. .EFWS(x) = N(+x) - N(-x),. .where N(x) is the number of words in the tweet with polarity score x..For example, if a tweet has one word with score 5, three words with score 4, two with score 2, three with with score -2, one with score -3, and finally two with score -4, then the effective word scores are:.EFWS(5) = N(5) - N(-5) = 1 - 0 = 1.EFWS(4) = N(4) - N(-4) = 3 - 2 = 1.EFWS(3) = N(3) - N(-3) = 0 - 1 = -1.EFWS(2) = N(2) - N(-2) = 2 - 3 = -1.EFWS(1) = N(1) - N(-1) = 2 - 0 = 2.We now define the heuristic for obtaining the label of a Tweet.. (EFWS(5) INLINEFORM0 1 or EFWS(4) INLINEFORM1 1) and (EFWS(2) INLINEFORM2 1) Label = positive .Similarly,. (EFWS(5) INLINEFORM0 -1 or EFWS(4) INLINEFORM1 -1) and (EFWS(2) INLINEFORM2 -1) Label = negative .The basic intuition behind such a heuristic is that we found tweets having one strongly positive and one moderately positive word more than the number of strongly negative and the moderately negative words respectively, usually conveyed a positive sentiment. Similar was the case for negative sentiments. The tweets getting a label from this heuristic are not sent into the training phase. After considerable amount of experimenting, and analyzing the nature of our dataset, which is not domain specific, we have reached the conclusion that the heuristic mentioned above is optimal for obtaining labels. We found that the heuristic accuracy was around 85% for a training dataset of 100K and a test dataset of 5K, where the total number of test tweets labelled by the heuristic were around 500. This means that around 425 out of the 500 tweets received a correct prediction of sentiment using this heuristic..Thus, using this heuristic improves the overall accuracy, as well as saves time by reducing the number of tweets to be tested by the ML algorithms. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "14\n",
      "What is the subjectivity threshold parameter and how does it affect the training dataset?\n",
      "\n",
      "The subjectivity threshold parameter is a filtering parameter used to exclude tweets with low subjective language in the training dataset. As the subjectivity threshold parameter increases, more tweets are filtered out, resulting in a smaller and more polarized training dataset. This can lead to higher accuracy in the model, but also runs the risk of excluding important information. Researchers must decide on an optimal threshold value for their specific experiment.\n",
      "Question : for the text In this section, we present the collated results of our experiments. To show that our model achieves higher accuracy than the baseline model and on a smaller training dataset, we first fix the test dataset. Our test dataset, as mentioned before, consists of 5000 tweets. We conducted our experiments on an Intel Core i5 machine (4 cores), with 8 GB RAM. The following are the accuracies of the baseline model on a training set of 200K tweets:..We filtered the training set with a subjectivity threshold of 0.5. By doing this, we saw that the number of tweets reduced to approximately 0.6 million tweets from an earlier total of 1.6 million. We then trained our model described in earlier sections on a 100K tweets randomly picked from this filtered training dataset, and observed the following accuracies:..Note that all the accuracies in the tables above have been recorded as the average of 3 iterations of our experiment. We achieve higher accuracy for all feature vectors, on all classifiers, and that too from a training dataset half the size of the baseline one..We now see the intricacies of the subjectivity threshold parameter. It is clear that more and more tweets get filtered as the subjectivity threshold parameter increases. This can be seen in the Figure 1 shown below. We have plotted the number of tweets that remain after filtering from two sources: TextBlob, Opinion Finder Tool. TextBlob has an inbuilt function that provides us the subjectivity level of a tweet. On the other hand, Opinion Finder only provides the information of which parts of the text are subjective, and which are objective. From that, we define the subjectivity level of that text as:.Subjectivity level = INLINEFORM0 .[ xlabel=Subjectivity Threshold, ylabel=Tweets (in millions), xmin=0, xmax=1, ymin=0, ymax=2000000, xtick=0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1, ytick=0,200000,400000,600000,800000,1000000,1200000,1400000,1600000,1800000, legend pos=north east, ] [color=red] coordinates (0, 1600000) (0.1, 939785) (0.2, 873054) (0.3, 804820) (0.4, 712485) (0.5, 571864) (0.6, 449286) (0.7, 304874) (0.8, 211217) (0.9, 135788) ;.[color=blue] coordinates (0, 1600000) (0.1, 602313) (0.2, 499173) (0.3, 392223) (0.4, 262109) (0.5, 169477) (0.6, 154667) (0.7, 139613) (0.8, 126148) (0.9, 116842) ; Textblob, Opinion Finder.Figure 1: Number of tweets with subjectivity greater than the subjectivity threshold.[ xlabel=Subjectivity Threshold, ylabel=Accuracy (from 0 to 1), xmin=0, xmax=1, ymin=0.7, ymax=1, xtick=0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1, ytick=0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1, legend pos=north east, ] [color=red] coordinates (0.1, 0.753871866) (0.2, 0.779442897) (0.3, 0.763421155) (0.4, 0.783231198) (0.5,0.805132645) (0.6,0.807373259) (0.7,0.808587744) (0.8,0.817799443) (0.9,0.823872989) ; Figure 2: Variation of accuracy (*Training data of 100K, Test data of 5K) with subjectivity threshold. *TextBlob is used to filter the tweets to form the training dataset..We now focus on the issue of choosing the optimum threshold value. As the subjectivity threshold parameter increases, our model trains on tweets with a higher subjectivity level, and the overall accuracy increases. We observed the following accuracies on subjectivity level 0.8 (Unigrams as features):.Naive Bayes: 80.32%.Non-linear SVM: 80.15 %.Logistic Regression: 81.77%.We should consider the fact that a lot of useful tweets are also lost in the process of gradually increasing the parameter, and this could cause a problem in cases when the test data is very large, because the model will not train on a generic dataset. Researchers may use a higher subjectivity threshold for their experiments if they are confident that most of the important information would be retained. This is most likely to happen in case of topic-specific or domain-specific data..[ ybar, enlargelimits=0.15, legend style=anchor=north, legend pos= north east, ylabel=Training time (in minutes), symbolic x coords=baseline,subjectivity=0.5,subjectivity=0.8, xtick=data, ] coordinates (baseline,17.4) (subjectivity=0.5,12.55) (subjectivity=0.8,10.68); coordinates (baseline,16.23) (subjectivity=0.5,12.31) (subjectivity=0.8,10.34); coordinates (baseline,31.9) (subjectivity=0.5,18.24) (subjectivity=0.8,16.3); Logistic Regression,Naive Bayes,SVM.Figure 3: Comparison of training times for Unigrams.[ ybar, enlargelimits=0.15, legend style=anchor=north, legend pos= north east, ylabel=Training time (in minutes), symbolic x coords=baseline,subjectivity=0.5,subjectivity=0.8, xtick=data, ] coordinates (baseline,28.41) (subjectivity=0.5,14.09) (subjectivity=0.8,11.3); coordinates (baseline,16.6) (subjectivity=0.5,13.51) (subjectivity=0.8,12.66); coordinates (baseline,35.2) (subjectivity=0.5,20.6) (subjectivity=0.8,19.2); Logistic Regression,Naive Bayes,SVM Figure 4: Comparison of training times for Unigrams + Bigrams.We use Logistic regression for classification and unigrams as the feature vector with K-fold cross validation for determining the accuracy. We choose an optimal threshold value of 0.5 for our experiment, considering the fact that the model should train on a more generic dataset. Figure 2 shows the variation of accuracy with the subjectivity threshold. The training size is fixed at 100K and the test dataset (5K tweets) is also same for all the experiments..We also measure the time taken to train our model, and compare it to the baseline model. Our observation was that our model took roughly half the amount of time in some cases and yet obtained a higher accuracy. Figures 3 and 4 show the difference in training time of the baseline model, our model on a 0.5 subjectivity-filtered dataset, and our model on a 0.8 subjectivity-filtered dataset on unigrams and unigrams + bigrams respectively. The times recorded are on a training dataset of 100K for our model and 200K for the baseline model, and a test dataset of 5K was fixed in all the recordings. The winning point, which can be seen from the plots, is that our model is considerably faster, and even has twofold speed in some cases. And alongside saving computation time, it achieves higher accuracy. This can be attributed to the fact that as the subjectivity threshold increases, only the tweets with highly polar words are retained in the training set and this makes the whole process faster. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "15\n",
      "Question 1: What is the proposed additional heuristic for sentiment classification mentioned in the text?\n",
      "\n",
      "Answer 1: The text mentions an additional heuristic for sentiment classification that can be used alongside learning heuristics. The details of this heuristic, however, are not provided.\n",
      "Question : for the text In this section, we explain the various preprocessing techniques used for feature reduction, and also the additional step of filtering the training dataset using the subjectivity score of tweets. We further describe our approach of using different machine learning classifiers and feature extractors. We also propose an additional heuristic for sentiment classification which can be used as a tag-along with the learning heuristics. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "16\n",
      "Question 1: What is the proposed technique for sampling tweets for building a sentiment classification model? \n",
      "\n",
      "Answer 1: The proposed technique for sampling tweets for building a sentiment classification model is to use tweet subjectivity to select the best training tweets. This ensures reliable prediction on a smaller training dataset and eventually requires less computational time.\n",
      "Question : for the text A lot of work has been done in the field of Twitter sentiment analysis till date. Sentiment analysis has been handled as a Natural Language Processing task at many levels of granularity. Most of these techniques use Machine Learning algorithms with features such as unigrams, n-grams, Part-Of-Speech (POS) tags. However, the training datasets are often very large, and hence with such a large number of features, this process requires a lot of computation power and time. The following question arises: What to do if we do not have resources that provide such a great amount of computation power? The existing solution to this problem is to use a smaller sample of the dataset. For sentiment analysis, if we train the model using a smaller randomly chosen sample, then we get low accuracy [16, 17]. In this paper, we propose a novel technique to sample tweets for building a sentiment classification model so that we get higher accuracy than the state-of-the-art baseline method, namely Distant Supervision, using a smaller set of tweets. Our model has lower computation time and higher accuracy compared to baseline model..Users often express sentiment using subjective expression. Although objective expressions can also have sentiment, it is much rare. Determining subjectivity is quite efficient compared to determining sentiment. Subjectivity can be determined for individual tweets. But to do sentiment classification, we need to build a classification model with positive and negative sentiment tweets. The time to train a sentiment classification model increases with the increase in the number of training tweets. In this paper, we use tweet subjectivity to select the best training tweets. This not only lowers the computation time but also increases the accuracy because we have training data with less noise. Even the created features will be more relevant to the classification task. The computation cost will reduce due to small training data size and better set of features. Thus if users do not have enough computational resources, they can filter the training dataset using a high value of subjectivf ity threshold. This ensures reliable prediction on a smaller training dataset, and eventually requires less computational time. The above approach, and some of the intricacies that invariably seep in, need to be considered, and are described in the later sections of the paper. In this paper we also integrate a lot of meticulous preprocessing steps. This makes our model more robust, and hence leads to higher accuracy..Along with the machine learning algorithms being used, we use a heuristic-based classification of tweets. This is based on the EFWS of a tweet, which is described in later sections. This heuristic basically takes into account the polarity scores of frequently used words in tweets, and is able to achieve around 85% accuracy on our dataset, hence boosting the overall accuracy by a considerable amount..Our training data consists of generic (not topic-specific) Twitter messages with emoticons, which are used as noisy labels. We show that the accuracy obtained on a training dataset comprising 100K tweets, and a test dataset of 5000 tweets gives an accuracy of around 80% on the following classifiers: Naive Bayes, RBF-kernel Support Vector Machine, and Logistic Regression. Our model takes roughly half the time to train and achieves higher accuracy (than the baseline model) on all the classifiers. Because the amount of training time is expected to increase exponentially as the training data increases, we expect our model to outperform (in terms of higher accuracy) the baseline model at a speed which is at least twofold the speed of the baseline model on larger datasets. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "17\n",
      "What is the purpose of stripping off usernames and URLs in the preprocessing stage of the Twitter language model?\n",
      "\n",
      "The purpose of stripping off usernames and URLs in the preprocessing stage of the Twitter language model is to reduce the feature space and eliminate information that does not help in sentiment classification. These types of information, such as twitter usernames and URLs, do not provide useful insights into the sentiment of the tweet.\n",
      "Question : for the text The Twitter language model has many unique properties. We take advantage of the following properties to reduce the feature space. Most of the preprocessing steps are common to most of the previous works in the field. However, we have added some more steps to this stage of our model..We first strip off the emoticons from the data. Users often include twitter usernames in their tweets in order to direct their messages. We also strip off usernames (e.g. @Chinmay) and URLs present in tweets because they do not help us in sentiment classification. Apart from full stops, which are dealt in the next point, other punctuations and special symbols are also removed. Repeated whitespaces are replaced with a single space. We also perform stemming to reduce the size of the feature space..In the previous works, full stops are just usually replaced by a space. However, we have observed that casual language in tweets is often seen in form of repeated punctuations. For example, “this is so cool...wow\". We take into consideration this format, and replace two or more occurrences of “.\" and “-\" with a space. Also, full stops are also quite different in usage. Sometimes, there isn't any space in between sentences. For example, “It’s raining.Feeling awesome\". We replace a single occurrence of a full stop with a space to ensure correct feature incorporation..In the case of hashtags, most of the previous works just consider the case of hashtags followed by a single word; they just remove the hashtag and add the word to the feature vector. However, sometimes, there are multiple words after a hashtag, and more often than not, these words form an important, conclusive part of the Tweet. For example, #ThisSucks, or #BestMomentEver. These hashtags need to be dealt with in a proper fashion. We split the text after hashtags after before each capital letter, and add these as tokens to the feature vector. For hashtags followed by a single word, we just replace the pattern #word with the word, as conventional models do. The intuition behind this step is that quite often, the sentiment of a tweet is expressed in form of a hashtag. For example, #happy or #disappointed are frequently used hashtags, and we don’t want to lose this information during sentiment classification..Tweets contain very casual language as mentioned earlier. For example, if we search “wow\" with an arbitrary number of o's in the middle (e.g. wooow, woooow) on Twitter, there will most likely be a non-empty result set. We use preprocessing so that any letter occurring more than two times in a row is replaced with two occurrences. In the samples above, these words would be converted into the token “woow\". After all the above modifications, tweets are converted into lowercase to avoid confusion between features having same content, but are different in capitalization..We gather a list of 400 stopwords. These words, if present in the tweets, are not considered in the feature vector..We store an acronym dictionary which has over 5000, frequently-used acronyms and their abbreviations. We replace such acronyms in tweets with their abbreviation, since these can be of great use while sentiment classification..All negative words like 'cannot', 'can't', 'won't', 'don't' are replaced by 'not', which effectively keeps the sentiment stable. It is observed that doing this makes the training faster, since the model has to deal with a smaller feature vector. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "18\n",
      "Question 1: What was the inspiration behind using the polarity scores of frequently used tweet words in the model described in the text?\n",
      "\n",
      "Answer 1: The inspiration behind using the polarity scores of frequently used tweet words came from Ayushi Dalmia's work, which proposed a model with a more involved preprocessing stage and used features like scores from Bing Liu’s Opinion Lexicon and number of positive and negative POS tags. This model achieved high accuracies considering their features were not the conventional bag-of-words or any n-grams. The thought of using the polarity scores of frequently used tweet words was inspired by their work.\n",
      "Question : for the text There has been a large amount of prior research in sentiment analysis of tweets. Read [10] shows that using emoticons as labels for positive and sentiment is effective for reducing dependencies in machine learning techniques. Alec Go [1] used Naive Bayes, SVM, and MaxEnt classifiers to train their model. This, as mentioned earlier, is our baseline model. Our model builds on this and achieves higher accuracy on a much smaller training dataset..Ayushi Dalmia [6] proposed a model with a more involved preprocessing stage, and used features like scores from Bing Liu’s Opinion Lexicon, and number of positive, negative POS tags. This model achieved considerably high accuracies considering the fact that their features were the not the conventional bag-of-words, or any n-grams. The thought of using the polarity scores of frequently used tweet words (as described in our EFWS heuristic) was inspired from this work. [14] created prior probabilities using the datasets for the average sentiment of tweets in different spatial, temporal and authorial contexts. They then used a Bayesian approach to combine these priors with standard bigram language models..Another significant effort in sentiment analysis on Twitter data is by Barbosa [16]. They use polarity predictions from three websites as noisy labels to train a model and use 1000 manually labelled tweets for tuning and another 1000 for testing. They propose the use of syntax features of tweets like punctuation, retweet, hashtags, link, and exclamation marks in addition with features like prior polarity of words and POS of words..Some works leveraged the use of existing hashtags in the Twitter data for building the training data. (Davidov, Tsur, and Rappoport 2010) also use hashtags for creating training data, but they limit their experiments to sentiment/non-sentiment classification, rather than 3-way polarity classification, as [15] does. Our model integrates some of the preprocessing techniques this work used. Hassan Saif [9] introduced a novel approach of adding semantics as additional features into the training set for sentiment analysis. This approach works well for topic specific data. Hence, we thought of taking a different approach for a generic tweet dataset like ours. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "19\n",
      "What is the difference between subjective and objective perspectives?\n",
      "Subjectivity refers to how personal opinions and feelings shape someone's judgment, while an objective perspective is based on facts, quantifiable, and not influenced by emotions, opinions, or personal feelings.\n",
      "Question : for the text Subjectivity refers to how someone's judgment is shaped by personal opinions and feelings instead of outside influences. An objective perspective is one that is not influenced by emotions, opinions, or personal feelings - it is a perspective based in fact, in things quantifiable and measurable. A subjective perspective is one open to greater interpretation based on personal feeling, emotion, aesthetics, etc..Subjectivity classification is another topic in the domain of text classification which is garnering more and more interest in the field of sentiment analysis. Since a single sentence may contain multiple opinions and subjective and factual clauses, this problem is not as straightforward as it seems. Below are some examples of subjective and objective sentences..Objective sentence with no sentiment: So, the Earth revolves around the Sun..Objective sentence with sentiment: The drug relieved my pain..Subjective sentence with no sentiment: I believe he went home yesterday..Subjective sentence with sentiment: I am so happy you got the scholarship..Classifying a sentence as subjective or objective provides certain conclusions. Purely objective sentences do not usually convey any sentiment, while most of the purely subjective sentences have a clear inclination towards either the positive or negative sentiment. Sentences which are not completely subjective or objective may or may not convey a sentiment. Libraries like TextBlob, and tools like Opinion Finder can be used to find the extent to which a sentence can be considered subjective..Since tweets are usually person-specific, or subjective, we use this intuition to reduce the size of the training set by filtering the sentences with a subjectivity level below a certain threshold (fairly objective tweets). generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "20\n",
      "Question 1: What tool was used to classify tweets as objective or subjective in this proposed step?\n",
      "\n",
      "Answer 1: TextBlob was used to classify each tweet as either objective or subjective.\n",
      "Question : for the text This is a new step we propose to achieve higher accuracy on a smaller training dataset. We use TextBlob to classify each tweet as subjective or objective. We then remove all tweets which have a subjectivity level/score (score lies between 0 and 1) below a specified threshold. The remaining tweets are used for training purposes. We observe that a considerable number of tweets are removed as the subjectivity threshold increases. We show the effect of doing this procedure on the overall accuracy in the evaluation section of the paper. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "21\n",
      "What is the difference between Naive Bayes and Maximum Entropy models?\n",
      "\n",
      "The main difference between Naive Bayes and Maximum Entropy models is that Naive Bayes is a generative model which estimates the likelihood of a class given the feature set, while Maximum Entropy is a discriminative model which directly estimates the best class given the feature set. Naive Bayes assumes that all features are independent, while Maximum Entropy can model dependencies between features.\n",
      "Question : for the text We use the following classifiers for our model..Naive Bayes is a simple model which works well on text categorization. We use a Naive Bayes model. Class c* is assigned to tweet d, where c* = argmax P(c INLINEFORM0 d). INLINEFORM1 .And INLINEFORM0 is calculated using Bayes Rule. In this formula, f represents a feature and INLINEFORM1 represents the count of feature INLINEFORM2 found in tweet d. There are a total of m features. Parameters P(c) and INLINEFORM3 are obtained through maximum likelihood estimates..Support vector machines are based on the Structural Risk Minimization principle from computational learning theory. SVM classification algorithms for binary classification is based on finding a separation between hyperplanes defined by classes of data. One remarkable property of SVMs is that their ability to learn can be independent of the dimensionality of the feature space. SVMs can generalize even in the presence of many features as in the case of text data classification. We use a non-linear Support Vector Machine with an RBF kernel..Maximum Entropy Model belongs to the family of discriminative classifiers also known as the exponential or log-linear classifiers.. In the naive Bayes classifier, Bayes rule is used to estimate this best y indirectly from the likelihood INLINEFORM0 (and the prior INLINEFORM1 ) but a discriminative model takes this direct approach, computing INLINEFORM2 by discriminating among the different possible values of the class y rather than first computing a likelihood. INLINEFORM3 .Logistic regression estimates INLINEFORM0 by combining the feature set linearly (multiplying each feature by a weight and adding them up), and then applying a function to this combination. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "22\n",
      "Question 1: What is the accuracy of VQA and how is it evaluated?\n",
      "\n",
      "Answer 1: The accuracy of VQA is evaluated using human consensus. Each question is answered by multiple people and the generated answers are compared to the target answer set. An answer is considered 100% accurate if at least 3 people provide that exact answer. The accuracy of VQA is calculated as the average of the minimum ratio of correct answers to the target set for each question.\n",
      "Question : for the text For the VQA dataset, each question is answered by multiple people and the answers may not be the same, the generated answers are evaluated using human consensus. For each predicted answer $a_i$ for the $i_{th}$ question with target answer set $T^{i}$ , the accuracy of VQA: $Acc_{VQA} = \\frac{1}{N}\\sum _{i=1}^Nmin(\\frac{\\sum _{t\\in T^i}{1}_{(a_i==t)}}{3},1)$ where ${1}_{(\\cdot )}$ is the indicator function. Simply put, the answer $a_i$ is only 100 $\\%$ accurate if at least 3 people provide that exact answer..Training Details We use the Adam optimizer BIBREF35 with a learning rate of 0.003 and batch size of 100. Training runs for up to 256 epochs with early stopping if the validation loss has not improved in the last 10 epochs. For weight initialization, we sampled from a random uniform distribution with range $[-0.08, 0.08]$ . Both the word embedding and hidden layers were vectors of size $d=512$ . We apply dropout on the initial image output from the VGG convolutional neural network BIBREF11 as well as the input to the answer module, keeping input with probability $p=0.5$ ..Results and Analysis.The VQA dataset is composed of three question domains: Yes/No, Number, and Other. This enables us to analyze the performance of the models on various tasks that require different reasoning abilities..The comparison models are separated into two broad classes: those that utilize a full connected image feature for classification and those that perform reasoning over multiple small image patches. Only the SAN and DMN approach use small image patches, while the rest use the fully-connected whole image feature approach..Here, we show the quantitative and qualitative results in Table 3 and Fig. 6 , respectively. The images in Fig. 6 illustrate how the attention gate $g^t_i$ selectively activates over relevant portions of the image according to the query. In Table 3 , our method outperforms baseline and other state-of-the-art methods across all question domains (All) in both test-dev and test-std, and especially for Other questions, achieves a wide margin compared to the other architectures, which is likely as the small image patches allow for finely detailed reasoning over the image..However, the granularity offered by small image patches does not always offer an advantage. The Number questions may be not solvable for both the SAN and DMN architectures, potentially as counting objects is not a simple task when an object crosses image patch boundaries. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "23\n",
      "Question 1: What optimizer was used to train the models and what was the learning rate and batch size?\n",
      "\n",
      "Answer 1: The models were trained using the Adam optimizer BIBREF35 with a learning rate of 0.001 and batch size of 128.\n",
      "Question : for the text We trained our models using the Adam optimizer BIBREF35 with a learning rate of 0.001 and batch size of 128. Training runs for up to 256 epochs with early stopping if the validation loss had not improved within the last 20 epochs. The model from the epoch with the lowest validation loss was then selected. Xavier initialization was used for all weights except for the word embeddings, which used random uniform initialization with range $[-\\sqrt{3}, \\sqrt{3}]$ . Both the embedding and hidden dimensions were of size $d = 80$ . We used $\\ell _2$ regularization on all weights except bias and used dropout on the initial sentence encodings and the answer module, keeping the input with probability $p=0.9$ . The last 10% of the training data on each task was chosen as the validation set. For all tasks, three passes were used for the episodic memory module, allowing direct comparison to other state of the art methods. Finally, we limited the input to the last 70 sentences for all tasks except QA3 for which we limited input to the last 130 sentences, similar to BIBREF10 ..On some tasks, the accuracy was not stable across multiple runs. This was particularly problematic on QA3, QA17, and QA18. To solve this, we repeated training 10 times using random initializations and evaluated the model that achieved the lowest validation set loss..Text QA Results.We compare our best performing approach, DMN+, to two state of the art question answering architectures: the end to end memory network (E2E) BIBREF10 and the neural reasoner framework (NR) BIBREF12 . Neither approach use supporting facts for training..The end-to-end memory network is a form of memory network BIBREF2 tested on both textual question answering and language modeling. The model features both explicit memory and a recurrent attention mechanism. We select the model from the paper that achieves the lowest mean error over the bAbI-10k dataset. This model utilizes positional encoding for input, RNN-style tied weights for the episode module, and a ReLU non-linearity for the memory update component..The neural reasoner framework is an end-to-end trainable model which features a deep architecture for logical reasoning and an interaction-pooling mechanism for allowing interaction over multiple facts. While the neural reasoner framework was only tested on QA17 and QA19, these were two of the most challenging question types at the time..In Table 2 we compare the accuracy of these question answering architectures, both as mean error and error on individual tasks. The DMN+ model reduces mean error by 1.4% compared to the the end-to-end memory network, achieving a new state of the art for the bAbI-10k dataset..One notable deficiency in our model is that of QA16: Basic Induction. In BIBREF10 , an untied model using only summation for memory updates was able to achieve a near perfect error rate of $0.4$ . When the memory update was replaced with a linear layer with ReLU activation, the end-to-end memory network's overall mean error decreased but the error for QA16 rose sharply. Our model experiences the same difficulties, suggesting that the more complex memory update component may prevent convergence on certain simpler tasks..The neural reasoner model outperforms both the DMN and end-to-end memory network on QA17: Positional Reasoning. This is likely as the positional reasoning task only involves minimal supervision - two sentences for input, yes/no answers for supervision, and only 5,812 unique examples after removing duplicates from the initial 10,000 training examples. BIBREF12 add an auxiliary task of reconstructing both the original sentences and question from their representations. This auxiliary task likely improves performance by preventing overfitting. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "24\n",
      "Question 1: What are the improvements in the proposed DMN framework?\n",
      "\n",
      "Answer 1: The proposed DMN framework has new modules that include an input fusion layer to allow interactions between input facts and a novel attention based GRU that allows for logical reasoning over ordered inputs. These improvements have achieved strong results without the supervision of supporting facts, and the resulting model obtained state of the art results on both the VQA dataset and the bAbI-10k text question-answering dataset, proving the framework can be generalized across input domains.\n",
      "Question : for the text We have proposed new modules for the DMN framework to achieve strong results without supervision of supporting facts. These improvements include the input fusion layer to allow interactions between input facts and a novel attention based GRU that allows for logical reasoning over ordered inputs. Our resulting model obtains state of the art results on both the VQA dataset and the bAbI-10k text question-answering dataset, proving the framework can be generalized across input domains. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "25\n",
      "Question 1: What color is the bike in the image?\n",
      "Answer 1: The bike in the image is red.\n",
      "Question : for the text The DAtaset for QUestion Answering on Real-world images (DAQUAR) BIBREF23 consists of 795 training images and 654 test images. Based upon these images, 6,795 training questions and 5,673 test questions were generated. Following the previously defined experimental method, we exclude multiple word answers BIBREF32 , BIBREF33 . The resulting dataset covers 90% of the original data. The evaluation method uses classification accuracy over the single words. We use this as a development dataset for model analysis (Sec. \"Model Analysis\" ). generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "26\n",
      "Question 1: How many datasets were used to analyze the proposed model changes in comparison with other architectures?\n",
      "\n",
      "Answer 1: Three datasets were used to analyze the proposed model changes and compare performance with other architectures.\n",
      "Question : for the text To analyze our proposed model changes and compare our performance with other architectures, we use three datasets. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "27\n",
      "Question 1: What is the Input Module in the DMN for Question Answering?\n",
      "Answer 1: The Input Module processes the input data into a set of vectors termed facts, represented as F=[f1,…,fN], where N is the total number of facts. For text QA, it consists of a GRU over the input words.\n",
      "Question : for the text We begin by outlining the DMN for question answering and the modules as presented in BIBREF6 ..The DMN is a general architecture for question answering (QA). It is composed of modules that allow different aspects such as input representations or memory components to be analyzed and improved independently. The modules, depicted in Fig. 1 , are as follows:.Input Module: This module processes the input data about which a question is being asked into a set of vectors termed facts, represented as $F=[f_1,\\hdots ,f_N]$ , where $N$ is the total number of facts. These vectors are ordered, resulting in additional information that can be used by later components. For text QA in BIBREF6 , the module consists of a GRU over the input words..As the GRU is used in many components of the DMN, it is useful to provide the full definition. For each time step $i$ with input $x_i$ and previous hidden state $h_{i-1}$ , we compute the updated hidden state $h_i = GRU(x_i,h_{i-1})$ by .$$u_i &=& \\sigma \\left(W^{(u)}x_{i} + U^{(u)} h_{i-1} + b^{(u)} \\right)\\\\\n",
      "r_i &=& \\sigma \\left(W^{(r)}x_{i} + U^{(r)} h_{i-1} + b^{(r)} \\right)\\\\\n",
      "\\tilde{h}_i &=& \\tanh \\left(Wx_{i} + r_i \\circ U h_{i-1} + b^{(h)}\\right)\\\\\n",
      "h_i &=& u_i\\circ \\tilde{h}_i + (1-u_i) \\circ h_{i-1}$$   (Eq. 2) .where $\\sigma $ is the sigmoid activation function, $\\circ $ is an element-wise product, $W^{(z)}, W^{(r)}, W \\in \\mathbb {R}^{n_H \\times n_I}$ , $U^{(z)}, U^{(r)}, U \\in \\mathbb {R}^{n_H \\times n_H}$ , $n_H$ is the hidden size, and $n_I$ is the input size..Question Module: This module computes a vector representation $q$ of the question, where $q \\in \\mathbb {R}^{n_H}$ is the final hidden state of a GRU over the words in the question..Episodic Memory Module: Episode memory aims to retrieve the information required to answer the question $q$ from the input facts. To improve our understanding of both the question and input, especially if questions require transitive reasoning, the episode memory module may pass over the input multiple times, updating episode memory after each pass. We refer to the episode memory on the $t^{th}$ pass over the inputs as $m^t$ , where $m^t \\in \\mathbb {R}^{n_H}$ , the initial memory vector is set to the question vector: $m^0 = q$ ..The episodic memory module consists of two separate components: the attention mechanism and the memory update mechanism. The attention mechanism is responsible for producing a contextual vector $c^t$ , where $c^t \\in \\mathbb {R}^{n_H}$ is a summary of relevant input for pass $t$ , with relevance inferred by the question $q$ and previous episode memory $m^{t-1}$ . The memory update mechanism is responsible for generating the episode memory $m^t$ based upon the contextual vector $c^t$ and previous episode memory $m^{t-1}$ . By the final pass $T$ , the episodic memory $m^T$ should contain all the information required to answer the question $c^t \\in \\mathbb {R}^{n_H}$0 ..Answer Module: The answer module receives both $q$ and $m^T$ to generate the model's predicted answer. For simple answers, such as a single word, a linear layer with softmax activation may be used. For tasks requiring a sequence output, an RNN may be used to decode $a = [q ; m^T]$ , the concatenation of vectors $q$ and $m^T$ , to an ordered set of tokens. The cross entropy error on the answers is used for training and backpropagated through the entire network. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "28\n",
      "Question 1: What is the final model proposed in the text and what datasets does it perform well on?\n",
      "\n",
      "Answer 1: The final model proposed in the text is called DMN+ and it performs well on the bAbI-10k dataset without supporting facts and the VQA dataset. It obtains the highest accuracy among the modeling choices for input representation, attention mechanism and memory update that were compared in the text.\n",
      "Question : for the text We propose and compare several modeling choices for two crucial components: input representation, attention mechanism and memory update. The final DMN+ model obtains the highest accuracy on the bAbI-10k dataset without supporting facts and the VQA dataset BIBREF8 . Several design choices are motivated by intuition and accuracy improvements on that dataset. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "29\n",
      "What is the input fusion layer in the DMN+ responsible for? \n",
      "\n",
      "The input fusion layer in the DMN+ is responsible for allowing interactions between sentences and enabling an information exchange between them by applying a bi-directional GRU. This allows contextual information from both future and past facts to impact the overall sentence representation.\n",
      "Question : for the text In the DMN specified in BIBREF6 , a single GRU is used to process all the words in the story, extracting sentence representations by storing the hidden states produced at the end of sentence markers. The GRU also provides a temporal component by allowing a sentence to know the content of the sentences that came before them. Whilst this input module worked well for bAbI-1k with supporting facts, as reported in BIBREF6 , it did not perform well on bAbI-10k without supporting facts (Sec. \"Model Analysis\" )..We speculate that there are two main reasons for this performance disparity, all exacerbated by the removal of supporting facts. First, the GRU only allows sentences to have context from sentences before them, but not after them. This prevents information propagation from future sentences. Second, the supporting sentences may be too far away from each other on a word level to allow for these distant sentences to interact through the word level GRU..Input Fusion Layer.For the DMN+, we propose replacing this single GRU with two different components. The first component is a sentence reader, responsible only for encoding the words into a sentence embedding. The second component is the input fusion layer, allowing for interactions between sentences. This resembles the hierarchical neural auto-encoder architecture of BIBREF9 and allows content interaction between sentences. We adopt the bi-directional GRU for this input fusion layer because it allows information from both past and future sentences to be used. As gradients do not need to propagate through the words between sentences, the fusion layer also allows for distant supporting sentences to have a more direct interaction..Fig. 2 shows an illustration of an input module, where a positional encoder is used for the sentence reader and a bi-directional GRU is adopted for the input fusion layer. Each sentence encoding $f_i$ is the output of an encoding scheme taking the word tokens $[w^i_1, \\hdots , w^i_{M_i}]$ , where $M_i$ is the length of the sentence..The sentence reader could be based on any variety of encoding schemes. We selected positional encoding described in BIBREF10 to allow for a comparison to their work. GRUs and LSTMs were also considered but required more computational resources and were prone to overfitting if auxiliary tasks, such as reconstructing the original sentence, were not used..For the positional encoding scheme, the sentence representation is produced by $f_i = \\sum ^{j=1}_M l_j \\circ w^i_j$ , where $\\circ $ is element-wise multiplication and $l_j$ is a column vector with structure $l_{jd} = (1 - j / M) - (d / D) (1 - 2j / M)$ , where $d$ is the embedding index and $D$ is the dimension of the embedding..The input fusion layer takes these input facts and enables an information exchange between them by applying a bi-directional GRU. .$$\\overrightarrow{f_i} = GRU_{fwd}(f_i, \\overrightarrow{f_{i-1}}) \\\\\n",
      "\\overleftarrow{f_{i}} = GRU_{bwd}(f_{i}, \\overleftarrow{f_{i+1}}) \\\\\n",
      "\\overleftrightarrow{f_i} = \\overleftarrow{f_i} + \\overrightarrow{f_i}$$   (Eq. 5) .where $f_i$ is the input fact at timestep $i$ , $ \\overrightarrow{f_i}$ is the hidden state of the forward GRU at timestep $i$ , and $\\overleftarrow{f_i}$ is the hidden state of the backward GRU at timestep $i$ . This allows contextual information from both future and past facts to impact $\\overleftrightarrow{f_i}$ ..We explored a variety of encoding schemes for the sentence reader, including GRUs, LSTMs, and the positional encoding scheme described in BIBREF10 . For simplicity and speed, we selected the positional encoding scheme. GRUs and LSTMs were also considered but required more computational resources and were prone to overfitting if auxiliary tasks, such as reconstructing the original sentence, were not used. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "30\n",
      "Question 1: How are features extracted from the image in the input module for VQA?\n",
      "\n",
      "Answer 1: Features from the image are extracted using a convolutional neural network based on the VGG-19 model. The image is first rescaled to $448 \\times 448$ and the output is taken from the last pooling layer with dimensionality $d = 512 \\times 14 \\times 14$. The pooling layer divides the image into a grid of $14 \\times 14$, resulting in 196 local regional vectors of $d = 512$.\n",
      "Question : for the text To apply the DMN to visual question answering, we introduce a new input module for images. The module splits an image into small local regions and considers each region equivalent to a sentence in the input module for text. The input module for VQA is composed of three parts, illustrated in Fig. 3 : local region feature extraction, visual feature embedding, and the input fusion layer introduced in Sec. \"Input Module for Text QA\" ..Local region feature extraction: To extract features from the image, we use a convolutional neural network BIBREF0 based upon the VGG-19 model BIBREF11 . We first rescale the input image to $448 \\times 448$ and take the output from the last pooling layer which has dimensionality $d = 512 \\times 14 \\times 14$ . The pooling layer divides the image into a grid of $14 \\times 14$ , resulting in 196 local regional vectors of $d = 512$ ..Visual feature embedding: As the VQA task involves both image features and text features, we add a linear layer with tanh activation to project the local regional vectors to the textual feature space used by the question vector $q$ ..Input fusion layer: The local regional vectors extracted from above do not yet have global information available to them. Without global information, their representational power is quite limited, with simple issues like object scaling or locational variance causing accuracy problems..To solve this, we add an input fusion layer similar to that of the textual input module described in Sec. \"Input Module for Text QA\" . First, to produce the input facts $F$ , we traverse the image in a snake like fashion, as seen in Figure 3 . We then apply a bi-directional GRU over these input facts $F$ to produce the globally aware input facts $\\overleftrightarrow{F}$ . The bi-directional GRU allows for information propagation from neighboring image patches, capturing spatial information. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "31\n",
      "What is the dynamic memory network (DMN)?\n",
      "\n",
      "The DMN is a neural network model that combines memory and attention mechanisms to yield state of the art results on tasks such as question answering, sentiment analysis, and part-of-speech tagging.\n",
      "Question : for the text Neural network based methods have made tremendous progress in image and text classification BIBREF0 , BIBREF1 . However, only recently has progress been made on more complex tasks that require logical reasoning. This success is based in part on the addition of memory and attention components to complex neural networks. For instance, memory networks BIBREF2 are able to reason over several facts written in natural language or (subject, relation, object) triplets. Attention mechanisms have been successful components in both machine translation BIBREF3 , BIBREF4 and image captioning models BIBREF5 ..The dynamic memory network BIBREF6 (DMN) is one example of a neural network model that has both a memory component and an attention mechanism. The DMN yields state of the art results on question answering with supporting facts marked during training, sentiment analysis, and part-of-speech tagging..We analyze the DMN components, specifically the input module and memory module, to improve question answering. We propose a new input module which uses a two level encoder with a sentence reader and input fusion layer to allow for information flow between sentences. For the memory, we propose a modification to gated recurrent units (GRU) BIBREF7 . The new GRU formulation incorporates attention gates that are computed using global knowledge over the facts. Unlike before, the new DMN+ model does not require that supporting facts (i.e. the facts that are relevant for answering a particular question) are labeled during training. The model learns to select the important facts from a larger set..In addition, we introduce a new input module to represent images. This module is compatible with the rest of the DMN architecture and its output is fed into the memory module. We show that the changes in the memory module that improved textual question answering also improve visual question answering. Both tasks are illustrated in Fig. 1 . generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "32\n",
      "Question 1: What is the difference between DMN2 and ODMN?\n",
      "\n",
      "Answer 1: DMN2 replaces the input module with the input fusion layer, while ODMN does not have any modifications to its architecture. This change improves interaction between distant facts, resulting in a significant improvement in accuracy on both the bAbI-10k textual and DAQUAR visual datasets.\n",
      "Question : for the text To understand the impact of the proposed module changes, we analyze the performance of a variety of DMN models on textual and visual question answering datasets..The original DMN (ODMN) is the architecture presented in BIBREF6 without any modifications. DMN2 only replaces the input module with the input fusion layer (Sec. \"Input Module for Text QA\" ). DMN3, based upon DMN2, replaces the soft attention mechanism with the attention based GRU proposed in Sec. \"The Episodic Memory Module\" . Finally, DMN+, based upon DMN3, is an untied model, using a unique set of weights for each pass and a linear layer with a ReLU activation to compute the memory update. We report the performance of the model variations in Table 1 ..A large improvement to accuracy on both the bAbI-10k textual and DAQUAR visual datasets results from updating the input module, seen when comparing ODMN to DMN2. On both datasets, the input fusion layer improves interaction between distant facts. In the visual dataset, this improvement is purely from providing contextual information from neighboring image patches, allowing it to handle objects of varying scale or questions with a locality aspect. For the textual dataset, the improved interaction between sentences likely helps the path finding required for logical reasoning when multiple transitive steps are required..The addition of the attention GRU in DMN3 helps answer questions where complex positional or ordering information may be required. This change impacts the textual dataset the most as few questions in the visual dataset are likely to require this form of logical reasoning. Finally, the untied model in the DMN+ overfits on some tasks compared to DMN3, but on average the error rate decreases..From these experimental results, we find that the combination of all the proposed model changes results, culminating in DMN+, achieves the highest performance across both the visual and textual datasets. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "33\n",
      "Question 1: What are the two major lines of recent work related to the DMN?\n",
      "\n",
      "Answer 1: The two major lines of recent work related to the DMN are memory and attention mechanisms, which have been developed separately for visual and textual question answering.\n",
      "Question : for the text The DMN is related to two major lines of recent work: memory and attention mechanisms. We work on both visual and textual question answering which have, until now, been developed in separate communities..Neural Memory Models The earliest recent work with a memory component that is applied to language processing is that of memory networks BIBREF2 which adds a memory component for question answering over simple facts. They are similar to DMNs in that they also have input, scoring, attention and response mechanisms. However, unlike the DMN their input module computes sentence representations independently and hence cannot easily be used for other tasks such as sequence labeling. Like the original DMN, this memory network requires that supporting facts are labeled during QA training. End-to-end memory networks BIBREF10 do not have this limitation. In contrast to previous memory models with a variety of different functions for memory attention retrieval and representations, DMNs BIBREF6 have shown that neural sequence models can be used for input representation, attention and response mechanisms. Sequence models naturally capture position and temporality of both the inputs and transitive reasoning steps..Neural Attention Mechanisms Attention mechanisms allow neural network models to use a question to selectively pay attention to specific inputs. They can benefit image classification BIBREF13 , generating captions for images BIBREF5 , among others mentioned below, and machine translation BIBREF14 , BIBREF3 , BIBREF4 . Other recent neural architectures with memory or attention which have proposed include neural Turing machines BIBREF15 , neural GPUs BIBREF16 and stack-augmented RNNs BIBREF17 ..Question Answering in NLP Question answering involving natural language can be solved in a variety of ways to which we cannot all do justice. If the potential input is a large text corpus, QA becomes a combination of information retrieval and extraction BIBREF18 . Neural approaches can include reasoning over knowledge bases, BIBREF19 , BIBREF20 or directly via sentences for trivia competitions BIBREF21 ..Visual Question Answering (VQA) In comparison to QA in NLP, VQA is still a relatively young task that is feasible only now that objects can be identified with high accuracy. The first large scale database with unconstrained questions about images was introduced by BIBREF8 . While VQA datasets existed before they did not include open-ended, free-form questions about general images BIBREF22 . Others are were too small to be viable for a deep learning approach BIBREF23 . The only VQA model which also has an attention component is the stacked attention network BIBREF24 . Their work also uses CNN based features. However, unlike our input fusion layer, they use a single layer neural network to map the features of each patch to the dimensionality of the question vector. Hence, the model cannot easily incorporate adjacency of local information in its hidden state. A model that also uses neural modules, albeit logically inspired ones, is that by BIBREF25 who evaluate on knowledgebase reasoning and visual question answering. We compare directly to their method on the latter task and dataset..Related to visual question answering is the task of describing images with sentences BIBREF26 . BIBREF27 used deep learning methods to map images and sentences into the same space in order to describe images with sentences and to find images that best visualize a sentence. This was the first work to map both modalities into a joint space with deep learning methods, but it could only select an existing sentence to describe an image. Shortly thereafter, recurrent neural networks were used to generate often novel sentences based on images BIBREF28 , BIBREF29 , BIBREF30 , BIBREF5 . generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "34\n",
      "Question 1: How is the attention gate computed in the DMN+ model?\n",
      "\n",
      "Answer 1: The attention gate $g^t_i$ is computed by allowing interactions between the input fact $\\overleftrightarrow{f}_i$, the question representation, and the episode memory state during pass $t$. A scalar value is associated with each fact, which is then used in an attention mechanism to extract a contextual vector based on the current focus. The attention gate is computed using Equation 10 in the text.\n",
      "Question : for the text The episodic memory module, as depicted in Fig. 4 , retrieves information from the input facts $\\overleftrightarrow{F} = [\\overleftrightarrow{f_1}, \\hdots , \\overleftrightarrow{f_N}]$ provided to it by focusing attention on a subset of these facts. We implement this attention by associating a single scalar value, the attention gate $g^t_i$ , with each fact $\\overleftrightarrow{f}_i$ during pass $t$ . This is computed by allowing interactions between the fact and both the question representation and the episode memory state. .$$z^t_i &=& [\\overleftrightarrow{f_i} \\circ q; \\overleftrightarrow{f_i} \\circ m^{t-1}; \\vert \\overleftrightarrow{f_i} - q \\vert ; \\vert \\overleftrightarrow{f_i} - m^{t-1} \\vert ] \\\\\n",
      "Z^t_i &=& W^{(2)} \\tanh \\left(W^{(1)}z^t_i + b^{(1)} \\right)+ b^{(2)} \\\\\n",
      "g^t_i &=& \\frac{\\exp (Z^t_i)}{\\sum _{k=1}^{M_i} \\exp (Z^t_k)} $$   (Eq. 10) .where $\\overleftrightarrow{f_i}$ is the $i^{th}$ fact, $m^{t-1}$ is the previous episode memory, $q$ is the original question, $\\circ $ is the element-wise product, $|\\cdot |$ is the element-wise absolute value, and $;$ represents concatenation of the vectors..The DMN implemented in BIBREF6 involved a more complex set of interactions within $z$ , containing the additional terms $[f; m^{t-1}; q; f^T W^{(b)} q; f^T W^{(b)} m^{t-1}]$ . After an initial analysis, we found these additional terms were not required..Attention Mechanism.Once we have the attention gate $g^t_i$ we use an attention mechanism to extract a contextual vector $c^t$ based upon the current focus. We focus on two types of attention: soft attention and a new attention based GRU. The latter improves performance and is hence the final modeling choice for the DMN+..Soft attention: Soft attention produces a contextual vector $c^t$ through a weighted summation of the sorted list of vectors $\\overleftrightarrow{F}$ and corresponding attention gates $g_i^t$ : $c^t = \\sum _{i=1}^N g^t_i \\overleftrightarrow{f}_i$ This method has two advantages. First, it is easy to compute. Second, if the softmax activation is spiky it can approximate a hard attention function by selecting only a single fact for the contextual vector whilst still being differentiable. However the main disadvantage to soft attention is that the summation process loses both positional and ordering information. Whilst multiple attention passes can retrieve some of this information, this is inefficient..Attention based GRU: For more complex queries, we would like for the attention mechanism to be sensitive to both the position and ordering of the input facts $\\overleftrightarrow{F}$ . An RNN would be advantageous in this situation except they cannot make use of the attention gate from Equation ..We propose a modification to the GRU architecture by embedding information from the attention mechanism. The update gate $u_i$ in Equation 2 decides how much of each dimension of the hidden state to retain and how much should be updated with the transformed input $x_i$ from the current timestep. As $u_i$ is computed using only the current input and the hidden state from previous timesteps, it lacks any knowledge from the question or previous episode memory..By replacing the update gate $u_i$ in the GRU (Equation 2 ) with the output of the attention gate $g^t_i$ (Equation ) in Equation , the GRU can now use the attention gate for updating its internal state. This change is depicted in Fig 5 . .$$h_i &=& g^t_i \\circ \\tilde{h}_i + (1-g^t_i) \\circ h_{i-1}$$   (Eq. 12) .An important consideration is that $g^t_i$ is a scalar, generated using a softmax activation, as opposed to the vector $u_i \\in \\mathbb {R}^{n_H}$ , generated using a sigmoid activation. This allows us to easily visualize how the attention gates activate over the input, later shown for visual QA in Fig. 6 . Though not explored, replacing the softmax activation in Equation with a sigmoid activation would result in $g^t_i \\in \\mathbb {R}^{n_H}$ . To produce the contextual vector $c^t$ used for updating the episodic memory state $m^t$ , we use the final hidden state of the attention based GRU..Episode Memory Updates.After each pass through the attention mechanism, we wish to update the episode memory $m^{t-1}$ with the newly constructed contextual vector $c^t$ , producing $m^t$ . In the DMN, a GRU with the initial hidden state set to the question vector $q$ is used for this purpose. The episodic memory for pass $t$ is computed by .$$m^t = GRU(c^t, m^{t-1})$$   (Eq. 13) .The work of BIBREF10 suggests that using different weights for each pass through the episodic memory may be advantageous. When the model contains only one set of weights for all episodic passes over the input, it is referred to as a tied model, as in the “Mem Weights” row in Table 1 ..Following the memory update component used in BIBREF10 and BIBREF12 we experiment with using a ReLU layer for the memory update, calculating the new episode memory state by .$$m^t = ReLU\\left(W^t [m^{t-1} ; c^t ; q] + b\\right)$$   (Eq. 14) .where $;$ is the concatenation operator, $W^t \\in \\mathbb {R}^{n_H \\times n_H}$ , $b \\in \\mathbb {R}^{n_H}$ , and $n_H$ is the hidden size. The untying of weights and using this ReLU formulation for the memory update improves accuracy by another 0.5% as shown in Table 1 in the last column. The final output of the memory network is passed to the answer module as in the original DMN. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "35\n",
      "Question 1: How many training questions are present in the VQA dataset?\n",
      "Answer 1: There are 248,349 training questions present in the VQA dataset.\n",
      "Question : for the text The Visual Question Answering (VQA) dataset was constructed using the Microsoft COCO dataset BIBREF34 which contained 123,287 training/validation images and 81,434 test images. Each image has several related questions with each question answered by multiple people. This dataset contains 248,349 training questions, 121,512 validation questions, and 244,302 for testing. The testing data was split into test-development, test-standard and test-challenge in BIBREF8 ..Evaluation on both test-standard and test-challenge are implemented via a submission system. test-standard may only be evaluated 5 times and test-challenge is only evaluated at the end of the competition. To the best of our knowledge, VQA is the largest and most complex image dataset for the visual question answering task. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "36\n",
      "Question 1: What type of dataset is used to evaluate the DMN on textual question answering?\n",
      "\n",
      "Answer 1: The DMN is evaluated on bAbI-10k English, a synthetic dataset which features 20 different tasks.\n",
      "Question : for the text For evaluating the DMN on textual question answering, we use bAbI-10k English BIBREF31 , a synthetic dataset which features 20 different tasks. Each example is composed of a set of facts, a question, the answer, and the supporting facts that lead to the answer. The dataset comes in two sizes, referring to the number of training examples each task has: bAbI-1k and bAbI-10k. The experiments in BIBREF10 found that their lowest error rates on the smaller bAbI-1k dataset were on average three times higher than on bAbI-10k. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "37\n",
      "Question 1: Who is Katy Gero supported by?\n",
      "Answer 1: Katy Gero is supported by an NSF GRF (DGE - 1644869).\n",
      "Question : for the text Katy Gero is supported by an NSF GRF (DGE - 1644869). We would also like to thank Elsbeth Turcan for her helpful comments. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "38\n",
      "Question 1: What is the purpose of using the Ablated NVA classifier in the experiment?\n",
      "\n",
      "Answer 1: The purpose of using the Ablated NVA classifier in the experiment is to look at the classifier prediction accuracy of reconstructed and transferred sentences. It is used in particular because it is the most content-blind one.\n",
      "Question : for the text For each model we look at the classifier prediction accuracy of reconstructed and transferred sentences. In particular we use the Ablated NVA classifier, as this is the most content-blind one..We produce 16 outputs from both the Baseline and StyleEq models. For the Baseline, we use a beam search of size 16. For the StyleEQ model, we use the method described in Section SECREF25 to select 16 `sibling' sentences in the target style, and generated a transferred sentence for each. We look at three different methods for selection: all, which uses all output sentences; top, which selects the top ranked sentence based on the score from the model; and oracle, which selects the sentence with the highest classifier likelihood for the intended style..The reason for the third method, which indeed acts as an oracle, is that using the score from the model didn't always surface a transferred sentence that best reflected the desired style. Partially this was because the model score was mostly a function of how well a transferred sentence reflected the distribution of the training data. But additionally, some control settings are more indicative of a target style than others. The use of the classifier allows us to identify the most suitable control setting for a target style that was roughly compatible with the number of content words..In table:fasttext-results we see the results. Note that for both models, the all and top classification accuracy tends to be quite similar, though for the Baseline they are often almost exactly the same when the Baseline has little to no diversity in the outputs..However, the oracle introduces a huge jump in accuracy for the StyleEQ model, especially compared to the Baseline, partially because the diversity of outputs from StyleEQ is much higher; often the Baseline model produces no diversity – the 16 output sentences may be nearly identical, save a single word or two. It's important to note that neither model uses the classifier in any way except to select the sentence from 16 candidate outputs..What this implies is that lurking within the StyleEQ model outputs are great sentences, even if they are hard to find. In many cases, the StyleEQ model has a classification accuracy above the base rate from the test data, which is 75% (see table:classifiers). generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "39\n",
      "Question 1: What is the beam size used during the decoding process for the models evaluated in tab:blueperpl? \n",
      "\n",
      "Answer 1: The beam size used during the decoding process for both models is eight.\n",
      "Question : for the text In tab:blueperpl we report BLEU scores for the reconstruction of test set sentences from their content and feature representations, as well as the model perplexities of the reconstruction. For both models, we use beam decoding with a beam size of eight. Beam candidates are ranked according to their length normalized log-likelihood. On these automatic measures we see that StyleEQ is better able to reconstruct the original sentences. In some sense this evaluation is mostly a sanity check, as the feature controls contain more locally specific information than the genre embeddings, which say very little about how many specific function words one should expect to see in the output. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "40\n",
      "Question 1: What does the Exact column in table:autoeval:ctrl indicate?\n",
      "\n",
      "Answer 1: The Exact column in table:autoeval:ctrl displays the percentage of generated texts that realize the exact number of control features specified by the perturbed control. High percentages in the Exact column indicate greater one-to-one correspondence between the control and surface realization.\n",
      "Question : for the text Designing controllable language models is often difficult because of the various dependencies between tokens; when changing one control value it may effect other aspects of the surface realization. For example, increasing the number of conjunctions may effect how the generator places prepositions to compensate for structural changes in the sentence. Since our features are deterministically recoverable, we can perturb an individual control value and check to see that the desired change was realized in the output. Moreover, we can check the amount of change in the other non-perturbed features to measure the independence of the controls..We sample 50 sentences from each genre from the test set. For each sample, we create a perturbed control setting for each control by adding $\\delta $ to the original control value. This is done for $\\delta \\in \\lbrace -3, -2, -1, 0, 1, 2, 3\\rbrace $, skipping any settings where the new control value would be negative..table:autoeval:ctrl shows the results of this experiment. The Exact column displays the percentage of generated texts that realize the exact number of control features specified by the perturbed control. High percentages in the Exact column indicate greater one-to-one correspondence between the control and surface realization. For example, if the input was “Dracula and Frankenstein and the mummy,” and we change the conjunction feature by $\\delta =-1$, an output of “Dracula, Frankenstein and the mummy,” would count towards the Exact category, while “Dracula, Frankenstein, the mummy,” would not..The Direction column specifies the percentage of cases where the generated text produces a changed number of the control features that, while not exactly matching the specified value of the perturbed control, does change from the original in the correct direction. For example, if the input again was “Dracula and Frankenstein and the mummy,” and we change the conjunction feature by $\\delta =-1$, both outputs of “Dracula, Frankenstein and the mummy,” and “Dracula, Frankenstein, the mummy,” would count towards Direction. High percentages in Direction mean that we could roughly ensure desired surface realizations by modifying the control by a larger $\\delta $..Finally, the Atomic column specifies the percentage of cases where the generated text with the perturbed control only realizes changes to that specific control, while other features remain constant. For example, if the input was “Dracula and Frankenstein in the castle,” and we set the conjunction feature to $\\delta =-1$, an output of “Dracula near Frankenstein in the castle,” would not count as Atomic because, while the number of conjunctions did decrease by one, the number of simple preposition changed. An output of “Dracula, Frankenstein in the castle,” would count as Atomic. High percentages in the Atomic column indicate this feature is only loosely coupled to the other features and can be changed without modifying other aspects of the sentence..Controls such as conjunction, determiner, and punctuation are highly controllable, with Exact rates above 80%. But with the exception of the constituency parse features, all controls have high Direction rates, many in the 90s. These results indicate our model successfully controls these features. The fact that the Atomic rates are relatively low is to be expected, as controls are highly coupled – e.g. to increase 1stPer, it is likely another pronoun control will have to decrease. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "41\n",
      "Question 1: What is the success rate for fooling a style classifier using the presented model?\n",
      "\n",
      "Answer 1: The presented model was able to fool a style classifier 84% of the time in automatic evaluations.\n",
      "Question : for the text We present a formal, extendable model of style that can add control to any neural text generation system. We model style as a suite of low-level linguistic controls, and train a neural encoder-decoder model to reconstruct reference sentences given only content words and the setting of the controls. In automatic evaluations, we show that our model can fool a style classifier 84% of the time and outperforms a baseline genre-embedding model. In human evaluations, we encounter the `vampires in space' problem in which content and style are equally discriminative but people focus more on the content..In future work we would like to model higher-level syntactic controls. BIBREF20 show that differences in clausal constructions, for instance having a dependent clause before an independent clause or vice versa, is a marker of style appreciated by the reader. Such features would likely interact with our lower-level controls in an interesting way, and provide further insight into style transfer in text. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "42\n",
      "What is the difference between the StyleEQ and Baseline models in terms of their ability to change syntactic constructions in a stylistically distinctive way?\n",
      "\n",
      "The StyleEQ model successfully changes syntactic constructions in a stylistically distinctive way, while the Baseline model only makes minor modifications such as changing the type of a single pronoun.\n",
      "Question : for the text table:cherrypicking shows example outputs for the StyleEQ and Baseline models. Through inspection we see that the StyleEQ model successfully changes syntactic constructions in stylistically distinctive ways, such as increasing syntactic complexity when transferring to philosophy, or changing relevant pronouns when transferring to sci-fi. In contrast, the Baseline model doesn't create outputs that move far from the reference sentence, making only minor modifications such changing the type of a single pronoun..To determine how readers would classify our transferred sentences, we recruited three English Literature PhD candidates, all of whom had passed qualifying exams that included determining both genre and era of various literary texts. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "43\n",
      "Question 1: What was the reason for the Baseline model having slightly higher fluency scores than the StyleEQ model?\n",
      "\n",
      "Answer 1: The Baseline model often reconstructed the reference sentence even when performing style transfer, and was far less constrained in how to construct the output sentence, resulting in slightly higher fluency scores. In contrast, the StyleEQ struggled to incorporate the controls into a fluent sentence, leading to lower fluency scores.\n",
      "Question : for the text To evaluate the fluency of our outputs, we had the annotators score reference sentences, reconstructed sentences, and transferred sentences on a 0-5 scale, where 0 was incoherent and 5 was a well-written human sentence..table:fluency shows the average fluency of various conditions from all three annotators. Both models have fluency scores around 3. Upon inspection of the outputs, it is clear that many have fluency errors, resulting in ungrammatical sentences..Notably the Baseline often has slightly higher fluency scores than the StyleEQ model. This is likely because the Baseline model is far less constrained in how to construct the output sentence, and upon inspection often reconstructs the reference sentence even when performing style transfer. In contrast, the StyleEQ is encouraged to follow the controls, but can struggle to incorporate these controls into a fluent sentence..The fluency of all outputs is lower than desired. We expect that incorporating pre-trained language models would increase the fluency of all outputs without requiring larger datasets. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "44\n",
      "Question 1: What was the accuracy of annotators A1, A2, and A3 on the baseline task of identifying the style of sentences from the training corpus?\n",
      "\n",
      "Answer 1: The accuracy of annotators A1, A2, and A3 on the baseline task of identifying the style of sentences from the training corpus was 80%, 88%, and 80% respectively.\n",
      "Question : for the text Each annotator annotated 90 reference sentences (i.e. from the training corpus) with which style they thought the sentence was from. The accuracy on this baseline task for annotators A1, A2, and A3 was 80%, 88%, and 80% respectively, giving us an upper expected bound on the human evaluation..In discussing this task with the annotators, they noted that content is a heavy predictor of genre, and that would certainly confound their annotations. To attempt to mitigate this, we gave them two annotation tasks: which-of-3 where they simply marked which style they thought a sentence was from, and which-of-2 where they were given the original style and marked which style they thought the sentence was transferred into..For each task, each annotator marked 180 sentences: 90 from each model, with an even split across the three genres. Annotators were presented the sentences in a random order, without information about the models. In total, each marked 270 sentences. (Note there were no reconstructions in this annotation task.).table:humanclassifiers shows the results. In both tasks, accuracy of annotators classifying the sentence as its intended style was low. In which-of-3, scores were around 20%, below the chance rate of 33%. In which-of-2, scores were in the 50s, slightly above the chance rate of 50%. This was the case for both models. There was a slight increase in accuracy for the StyleEQ model over the Baseline for which-of-3, but the opposite trend for which-of-2, suggesting these differences are not significant..It's clear that it's hard to fool the annotators. Introspecting on their approach, the annotators expressed having immediate responses based on key words – for instance any references of `space' implied `sci-fi'. We call this the `vampires in space' problem, because no matter how well a gothic sentence is rewritten as a sci-fi one, it's impossible to ignore the fact that there is a vampire in space. The transferred sentences, in the eyes of the Ablated NVA classifier (with no access to content words), did quite well transferring into their intended style. But people are not blind to content. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "45\n",
      "What is the \"vampires in space\" problem that the annotators encountered while working on stylometrics?\n",
      "\n",
      "Answer 1: The \"vampires in space\" problem refers to the challenge of separating syntactic constructions, which are important for distinguishing literary styles, from distinctive content that often co-occurs with these constructions. Thus, while syntactic constructions are useful for creating fingerprints of styles, they are often surface realizations of higher-level stylistic decisions.\n",
      "Question : for the text Working with the annotators, we regularly came up against the 'vampires in space' problem: while syntactic constructions account for much of the distinction of literary styles, these constructions often co-occur with distinctive content..Stylometrics finds syntactic constructions are great at fingerprinting, but suggests that these constructions are surface realizations of higher-level stylistic decisions. The number and type of personal pronouns is a reflection of how characters feature in a text. A large number of positional prepositions may be the result of a writer focusing on physical descriptions of scenes. In our attempt to decouple these, we create Frankenstein sentences, which piece together features of different styles – we are putting vampires in space..Another way to validate our approach would be to select data that is stylistically distinctive but with similar content: perhaps genres in which content is static but language use changes over time, stylistically distinct authors within a single genre, or parodies of a distinctive genre. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "46\n",
      "What is the main challenge in performing style transfer in the text domain?\n",
      "\n",
      "The main challenge in performing style transfer in the text domain is disentangling style from content, which is particularly difficult as most work in style transfer relies on the availability of meta-data that often conflates style with content. Generalizing style transfer requires separating style from the meaning of the text itself.\n",
      "Question : for the text All text has style, whether it be formal or informal, polite or aggressive, colloquial, persuasive, or even robotic. Despite the success of style transfer in image processing BIBREF0, BIBREF1, there has been limited progress in the text domain, where disentangling style from content is particularly difficult..To date, most work in style transfer relies on the availability of meta-data, such as sentiment, authorship, or formality. While meta-data can provide insight into the style of a text, it often conflates style with content, limiting the ability to perform style transfer while preserving content. Generalizing style transfer requires separating style from the meaning of the text itself. The study of literary style can guide us. For example, in the digital humanities and its subfield of stylometry, content doesn't figure prominently in practical methods of discriminating authorship and genres, which can be thought of as style at the level of the individual and population, respectively. Rather, syntactic and functional constructions are the most salient features..In this work, we turn to literary style as a test-bed for style transfer, and build on work from literature scholars using computational techniques for analysis. In particular we draw on stylometry: the use of surface level features, often counts of function words, to discriminate between literary styles. Stylometry first saw success in attributing authorship to the disputed Federalist Papers BIBREF2, but is recently used by scholars to study things such as the birth of genres BIBREF3 and the change of author styles over time BIBREF4. The use of function words is likely not the way writers intend to express style, but they appear to be downstream realizations of higher-level stylistic decisions..We hypothesize that surface-level linguistic features, such as counts of personal pronouns, prepositions, and punctuation, are an excellent definition of literary style, as borne out by their use in the digital humanities, and our own style classification experiments. We propose a controllable neural encoder-decoder model in which these features are modelled explicitly as decoder feature embeddings. In training, the model learns to reconstruct a text using only the content words and the linguistic feature embeddings. We can then transfer arbitrary content words to a new style without parallel data by setting the low-level style feature embeddings to be indicative of the target style..This paper makes the following contributions:.A formal model of style as a suite of controllable, low-level linguistic features that are independent of content..An automatic evaluation showing that our model fools a style classifier 84% of the time..A human evaluation with English literature experts, including recommendations for dealing with the entanglement of content with style. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "47\n",
      "Question 1: What are the proposed low-level linguistic feature counts that define style in the given text?\n",
      "\n",
      "Answer 1: The proposed low-level linguistic feature counts are counts of closed word classes (like personal pronouns) and counts of syntactic features like the number of SBAR non-terminals in the constituency parse of a sentence. These controls are extracted heuristically and rely on pre-defined word lists.\n",
      "Question : for the text Given that non-content words are distinctive enough for a classifier to determine style, we propose a suite of low-level linguistic feature counts (henceforth, controls) as our formal, content-blind definition of style. The style of a sentence is represented as a vector of counts of closed word classes (like personal pronouns) as well as counts of syntactic features like the number of SBAR non-terminals in its constituency parse, since clause structure has been shown to be indicative of style BIBREF20. Controls are extracted heuristically, and almost all rely on counts of pre-defined word lists. For constituency parses we use the Stanford Parser BIBREF21. table:controlexamples lists all the controls along with examples. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "48\n",
      "Question 1: What is the reconstruction task used to train models for stylistic transfer?\n",
      "\n",
      "Answer 1: The reconstruction task involves inputting a distorted version of a reference sentence and outputting the original reference. Models are trained using this task to learn how to construct a sentence using content and style independently.\n",
      "Question : for the text Models are trained with a reconstruction task, in which a distorted version of a reference sentence is input and the goal is to output the original reference..fig:sentenceinput illustrates the process. Controls are calculated heuristically. All words found in the control word lists are then removed from the reference sentence. The remaining words, which represent the content, are used as input into the model, along with their POS tags and lemmas..In this way we encourage models to construct a sentence using content and style independently. This will allow us to vary the stylistic controls while keeping the content constant, and successfully perform style transfer. When generating a new sentence, the controls correspond to the counts of the corresponding syntactic features that we expect to be realized in the output. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "49\n",
      "Question 1: What is the input to the encoder in the StyleEQ model?\n",
      "\n",
      "Answer 1: The input to the encoder in the StyleEQ model is a sequence of content words with their lemmas, fine and coarse grained part-of-speech tags, denoted as $X_{.,j}$ for $j \\in \\mathcal {T} = \\lbrace \\textrm {word, lemma, fine-pos, coarse-pos}\\rbrace $. Each token (and its lemma and POS) is embedded before concatenating and feeding into the encoder GRU to obtain encoder hidden states.\n",
      "Question : for the text We implement our feature controlled language model using a neural encoder-decoder with attention BIBREF22, using 2-layer uni-directional gated recurrent units (GRUs) for the encoder and decoder BIBREF23..The input to the encoder is a sequence of $M$ content words, along with their lemmas, and fine and coarse grained part-of-speech (POS) tags, i.e. $X_{.,j} = (x_{1,j},\\ldots ,x_{M,j})$ for $j \\in \\mathcal {T} = \\lbrace \\textrm {word, lemma, fine-pos, coarse-pos}\\rbrace $. We embed each token (and its lemma and POS) before concatenating, and feeding into the encoder GRU to obtain encoder hidden states, $ c_i = \\operatorname{gru}(c_{i-1}, \\left[E_j(X_{i,j}), \\; j\\in \\mathcal {T} \\right]; \\omega _{enc}) $ for $i \\in {1,\\ldots ,M},$ where initial state $c_0$, encoder GRU parameters $\\omega _{enc}$ and embedding matrices $E_j$ are learned parameters..The decoder sequentially generates the outputs, i.e. a sequence of $N$ tokens $y =(y_1,\\ldots ,y_N)$, where all tokens $y_i$ are drawn from a finite output vocabulary $\\mathcal {V}$. To generate the each token we first embed the previously generated token $y_{i-1}$ and a vector of $K$ control features $z = ( z_1,\\ldots , z_K)$ (using embedding matrices $E_{dec}$ and $E_{\\textrm {ctrl-1}}, \\ldots , E_{\\textrm {ctrl-K}}$ respectively), before concatenating them into a vector $\\rho _i,$ and feeding them into the decoder side GRU along with the previous decoder state $h_{i-1}$:.where $\\omega _{dec}$ are the decoder side GRU parameters..Using the decoder hidden state $h_i$ we then attend to the encoder context vectors $c_j$, computing attention scores $\\alpha _{i,j}$, where.before passing $h_i$ and the attention weighted context $\\bar{c}_i=\\sum _{j=1}^M \\alpha _{i,j} c_j$ into a single hidden-layer perceptron with softmax output to compute the next token prediction probability,.where $W,U,V$ and $u,v, \\nu $ are parameter matrices and vectors respectively..Crucially, the controls $z$ remain fixed for all input decoder steps. Each $z_k$ represents the frequency of one of the low-level features described in sec:formalstyle. During training on the reconstruction task, we can observe the full output sequence $y,$ and so we can obtain counts for each control feature directly. Controls receive a different embedding depending on their frequency, where counts of 0-20 each get a unique embedding, and counts greater than 20 are assigned to the same embedding. At test time, we set the values of the controls according to procedure described in Section SECREF25..We use embedding sizes of 128, 128, 64, and 32 for token, lemma, fine, and coarse grained POS embedding matrices respectively. Output token embeddings $E_{dec}$ have size 512, and 50 for the control feature embeddings. We set 512 for all GRU and perceptron output sizes. We refer to this model as the StyleEQ model. See fig:model for a visual depiction of the model. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "50\n",
      "Question 1: How does the model with genre embeddings compare to the StyleEQ model with explicitly represented features as input?\n",
      "\n",
      "Answer 1: The model with genre embeddings represents $K$ features in the form of a genre embedding and allows for generating in a specific style by setting the appropriate embedding. It was compared to the StyleEQ model with explicitly represented features as input and found to be similar in performance. Both models were able to generate text in different styles effectively.\n",
      "Question : for the text We compare the above model to a similar model, where rather than explicitly represent $K$ features as input, we have $K$ features in the form of a genre embedding, i.e. we learn a genre specific embedding for each of the gothic, scifi, and philosophy genres, as studied in BIBREF8 and BIBREF7. To generate in a specific style, we simply set the appropriate embedding. We use genre embeddings of size 850 which is equivalent to the total size of the $K$ feature embeddings in the StyleEQ model. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "1\n",
      "Question 1: What is the purpose of the method used in the StyleEQ model for selecting controls?\n",
      "\n",
      "Answer 1: The purpose of the method used in the StyleEQ model for selecting controls is to ensure that the controls match the reference sentence in magnitude by finding all sentences in the target style with the same number of words as the reference sentence and adding constraints for the same number of proper nouns, nouns, verbs, and adjectives. This method captures the natural distribution of the corpora and encourages a diversity of outputs.\n",
      "Question : for the text In the Baseline model, style transfer is straightforward: given an input sentence in one style, fix the encoder content features while selecting a different genre embedding. In contrast, the StyleEQ model requires selecting the counts for each control. Although there are a variety of ways to do this, we use a method that encourages a diversity of outputs..In order to ensure the controls match the reference sentence in magnitude, we first find all sentences in the target style with the same number of words as the reference sentence. Then, we add the following constraints: the same number of proper nouns, the same number of nouns, the same number of verbs, and the same number of adjectives. We randomly sample $n$ of the remaining sentences, and for each of these `sibling' sentences, we compute the controls. For each of the new controls, we generate a sentence using the original input sentence content features. The generated sentences are then reranked using the length normalized log-likelihood under the model. We can then select the highest scoring sentence as our style-transferred output, or take the top-$k$ when we need a diverse set of outputs..The reason for this process is that although there are group-level distinctive controls for each style, e.g. the high use of punctuation in philosophy books or of first person pronouns in gothic novels, at the sentence level it can understandably be quite varied. This method matches sentences between styles, capturing the natural distribution of the corpora. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "2\n",
      "Question 1: What is the validation set BLEU score used for in training the models?\n",
      "\n",
      "Answer 1: The validation set BLEU score is used to select the final model iteration for evaluation after training for a maximum of 200 epochs.\n",
      "Question : for the text We train both models with minibatch stochastic gradient descent with a learning rate of 0.25, weight decay penalty of 0.0001, and batch size of 64. We also apply dropout with a drop rate of 0.25 to all embedding layers, the GRUs, and preceptron hidden layer. We train for a maximum of 200 epochs, using validation set BLEU score BIBREF26 to select the final model iteration for evaluation. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "3\n",
      "Question 1: What are the four models used to replace content words with part-of-speech (POS) tag placeholder tokens in the study?\n",
      "\n",
      "Answer 1: The four models used to replace content words with part-of-speech (POS) tag placeholder tokens in the study are Ablated N, Ablated NV, Ablated NVA, and Content-only.\n",
      "Question : for the text The stylometric research cited above suggests that the most frequently used words, e.g. function words, are most discriminating of authorship and literary style. We investigate these claims using three corpora that have distinctive styles in the literary community: gothic novels, philosophy books, and pulp science fiction, hereafter sci-fi..We retrieve gothic novels and philosophy books from Project Gutenberg and pulp sci-fi from Internet Archive's Pulp Magazine Archive. We partition this corpus into train, validation, and test sets the sizes of which can be found in Table TABREF12..In order to validate the above claims, we train five different classifiers to predict the literary style of sentences from our corpus. Each classifier has gradually more content words replaced with part-of-speech (POS) tag placeholder tokens. The All model is trained on sentences with all proper nouns replaced by `PROPN'. The models Ablated N, Ablated NV, and Ablated NVA replace nouns, nouns & verbs, and nouns, verbs, & adjectives with the corresponding POS tag respectively. Finally, Content-only is trained on sentences with all words that are not tagged as NOUN, VERB, ADJ removed; the remaining words are not ablated..We train the classifiers on the training set, balancing the class distribution to make sure there are the same number of sentences from each style. Classifiers are trained using fastText BIBREF19, using tri-gram features with all other settings as default. table:classifiers shows the accuracies of the classifiers..The styles are highly distinctive: the All classifier has an accuracy of 86%. Additionally, even the Ablated NVA is quite successful, with 75% accuracy, even without access to any content words. The Content only classifier is also quite successful, at 80% accuracy. This indicates that these stylistic genres are distinctive at both the content level and at the syntactic level. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "4\n",
      "Question 1: What is the main difference between the approaches used in BIBREF11 and BIBREF13 for controlling style in text generation?\n",
      "\n",
      "Answer 1: The main difference is that BIBREF11 focuses on controlling style when generating sentences from meaning representations for restaurants, with limited diversity in outputs and constraints on style. On the other hand, BIBREF13 investigates using word-level heuristics to create stylistic parameters and content parameters for controlling text generation using a movie review dataset, with successful control over parameters in the outputs. Their approach is related to style transfer, but with the content held fixed.\n",
      "Question : for the text Several papers have worked on controlling style when generating sentences from restaurant meaning representations BIBREF11, BIBREF12. In each of these cases, the diversity in outputs is quite small given the constraints of the meaning representation, style is often constrained to interjections (like “yeah”), and there is no original style from which to transfer..BIBREF13 investigate using stylistic parameters and content parameters to control text generation using a movie review dataset. Their stylistic parameters are created using word-level heuristics and they are successful in controlling these parameters in the outputs. Their success bodes well for our related approach in a style transfer setting, in which the content (not merely content parameters) is held fixed. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "5\n",
      "Question 1: What is the main limitation of using parallel data in style transfer for text?\n",
      "\n",
      "Answer 1: The main limitation of using parallel data in style transfer for text is the difficulty of finding parallel texts, which dramatically limits this approach. While it may work for very specific styles, it is not a feasible option for most cases.\n",
      "Question : for the text Following in the footsteps of machine translation, style transfer in text has seen success by using parallel data. BIBREF5 use modern translations of Shakespeare plays to build a modern-to-Shakespearan model. BIBREF6 compile parallel data for formal and informal sentences, allowing them to successfully use various machine translation techniques. While parallel data may work for very specific styles, the difficulty of finding parallel texts dramatically limits this approach. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "6\n",
      "Question 1: What is the main obstacle in models that seek to disentangle style and content in text generation? \n",
      "\n",
      "Answer 1: The main obstacle is often to disentangle style and content, as style is often modeled as a monolithic style embedding, which struggles to capture the existing knowledge we have about style and disentangle content.\n",
      "Question : for the text There has been a decent amount of work on this approach in the past few years BIBREF7, BIBREF8, mostly focusing on variations of an encoder-decoder framework in which style is modeled as a monolithic style embedding. The main obstacle is often to disentangle style and content. However, it remains a challenging problem..Perhaps the most successful is BIBREF9, who use a de-noising auto encoder and back translation to learn style without parallel data. BIBREF10 outline the benefits of automatically extracting style, and suggest there is a formal weakness of using linguistic heuristics. In contrast, we believe that monolithic style embeddings don't capture the existing knowledge we have about style, and will struggle to disentangle content. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "7\n",
      "Question 1: What is stylometry? \n",
      "\n",
      "Answer 1: Stylometry is a statistical approach to studying style in literature, which involves analyzing and comparing an author's use of vocabulary and structural properties in their texts. It has been used in debates over authorship and can be used to identify patterns and stylistic attributes in literary works.\n",
      "Question : for the text Style, in literary research, is anything but a stable concept, but it nonetheless has a long tradition of study in the digital humanities. In a remarkably early quantitative study of literature, BIBREF14 charts sentence-level stylistic attributes specific to a number of novelists. Half a century later, BIBREF15 builds on earlier work in information theory by BIBREF16, and defines a literary text as consisting of two “materials\": “the vocabulary, and some structural properties, the style, of its author.\".Beginning with BIBREF2, statistical approaches to style, or stylometry, join the already-heated debates over the authorship of literary works. A noteable example of this is the “Delta\" measure, which uses z-scores of function word frequencies BIBREF17. BIBREF18 find that Shakespeare added some material to a later edition of Thomas Kyd's The Spanish Tragedy, and that Christopher Marlowe collaborated with Shakespeare on Henry VI. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "8\n",
      "What is the significance of facial expressions and visual content in assessing a user's depression status online?\n",
      "\n",
      "The text highlights that visual content in images from posts and profiles can provide valuable psychological cues for understanding a user's depression status, particularly emotions reflected in facial expressions, attributes contributing to computational aesthetics, and sentimental quotes they share. Facial presence and expression, color, brightness, and contrast of images, and naturalness are some of the features that can be analyzed to assess a user's behavior. Facial expressions can also be correlated with emotional signals captured from textual content.\n",
      "Question : for the text We now provide an in-depth analysis of visual and textual content of vulnerable users..Visual Content Analysis: We show that the visual content in images from posts as well as profiles provide valuable psychological cues for understanding a user's depression status. Profile/posted images can surface self-stigmatization BIBREF53 . Additionally, as opposed to typical computer vision framework for object recognition that often relies on thousands of predetermined low-level features, what matters more for assessing user's online behavior is the emotions reflected in facial expressions BIBREF54 , attributes contributing to the computational aesthetics BIBREF55 , and sentimental quotes they may subscribe to (Figure FIGREF15 ) BIBREF8 ..Facial Presence: .For capturing facial presence, we rely on BIBREF56 's approach that uses multilevel convolutional coarse-to-fine network cascade to tackle facial landmark localization. We identify facial presentation, emotion from facial expression, and demographic features from profile/posted images . Table TABREF21 illustrates facial presentation differences in both profile and posted images (media) for depressed and control users in INLINEFORM0 . With control class showing significantly higher in both profile and media (8%, 9% respectively) compared to that for the depressed class. In contrast with age and gender disclosure, vulnerable users are less likely to disclose their facial identity, possibly due to lack of confidence or fear of stigma..Facial Expression:.Following BIBREF8 's approach, we adopt Ekman's model of six emotions: anger, disgust, fear, joy, sadness and surprise, and use the Face++ API to automatically capture them from the shared images. Positive emotions are joy and surprise, and negative emotions are anger, disgust, fear, and sadness. In general, for each user u in INLINEFORM0 , we process profile/shared images for both the depressed and the control groups with at least one face from the shared images (Table TABREF23 ). For the photos that contain multiple faces, we measure the average emotion..Figure FIGREF27 illustrates the inter-correlation of these features. Additionally, we observe that emotions gleaned from facial expressions correlated with emotional signals captured from textual content utilizing LIWC. This indicates visual imagery can be harnessed as a complementary channel for measuring online emotional signals..General Image Features:.The importance of interpretable computational aesthetic features for studying users' online behavior has been highlighted by several efforts BIBREF55 , BIBREF8 , BIBREF57 . Color, as a pillar of the human vision system, has a strong association with conceptual ideas like emotion BIBREF58 , BIBREF59 . We measured the normalized red, green, blue and the mean of original colors, and brightness and contrast relative to variations of luminance. We represent images in Hue-Saturation-Value color space that seems intuitive for humans, and measure mean and variance for saturation and hue. Saturation is defined as the difference in the intensities of the different light wavelengths that compose the color. Although hue is not interpretable, high saturation indicates vividness and chromatic purity which are more appealing to the human eye BIBREF8 . Colorfulness is measured as a difference against gray background BIBREF60 . Naturalness is a measure of the degree of correspondence between images and the human perception of reality BIBREF60 . In color reproduction, naturalness is measured from the mental recollection of the colors of familiar objects. Additionally, there is a tendency among vulnerable users to share sentimental quotes bearing negative emotions. We performed optical character recognition (OCR) with python-tesseract to extract text and their sentiment score. As illustrated in Table TABREF26 , vulnerable users tend to use less colorful (higher grayscale) profile as well as shared images to convey their negative feelings, and share images that are less natural (Figure FIGREF15 ). With respect to the aesthetic quality of images (saturation, brightness, and hue), depressed users use images that are less appealing to the human eye. We employ independent t-test, while adopting Bonferroni Correction as a conservative approach to adjust the confidence intervals. Overall, we have 223 features, and choose Bonferroni-corrected INLINEFORM0 level of INLINEFORM1 (*** INLINEFORM2 , ** INLINEFORM3 )..** alpha= 0.05, *** alpha = 0.05/223.Demographics Inference & Language Cues: LIWC has been used extensively for examining the latent dimensions of self-expression for analyzing personality BIBREF61 , depressive behavior, demographic differences BIBREF43 , BIBREF40 , etc. Several studies highlight that females employ more first-person singular pronouns BIBREF62 , and deictic language BIBREF63 , while males tend to use more articles BIBREF64 which characterizes concrete thinking, and formal, informational and affirmation words BIBREF65 . For age analysis, the salient findings include older individuals using more future tense verbs BIBREF62 triggering a shift in focus while aging. They also show positive emotions BIBREF66 and employ fewer self-references (i.e. 'I', 'me') with greater first person plural BIBREF62 . Depressed users employ first person pronouns more frequently BIBREF67 , repeatedly use negative emotions and anger words. We analyzed psycholinguistic cues and language style to study the association between depressive behavior as well as demographics. Particularly, we adopt Levinson's adult development grouping that partitions users in INLINEFORM0 into 5 age groups: (14,19],(19,23], (23,34],(34,46], and (46,60]. Then, we apply LIWC for characterizing linguistic styles for each age group for users in INLINEFORM1 ..Qualitative Language Analysis: The recent LIWC version summarizes textual content in terms of language variables such as analytical thinking, clout, authenticity, and emotional tone. It also measures other linguistic dimensions such as descriptors categories (e.g., percent of target words gleaned by dictionary, or longer than six letters - Sixltr) and informal language markers (e.g., swear words, netspeak), and other linguistic aspects (e.g., 1st person singular pronouns.).Thinking Style:.Measuring people's natural ways of trying to analyze, and organize complex events have strong association with analytical thinking. LIWC relates higher analytic thinking to more formal and logical reasoning whereas a lower value indicates focus on narratives. Also, cognitive processing measures problem solving in mind. Words such as \"think,\" \"realize,\" and \"know\" indicates the degree of \"certainty\" in communications. Critical thinking ability relates to education BIBREF68 , and is impacted by different stages of cognitive development at different ages . It has been shown that older people communicate with greater cognitive complexity while comprehending nuances and subtle differences BIBREF62 . We observe a similar pattern in our data (Table TABREF40 .) A recent study highlights how depression affects brain and thinking at molecular level using a rat model BIBREF69 . Depression can promote cognitive dysfunction including difficulty in concentrating and making decisions. We observed a notable differences in the ability to think analytically in depressed and control users in different age groups (see Figure FIGREF39 - A, F and Table TABREF40 ). Overall, vulnerable younger users are not logical thinkers based on their relative analytical score and cognitive processing ability..Authenticity:.Authenticity measures the degree of honesty. Authenticity is often assessed by measuring present tense verbs, 1st person singular pronouns (I, me, my), and by examining the linguistic manifestations of false stories BIBREF70 . Liars use fewer self-references and fewer complex words. Psychologists often see a child's first successfull lie as a mental growth. There is a decreasing trend of the Authenticity with aging (see Figure FIGREF39 -B.) Authenticity for depressed youngsters is strikingly higher than their control peers. It decreases with age (Figure FIGREF39 -B.).Clout:.People with high clout speak more confidently and with certainty, employing more social words with fewer negations (e.g., no, not) and swear words. In general, midlife is relatively stable w.r.t. relationships and work. A recent study shows that age 60 to be best for self-esteem BIBREF71 as people take on managerial roles at work and maintain a satisfying relationship with their spouse. We see the same pattern in our data (see Figure FIGREF39 -C and Table TABREF40 ). Unsurprisingly, lack of confidence (the 6th PHQ-9 symptom) is a distinguishable characteristic of vulnerable users, leading to their lower clout scores, especially among depressed users before middle age (34 years old)..Self-references:.First person singular words are often seen as indicating interpersonal involvement and their high usage is associated with negative affective states implying nervousness and depression BIBREF66 . Consistent with prior studies, frequency of first person singular for depressed people is significantly higher compared to that of control class. Similarly to BIBREF66 , youngsters tend to use more first-person (e.g. I) and second person singular (e.g. you) pronouns (Figure FIGREF39 -G)..Informal Language Markers; Swear, Netspeak:.Several studies highlighted the use of profanity by young adults has significantly increased over the last decade BIBREF72 . We observed the same pattern in both the depressed and the control classes (Table TABREF40 ), although it's rate is higher for depressed users BIBREF1 . Psychologists have also shown that swearing can indicate that an individual is not a fragmented member of a society. Depressed youngsters, showing higher rate of interpersonal involvement and relationships, have a higher rate of cursing (Figure FIGREF39 -E). Also, Netspeak lexicon measures the frequency of terms such as lol and thx..Sexual, Body: .Sexual lexicon contains terms like \"horny\", \"love\" and \"incest\", and body terms like \"ache\", \"heart\", and \"cough\". Both start with a higher rate for depressed users while decreasing gradually while growing up, possibly due to changes in sexual desire as we age (Figure FIGREF39 -H,I and Table TABREF40 .).Quantitative Language Analysis:.We employ one-way ANOVA to compare the impact of various factors and validate our findings above. Table TABREF40 illustrates our findings, with a degree of freedom (df) of 1055. The null hypothesis is that the sample means' for each age group are similar for each of the LIWC features..*** alpha = 0.001, ** alpha = 0.01, * alpha = 0.05 generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "9\n",
      "What is the purpose of utilizing self-disclosure clues in social media analytic studies?\n",
      "\n",
      "The purpose of utilizing self-disclosure clues in social media analytic studies is to create ground-truth data for predicting demographics and user's depressive behavior, as well as to collect metadata values of each user such as profile descriptions, followers_count, created_at, and profile_image_url.\n",
      "Question : for the text Self-disclosure clues have been extensively utilized for creating ground-truth data for numerous social media analytic studies e.g., for predicting demographics BIBREF36 , BIBREF41 , and user's depressive behavior BIBREF46 , BIBREF47 , BIBREF48 . For instance, vulnerable individuals may employ depressive-indicative terms in their Twitter profile descriptions. Others may share their age and gender, e.g., \"16 years old suicidal girl\"(see Figure FIGREF15 ). We employ a huge dataset of 45,000 self-reported depressed users introduced in BIBREF46 where a lexicon of depression symptoms consisting of 1500 depression-indicative terms was created with the help of psychologist clinician and employed for collecting self-declared depressed individual's profiles. A subset of 8,770 users (24 million time-stamped tweets) containing 3981 depressed and 4789 control users (that do not show any depressive behavior) were verified by two human judges BIBREF46 . This dataset INLINEFORM0 contains the metadata values of each user such as profile descriptions, followers_count, created_at, and profile_image_url..Age Enabled Ground-truth Dataset: We extract user's age by applying regular expression patterns to profile descriptions (such as \"17 years old, self-harm, anxiety, depression\") BIBREF41 . We compile \"age prefixes\" and \"age suffixes\", and use three age-extraction rules: 1. I am X years old 2. Born in X 3. X years old, where X is a \"date\" or age (e.g., 1994). We selected a subset of 1061 users among INLINEFORM0 as gold standard dataset INLINEFORM1 who disclose their age. From these 1061 users, 822 belong to depressed class and 239 belong to control class. From 3981 depressed users, 20.6% disclose their age in contrast with only 4% (239/4789) among control group. So self-disclosure of age is more prevalent among vulnerable users. Figure FIGREF18 depicts the age distribution in INLINEFORM2 . The general trend, consistent with the results in BIBREF42 , BIBREF49 , is biased toward young people. Indeed, according to Pew, 47% of Twitter users are younger than 30 years old BIBREF50 . Similar data collection procedure with comparable distribution have been used in many prior efforts BIBREF51 , BIBREF49 , BIBREF42 . We discuss our approach to mitigate the impact of the bias in Section 4.1. The median age is 17 for depressed class versus 19 for control class suggesting either likely depressed-user population is younger, or depressed youngsters are more likely to disclose their age for connecting to their peers (social homophily.) BIBREF51 .Gender Enabled Ground-truth Dataset: We selected a subset of 1464 users INLINEFORM0 from INLINEFORM1 who disclose their gender in their profile description. From 1464 users 64% belonged to the depressed group, and the rest (36%) to the control group. 23% of the likely depressed users disclose their gender which is considerably higher (12%) than that for the control class. Once again, gender disclosure varies among the two gender groups. For statistical significance, we performed chi-square test (null hypothesis: gender and depression are two independent variables). Figure FIGREF19 illustrates gender association with each of the two classes. Blue circles (positive residuals, see Figure FIGREF19 -A,D) show positive association among corresponding row and column variables while red circles (negative residuals, see Figure FIGREF19 -B,C) imply a repulsion. Our findings are consistent with the medical literature BIBREF10 as according to BIBREF52 more women than men were given a diagnosis of depression. In particular, the female-to-male ratio is 2.1 and 1.9 for Major Depressive Disorder and Dysthymic Disorder respectively. Our findings from Twitter data indicate there is a strong association (Chi-square: 32.75, p-value:1.04e-08) between being female and showing depressive behavior on Twitter. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "10\n",
      "What is the average accuracy of the model for predicting gender on the ground-truth dataset?\n",
      "\n",
      "The average accuracy of the model for predicting gender on the ground-truth dataset is 0.82. (as mentioned in the text, in the section on \"Prediction with Textual Content\" and \"Prediction with Visual Imagery\")\n",
      "Question : for the text We leverage both the visual and textual content for predicting age and gender..Prediction with Textual Content:.We employ BIBREF73 's weighted lexicon of terms that uses the dataset of 75,394 Facebook users who shared their status, age and gender. The predictive power of this lexica was evaluated on Twitter, blog, and Facebook, showing promising results BIBREF73 . Utilizing these two weighted lexicon of terms, we are predicting the demographic information (age or gender) of INLINEFORM0 (denoted by INLINEFORM1 ) using following equation: INLINEFORM2 .where INLINEFORM0 is the lexicon weight of the term, and INLINEFORM1 represents the frequency of the term in the user generated INLINEFORM2 , and INLINEFORM3 measures total word count in INLINEFORM4 . As our data is biased toward young people, we report age prediction performance for each age group separately (Table TABREF42 ). Moreover, to measure the average accuracy of this model, we build a balanced dataset (keeping all the users above 23 -416 users), and then randomly sampling the same number of users from the age ranges (11,19] and (19,23]. The average accuracy of this model is 0.63 for depressed users and 0.64 for control class. Table TABREF44 illustrates the performance of gender prediction for each class. The average accuracy is 0.82 on INLINEFORM5 ground-truth dataset..Prediction with Visual Imagery:.Inspired by BIBREF56 's approach for facial landmark localization, we use their pretrained CNN consisting of convolutional layers, including unshared and fully-connected layers, to predict gender and age from both the profile and shared images. We evaluate the performance for gender and age prediction task on INLINEFORM0 and INLINEFORM1 respectively as shown in Table TABREF42 and Table TABREF44 ..Demographic Prediction Analysis:.We delve deeper into the benefits and drawbacks of each data modality for demographic information prediction. This is crucial as the differences between language cues between age groups above age 35 tend to become smaller (see Figure FIGREF39 -A,B,C) and making the prediction harder for older people BIBREF74 . In this case, the other data modality (e.g., visual content) can play integral role as a complementary source for age inference. For gender prediction (see Table TABREF44 ), on average, the profile image-based predictor provides a more accurate prediction for both the depressed and control class (0.92 and 0.90) compared to content-based predictor (0.82). For age prediction (see Table TABREF42 ), textual content-based predictor (on average 0.60) outperforms both of the visual-based predictors (on average profile:0.51, Media:0.53)..However, not every user provides facial identity on his account (see Table TABREF21 ). We studied facial presentation for each age-group to examine any association between age-group, facial presentation and depressive behavior (see Table TABREF43 ). We can see youngsters in both depressed and control class are not likely to present their face on profile image. Less than 3% of vulnerable users between 11-19 years reveal their facial identity. Although content-based gender predictor was not as accurate as image-based one, it is adequate for population-level analysis. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "11\n",
      "Question 1: What are some factors that can trigger depression in different age groups?\n",
      "\n",
      "Answer 1: Depression triggers for children include parental depression, domestic violence, and loss of a pet, friend or family member. For teenagers (ages 12-18), depression may arise from hormonal imbalance, sexuality concerns and rejection by peers. Young adults (ages 19-29) may develop depression due to life transitions, poverty, trauma, and work issues. Adult (ages 30-60) depression triggers include caring simultaneously for children and aging parents, financial burden, work and relationship issues. Senior adults develop depression from common late-life issues, social isolation, major life loses such as the death of a spouse, financial stress and other chronic health problems (e.g., cardiac disease, dementia).\n",
      "Question : for the text Depression is a highly prevalent public health challenge and a major cause of disability worldwide. Depression affects 6.7% (i.e., about 16 million) Americans each year . According to the World Mental Health Survey conducted in 17 countries, on average, about 5% of people reported having an episode of depression in 2011 BIBREF0 . Untreated or under-treated clinical depression can lead to suicide and other chronic risky behaviors such as drug or alcohol addiction..Global efforts to curb clinical depression involve identifying depression through survey-based methods employing phone or online questionnaires. These approaches suffer from under-representation as well as sampling bias (with very small group of respondents.) In contrast, the widespread adoption of social media where people voluntarily and publicly express their thoughts, moods, emotions, and feelings, and even share their daily struggles with mental health problems has not been adequately tapped into studying mental illnesses, such as depression. The visual and textual content shared on different social media platforms like Twitter offer new opportunities for a deeper understanding of self-expressed depression both at an individual as well as community-level. Previous research efforts have suggested that language style, sentiment, users' activities, and engagement expressed in social media posts can predict the likelihood of depression BIBREF1 , BIBREF2 . However, except for a few attempts BIBREF3 , BIBREF4 , BIBREF5 , BIBREF6 , these investigations have seldom studied extraction of emotional state from visual content of images in posted/profile images. Visual content can express users' emotions more vividly, and psychologists noted that imagery is an effective medium for communicating difficult emotions..According to eMarketer, photos accounted for 75% of content posted on Facebook worldwide and they are the most engaging type of content on Facebook (87%). Indeed, \"a picture is worth a thousand words\" and now \"photos are worth a million likes.\" Similarly, on Twitter, the tweets with image links get twice as much attention as those without , and video-linked tweets drive up engagement . The ease and naturalness of expression through visual imagery can serve to glean depression-indicators in vulnerable individuals who often seek social support through social media BIBREF7 . Further, as psychologist Carl Rogers highlights, we often pursue and promote our Ideal-Self . In this regard, the choice of profile image can be a proxy for the online persona BIBREF8 , providing a window into an individual's mental health status. For instance, choosing emaciated legs of girls covered with several cuts as profile image portrays negative self-view BIBREF9 ..Inferring demographic information like gender and age can be crucial for stratifying our understanding of population-level epidemiology of mental health disorders. Relying on electronic health records data, previous studies explored gender differences in depressive behavior from different angles including prevalence, age at onset, comorbidities, as well as biological and psychosocial factors. For instance, women have been diagnosed with depression twice as often as men BIBREF10 and national psychiatric morbidity survey in Britain has shown higher risk of depression in women BIBREF11 . On the other hand, suicide rates for men are three to five times higher compared to that of the women BIBREF12 ..Although depression can affect anyone at any age, signs and triggers of depression vary for different age groups . Depression triggers for children include parental depression, domestic violence, and loss of a pet, friend or family member. For teenagers (ages 12-18), depression may arise from hormonal imbalance, sexuality concerns and rejection by peers. Young adults (ages 19-29) may develop depression due to life transitions, poverty, trauma, and work issues. Adult (ages 30-60) depression triggers include caring simultaneously for children and aging parents, financial burden, work and relationship issues. Senior adults develop depression from common late-life issues, social isolation, major life loses such as the death of a spouse, financial stress and other chronic health problems (e.g., cardiac disease, dementia). Therefore, inferring demographic information while studying depressive behavior from passively sensed social data, can shed better light on the population-level epidemiology of depression..The recent advancements in deep neural networks, specifically for image analysis task, can lead to determining demographic features such as age and gender BIBREF13 . We show that by determining and integrating heterogeneous set of features from different modalities – aesthetic features from posted images (colorfulness, hue variance, sharpness, brightness, blurriness, naturalness), choice of profile picture (for gender, age, and facial expression), the screen name, the language features from both textual content and profile's description (n-gram, emotion, sentiment), and finally sociability from ego-network, and user engagement – we can reliably detect likely depressed individuals in a data set of 8,770 human-annotated Twitter users..We address and derive answers to the following research questions: 1) How well do the content of posted images (colors, aesthetic and facial presentation) reflect depressive behavior? 2) Does the choice of profile picture show any psychological traits of depressed online persona? Are they reliable enough to represent the demographic information such as age and gender? 3) Are there any underlying common themes among depressed individuals generated using multimodal content that can be used to detect depression reliably? generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "12\n",
      "What is the advantage of using the early fusion BIBREF32 technique in the multimodal framework?\n",
      "\n",
      "Answer 1: The advantage of using the early fusion BIBREF32 technique in the multimodal framework is that it reduces the learning effort and shows promising results compared to the computationally expensive late fusion scheme, where each modality requires a separate supervised modeling. Additionally, it allows for the modeling of each user as a vector concatenation of individual modality features.\n",
      "Question : for the text We use the above findings for predicting depressive behavior. Our model exploits early fusion BIBREF32 technique in feature space and requires modeling each user INLINEFORM0 in INLINEFORM1 as vector concatenation of individual modality features. As opposed to computationally expensive late fusion scheme where each modality requires a separate supervised modeling, this model reduces the learning effort and shows promising results BIBREF75 . To develop a generalizable model that avoids overfitting, we perform feature selection using statistical tests and all relevant ensemble learning models. It adds randomness to the data by creating shuffled copies of all features (shadow feature), and then trains Random Forest classifier on the extended data. Iteratively, it checks whether the actual feature has a higher Z-score than its shadow feature (See Algorithm SECREF6 and Figure FIGREF45 ) BIBREF76 ..Main each Feature INLINEFORM0 INLINEFORM1 .RndForrest( INLINEFORM0 ) Calculate Imp INLINEFORM1 INLINEFORM2 Generate next hypothesis , INLINEFORM3 Once all hypothesis generated Perform Statistical Test INLINEFORM4 //Binomial Distribution INLINEFORM5 Feature is important Feature is important. Ensemble Feature Selection.Next, we adopt an ensemble learning method that integrates the predictive power of multiple learners with two main advantages; its interpretability with respect to the contributions of each feature and its high predictive power. For prediction we have INLINEFORM0 where INLINEFORM1 is a weak learner and INLINEFORM2 denotes the final prediction..In particular, we optimize the loss function: INLINEFORM0 where INLINEFORM1 incorporates INLINEFORM2 and INLINEFORM3 regularization. In each iteration, the new INLINEFORM4 is obtained by fitting weak learner to the negative gradient of loss function. Particularly, by estimating the loss function with Taylor expansion : INLINEFORM5 where its first expression is constant, the second and the third expressions are first ( INLINEFORM6 ) and second order derivatives ( INLINEFORM7 ) of the loss. INLINEFORM8 .For exploring the weak learners, assume INLINEFORM0 has k leaf nodes, INLINEFORM1 be subset of users from INLINEFORM2 belongs to the node INLINEFORM3 , and INLINEFORM4 denotes the prediction for node INLINEFORM5 . Then, for each user INLINEFORM6 belonging to INLINEFORM7 , INLINEFORM8 and INLINEFORM9 INLINEFORM10 .Next, for each leaf node INLINEFORM0 , deriving w.r.t INLINEFORM1 : INLINEFORM2 .and by substituting weights: INLINEFORM0 .which represents the loss for fixed weak learners with INLINEFORM0 nodes. The trees are built sequentially such that each subsequent tree aims to reduce the errors of its predecessor tree. Although, the weak learners have high bias, the ensemble model produces a strong learner that effectively integrate the weak learners by reducing bias and variance (the ultimate goal of supervised models) BIBREF77 . Table TABREF48 illustrates our multimodal framework outperform the baselines for identifying depressed users in terms of average specificity, sensitivity, F-Measure, and accuracy in 10-fold cross-validation setting on INLINEFORM1 dataset. Figure FIGREF47 shows how the likelihood of being classified into the depressed class varies with each feature addition to the model for a sample user in the dataset. The prediction bar (the black bar) shows that the log-odds of prediction is 0.31, that is, the likelihood of this person being a depressed user is 57% (1 / (1 + exp(-0.3))). The figure also sheds light on the impact of each contributing feature. The waterfall charts represent how the probability of being depressed changes with the addition of each feature variable. For instance, the \"Analytic thinking\" of this user is considered high 48.43 (Median:36.95, Mean: 40.18) and this decreases the chance of this person being classified into the depressed group by the log-odds of -1.41. Depressed users have significantly lower \"Analytic thinking\" score compared to control class. Moreover, the 40.46 \"Clout\" score is a low value (Median: 62.22, Mean: 57.17) and it decreases the chance of being classified as depressed. With respect to the visual features, for instance, the mean and the median of 'shared_colorfulness' is 112.03 and 113 respectively. The value of 136.71 would be high; thus, it decreases the chance of being depressed for this specific user by log-odds of -0.54. Moreover, the 'profile_naturalness' of 0.46 is considered high compared to 0.36 as the mean for the depressed class which justifies pull down of the log-odds by INLINEFORM2 . For network features, for instance, 'two_hop_neighborhood' for depressed users (Mean : 84) are less than that of control users (Mean: 154), and is reflected in pulling down the log-odds by -0.27..Baselines:.To test the efficacy of our multi-modal framework for detecting depressed users, we compare it against existing content, content-network, and image-based models (based on the aforementioned general image feature, facial presence, and facial expressions.) generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "13\n",
      "What are some approaches for detecting depression from social media content?\n",
      "\n",
      "Answer 1: Some approaches for detecting depression from social media content include machine/deep learning and natural language processing, which analyze factors such as language, emotion, style, ego-network, and user engagement. There have also been advancements in identifying suicidal and self-harm signals, as well as predicting the severity of mental health from forum posts. The emergence of photo-sharing platforms, like Instagram, has also attracted researchers' attention to study people's behavior from their visual narratives.\n",
      "Question : for the text Mental Health Analysis using Social Media:.Several efforts have attempted to automatically detect depression from social media content utilizing machine/deep learning and natural language processing approaches. Conducting a retrospective study over tweets, BIBREF14 characterizes depression based on factors such as language, emotion, style, ego-network, and user engagement. They built a classifier to predict the likelihood of depression in a post BIBREF14 , BIBREF15 or in an individual BIBREF1 , BIBREF16 , BIBREF17 , BIBREF18 . Moreover, there have been significant advances due to the shared task BIBREF19 focusing on methods for identifying depressed users on Twitter at the Computational Linguistics and Clinical Psychology Workshop (CLP 2015). A corpus of nearly 1,800 Twitter users was built for evaluation, and the best models employed topic modeling BIBREF20 , Linguistic Inquiry and Word Count (LIWC) features, and other metadata BIBREF21 . More recently, a neural network architecture introduced by BIBREF22 combined posts into a representation of user's activities for detecting depressed users. Another active line of research has focused on capturing suicide and self-harm signals BIBREF23 , BIBREF24 , BIBREF25 , BIBREF26 , BIBREF2 , BIBREF27 . Moreover, the CLP 2016 BIBREF28 defined a shared task on detecting the severity of the mental health from forum posts. All of these studies derive discriminative features to classify depression in user-generated content at message-level, individual-level or community-level. Recent emergence of photo-sharing platforms such as Instagram, has attracted researchers attention to study people's behavior from their visual narratives – ranging from mining their emotions BIBREF29 , and happiness trend BIBREF30 , to studying medical concerns BIBREF31 . Researchers show that people use Instagram to engage in social exchange and storytelling about their difficult experiences BIBREF4 . The role of visual imagery as a mechanism of self-disclosure by relating visual attributes to mental health disclosures on Instagram was highlighted by BIBREF3 , BIBREF5 where individual Instagram profiles were utilized to build a prediction framework for identifying markers of depression. The importance of data modality to understand user behavior on social media was highlighted by BIBREF32 . More recently, a deep neural network sequence modeling approach that marries audio and text data modalities to analyze question-answer style interviews between an individual and an agent has been developed to study mental health BIBREF32 . Similarly, a multimodal depressive dictionary learning was proposed to detect depressed users on Twitter BIBREF33 . They provide a sparse user representations by defining a feature set consisting of social network features, user profile features, visual features, emotional features BIBREF34 , topic-level features, and domain-specific features. Particularly, our choice of multi-model prediction framework is intended to improve upon the prior works involving use of images in multimodal depression analysis BIBREF33 and prior works on studying Instagram photos BIBREF6 , BIBREF35 ..Demographic information inference on Social Media: .There is a growing interest in understanding online user's demographic information due to its numerous applications in healthcare BIBREF36 , BIBREF37 . A supervised model developed by BIBREF38 for determining users' gender by employing features such as screen-name, full-name, profile description and content on external resources (e.g., personal blog). Employing features including emoticons, acronyms, slangs, punctuations, capitalization, sentence length and included links/images, along with online behaviors such as number of friends, post time, and commenting activity, a supervised model was built for predicting user's age group BIBREF39 . Utilizing users life stage information such as secondary school student, college student, and employee, BIBREF40 builds age inference model for Dutch Twitter users. Similarly, relying on profile descriptions while devising a set of rules and patterns, a novel model introduced for extracting age for Twitter users BIBREF41 . They also parse description for occupation by consulting the SOC2010 list of occupations and validating it through social surveys. A novel age inference model was developed while relying on homophily interaction information and content for predicting age of Twitter users BIBREF42 . The limitations of textual content for predicting age and gender was highlighted by BIBREF43 . They distinguish language use based on social gender, age identity, biological sex and chronological age by collecting crowdsourced signals using a game in which players (crowd) guess the biological sex and age of a user based only on their tweets. Their findings indicate how linguistic markers can misguide (e.g., a heart represented as <3 can be misinterpreted as feminine when the writer is male.) Estimating age and gender from facial images by training a convolutional neural networks (CNN) for face recognition is an active line of research BIBREF44 , BIBREF13 , BIBREF45 . generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "14\n",
      "Question 1: What is the proposed model aiming to incorporate into the definition modeling task?\n",
      "\n",
      "Answer 1: The proposed model aims to incorporate sememes into the definition modeling task.\n",
      "Question : for the text Our proposed model aims to incorporate sememes into the definition modeling task. Given the word to be defined $x$ and its corresponding sememes $s=[s_1, \\dots , s_N ]$ , we define the probability of generating the definition $y=[y_1, \\dots , y_t ]$ as: .$$P(y | x, s) = \\prod _{t=1}^{T} P(y_t|y_{<t},x,s) $$   (Eq. 8) .Similar to Eq. 6 , we can maximize the log-likelihood with the definition corpus $D_{x,s,y}$ to learn model parameters: .$$\\hat{\\theta } = \\mathop {\\rm argmax}_{\\theta } \\sum _{\\langle x,s,y \\rangle \\in D_{x,s,y}}\\log P(y | x, s; \\theta ) $$   (Eq. 9) .The probability $P(y | x, s)$ can be implemented with an adaptive attention based encoder-decoder framework, which we call Adaptive-Attention Model (AAM). The new architecture consists of a bidirectional RNN as the encoder and a RNN decoder that adaptively attends to the sememes during decoding a definition..Similar to BIBREF13 , the encoder is a bidirectional RNN, consisting of forward and backward RNNs. Given the word to be defined $x$ and its corresponding sememes $s=[s_1, \\dots , s_N ]$ , we define the input sequence of vectors for the encoder as $\\mathbf {v}=[\\mathbf {v}_1,\\dots ,\\mathbf {v}_{N}]$ . The vector $\\mathbf {v}_n$ is given as follows: .$$\\mathbf {v}_n = [\\mathbf {x}; \\mathbf {s}_n ]$$   (Eq. 11) .where $\\mathbf {x}$ is the vector representation of the word $x$ , $\\mathbf {s}_n$ is the vector representation of the $n$ -th sememe $s_n$ , and $[\\mathbf {a};\\mathbf {b}]$ denote concatenation of vector $\\mathbf {a}$ and $\\mathbf {b}$ ..The forward RNN $\\overrightarrow{f}$ reads the input sequence of vectors from $\\mathbf {v}_1$ to $\\mathbf {v}_N$ and calculates a forward hidden state for position $n$ as: .$$\\overrightarrow{\\mathbf {h}_{n}} &=& f(\\mathbf {v}_n, \\overrightarrow{\\mathbf {h}_{n-1}})$$   (Eq. 12) .where $f$ is an LSTM or GRU. Similarly, the backward RNN $\\overleftarrow{f}$ reads the input sequence of vectors from $\\mathbf {v}_N$ to $\\mathbf {v}_1$ and obtain a backward hidden state for position $n$ as: .$$\\overleftarrow{\\mathbf {h}_{n}} &=& f(\\mathbf {h}_n, \\overleftarrow{\\mathbf {h}_{n+1}})$$   (Eq. 13) .In this way, we obtain a sequence of encoder hidden states $\\mathbf {h}=\\left[\\mathbf {h}_1,...,\\mathbf {h}_N\\right]$ , by concatenating the forward hidden state $\\overrightarrow{\\mathbf {h}_{n}}$ and the backward one $\\overleftarrow{\\mathbf {h}_{n}}$ at each position $n$ : .$$\\mathbf {h}_n=\\left[\\overrightarrow{\\mathbf {h}_{n}}, \\overleftarrow{\\mathbf {h}_{n}}\\right]$$   (Eq. 14) .The hidden state $\\mathbf {h}_n$ captures the sememe- and word-aware information of the $n$ -th sememe..As attention-based neural encoder-decoder frameworks have shown great success in image captioning BIBREF14 , document summarization BIBREF15 and neural machine translation BIBREF13 , it is natural to adopt the attention-based recurrent decoder in BIBREF13 as our decoder. The vanilla attention attends to the sememes at every time step. However, not all words in the definition have corresponding sememes. For example, sememe “住下” (reside) could be useful when generating “食宿” (residence), but none of the sememes is useful when generating “提供” (provide). Besides, language correlations make the sememes unnecessary when generating words like “和” (and) and “给” (for)..Inspired by BIBREF9 , we introduce the adaptive attention mechanism for the decoder. At each time step $t$ , we summarize the time-varying sememes' information as sememe context, and the language model's information as LM context. Then, we use another attention to obtain the context vector, relying on either the sememe context or LM context..More concretely, we define each conditional probability in Eq. 8 as: .$$& P(y_t|y_{<t},x,s) \\propto \\exp {(y_t;\\mathbf {z}_t,\\mathbf {c}_t)} &  \\\\\n",
      "& \\mathbf {z}_t = f(\\mathbf {z}_{t-1},y_{t-1},\\mathbf {c}_t) & $$   (Eq. 17) .where $\\mathbf {c}_t$ is the context vector from the output of the adaptive attention module at time $t$ , $\\mathbf {z}_t$ is a decoder's hidden state at time $t$ ..To obtain the context vector $\\mathbf {c}_t$ , we first compute the sememe context vector $\\hat{\\mathbf {c}_t}$ and the LM context $\\mathbf {o}_t$ . Similar to the vanilla attention, the sememe context $\\hat{\\mathbf {c}_t}$ is obtained with a soft attention mechanism as: .$$\\hat{\\mathbf {c}_t} = \\sum _{n=1}^{N} \\alpha _{tn} \\mathbf {h}_n,$$   (Eq. 18) .where .$$\\alpha _{tn} &=& \\frac{\\mathrm {exp}(e_{tn})}{\\sum _{i=1}^{N} \\mathrm {exp}(e_{ti})} \\nonumber \\\\\n",
      "e_{tn} &=& \\mathbf {w}_{\\hat{c}}^T[\\mathbf {h}_n; \\mathbf {z}_{t-1}].$$   (Eq. 19) .Since the decoder's hidden states store syntax and semantic information for language modeling, we compute the LM context $\\mathbf {o}_t$ with a gated unit, whose input is the definition word $y_t$ and the previous hidden state $\\mathbf {z}_{t-1}$ : .$$\\mathbf {g}_t &=& \\sigma (\\mathbf {W}_g [y_{t-1}; \\mathbf {z}_{t-1}] + \\mathbf {b}_g) \\nonumber \\\\\n",
      "\\mathbf {o}_t &=& \\mathbf {g}_t \\odot \\mathrm {tanh} (\\mathbf {z}_{t-1}) $$   (Eq. 20) .Once the sememe context vector $\\hat{\\mathbf {c}_t}$ and the LM context $\\mathbf {o}_t$ are ready, we can generate the context vector with an adaptive attention layer as: .$$\\mathbf {c}_t = \\beta _t \\mathbf {o}_t + (1-\\beta _t)\\hat{\\mathbf {c}_t}, $$   (Eq. 21) .where .$$\\beta _{t} &=& \\frac{\\mathrm {exp}(e_{to})}{\\mathrm {exp}(e_{to})+\\mathrm {exp}(e_{t\\hat{c}})} \\nonumber \\\\\n",
      "e_{to} &=& (\\mathbf {w}_c)^T[\\mathbf {o}_t;\\mathbf {z}_t] \\nonumber \\\\\n",
      "e_{t\\hat{c}} &=& (\\mathbf {w}_c)^T[\\hat{\\mathbf {c}_t};\\mathbf {z}_t] $$   (Eq. 22) . $\\beta _{t}$ is a scalar in range $[0,1]$ , which controls the relative importance of LM context and sememe context..Once we obtain the context vector $\\mathbf {c}_t$ , we can update the decoder's hidden state and generate the next word according to Eq. and Eq. 17 , respectively. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "15\n",
      "Question 1: How is the baseline model BIBREF3 implemented?\n",
      "\n",
      "Answer 1: The baseline model BIBREF3 is implemented with a recurrent neural network based encoder-decoder framework, which learns a probabilistic mapping from a word to its definition without utilizing the information of sememes. The model computes the conditional probability of each definition word depending on the previous definition words and the word being defined, and the probability of the word-definition pair is computed according to the probability chain rule. The model parameters are learned by maximizing the log-likelihood.\n",
      "Question : for the text The baseline model BIBREF3 is implemented with a recurrent neural network based encoder-decoder framework. Without utilizing the information of sememes, it learns a probabilistic mapping $P(y | x)$ from the word $x$ to be defined to a definition $y = [y_1, \\dots , y_T ]$ , in which $y_t$ is the $t$ -th word of definition $y$ ..More concretely, given a word $x$ to be defined, the encoder reads the word and generates its word embedding $\\mathbf {x}$ as the encoded information. Afterward, the decoder computes the conditional probability of each definition word $y_t$ depending on the previous definition words $y_{<t}$ , as well as the word being defined $x$ , i.e., $P(y_t|y_{<t},x)$ . $P(y_t|y_{<t},x)$ is given as: .$$& P(y_t|y_{<t},x) \\propto \\exp {(y_t;\\mathbf {z}_t,\\mathbf {x})} & \\\\\n",
      "& \\mathbf {z}_t = f(\\mathbf {z}_{t-1},y_{t-1},\\mathbf {x}) &$$   (Eq. 4) .where $\\mathbf {z}_t$ is the decoder's hidden state at time $t$ , $f$ is a recurrent nonlinear function such as LSTM and GRU, and $\\mathbf {x}$ is the embedding of the word being defined. Then the probability of $P(y | x)$ can be computed according to the probability chain rule: .$$P(y | x) = \\prod _{t=1}^{T} P(y_t|y_{<t},x)$$   (Eq. 5) .We denote all the parameters in the model as $\\theta $ and the definition corpus as $D_{x,y}$ , which is a set of word-definition pairs. Then the model parameters can be learned by maximizing the log-likelihood: .$$\\hat{\\theta } = \\mathop {\\rm argmax}_{\\theta } \\sum _{\\langle x, y \\rangle \\in D_{x,y}}\\log P(y | x; \\theta ) $$   (Eq. 6)  generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "16\n",
      "Question 1: What is the purpose of the Chinese definition modeling task introduced in this text?\n",
      "\n",
      "Answer 1: The purpose of the Chinese definition modeling task is to generate a definition in Chinese for a given word and sememes of a specific word sense, which is useful for dictionary compilation.\n",
      "Question : for the text We introduce the Chinese definition modeling task that generates a definition in Chinese for a given word and sememes of a specific word sense. This task is useful for dictionary compilation. To achieve this, we constructed the CDM dataset with word-sememes-definition triples. We propose two novel methods, AAM and SAAM, to generate word sense aware definition by utilizing sememes. In experiments on the CDM dataset we show that our proposed AAM and SAAM outperform the state-of-the-art approach with a large margin. By efficiently incorporating sememes, the SAAM achieves the best performance with improvement over the state-of-the-art method. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "17\n",
      "Question 1: How was the CDM dataset constructed for the Chinese definition modeling task?\n",
      "\n",
      "Answer 1: The CDM dataset was constructed by aligning definitions and sememe groups for each word sense, and filtering out entries that contained the interpreted word or certain types of words. Each entry consists of a triple with the word, sememes, and definition, and the dataset contains 104,517 entries with 30,052 unique interpreted words.\n",
      "Question : for the text To verify our proposed models, we construct the CDM dataset for the Chinese definition modeling task. cdmEach entry in the dataset is a triple that consists of: the interpreted word, sememes and a definition for a specific word sense, where the sememes are annotated with HowNet BIBREF5 , and the definition are annotated with Chinese Concept Dictionary (CCD) BIBREF6 ..Concretely, for a common word in HowNet and CCD, we first align its definitions from CCD and sememe groups from HowNet, where each group represents one word sense. We define the sememes of a definition as the combined sememes associated with any token of the definition. Then for each definition of a word, we align it with the sememe group that has the largest number of overlapping sememes with the definition's sememes. With such aligned definition and sememe group, we add an entry that consists of the word, the sememes of the aligned sememe group and the aligned definition. Each word can have multiple entries in the dataset, especially the polysemous word. To improve the quality of the created dataset, we filter out entries that the definition contains the interpreted word, or the interpreted word is among function words, numeral words and proper nouns..After processing, we obtain the dataset that contains 104,517 entries with 30,052 unique interpreted words. We divide the dataset according to the unique interpreted words into training set, validation set and test set with a ratio of 18:1:1. Table 1 shows the detailed data statistics. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "18\n",
      "Question 1: What is the difference between the previous works on word embeddings and definition modeling and the current research incorporating sememes?\n",
      "\n",
      "Answer 1: The previous works on word embeddings and definition modeling focused on evaluating and interpreting word embeddings, while the current research incorporates sememes to generate word sense aware word definition for dictionary compilation.\n",
      "Question : for the text Distributed representations of words, or word embeddings BIBREF18 were widely used in the field of NLP in recent years. Since word embeddings have been shown to capture lexical semantics, BIBREF3 proposed the definition modeling task as a more transparent and direct representation of word embeddings. This work is followed by BIBREF4 , who studied the problem of word ambiguities in definition modeling by employing latent variable modeling and soft attention mechanisms. Both works focus on evaluating and interpreting word embeddings. In contrast, we incorporate sememes to generate word sense aware word definition for dictionary compilation. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "19\n",
      "Question 1: What is the focus of this section?\n",
      "Answer 1: This section focuses on the construction process of the CDM dataset, experimental results, and analysis.\n",
      "Question : for the text In this section, we will first introduce the construction process of the CDM dataset, then the experimental results and analysis. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "20\n",
      "What is the purpose of Chinese definition modeling?\n",
      "Chinese definition modeling is the task of generating a definition in Chinese for a given Chinese word, and its purpose is to benefit the compilation of dictionaries, especially for Chinese as a foreign language learners.\n",
      "Question : for the text Chinese definition modeling is the task of generating a definition in Chinese for a given Chinese word. This task can benefit the compilation of dictionaries, especially dictionaries for Chinese as a foreign language (CFL) learners..In recent years, the number of CFL learners has risen sharply. In 2017, 770,000 people took the Chinese Proficiency Test, an increase of 38% from 2016. However, most Chinese dictionaries are for native speakers. Since these dictionaries usually require a fairly high level of Chinese, it is necessary to build a dictionary specifically for CFL learners. Manually writing definitions relies on the knowledge of lexicographers and linguists, which is expensive and time-consuming BIBREF0 , BIBREF1 , BIBREF2 . Therefore, the study on writing definitions automatically is of practical significance..Definition modeling was first proposed by BIBREF3 as a tool to evaluate different word embeddings. BIBREF4 extended the work by incorporating word sense disambiguation to generate context-aware word definition. Both methods are based on recurrent neural network encoder-decoder framework without attention. In contrast, this paper formulates the definition modeling task as an automatic way to accelerate dictionary compilation..In this work, we introduce a new dataset for the Chinese definition modeling task that we call Chinese Definition Modeling Corpus cdm(CDM). CDM consists of 104,517 entries, where each entry contains a word, the sememes of a specific word sense, and the definition in Chinese of the same word sense. Sememes are minimum semantic units of word meanings, and the meaning of each word sense is typically composed of several sememes, as is illustrated in Figure 1 . For a given word sense, CDM annotates the sememes according to HowNet BIBREF5 , and the definition according to Chinese Concept Dictionary (CCD) BIBREF6 . Since sememes have been widely used in improving word representation learning BIBREF7 and word similarity computation BIBREF8 , we argue that sememes can benefit the task of definition modeling..We propose two novel models to incorporate sememes into Chinese definition modeling: the Adaptive-Attention Model (AAM) and the Self- and Adaptive-Attention Model (SAAM). Both models are based on the encoder-decoder framework. The encoder maps word and sememes into a sequence of continuous representations, and the decoder then attends to the output of the encoder and generates the definition one word at a time. Different from the vanilla attention mechanism, the decoder of both models employs the adaptive attention mechanism to decide which sememes to focus on and when to pay attention to sememes at one time BIBREF9 . Following BIBREF3 , BIBREF4 , the AAM is built using recurrent neural networks (RNNs). However, recent works demonstrate that attention-based architecture that entirely eliminates recurrent connections can obtain new state-of-the-art in neural machine translation BIBREF10 , constituency parsing BIBREF11 and semantic role labeling BIBREF12 . In the SAAM, we replace the LSTM-based encoder and decoder with an architecture based on self-attention. This fully attention-based model allows for more parallelization, reduces the path length between word, sememes and the definition, and can reach a new state-of-the-art on the definition modeling task. To the best of our knowledge, this is the first work to introduce the attention mechanism and utilize external resource for the definition modeling task..In experiments on the CDM dataset we show that our proposed AAM and SAAM outperform the state-of-the-art approach with a large margin. By efficiently incorporating sememes, the SAAM achieves the best performance with improvement over the state-of-the-art method by +6.0 BLEU. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "21\n",
      "What is HowNet and how is it used in natural language processing?\n",
      "\n",
      "HowNet is a knowledge base that annotates Chinese concepts with one or more sememes, playing an important role in understanding the semantic meanings of concepts in human languages. It has been widely used in NLP tasks such as word representation learning, word similarity computation, and sentiment analysis.\n",
      "Question : for the text Recently many knowledge bases (KBs) are established in order to organize human knowledge in structural forms. By providing human experiential knowledge, KBs are playing an increasingly important role as infrastructural facilities of natural language processing..HowNet BIBREF19 is a knowledge base that annotates each concept in Chinese with one or more sememes. HowNet plays an important role in understanding the semantic meanings of concepts in human languages, and has been widely used in word representation learning BIBREF7 , word similarity computation BIBREF20 and sentiment analysis BIBREF21 . For example, BIBREF7 improved word representation learning by utilizing sememes to represent various senses of each word and selecting suitable senses in contexts with an attention mechanism..Chinese Concept Dictionary (CCD) is a WordNet-like semantic lexicon BIBREF22 , BIBREF23 , where each concept is defined by a set of synonyms (SynSet). CCD has been widely used in many NLP tasks, such as word sense disambiguation BIBREF23 ..In this work, we annotate the word with aligned sememes from HowNet and definition from CCD. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "22\n",
      "What is the definition modeling task? \n",
      "\n",
      "The definition modeling task is to generate an explanatory sentence for an interpreted word.\n",
      "Question : for the text The definition modeling task is to generate an explanatory sentence for the interpreted word. For example, given the word “旅馆” (hotel), a model should generate a sentence like this: “给旅行者提供食宿和其他服务的地方” (A place to provide residence and other services for tourists). Since distributed representations of words have been shown to capture lexical syntax and semantics, it is intuitive to employ word embeddings to generate natural language definitions..Previously, BIBREF3 proposed several model architectures to generate a definition according to the distributed representation of a word. We briefly summarize their model with the best performance in Section \"Experiments\" and adopt it as our baseline model..Inspired by the works that use sememes to improve word representation learning BIBREF7 and word similarity computation BIBREF8 , we propose the idea of incorporating sememes into definition modeling. Sememes can provide additional semantic information for the task. As shown in Figure 1 , sememes are highly correlated to the definition. For example, the sememe “场所” (place) is related with the word “地方” (place) of the definition, and the sememe “旅游” (tour) is correlated to the word “旅行者” (tourists) of the definition..Therefore, to make full use of the sememes in CDM dataset, we propose AAM and SAAM for the task, in Section \"Adaptive-Attention Model\" and Section \"Self- and Adaptive-Attention Model\" , respectively. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "23\n",
      "What is the impact of incorporating sememes in the proposed models AAM and SAAM for generating definitions?\n",
      "The incorporation of sememes in AAM and SAAM improves their performance compared to the baseline model by +3.1 BLEU and +6.65 BLEU respectively. Sememes are particularly useful in disambiguating the word sense associated with the target definition. The SAAM model that incorporates sememes achieves the highest BLEU score of 36.36 on the test set, demonstrating the effectiveness of sememes and the architecture of SAAM. The generated definitions with sememes are more accurate and concrete. Removing sememes from SAAM results in a performance drop of 3.53 BLEU on the test set.\n",
      "Question : for the text We report the experimental results on CDM test set in Figure 3 . It shows that both of our proposed models, namely AAM and SAAM, achieve good results and outperform the baseline by a large margin. With sememes, AAM and SAAM can improve over the baseline with +3.1 BLEU and +6.65 BLEU, respectively..We also find that sememes are very useful for generating the definition. The incorporation of sememes improves the AAM with +3.32 BLEU and the SAAM with +3.53 BLEU. This can be explained by that sememes help to disambiguate the word sense associated with the target definition..Among all models, SAAM which incorporates sememes achieves the new state-of-the-art, with a BLEU score of 36.36 on the test set, demonstrating the effectiveness of sememes and the architecture of SAAM..Table 2 lists some example definitions generated with different models. For each word-sememes pair, the generated three definitions are ordered according to the order: Baseline, AAM and SAAM. For AAM and SAAM, we use the model that incorporates sememes. These examples show that with sememes, the model can generate more accurate and concrete definitions. For example, for the word “旅馆” (hotel), the baseline model fails to generate definition containing the token “旅行者”(tourists). However, by incoporating sememes' information, especially the sememe “旅游” (tour), AAM and SAAM successfully generate “旅行者”(tourists). Manual inspection of others examples also supports our claim..We also conduct an ablation study to evaluate the various choices we made for SAAM. We consider three key components: position embedding, the adaptive attention layer, and the incorporated sememes. As illustrated in table 3 , we remove one of these components and report the performance of the resulting model on validation set and test set. We also list the performance of the baseline and AAM for reference..It demonstrates that all components benefit the SAAM. Removing position embedding is 0.31 BLEU below the SAAM on the test set. Removing the adaptive attention layer is 0.43 BLEU below the SAAM on the test set. Sememes affects the most. Without incoporating sememes, the performance drops 3.53 BLEU on the test set. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "24\n",
      "Question 1: What is the architecture used in SAAM to reduce training time and learn better dependency between word, sememes, and tokens of the definition?\n",
      "\n",
      "Answer 1: SAAM adopts an architecture based on attention, which replaces the recurrent connections with self-attention. This architecture not only reduces training time by allowing for more parallelization but also learns better dependency between word, sememes, and tokens of the definition by reducing the path length between them.\n",
      "Question : for the text Recent works demonstrate that an architecture entirely based on attention can obtain new state-of-the-art in neural machine translation BIBREF10 , constituency parsing BIBREF11 and semantic role labeling BIBREF12 . SAAM adopts similar architecture and replaces the recurrent connections in AAM with self-attention. Such architecture not only reduces the training time by allowing for more parallelization, but also learns better the dependency between word, sememes and tokens of the definition by reducing the path length between them..Given the word to be defined $x$ and its corresponding ordered sememes $s=[s_1, \\dots , s_{N}]$ , we combine them as the input sequence of embeddings for the encoder, i.e., $\\mathbf {v}=[\\mathbf {v}_0, \\mathbf {v}_1, \\dots , \\mathbf {v}_{N}]$ . The $n$ -th vector $\\mathbf {v}_n$ is defined as: .$$\\mathbf {v}_n =\n",
      "{\\left\\lbrace \\begin{array}{ll}\n",
      "\\mathbf {x}, &n=0 \\cr \\mathbf {s}_n, &n>0\n",
      "\\end{array}\\right.}$$   (Eq. 25) .where $\\mathbf {x}$ is the vector representation of the given word $x$ , and $\\mathbf {s}_n$ is the vector representation of the $n$ -th sememe $s_n$ ..Although the input sequence is not time ordered, position $n$ in the sequence carries some useful information. First, position 0 corresponds to the word to be defined, while other positions correspond to the sememes. Secondly, sememes are sorted into a logical order in HowNet. For example, as the first sememe of the word “旅馆” (hotel), the sememe “场所” (place) describes its most important aspect, namely, the definition of “旅馆” (hotel) should be “…… 的地方” (a place for ...). Therefore, we add learned position embedding to the input embeddings for the encoder: .$$\\mathbf {v}_n = \\mathbf {v}_n + \\mathbf {p}_n$$   (Eq. 26) .where $\\mathbf {p}_n$ is the position embedding that can be learned during training..Then the vectors $\\mathbf {v}=[\\mathbf {v}_0, \\mathbf {v}_1, \\dots , \\mathbf {v}_{N}]$ are transformed by a stack of identical layers, where each layers consists of two sublayers: multi-head self-attention layer and position-wise fully connected feed-forward layer. Each of the layers are connected by residual connections, followed by layer normalization BIBREF16 . We refer the readers to BIBREF10 for the detail of the layers. The output of the encoder stack is a sequence of hidden states, denoted as $\\mathbf {h}=[\\mathbf {h}_0, \\mathbf {h}_1, \\dots , \\mathbf {h}_{N}]$ ..The decoder is also composed of a stack of identical layers. In BIBREF10 , each layer includes three sublayers: masked multi-head self-attention layer, multi-head attention layer that attends over the output of the encoder stack and position-wise fully connected feed-forward layer. In our model, we replace the two multi-head attention layers with an adaptive multi-head attention layer. Similarly to the adaptive attention layer in AAM, the adaptive multi-head attention layer can adaptivelly decide which sememes to focus on and when to attend to sememes at each time and each layer. Figure 2 shows the architecture of the decoder..Different from the adaptive attention layer in AAM that uses single head attention to obtain the sememe context and gate unit to obtain the LM context, the adaptive multi-head attention layer utilizes multi-head attention to obtain both contexts. Multi-head attention performs multiple single head attentions in parallel with linearly projected keys, values and queries, and then combines the outputs of all heads to obtain the final attention result. We omit the detail here and refer the readers to BIBREF10 . Formally, given the hidden state $\\mathbf {z}_t^{l-1}$ at time $t$ , layer $l-1$ of the decoder, we obtain the LM context with multi-head self-attention: .$$\\mathbf {o}_t^l = \\textit {MultiHead}(\\mathbf {z}_t^{l-1},\\mathbf {z}_{\\le t}^{l-1},\\mathbf {z}_{\\le t}^{l-1})$$   (Eq. 28) .where the decoder's hidden state $\\mathbf {z}_t^{l-1}$ at time $t$ , layer $l-1$ is the query, and $\\mathbf {z}_{\\le t}^{l-1}=[\\mathbf {z}_1^{l-1},...,\\mathbf {z}_t^{l-1}]$ , the decoder's hidden states from time 1 to time $t$ at layer $l-1$ , are the keys and values. To obtain better LM context, we employ residual connection and layer normalization after the multi-head self-attention. Similarly, the sememe context can be computed by attending to the encoder's outputs with multi-head attention: .$$\\hat{\\mathbf {c}_t}^l = \\textit {MultiHead}(\\mathbf {o}_t^l,\\mathbf {h},\\mathbf {h})$$   (Eq. 29) .where $\\mathbf {o}_t^l$ is the query, and the output from the encoder stack $\\mathbf {h}=[\\mathbf {h}_0, \\mathbf {h}_1, \\dots , \\mathbf {h}_{N}]$ , are the values and keys..Once obtaining the sememe context vector $\\hat{\\mathbf {c}_t}^l$ and the LM context $\\mathbf {o}_t^l$ , we compute the output from the adaptive attention layer with: .$$\\mathbf {c}_t^l = \\beta _t^l \\mathbf {o}_t^l + (1-\\beta _t^l)\\hat{\\mathbf {c}_t}^l, $$   (Eq. 30) .where .$$\\beta _{t}^l &=& \\frac{\\mathrm {exp}(e_{to})}{\\mathrm {exp}(e_{to})+\\mathrm {exp}(e_{t\\hat{c}})} \\nonumber \\\\\n",
      "e_{to}^l &=& (\\mathbf {w}_c^l)^T[\\mathbf {o}_t^l;\\mathbf {z}_t^{l-1}] \\nonumber \\\\\n",
      "e_{t\\hat{c}}^l &=& (\\mathbf {w}_c^l)^T[\\hat{\\mathbf {c}_t}^l;\\mathbf {z}_t^{l-1}] $$   (Eq. 31)  generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "25\n",
      "What is self-attention?\n",
      "Self-attention is a special case of attention mechanism that relates different positions of a single sequence to compute a representation for the sequence. It has been successfully applied to many NLP tasks and has many advantages such as reducing computation complexity per layer and allowing for more parallelization.\n",
      "Question : for the text Self-attention is a special case of attention mechanism that relates different positions of a single sequence in order to compute a representation for the sequence. Self-attention has been successfully applied to many tasks recently BIBREF24 , BIBREF25 , BIBREF26 , BIBREF10 , BIBREF12 , BIBREF11 .. BIBREF10 introduced the first transduction model based on self-attention by replacing the recurrent layers commonly used in encoder-decoder architectures with multi-head self-attention. The proposed model called Transformer achieved the state-of-the-art performance on neural machine translation with reduced training time. After that, BIBREF12 demonstrated that self-attention can improve semantic role labeling by handling structural information and long range dependencies. BIBREF11 further extended self-attention to constituency parsing and showed that the use of self-attention helped to analyze the model by making explicit the manner in which information is propagated between different locations in the sentence..Self-attention has many good properties. It reduces the computation complexity per layer, allows for more parallelization and reduces the path length between long-range dependencies in the network. In this paper, we use self-attention based architecture in SAAM to learn the relations of word, sememes and definition automatically. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "26\n",
      "Question 1: What pre-trained embeddings are used for all models on the CDM dataset?\n",
      "\n",
      "Answer 1: All the embeddings, including word and sememe embedding, are fixed 300 dimensional word embeddings pretrained on the Chinese Gigaword corpus (LDC2011T13).\n",
      "Question : for the text We show the effectiveness of all models on the CDM dataset. All the embeddings, including word and sememe embedding, are fixed 300 dimensional word embeddings pretrained on the Chinese Gigaword corpus (LDC2011T13). All definitions are segmented with Jiaba Chinese text segmentation tool and we use the resulting unique segments as the decoder vocabulary. To evaluate the difference between the generated results and the gold-standard definitions, we compute BLEU score using a script provided by Moses, following BIBREF3 . We implement the Baseline and AAM by modifying the code of BIBREF9 , and SAAM with fairseq-py ..We use two-layer LSTM network as the recurrent component. We set batch size to 128, and the dimension of the hidden state to 300 for the decoder. Adam optimizer is employed with an initial learning rate of $1\\times 10^{-3}$ . Since the morphemes of the word to be defined can benefit definition modeling, BIBREF3 obtain the model with the best performance by adding a trainable embedding from character-level CNN to the fixed word embedding. To obtain the state-of-the-art result as the baseline, we follow BIBREF3 and experiment with the character-level CNN with the same hyperparameters..To be comparable with the baseline, we also use two-layer LSTM network as the recurrent component.We set batch size to 128, and the dimension of the hidden state to 300 for both the encoder and the decoder. Adam optimizer is employed with an initial learning rate of $1\\times 10^{-3}$ ..We have the same hyperparameters as BIBREF10 , and set these hyperparameters as $(d_{\\text{model}}=300, d_{\\text{hidden}}=2048, n_{\\text{head}}=5, n_{\\text{layer}}=6)$ . To be comparable with AAM, we use the same batch size as 128. We also employ label smoothing technique BIBREF17 with a smoothing value of 0.1 during training. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "27\n",
      "Question 1: Who provided most of the computational resources and services for this project?\n",
      "\n",
      "Answer 1: The VSC (Flemish Supercomputer Center), which was funded by the Research Foundation - Flanders (FWO) and the Flemish Government – department EWI, provided most of the computational resources and services for this project.\n",
      "Question : for the text Pieter Delobelle was supported by the Research Foundation - Flanders under EOS No. 30992574 and received funding from the Flemish Government under the “Onderzoeksprogramma Artificiële Intelligentie (AI) Vlaanderen” programme. Thomas Winters is a fellow of the Research Foundation-Flanders (FWO-Vlaanderen). Most computational resources and services used in this work were provided by the VSC (Flemish Supercomputer Center), funded by the Research Foundation - Flanders (FWO) and the Flemish Government – department EWI. We are especially grateful to Luc De Raedt for his guidance as well as for providing the facilities to complete this project. We are thankful to Liesbeth Allein and her supervisors for inspiring us to use the die/dat task. We are also grateful to BIBREF27, BIBREF28, BIBREF29, BIBREF23 for their software packages. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "28\n",
      "Question 1: Where can we find the training and evaluation code, and the RobBERT model and fine-tuned models?\n",
      "\n",
      "Answer 1: They are publicly available for download on https://github.com/iPieter/RobBERT.\n",
      "Question : for the text The training and evaluation code of this paper as well as the RobBERT model and the fine-tuned models are publicly available for download on https://github.com/iPieter/RobBERT. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "29\n",
      "What is RobBERT and how does it perform compared to other language models for Dutch tasks? \n",
      "\n",
      "RobBERT is a new language model for Dutch that is based on RoBERTa. It has been shown to outperform earlier approaches for Dutch language tasks, as well as other BERT-based language models. This suggests that it can serve as a base for fine-tuning on other tasks and potentially advance results for Dutch language tasks.\n",
      "Question : for the text We introduced a new language model for Dutch based on RoBERTa, called RobBERT, and showed that it outperforms earlier approaches for Dutch language tasks, as well as other BERT-based language models. We thus hope this model can serve as a base for fine-tuning on other tasks, and thus help foster new models that might advance results for Dutch language tasks. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "30\n",
      "Question 1: In what type of task was RobBERT's zero-shot performance evaluated?\n",
      "\n",
      "Answer 1: RobBERT's zero-shot performance was evaluated in the disambiguation of demonstrative pronouns task in Dutch language.\n",
      "Question : for the text We evaluated RobBERT in several different settings on multiple downstream tasks. First, we compare its performance with other BERT-models and state-of-the-art systems in sentiment analysis, to show its performance for classification tasks. Second, we compare its performance in a recent Dutch language task, namely the disambiguation of demonstrative pronouns, which allows us to additionally compare the zero-shot performance of our and other BERT models, i.e. using only the pre-trained model without any fine-tuning. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "31\n",
      "What is the task specific to Dutch that was evaluated in this section?\n",
      "\n",
      "The task evaluated in this section is disambiguating \"die\" and \"dat\" in Dutch, which can function as both demonstrative and relative pronouns as well as subordinating conjunctions.\n",
      "Question : for the text Aside from classic natural language processing tasks in previous subsections, we also evaluated its performance on a task that is specific to Dutch, namely disambiguating “die” and “dat” (= “that” in English). In Dutch, depending on the sentence, both terms can be either demonstrative or relative pronouns; in addition they can also be used in a subordinating conjunction, i.e. to introduce a clause. The use of either of these words depends on the gender of the word it refers to. Distinguishing these words is a task introduced by BIBREF20, who presented multiple models trained on the Europarl BIBREF21 and SoNaR corpora BIBREF22. The results ranged from an accuracy of 75.03% on Europarl to 84.56% on SoNaR..For this task, we use the Dutch version of the Europarl corpus BIBREF21, which we split in 1.3M utterances for training, 319k for validation, and 399k for testing. We then process every sentence by checking if it contains “die” or “dat”, and if so, add a training example for every occurrence of this word in the sentence, where a single occurrence is masked. For the test set for example, this resulted in about 289k masked sentences. We then test two different approaches for solving this task on this dataset. The first approach is making the BERT models use their MLM task and guess which word should be filled in this spot, and check if it has more confidence in either “die” or “dat” (by checking the first 2,048 guesses at most, as this seemed sufficiently large). This allows us to compare the zero-shot BERT models, i.e. without any fine-tuning after pre-training, for which the results can be seen in Table TABREF7. The second approach uses the same data, but creates two sentences by filling in the mask with both “die” and “dat”, appending both with the [SEP] token and making the model predict which of the two sentences is correct. The fine-tuning was performed using 4 Nvidia GTX 1080 Ti GPUs and evaluated against the same test set of 399k utterances. As before, we fine-tuned the model twice: once with the full training set and once with a subset of 10k utterances from the training set for illustrating the benefits of pre-training on low-resource tasks..RobBERT outperforms previous models as well as other BERT models both with as well as without fine-tuning (see Table TABREF4 and Table TABREF7). It is also able to reach similar performance using less data. The fact that zero-shot RobBERT outperforms other zero-shot BERT models is also an indication that the base model has internalised more knowledge about Dutch than the other two have. The reason RobBERT and other BERT models outperform the previous RNN-based approach is likely the transformers ability to deal better with coreference resolution BIBREF12, and by extension better in deciding which word the “die” or “dat” belongs to. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "32\n",
      "Question 1: What dataset was used to evaluate the performance of RobBERT and how many of the reviews in this dataset were labeled as positive or negative?\n",
      "\n",
      "Answer 1: The Dutch Book Reviews Dataset (DBRD) was used to evaluate the performance of RobBERT, which contains book reviews scraped from hebban.nl and labeled as positive or negative. Out of the 118,516 reviews in the dataset, only 22,252 are labeled.\n",
      "Question : for the text We replicated the high-level sentiment analysis task used to evaluate BERTje BIBREF8 to be able to compare our methods. This task uses a dataset called Dutch Book Reviews Dataset (DBRD), in which book reviews scraped from hebban.nl are labeled as positive or negative BIBREF19. Although the dataset contains 118,516 reviews, only 22,252 of these reviews are actually labeled as positive or negative. The DBRD dataset is already split in a balanced 10% test and 90% train split, allowing us to easily compare to other models trained for solving this task. This dataset was released in a paper analysing the performance of an ULMFiT model (Universal Language Model Fine-tuning for Text Classification model) BIBREF19..We fine-tuned RobBERT on the first 10,000 training examples as well as on the full data set. While the ULMFiT model is first fine-tuned using the unlabeled reviews before training the classifier BIBREF19, it is unclear whether BERTje also first fine-tuned on the unlabeled reviews or only used the labeled data for fine-tuning the pretrained model. It is also unclear how it dealt with reviews being longer than the maximum number of tokens allowed as input in BERT models, as the average book review length is 547 tokens, with 40% of the documents being longer than our RobBERT model can handle. For a safe comparison, we thus decided to discard the unlabeled data and only use the labeled data for training and test purposes (20,028 and 2,224 examples respectively), and compare approaches for dealing with too long input sequences. We trained our model for 2000 iterations with a batch size of 128 and a warm-up of 500 iterations, reaching a learning rate of $10^{-5}$. We found that our model performed better when trained on the last part of the book reviews than on the first part. This is likely due to this part containing concluding remarks summarizing the overall sentiment. While BERTje was slightly outperformed by ULMFiT BIBREF8, BIBREF19, we can see that RobBERT achieves better performance than both on the test set, although the performance difference is not statistically significantly better than the ULMFiT model, as can be seen in Table TABREF4. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "33\n",
      "Question 1: What are some potential future improvements for RobBERT?\n",
      "\n",
      "Answer 1: Some potential future improvements for RobBERT include experimenting with new pre-training tasks, training on an unshuffled corpus, using a custom Dutch tokenizer, and exploring its potential use in sequence-to-sequence models. Additionally, there are many Dutch language tasks that could potentially achieve state-of-the-art results when fine-tuned on this pre-trained model.\n",
      "Question : for the text There are several possible improvements as well as interesting future directions for this research, for example in training similar models. First, as BERT-based models are a very active field of research, it is interesting to experiment with change the pre-training tasks with new unsupervised tasks when they are discovered, such as the sentence order prediction BIBREF14. Second, while RobBERT is trained on lines that contain multiple sentences, it does not put subsequent lines of the corpus after each other due to the shuffled nature of the OSCAR corpus BIBREF16. This is unlike RoBERTa, which does put full sentences next to each other if they fit, in order to learn the long-range dependencies between words that the original BERT learned using its controversial NSP task. It could be interesting to use the processor used to create OSCAR in order to create an unshuffled version to train on, such that this technique can be used on the data set. Third, RobBERT uses the same tokenizer as RoBERTa, meaning it uses a tokenizer built for the English language. Training a new model using a custom Dutch tokenizer, e.g. using the newly released HuggingFace tokenizers library BIBREF23, could increase the performance even further. On the same note, incorporating more Unicode glyphs as separate tokens can also be beneficial for example for tasks related to conversational agents BIBREF24..RobBERT itself could also be used in new settings to help future research. First, RobBERT could be used in different settings thanks to the renewed interest of sequence-to-sequence models due to their results on a vast range of language tasks BIBREF25, BIBREF26. These models use a BERT-like transformer stack for the encoder and depending on the task a generative model as a decoder. These advances once again highlight the flexibility of the self-attention mechanism and it might be interesting to research the re-usability of RobBERT in these type of architectures. Second, there are many Dutch language tasks that we did not examine in this paper, for which it may also be possible to achieve state-of-the-art results when fine-tuned on this pre-trained model. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "34\n",
      "What is RobBERT? \n",
      "\n",
      "RobBERT is a Dutch RoBERTa-based pre-trained language model introduced in the paper, which has the potential to increase the performance of downstream Dutch NLP tasks.\n",
      "Question : for the text The advent of neural networks in natural language processing (NLP) has significantly improved state-of-the-art results within the field. While recurrent neural networks (RNNs) and long short-term memory networks (LSTMs) initially dominated the field, recent models started incorporating attention mechanisms and then later dropped the recurrent part and just kept the attention mechanisms in so-called transformer models BIBREF0. This latter type of model caused a new revolution in NLP and led to popular language models like GPT-2 BIBREF1, BIBREF2 and ELMo BIBREF3. BERT BIBREF4 improved over previous transformer models and recurrent networks by allowing the system to learn from input text in a bidirectional way, rather than only from left-to-right or the other way around. This model was later re-implemented, critically evaluated and improved in the RoBERTa model BIBREF5..These large-scale transformer models provide the advantage of being able to solve NLP tasks by having a common, expensive pre-training phase, followed by a smaller fine-tuning phase. The pre-training happens in an unsupervised way by providing large corpora of text in the desired language. The second phase only needs a relatively small annotated data set for fine-tuning to outperform previous popular approaches in one of a large number of possible language tasks..While language models are usually trained on English data, some multilingual models also exist. These are usually trained on a large quantity of text in different languages. For example, Multilingual-BERT is trained on a collection of corpora in 104 different languages BIBREF4, and generalizes language components well across languages BIBREF6. However, models trained on data from one specific language usually improve the performance of multilingual models for this particular language BIBREF7, BIBREF8. Training a RoBERTa model BIBREF5 on a Dutch dataset thus has a lot of potential for increasing performance for many downstream Dutch NLP tasks. In this paper, we introduce RobBERT, a Dutch RoBERTa-based pre-trained language model, and critically test its performance using natural language tasks against other Dutch languages models. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "35\n",
      "Question 1: What is the name of the Dutch RoBERTa-based language model described in the text?\n",
      "Answer 1: The name of the Dutch RoBERTa-based language model described in the text is RobBERT.\n",
      "Question : for the text This section describes the data and training regime we used to train our Dutch RoBERTa-based language model called RobBERT. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "36\n",
      "Question 1: What is the difference between the data used to train RobBERT and BERTje?\n",
      "\n",
      "Answer 1: RobBERT was pre-trained on the Dutch section of the OSCAR corpus, which contains 6.6 billion words and 126,064,722 lines of text. BERTje, on the other hand, was trained on a collection of multiple Dutch corpora that totaled only 12 GB in size. Additionally, BERTje used WordPiece as subword embeddings while RobBERT used Byte Pair Encoding (BPE).\n",
      "Question : for the text We pre-trained our model on the Dutch section of the OSCAR corpus, a large multilingual corpus which was obtained by language classification in the Common Crawl corpus BIBREF16. This Dutch corpus has 6.6 billion words, totalling 39 GB of text. It contains 126,064,722 lines of text, where each line can contain multiple sentences. Subsequent lines are however not related to each other, due to the shuffled nature of the OSCAR data set. For comparison, the French RoBERTa-based language model CamemBERT BIBREF7 has been trained on the French portion of OSCAR, which consists of 138 GB of scraped text..Our data differs in several ways from the data used to train BERTje, a BERT-based Dutch language model BIBREF8. Firstly, they trained the model on an assembly of multiple Dutch corpora totalling only 12 GB. Secondly, they used WordPiece as subword embeddings, since this is what the original BERT architecture uses. RobBERT on the other hand uses Byte Pair Encoding (BPE), which is also used by GPT-2 BIBREF2 and RoBERTa BIBREF5. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "37\n",
      "What pre-training task did RobBERT use? \n",
      "\n",
      "RobBERT used only the MLM task (masking words and predicting the masked words) for pre-training, unlike the original BERT model which also used the NSP task (determining whether two sentences are consecutive in a document).\n",
      "Question : for the text RobBERT shares its architecture with RoBERTa's base model, which itself is a replication and improvement over BERT BIBREF5. The architecture of our language model is thus equal to the original BERT model with 12 self-attention layers with 12 heads BIBREF4. One difference with the original BERT is due to the different pre-training task specified by RoBERTa, using only the MLM task and not the NSP task. The training thus only uses word masking, where the model has to predict which words were masked in certain positions of a given line of text. The training process uses the Adam optimizer BIBREF17 with polynomial decay of the learning rate $l_r=10^{-6}$ and a ramp-up period of 1000 iterations, with parameters $\\beta _1=0.9$ (a common default) and RoBERTa's default $\\beta _2=0.98$. Additionally, we also used a weight decay of 0.1 as well as a small dropout of 0.1 to help prevent the model from overfitting BIBREF18..We used a computing cluster in order to efficiently pre-train our model. More specifically, the pre-training was executed on a computing cluster with 20 nodes with 4 Nvidia Tesla P100 GPUs (16 GB VRAM each) and 2 nodes with 8 Nvidia V100 GPUs (having 32 GB VRAM each). This pre-training happened in fixed batches of 8192 sentences by rescaling each GPUs batch size depending on the number of GPUs available, in order to maximally utilize the cluster without blocking it entirely for other users. The model trained for two epochs, which is over 16k batches in total. With the large batch size of 8192, this equates to 0.5M updates for a traditional BERT model. At this point, the perplexity did not decrease any further. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "38\n",
      "What tasks is BERT pre-trained on and how does it create internal representations of language? \n",
      "\n",
      "Answer 1: BERT is pre-trained on two unsupervised tasks: word masking and next sentence prediction. The model has to guess which word is masked in a certain position in the text and predict if two sentences are subsequent in the corpus, or if they are randomly sampled from the corpus. These tasks allowed the model to create internal representations about a language, which could thereafter be reused for different language tasks.\n",
      "Question : for the text Transformer models have been successfully used for a wide range of language tasks. Initially, transformers were introduced for use in machine translation, where they vastly improved state-of-the-art results for English to German in an efficient manner BIBREF0. This transformer model architecture resulted in a new paradigm in NLP with the migration from sequence-to-sequence recurrent neural networks to transformer-based models by removing the recurrent component and only keeping attention. This cornerstone was used for BERT, a transformer model that obtained state-of-the-art results for eleven natural language processing tasks, such as question answering and natural language inference BIBREF4. BERT is pre-trained with large corpora of text using two unsupervised tasks. The first task is word masking (also called the Cloze task BIBREF9 or masked language model (MLM)), where the model has to guess which word is masked in certain position in the text. The second task is next sentence prediction. This is done by predicting if two sentences are subsequent in the corpus, or if they are randomly sampled from the corpus. These tasks allowed the model to create internal representations about a language, which could thereafter be reused for different language tasks. This architecture has been shown to be a general language model that could be fine-tuned with little data in a relatively efficient way for a very distinct range of tasks and still outperform previous architectures BIBREF4..Transformer models are also capable of generating contextualized word embeddings. These contextualized embeddings were presented by BIBREF3 and addressed the well known issue with a word's meaning being defined by its context (e.g. “a stick” versus “let's stick to”). This lack of context is something that traditional word embeddings like word2vec BIBREF10 or GloVe BIBREF11 lack, whereas BERT automatically incorporates the context a word occurs in..Another advantage of transformer models is that attention allows them to better resolve coreferences between words BIBREF12. A typical example for the importance of coreference resolution is “The trophy doesn’t fit in the brown suitcase because it’s too big.”, where the word “it” would refer to the the suitcase instead of the trophy if the last word was changed to “small” BIBREF13. Being able to resolve these coreferences is for example important for translating to languages with gender, as suitcase and trophy have different genders in French..Although BERT has been shown to be a useful language model, it has also received some scrutiny on the training and pre-processing of the language model. As mentioned before, BERT uses next sentence prediction (NSP) as one of its two training tasks. In NSP, the model has to predict whether two sentences follow each other in the training text, or are just randomly selected from the corpora. The authors of RoBERTa BIBREF5 showed that while this task made the model achieve a better performance, it was not due to its intended reason, as it might merely predict relatedness rather than subsequent sentences. That BIBREF4 trained a better model when using NSP than without NSP is likely due to the model learning long-range dependencies in text from its inputs, which are longer than just the single sentence on itself. As such, the RoBERTa model uses only the MLM task, and uses multiple full sentences in every input. Other research improved the NSP task by instead making the model predict the correct order of two sentences, where the model thus has to predict whether the sentences occur in the given order in the corpus, or occur in flipped order BIBREF14..BIBREF4 also presented a multilingual model (mBERT) with the same architecture as BERT, but trained on Wikipedia corpora in 104 languages. Unfortunately, the quality of these multilingual embeddings is often considered worse than their monolingual counterparts. BIBREF15 illustrated this difference in quality for German and English models in a generative setting. The monolingual French CamemBERT model BIBREF7 also compared their model to mBERT, which performed poorer on all tasks. More recently, BIBREF8 also showed similar results for Dutch using their BERTje model, outperforming multilingual BERT in a wide range of tasks, such as sentiment analysis and part-of-speech tagging. Since this work is concurrent with ours, we compare our results with BERTje in this paper. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "39\n",
      "Question 1: How does the implementation convert each frame into a natural language representation?\n",
      "\n",
      "Answer 1: The implementation iterates through the list of available labels and builds a string according to the objects detected in multiple vertical patches on the screen. The objects can be classified as close, mid, or far based on distance from the player using two threshold levels. The object descriptions can be concise or detailed as needed, and the resulting sentence is transformed into an embedding vector for use by the agent.\n",
      "Question : for the text To convert each frame to a natural language representation state, the list of available labels is iterated, and a string is built accordingly. The main idea of our implementation is to divide the screen into multiple vertical patches, count the amount of different objects inside by their types, and parse it as a sentence. The decision as to whether an object is close or far can be determined by calculating the distance from it to the player, and using two threshold levels. Object descriptions can be concise or detailed, as needed. We experimented with the following mechanics:.the screen can be divided between patches equally, or by determined ratios. Here, our main guideline was to keep the \"front\" patch narrow enough so it can be used as \"sights\"..our initial experiment was with 3 patches, and later we added 2 more patches classified as \"outer left\" and \"outer right\". In our experiments we have tested up to 51 patches, referred to as left or right patch with corresponding numbers..we used 2 thresholds, which allowed us to classify the distance of an object from the player as \"close\",\"mid\", and \"far. Depending on the task, the value of the threshold can be changed, as well as adding more thresholds..different states might generate sentence with different size. A maximum sentence length is another parameter that was tested. sentences-length-table presents some data regarding the average word count in some of the game sceanrios..After the sentence describing the state is generated, it is transformed to an embedding vector. Words that were not found in the vocabulary were replaced with an “OOV\" vector. All words were then concatenated to a NxDx1 matrix, representing the state. We experimented with both Word2Vec and GloVe pretrained embedding vectors. Eventually, we used the latter, as it consumes less memory and speeds up the training process. The length of the state sentence is one of the hyperparameters of the agents; shorter sentences are zero padded, where longer ones are trimmed. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "40\n",
      "Question 1: What are the dimensions of the semantic segmentation image used in the models?\n",
      "\n",
      "Answer 1: The semantic segmentation image used in the models has a resolution of 640X480X1, where the pixel value represents the object's class, as generated using the VizDoom label API.\n",
      "Question : for the text All of our models were implemented using PyTorch. The DQN agents used a single network that outputs the Q-Values of the available actions. The PPO agents used an Actor-Critic model with two networks; the first outputs the policy distribution for the input state, and the second network outputs it's value. As mentioned earlier, we used three common neural network architectures:.used for the raw image and semantic segmentation based agents. VizDoom's raw output image resolution is 640X480X3 RGB image. We experimented with both the original image and its down-sampled version. The semantic segmentation image was of resolution 640X480X1, where the pixel value represents the object's class, generated using the VizDoom label API. the network consisted of two convolutional layers, two hidden linear layers and an output layer. The first convolutional layer has 8 6X6 filters with stride 3 and ReLU activation. The second convolutional layer has 16 3X3 filters with stride 2 and ReLU activation. The fully connected layers has 32 and 16 units, both of them are followed by ReLU activation. The output layer's size is the amount of actions the agent has available in the trained scenario..Used in the feature vector based agent. Naturally, some discretization is needed in order to build a feature vector, so some of the state data is lost. the feature vector was made using features we extracted from the VizDoom API, and its dimensions was 90 X 1. The network is made up of two fully connected layers, each of them followed by a ReLU activation. The first layer has 32 units, and the second one one has 16 units. The output layer's size was the amount of actions available to the agent..Used in the natural language based agent. As previously mentioned, each word in the natural language state is transformed into a 200X50X1 matrix. The first layers of the TextCNN are convolutional layers with 8 filter which are designed to scan input sentence, and return convolution outputs of sequences of varying lengths. The filters vary in width, such that each of them learns to identify different lengths of sequences in words. Longer filters have higher capability of extracting features from longer word sequences. The filters we have chosen have the following dimensions: 3X50X1, 4X50X1, 5X50X1, 8X50X1,11X50X1. Following the convolution layer there is a ReLU activation and a max pool layer. Finally, there are two fully connected layers; The first layer has 32 units, and second one has 16 units. Both of them are followed by ReLU activation..All architectures have the same output, regardless of the input type. The DQN network is a regression network, with its output size the number of available actions. The PPO agent has 2 networks; actor and critic. The actor network has a Softmax activation with size equal to the available amount of actions. The critic network is a regression model with a single output representing the state's value. Reward plots for the PPO agent can be found in Figure FIGREF47. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "41\n",
      "Question 1: What kind of information should a semantic representation using natural language contain?\n",
      "\n",
      "Answer 1: A semantic representation using natural language should contain information which can be deduced by a human playing the game.\n",
      "Question : for the text A semantic representation using natural language should contain information which can be deduced by a human playing the game. For example, even though a human does not know the exact distance between objects, it can classify them as \"close\" or \"far\". However, objects that are outside the player's field of vision can not be a part of the state. Furthermore, a human would most likely refer to an object's location relative to itself, using directions such as \"right\" or \"left\". generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "42\n",
      "Question 1: What is the original purpose of VizDoom?\n",
      "\n",
      "Answer 1: The original purpose of VizDoom is to provide a research platform for vision based reinforcement learning.\n",
      "Question : for the text VizDoom is a \"Doom\" based research environment that was developed at the Poznań University of Technology. It is based on \"ZDoom\" game executable, and includes a Python based API. The API offers the user the ability to run game instances, query the game state, and execute actions. The original purpose of VizDoom is to provide a research platform for vision based reinforcement learning. Thus, a natural language representation for the game was needed to be implemented. ViZDoom emulates the \"Doom\" game and enables us to access data within a certain frame using Python dictionaries. This makes it possible to extract valuable data including player health, ammo, enemy locations etc. Each game frame contains \"labels\", which contain data on visible objects in the game (the player, enemies, medkits, etc). We used \"Doom Builder\" in order to edit some of the scenarios and design a new one. Enviroment rewards are presented in doom-scenarios-table. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "43\n",
      "Q1: What are some disadvantages of using natural language representations in reinforcement learning?\n",
      "A1: Natural language representations require the designer to accurately describe the state and may abstract important features that the designer may not realize are necessary. They should be carefully chosen, similar to reward shaping or choosing a training algorithm.\n",
      "Question : for the text Our results indicate that natural language can outperform, and sometime even replace, vision-based representations. Nevertheless, natural language representations can also have disadvantages in various scenarios. For one, they require the designer to be able to describe the state exactly, whether by a rule-based or learned parser. Second, they abstract notions of the state space that the designer may not realize are necessary for solving the problem. As such, semantic representations should be carefully chosen, similar to the process of reward shaping or choosing a training algorithm. Here, we enumerate three instances in which we believe natural language representations are beneficial:.Natural use-case: Information contained in both generic and task-specific textual corpora may be highly valuable for decision making. This case assumes the state can either be easily described using natural language or is already in a natural language state. This includes examples such as user-based domains, in which user profiles and comments are part of the state, or the stock market, in which stocks are described by analysts and other readily available text. 3D physical environments such as VizDoom also fall into this category, as semantic segmentation maps can be easily described using natural language..Subjective information: Subjectivity refers to aspects used to express opinions, evaluations, and speculations. These may include strategies for a game, the way a doctor feels about her patient, the mood of a driver, and more..Unstructured information: In these cases, features might be measured by different units, with an arbitrary position in the state's feature vector, rendering them sensitive to permutations. Such state representations are thus hard to process using neural networks. As an example, the medical domain may contain numerous features describing the vitals of a patient. These raw features, when observed by an expert, can be efficiently described using natural language. Moreover, they allow an expert to efficiently add subjective information..An orthogonal line of research considers automating the process of image annotation. The noise added from the supervised or unsupervised process serves as a great challenge for natural language representation. We suspect the noise accumulated by this procedure would require additional information to be added to the state (e.g., past information). Nevertheless, as we have shown in this paper, such information can be compressed using natural language. In addition, while we have only considered spatial features of the state, information such as movement directions and transient features can be efficiently encoded as well..Natural language representations help abstract information and interpret the state of an agent, improving its overall performance. Nevertheless, it is imperative to choose a representation that best fits the domain at hand. Designers of RL algorithms should consider searching for a semantic representation that fits their needs. While this work only takes a first step toward finding better semantic state representations, we believe the structure inherent in natural language can be considered a favorable candidate for achieving this goal. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "44\n",
      "Question 1: What is the main difference between previous representation techniques in RL and the approach suggested in this paper?\n",
      "\n",
      "Answer 1: Previous representation techniques in RL mainly focused on raw forms of the state, while the approach suggested in this paper is to represent the state using natural language, similar to the efficient way humans summarize and transfer information.\n",
      "Question : for the text “The world of our experiences must be enormously simplified and generalized before it is possible to make a symbolic inventory of all our experiences of things and relations.\".(Edward Sapir, Language: An Introduction to the Study of Speech, 1921).Deep Learning based algorithms use neural networks in order to learn feature representations that are good for solving high dimensional Machine Learning (ML) tasks. Reinforcement Learning (RL) is a subfield of ML that has been greatly affected by the use of deep neural networks as universal function approximators BIBREF0, BIBREF1. These deep neural networks are used in RL to estimate value functions, state-action value functions, policy mappings, next-state predictions, rewards, and more BIBREF2, BIBREF3, BIBREF4, thus combating the “curse of dimensionality\"..The term representation is used differently in different contexts. For the purpose of this paper we define a semantic representation of a state as one that reflects its meaning as it is understood by an expert. The semantic representation of a state should thus be paired with a reliable and computationally efficient method for extracting information from it. Previous success in RL has mainly focused on representing the state in its raw form (e.g., visual input in Atari-based games BIBREF2). This approach stems from the belief that neural networks (specifically convolutional networks) can extract meaningful features from complex inputs. In this work, we challenge current representation techniques and suggest to represent the state using natural language, similar to the way we, as humans, summarize and transfer information efficiently from one to the other BIBREF5..The ability to associate states with natural language sentences that describe them is a hallmark of understanding representations for reinforcement learning. Humans use rich natural language to describe and communicate their visual perceptions, feelings, beliefs, strategies, and more. The semantics inherent to natural language carry knowledge and cues of complex types of content, including: events, spatial relations, temporal relations, semantic roles, logical structures, support for inference and entailment, as well as predicates and arguments BIBREF6. The expressive nature of language can thus act as an alternative semantic state representation..Over the past few years, Natural Language Processing (NLP) has shown an acceleration in progress on a wide range of downstream applications ranging from Question Answering BIBREF7, BIBREF8, to Natural Language Inference BIBREF9, BIBREF10, BIBREF11 through Syntactic Parsing BIBREF12, BIBREF13, BIBREF14. Recent work has shown the ability to learn flexible, hierarchical, contextualized representations, obtaining state-of-the-art results on various natural language processing tasks BIBREF15. A basic observation of our work is that natural language representations are also beneficial for solving problems in which natural language is not the underlying source of input. Moreover, our results indicate that natural language is a strong alternative to current complementary methods for semantic representations of a state..In this work we assume a state can be described using natural language sentences. We use distributional embedding methods in order to represent sentences, processed with a standard Convolutional Neural Network for feature extraction. In Section SECREF2 we describe the basic frameworks we rely on. We discuss possible semantic representations in Section SECREF3, namely, raw visual inputs, semantic segmentation, feature vectors, and natural language representations. Then, in Section SECREF4 we compare NLP representations with their alternatives. Our results suggest that representation of the state using natural language can achieve better performance, even on difficult tasks, or tasks in which the description of the state is saturated with task-nuisances BIBREF17. Moreover, we observe that NLP representations are more robust to transfer and changes in the environment. We conclude the paper with a short discussion and related work. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "45\n",
      "Question 1: What is the distributional hypothesis in linguistics and how are word embeddings related to it?\n",
      "\n",
      "Answer 1: The distributional hypothesis in linguistics suggests that words that are used and occur in the same contexts tend to have similar meanings. Word embeddings are a way of representing words as vectors in a high-dimensional space that capture these patterns of co-occurrence. They are built upon the distributional hypothesis and are a fundamental building block for representing natural language sentences.\n",
      "Question : for the text A word embedding is a mapping from a word $w$ to a vector $\\mathbf {w} \\in \\mathbb {R}^d$. A simple form of word embedding is the Bag of Words (BoW), a vector $\\mathbf {w} \\in \\mathbb {N}^{|D|}$ ($|D|$ is the dictionary size), in which each word receives a unique 1-hot vector representation. Recently, more efficient methods have been proposed, in which the embedding vector is smaller than the dictionary size, $d \\ll |D|$. These methods are also known as distributional embeddings..The distributional hypothesis in linguistics is derived from the semantic theory of language usage (i.e. words that are used and occur in the same contexts tend to have similar meanings). Distributional word representations are a fundamental building block for representing natural language sentences. Word embeddings such as Word2vec BIBREF20 and GloVe BIBREF21 build upon the distributional hypothesis, improving efficiency of state-of-the-art language models..Convolutional Neural Networks (CNNs), originally invented for computer vision, have been shown to achieve strong performance on text classification tasks BIBREF22, BIBREF23, as well as other traditional NLP tasks BIBREF24. In this paper we consider a common architecture BIBREF25, in which each word in a sentence is represented as an embedding vector, a single convolutional layer with $m$ filters is applied, producing an $m$-dimensional vector for each $n$-gram. The vectors are combined using max-pooling followed by a ReLU activation. The result is then passed through multiple hidden linear layers with ReLU activation, eventually generating the final output. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "46\n",
      "What is the goal in Reinforcement Learning? \n",
      "The goal in Reinforcement Learning is to learn a policy that maps a state to a probability distribution over actions, with the objective to maximize reward provided by the environment. \n",
      "\n",
      "Question 2: What are the value and action-value functions in MDPs? \n",
      "The value function estimates the expected long-term reward starting from a given state, while the action-value function estimates the expected long-term reward starting from a given state-action pair. \n",
      "\n",
      "Question 3: What is the difference between DQN and PPO algorithms? \n",
      "DQN is a value-based algorithm that learns the action-value function using a neural network, while PPO is a policy-based algorithm that optimizes the policy directly using the policy gradient theorem with a trust-region update rule.\n",
      "Question : for the text In Reinforcement Learning the goal is to learn a policy $\\pi (s)$, which is a mapping from state $s$ to a probability distribution over actions $\\mathcal {A}$, with the objective to maximize a reward $r(s)$ that is provided by the environment. This is often solved by formulating the problem as a Markov Decision Process (MDP) BIBREF19. Two common quantities used to estimate the performance in MDPs are the value $v (s)$ and action-value $Q (s, a)$ functions, which are defined as follows: ${v(s) = \\mathbb {E}^{\\pi } [\\sum _t \\gamma ^t r_t | s_0 = s ]}$ and ${Q(s, a) = \\mathbb {E}^{\\pi } [\\sum _t \\gamma ^t r_t | s_0 = s, a_0 = a ]}$. Two prominent algorithms for solving RL tasks, which we use in this paper, are the value-based DQN BIBREF2 and the policy-based PPO BIBREF3..Deep Q Networks (DQN): The DQN algorithm is an extension of the classical Q-learning approach, to a deep learning regime. Q-learning learns the optimal policy by directly learning the value function, i.e., the action-value function. A neural network is used to estimate the $Q$-values and is trained to minimize the Bellman error, namely.Proximal Policy Optimization (PPO): While the DQN learns the optimal behavioral policy using a dynamic programming approach, PPO takes a different route. PPO builds upon the policy gradient theorem, which optimizes the policy directly, with an addition of a trust-region update rule. The policy gradient theorem updates the policy by generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "47\n",
      "Question 1: What is the purpose of representation learning in machine learning?\n",
      "\n",
      "Answer 1: The purpose of representation learning in machine learning is to find an appropriate representation of data in order to perform a machine learning task. Deep learning exploits this concept by its very nature. The work on representation learning includes Predictive State Representations (PSR) and a Heuristic Embedding of Markov Processes (HEMP).\n",
      "Question : for the text Work on representation learning is concerned with finding an appropriate representation of data in order to perform a machine learning task BIBREF33. In particular, deep learning exploits this concept by its very nature BIBREF2. Work on representation learning include Predictive State Representations (PSR) BIBREF34, BIBREF35, which capture the state as a vector of predictions of future outcomes, and a Heuristic Embedding of Markov Processes (HEMP) BIBREF36, which learns to embed transition probabilities using an energy-based optimization problem..There has been extensive work attempting to use natural language in RL. Efforts that integrate language in RL develop tools, approaches, and insights that are valuable for improving the generalization and sample efficiency of learning agents. Previous work on language-conditioned RL has considered the use of natural language in the observation and action space. Environments such as Zork and TextWorld BIBREF37 have been the standard benchmarks for testing text-based games. Nevertheless, these environments do not search for semantic state representations, in which an RL algorithm can be better evaluated and controlled..BIBREF38 use high-level semantic abstractions of documents in a representation to facilitate relational learning using Inductive Logic Programming and a generative language model. BIBREF39 use high-level guidance expressed in text to enrich a stochastic agent, playing against the built-in AI of Civilization II. They train an agent with the Monte-Carlo search framework in order to jointly learn to identify text that is relevant to a given game state as well as game strategies based only on environment feedback. BIBREF40 utilize natural language in a model-based approach to describe the dynamics and rewards of an environment, showing these can facilitate transfer between different domains..More recently, the structure and compositionality of natural language has been used for representing policies in hierarchical RL. In a paper by BIBREF41, instructions given in natural language were used in order to break down complex problems into high-level plans and lower-level actions. Their suggested framework leverages the structure inherent to natural language, allowing for transfer to unfamiliar tasks and situations. This use of semantic structure has also been leveraged by BIBREF42, where abstract actions (not necessarily words) were recognized as symbols of a natural and expressive language, improving performance and transfer of RL agents..Outside the context of RL, previous work has also shown that high-quality linguistic representations can assist in cross-modal transfer, such as using semantic relationships between labels for zero-shot transfer in image classification BIBREF43, BIBREF44. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "48\n",
      "Question 1: What are the three common approaches for semantic representation of states?\n",
      "\n",
      "Answer 1: The three common approaches for semantic representation of states are raw visual inputs, feature vectors, and semantic segmentation maps.\n",
      "Question : for the text Contemporary methods for semantic representation of states currently follow one of three approaches: (1) raw visual inputs BIBREF2, BIBREF26, in which raw sensory values of pixels are used from one or multiple sources, (2) feature vectors BIBREF27, BIBREF28, in which general features of the problem are chosen, with no specific structure, and (3) semantic segmentation maps BIBREF29, BIBREF30, in which discrete or logical values are used in one or many channels to represent the general features of the state..The common approach is to derive decisions (e.g., classification, action, etc.) based on information in its raw form. In RL, the raw form is often the pixels representing an image – however the image is only one form of a semantic representation. In Semantic Segmentation, the image is converted from a 3-channel (RGB) matrix into an $N$-channel matrix, where $N$ is the number of classes. In this case, each channel represents a class, and a binary value at each coordinate denotes whether or not this class is present in the image at this location. For instance, fig: semantic segmentation example considers an autonomous vehicle task. The raw image and segmentation maps are both sufficient for the task (i.e., both contain a sufficient semantic representation). Nevertheless, the semantic segmentation maps contain less task-nuisances BIBREF17, which are random variables that affect the observed data, but are not informative to the task we are trying to solve..In this paper we propose a forth method for representing a state, namely using natural language descriptions. One method to achieve such a representation is through Image Captioning BIBREF31, BIBREF32. Natural language is both rich as well as flexible. This flexibility enables the algorithm designer to represent the information present in the state as efficiently and compactly as possible. As an example, the top image in fig: semantic segmentation example can be represented using natural language as “There is a car in your lane two meters in front of you, a bicycle rider on your far left in the negative lane, a car in your direction in the opposite lane which is twenty meters away, and trees and pedestrians on the side walk.” or compactly by “There is a car two meters in front of you a pedestrian on the sidewalk to your right and a car inbound in the negative lane which is far away.”. Language also allows us to efficiently compress information. As an example, the segmentation map in the bottom image of fig: semantic segmentation example can be compactly described by “There are 13 pedestrians crossing the road in front of you”. In the next section we will demonstrate the benefits of using natural-language-based semantic state representation in a first person shooter enviornment. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "49\n",
      "Question 1: What are the three types of state representations provided by the ViZDoom environment?\n",
      "\n",
      "Answer 1: The three types of state representations provided by the ViZDoom environment are raw visual inputs, feature vector representation, and semantic segmentation map.\n",
      "Question : for the text In this section we compare the different types of semantic representations for representing states in the ViZDoom environment BIBREF26, as described in the previous section. More specifically, we use a semantic natural language parser in order to describe a state, over numerous instances of levels varying in difficulty, task-nuisances, and objectives. Our results show that, though semantic segmentation and feature vector representation techniques express a similar statistic of the state, natural language representation offers better performance, faster convergence, more robust solutions, as well as better transfer..The ViZDoom environment involves a 3D world that is significantly more real-world-like than Atari 2600 games, with a relatively realistic physics model. An agent in the ViZDoom environment must effectively perceive, interpret, and learn the 3D world in order to make tactical and strategic decisions of where to go and how to act. There are three types of state representations that are provided by the environment. The first, which is also most commonly used, is raw visual inputs, in which the state is represented by an image from a first person view of the agent. A feature vector representation is an additional state representation provided by the environment. The feature vector representation includes positions as well as labels of all objects and creatures in the vicinity of the agent. Lastly, the environment provides a semantic segmentation map based on the aforementioned feature vector. An example of the visual representations in VizDoom is shown in fig: representations in vizdoom..In order to incorporate natural language representation to the VizDoom environment we've constructed a semantic parser of the semantic segmentation maps provided by the environment. Each state of the environment was converted into a natural language sentence based on positions and labels of objects in the frame. To implement this, the screen was divided into several vertical and horizontal patches, as depicted in fig: patches. These patches describe relational aspects of the state, such as distance of objects and their direction with respect to the agent's point of view. In each patch, objects were counted, and a natural language description of the patch was constructed. This technique was repeated for all patches to form the final state representation. fig: nlp state rep depicts examples of natural language sentences of different states in the enviornment. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "50\n",
      "What types of scenarios were included in the experiments to test the natural language representation compared to other representation techniques?\n",
      "\n",
      "The experiments included a basic scenario, a health gathering scenario, a scenario in which the agent must take cover from fireballs, a scenario in which the agent must defend itself from charging enemies, and a super scenario that combined elements from all of the previous scenarios. Each scenario included varying degrees of difficulty and required the agent to navigate, shoot, and collect items such as weapons and medipacks. Enemies of different types also attacked the agent, and positive rewards were given when enemies were killed. The natural language representation was tested against visual-based and feature representations.\n",
      "Question : for the text We tested the natural language representation against the visual-based and feature representations on several tasks, with varying difficulty. In these tasks, the agent could navigate, shoot, and collect items such as weapons and medipacks. Often, enemies of different types attacked the agent, and a positive reward was given when an enemy was killed. Occasionally, the agent also suffered from health degeneration. The tasks included a basic scenario, a health gathering scenario, a scenario in which the agent must take cover from fireballs, a scenario in which the agent must defend itself from charging enemies, and a super scenario, where a mixture of the above scenarios was designed to challenge the agent..More specifically, in the basic scenario, a single monster is spawned in front of the agent. The purpose of this scenario is to teach the agent to aim at the enemy and shoot at it. In the health gathering scenario, the floor of the room is covered in toxin, causing the agent to gradually lose health. Medipacks are spawned randomly in the room and the agent's objective is to keep itself alive by collecting them. In the take cover scenario, multiple fireball shooting monsters are spawned in front of the agent. The goal of the agent is to stay alive as long as possible, dodging inbound fireballs. The difficulty of the task increases over time, as additional monsters are spawned. In the defend the center scenario, melee attacking monsters are randomly spawned in the room, and charge towards the agent. As opposed to other scenarios, the agent is incapable of moving, aside from turning left and right and shooting. In the defend the line scenario, both melee and fireball shooting monsters are spawned near the opposing wall. The agent can only step right, left or shoot. Finally, in the “super\" scenario both melee and fireball shooting monsters are repeatably spawned all over the room. the room contains various items the agent can pick up and use, such as medipacks, shotguns, ammunition and armor. Furthermore, the room is filled with unusable objects, various types of trees, pillars and other decorations. The agent can freely move and turn in any direction, as well as shoot. This scenario combines elements from all of the previous scenarios..Our agent was implemented using a Convolutional Neural Network as described in Section SECREF4. We converted the parsed state into embedded representations of fixed length. We tested both a DQN and a PPO based agent, and compared the natural language representation to the other representation techniques, namely the raw image, feature vector, and semantic segmentation representations..In order to effectively compare the performance of the different representation methods, we conducted our experiments under similar conditions for all agents. The same hyper-parameters were used under all tested representations. Moreover, to rule out effects of architectural expressiveness, the number of weights in all neural networks was approximately matched, regardless of the input type. Finally, we ensured the “super\" scenario was positively biased toward image-based representations. This was done by adding a large amount items to the game level, thereby filling the state with nuisances (these tests are denoted by `nuisance' in the scenario name). This was especially evident in the NLP representations, as sentences became extensively longer (average of over 250 words). This is contrary to image-based representations, which did not change in dimension..Results of the DQN-based agent are presented in fig: scenario comparison. Each plot depicts the average reward (across 5 seeds) of all representations methods. It can be seen that the NLP representation outperforms the other methods. This is contrary to the fact that it contains the same information as the semantic segmentation maps. More interestingly, comparing the vision-based and feature-based representations render inconsistent conclusions with respect to their relative performance. NLP representations remain robust to changes in the environment as well as task-nuisances in the state. As depicted in fig: nuisance scenarios, inflating the state space with task-nuisances impairs the performance of all representations. There, a large amount of unnecessary objects were spawned in the level, increasing the state's description length to over 250 words, whilst retaining the same amount of useful information. Nevertheless, the NLP representation outperformed the vision and feature based representations, with high robustness to the applied noise..In order to verify the performance of the natural language representation was not due to extensive discretization of patches, we've conducted experiments increasing the number of horizontal patches - ranging from 3 to 31 patches in the extreme case. Our results, as depicted in fig: patch count, indicate that the amount of discretization of patches did not affect the performance of the NLP agent, remaining a superior representation compared to the rest..To conclude, our experiments suggest that NLP representations, though they describe the same raw information of the semantic segmentation maps, are more robust to task-nuisances, allow for better transfer, and achieve higher performance in complex tasks, even when their description is long and convoluted. While we've only presented results for DQN agents, we include plots for a PPO agent in the Appendix, showing similar trends and conclusions. We thus deduce that NLP-based semantic state representations are a preferable choice for training VizDoom agents. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "1\n",
      "What is the difference between abstractive and extractive summarizers?\n",
      "Abstractive summarizers identify conceptual information in the corpus and reformulate a summary from scratch, while extractive summarizers identify relevant sentences in the original corpus and produce summaries by aggregating these sentences.\n",
      "Question : for the text While early models focused on the task of single document summarization, recent systems generally produce summaries of corpora of documents BIBREF10 . Similarly, the focus has shifted from generic summarization to the more realistic task of query-oriented summarization, in which a summary is produced with the essential information contained in a corpus that is also relevant to a user-defined query BIBREF11 ..Summarization systems are further divided into two classes, namely abstractive and extractive models. Extractive summarizers identify relevant sentences in the original corpus and produce summaries by aggregating these sentences BIBREF10 . In contrast, an abstractive summarizer identifies conceptual information in the corpus and reformulates a summary from scratch BIBREF11 . Since abstractive approaches require advanced natural language processing, the majority of existing summarization systems consist of extractive models..Extractive summarizers differ in the method used to identify relevant sentences, which leads to a classification of models as either feature-based or graph-based approaches. Feature-based methods represent the sentences with a set of predefined features such as the sentence position, the sentence length or the presence of cue phrases BIBREF12 . Then, they train a model to compute relevance scores for the sentences based on their features. Since feature-based approaches generally require datasets with labelled sentences which are hard to produce BIBREF11 , unsupervised graph-based methods have attracted growing interest in recent years..Graph-based summarizers represent the sentences of a corpus as the nodes of a graph with the edges modelling relationships of similarity between the sentences BIBREF0 . Then, graph-based algorithms are applied to identify relevant sentences. The models generally differ in the type of relationship captured by the graph or in the sentence selection approach. Most graph-based models define the edges connecting sentences based on the co-occurrence of terms in pairs of sentences BIBREF0 , BIBREF2 , BIBREF3 . Then, important sentences are identified either based on node ranking algorithms, or using a global optimization approach. Methods based on node ranking compute individual relevance scores for the sentences and build summaries with highly scored sentences. The earliest such summarizer, LexRank BIBREF0 , applies the PageRank algorithm to compute sentence scores. Introducing a query bias in the node ranking algorithm, this method can be adapted for query-oriented summarization as in BIBREF2 . A different graph model was proposed in BIBREF13 , where sentences and key phrases form the two classes of nodes of a bipartite graph. The sentences and the key phrases are then scored simultaneously by applying a mutual reinforcement algorithm. An extended bipartite graph ranking algorithm is also proposed in BIBREF1 in which the sentences represent one class of nodes and clusters of similar sentences represent the other class. The hubs and authorities algorithm is then applied to compute sentence scores. Adding terms as a third class of nodes, BIBREF14 propose to score terms, sentences and sentence clusters simultaneously, based on a mutual reinforcement algorithm which propagates the scores across the three node classes. A common drawback of the approaches based on node ranking is that they compute individual relevance scores for the sentences and they fail to model the information jointly carried by the sentences, which may result in redundant summaries. Hence, global optimization approaches were proposed to select a set of jointly relevant and non-redundant sentences as in BIBREF15 and BIBREF16 . For instance, BIBREF17 propose a greedy algorithm to find a dominating set of nodes in the sentence graph. A summary is then formed with the corresponding set of sentences. Similarly, BIBREF15 extract a set of sentences with a maximal similarity with the entire corpus and a minimal pairwise lexical similarity, which is modelled as a multi-objective optimization problem. In contrast, BIBREF9 propose a coverage approach in which a set of sentences maximizing the number of distinct relevant terms is selected. Finally, BIBREF16 propose a two step approach in which individual sentence relevance scores are computed first. Then a set of sentences with a maximal total relevance and a minimal joint redundancy is selected. All three methods attempt to solve NP-hard problems. Hence, they propose approximation algorithms based on the theory of submodular functions..Going beyond pairwise lexical similarities between sentences and relations based on the co-occurrence of terms, hypergraph models were proposed, in which nodes are sentences and hyperedges model group relationships between sentences BIBREF3 . The hyperedges of the hypergraph capture topical relationships among groups of sentences. Existing hypergraph-based systems BIBREF3 , BIBREF4 combine pairwise lexical similarities and clusters of lexically similar sentences to form the hyperedges of the hypergraph. Hypergraph ranking algorithms are then applied to identify important and query-relevant sentences. However, they do not provide any interpretation for the clusters of sentences discovered by their method. Moreover, these clusters do not overlap, which is incoherent with the fact that each sentence carries multiple information and hence belongs to multiple semantic groups of sentences. In contrast, each hyperedge in our proposed hypergraph connects sentences covering the same topic, and these hyperedges do overlap..A minimal hypergraph transversal is a subset of the nodes of hypergraph of minimum cardinality and such that each hyperedge of the hypergraph is incident to at least one node in the subset BIBREF5 . Theoretically equivalent to the minimum hitting set problem, the problem of finding a minimum hypergraph transversal can be viewed as finding a subset of representative nodes covering the essential information carried by each hyperedge. Hence, hypergraph transversals find applications in various areas such as computational biology, boolean algebra and data mining BIBREF18 . Extensions of hypergraph transversals to include hyperedge and node weights were also proposed in BIBREF19 . Since the associated optimization problems are generally NP-hard, various approximation algorithms were proposed, including greedy algorithms BIBREF20 and LP relaxations BIBREF21 . The problem of finding a hypergraph transversal is conceptually similar to that of finding a summarizing subset of a set of objects modelled as a hypergraph. However, to the best of our knowledge, there was no attempt to use hypergraph transversals for text summarization in the past. Since it seeks a set of jointly relevant sentences, our method shares some similarities with existing graph-based models that apply global optimization strategies for sentence selection BIBREF9 , BIBREF15 , BIBREF16 . However, our hypergraph better captures topical relationships among sentences than the simple graphs based on lexical similarities between sentences. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "2\n",
      "Q: How does TL-TranSum compare to other summarizers in terms of performance?\n",
      "\n",
      "A: TL-TranSum outperforms the best system and average ROUGE-SU4 score by $2\\%$ and $21\\%$ in DUC2007, and by $2\\%$ and $22\\%$ in DUC2006. However, it is outperformed by the human summarizer due to our system producing extracts, while humans naturally reformulate sentences to compress their content and produce more informative summaries. In DUC2005, our TL-TranSum method is outperformed by the best system but still yields a $15\\%$ higher ROUGE-SU4 score than the average score.\n",
      "Question : for the text As a final experiment, we compare our TL-TranSum approach to other summarizers presented at DUC contests. Table 3 displays the ROUGE-2 and ROUGE-SU4 scores for the worst summary produced by a human, for the top four systems submitted for the contests, for the baseline proposed by NIST (a summary consisting of the leading sentences of randomly selected documents) and the average score of all methods submitted, respectively for DUC2005, DUC2006 and DUC2007 contests. Regarding DUC2007, our method outperforms the best system by $2\\%$ and the average ROUGE-SU4 score by $21\\%$ . It also performs significantly better than the baseline of NIST. However, it is outperformed by the human summarizer since our systems produces extracts, while humans naturally reformulate the original sentences to compress their content and produce more informative summaries. Tests on DUC2006 dataset lead to similar conclusions, with our TL-TranSum algorithm outperforming the best other system and the average ROUGE-SU4 score by $2\\%$ and $22\\%$ , respectively. On DUC2005 dataset however, our TL-TranSum method is outperformed by the beset system which is due to the use of advanced NLP techniques (such as sentence trimming BIBREF37 ) which tend to increase the ROUGE-SU4 score. Nevertheless, the ROUGE-SU4 score produced by our TL-TranSum algorithm is still $15\\%$ higher than the average score for DUC2005 contest. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "3\n",
      "Question 1: What is the MaxCover approach in text summarization?\n",
      "\n",
      "Answer 1: The MaxCover approach seeks a summary by maximizing the number of distinct relevant terms appearing in the summary while not exceeding the target summary length. It uses equation 18 to compute the term relevance scores. However, it overlooks the semantic similarities between terms captured by the SEMCOT algorithm and the hypergraph model used by TL-TranSum.\n",
      "Question : for the text We compare the performance of our TL-TranSum algorithm \"Detection of hypergraph transversals for text summarization\" with that of five related summarization systems. Topic-sensitive LexRank of BIBREF2 (TS-LexRank) and HITS algorithms of BIBREF1 are early graph-based summarizers. TS-LexRank builds a sentence graph based on term co-occurrences in sentences, and it applies a query-biased PageRank algorithm for sentence scoring. HITS method additionally extracts clusters of sentences and it applies the hubs and authorities algorithm for sentence scoring, with the sentences as authorities and the clusters as hubs. As suggested in BIBREF3 , in order to extract query relevant sentences, only the top $5\\%$ of sentences that are most relevant to the query are considered. HyperSum extends early graph-based summarizers by defining a cluster-based hypergraph with the sentences as nodes and hyperedges as sentence clusters, as described in section \"Testing the hypergraph structure\" . The sentences are then scored using an iterative label propagation algorithm over the hypergraph, starting with the lexical similarity of each sentence with the query as initial labels. In all three methods, the sentences with highest scores and pairwise lexical similarity not exceeding a threshold are included in the summary. Finally, we test two methods that also build on the theory of submodular functions. First, the MaxCover approach BIBREF9 seeks a summary by maximizing the number of distinct relevant terms appearing in the summary while not exceeding the target summary length (using equation 18 to compute the term relevance scores). While the objective function of the method is similar to that of the problem of finding a maximal budgeted hypergraph transversal (equation 26 ) of BIBREF16 , they overlook the semantic similarities between terms which are captured by our SEMCOT algorithm and our hypergraph model. Similarly, the Maximal Relevance Minimal Redundancy (MRMR) first computes relevance scores of sentences as in equation 18 , then it seeks a summary with a maximal total relevance score and a minimal redundancy while not exceeding the target summary length. The problem is solved by an iterative algorithm building on the submodularity and non-decreasing property of the objective function..Table 2 displays the ROUGE-2 and ROUGE-SU4 scores with the corresponding $95\\%$ confidence intervals for all six systems, including our TL-TranSum method. We observe that our system outperforms other graph and hypergraph-based summarizers involving the computation of individual sentence scores: LexRank by $6\\%$ , HITS by $13\\%$ and HyperSum by $6\\%$ of ROUGE-SU4 score; which confirms both the relevance of our theme-based hypergraph model and the capacity of our transversal-based summarizer to identify jointly relevant sentences as opposed to methods based on the computation of individual sentence scores. Moreover, our TL-TranSum method also outperforms other approaches such as MaxCover ( $5\\%$ ) and MRMR ( $7\\%$ ). These methods are also based on a submodular and non-decreasing function expressing the information coverage of the summary, but they are limited to lexical similarities between sentences and fail to detect topics and themes to measure the information coverage of the summary. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "4\n",
      "What is the worst case time complexity of the sentence theme detection based on topic tagging algorithm?\n",
      "\n",
      "The worst case time complexity of the sentence theme detection based on topic tagging algorithm is O(N_cN_tlog(N_t)) steps, where N_c is the number of iterations of the algorithm and N_t is the number of terms.\n",
      "Question : for the text We analyse the worst case time complexity of each step of our method. The time complexity of DBSCAN algorithm BIBREF27 is $O(N_t\\log (N_t))$ . Hence, the theme detection algorithm \"Sentence theme detection based on topic tagging\" takes $O(N_cN_t\\log (N_t))$ steps, where $N_c$ is the number of iterations of algorithm \"Sentence theme detection based on topic tagging\" which is generally low compared to the number of terms. The time complexity for the hypergraph construction is $O(K(N_s+N_t))$ where $K$ is the number of topics, or $O(N_t^2)$ if $N_t\\ge N_s$ . The time complexity of the sentence selection algorithms \"Detection of hypergraph transversals for text summarization\" and \"Detection of hypergraph transversals for text summarization\" are bounded by $O(N_sKC^{\\max }L^{\\max })$ where $C^{\\max }$ is the number of sentences in the largest theme and $L^{\\max }$ is the length of the longest sentences. Assuming $O(N_cN_t\\log (N_t))$0 is larger than $O(N_cN_t\\log (N_t))$1 , the overall time complexity of the method is of $O(N_cN_t\\log (N_t))$2 steps in the worst case. Hence the method is essentially equivalent to early graph-based models for text summarization in terms of computational burden, such as the LexRank-based systems BIBREF0 , BIBREF2 or greedy approaches based on global optimization BIBREF17 , BIBREF15 , BIBREF16 . However, it is computationnally more efficient than traditional hypergraph-based summarizers such as the one in BIBREF4 which involves a Markov Chain Monte Carlo inference for its topic model or the one in BIBREF3 which is based on an iterative computation of scores involving costly matrix multiplications at each step. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "5\n",
      "Question 1: What is the key advantage of the hypergraph-based summarization model proposed in this paper?\n",
      "\n",
      "Answer 1: The key advantage of the hypergraph-based summarization model proposed in this paper is its ability to capture groups of semantically related sentences, which enables the extraction of informative summaries by selecting a subset of sentences that jointly cover the relevant themes of the corpus. The model also outperforms related graph- or hypergraph-based approaches by at least 10% of ROUGE-SU4 score.\n",
      "Question : for the text In this paper, a new hypergraph-based summarization model was proposed, in which the nodes are the sentences of the corpus and the hyperedges are themes grouping sentences covering the same topics. Going beyond existing methods based on simple graphs and pairwise lexical similarities, our hypergraph model captures groups of semantically related sentences. Moreover, two new method of sentence selection based on the detection of hypergraph transversals were proposed: one to generate summaries of minimal length and achieving a target coverage, and the other to generate a summary achieving a maximal coverage of relevant themes while not exceeding a target length. The approach generates informative summaries by extracting a subset of sentences jointly covering the relevant themes of the corpus. Experiments on a real-world dataset demonstrate the effectiveness of the approach. The hypergraph model itself is shown to produce more accurate summaries than other models based on term or sentence clustering. The overall system also outperforms related graph- or hypergraph-based approaches by at least $10\\%$ of ROUGE-SU4 score..As a future research direction, we may analyse the performance of other algorithms for the detection of hypergraph transversals, such as methods based on LP relaxations. We may also further extend our topic model to take the polysemy of terms into acount: since each term may carry multiple meanings, a given term could refer to different topics depending on its context. Finally, we intend to adapt our model for solving related problems, such as commmunity question answering. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "6\n",
      "Question 1: What datasets were used to test the algorithms and how many documents does each dataset have?\n",
      "\n",
      "Answer 1: The algorithms were tested on DUC2005 BIBREF32, DUC2006 BIBREF33, and DUC2007 BIBREF34 datasets, each consisting of 50, 50, and 45 corpora, respectively. Each corpus had 25 documents of approximately 1000 words, on average.\n",
      "Question : for the text We test our algorithms on DUC2005 BIBREF32 , DUC2006 BIBREF33 and DUC2007 BIBREF34 datasets which were produced by the Document Understanding Conference (DUC) and are widely used as benchmark datasets for the evaluation of query-oriented summarizers. The datasets consist respectively of 50, 50 and 45 corpora, each consisting of 25 documents of approximately 1000 words, on average. A query is associated to each corpus. For evaluation purposes, each corpus is associated with a set of query-relevant summaries written by humans called reference summaries. In each of our experiments, a candidate summary is produced for each corpus by one of our algorithms and it is compared with the reference summaries using the metrics described below. Moreover, in experiments involving algorithm \"Detection of hypergraph transversals for text summarization\" , the target summary length is set to 250 words as required in DUC evalutions..In order to evaluate the similarity of a candidate summary with a set of reference summaries, we make use of the ROUGE toolkit of BIBREF35 , and more specifically of ROUGE-2 and ROUGE-SU4 metrics, which were adopted by DUC for summary evaluation. ROUGE-2 measures the number of bigrams found both in the candidate summary and the set of reference summaries. ROUGE-SU4 extends this approach by counting the number of unigrams and the number of 4-skip-bigrams appearing in the candidate and the reference summaries, where a 4-skip-bigram is a pair of words that are separated by no more than 4 words in a text. We refer to ROUGE toolkit BIBREF35 for more details about the evaluation metrics. ROUGE-2 and ROUGE-SU4 metrics are computed following the same setting as in DUC evaluations, namely with word stemming and jackknife resampling but without stopword removal. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "7\n",
      "in except\n",
      "7\n",
      "Question 1: What are the five models being compared for their performance?\n",
      "\n",
      "Answer 1: The five models being compared for their performance are: (1) LSTM; (2) Naive transformer; (3) Transformer + Anti OT (RL loss); (4) Transformer + Anti UT (phrase segmentation-based padding); (5) Transformer + Anti OT&UT.\n",
      "Question : for the text We compare the performance of the following models: (1) LSTM BIBREF19; (2)Naive transformer BIBREF14; (3)Transformer + Anti OT (RL loss); (4)Transformer + Anti UT (phrase segmentation-based padding); (5)Transformer + Anti OT&UT. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "1\n",
      "Question 1: How many short paragraphs are included in the vernacular literature corpus? \n",
      "\n",
      "Answer 1: The vernacular literature corpus contains 337K short paragraphs from 281 famous books.\n",
      "Question : for the text Training and Validation Sets We collected a corpus of poems and a corpus of vernacular literature from online resources. The poem corpus contains 163K quatrain poems from Tang Poems and Song Poems, the vernacular literature corpus contains 337K short paragraphs from 281 famous books, the corpus covers various literary forms including prose, fiction and essay. Note that our poem corpus and a vernacular corpus are not aligned. We further split the two corpora into a training set and a validation set..Test Set From online resources, we collected 487 seven-character quatrain poems from Tang Poems and Song Poems, as well as their corresponding high quality vernacular translations. These poems could be used as gold standards for poems generated from their corresponding vernacular translations. Table TABREF11 shows the statistics of our training, validation and test set. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "2\n",
      "Question 1: How is perplexity used to evaluate a model's ability to generate a poem?\n",
      "Answer 1: Perplexity is used to reflect the probability a model generates a certain poem, with a better model yielding a higher probability (lower perplexity) on the gold poem.\n",
      "Question : for the text Perplexity Perplexity reflects the probability a model generates a certain poem. Intuitively, a better model would yield higher probability (lower perplexity) on the gold poem..BLEU As a standard evaluation metric for machine translation, BLEU BIBREF18 measures the intersection of n-grams between the generated poem and the gold poem. A better generated poem usually achieves higher BLEU score, as it shares more n-gram with the gold poem..Human evaluation While perplexity and BLEU are objective metrics that could be applied to large-volume test set, evaluating Chinese poems is after all a subjective task. We invited 30 human evaluators to join our human evaluation. The human evaluators were divided into two groups. The expert group contains 15 people who hold a bachelor degree in Chinese literature, and the amateur group contains 15 people who holds a bachelor degree in other fields. All 30 human evaluators are native Chinese speakers..We ask evaluators to grade each generated poem from four perspectives: 1) Fluency: Is the generated poem grammatically and rhythmically well formed, 2) Semantic coherence: Is the generated poem itself semantic coherent and meaningful, 3) Semantic preservability: Does the generated poem preserve the semantic of the modern Chinese translation, 4) Poeticness: Does the generated poem display the characteristic of a poem and does the poem build good poetic image. The grading scale for each perspective is from 1 to 5. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "3\n",
      "What was the accuracy rate of human evaluators in differentiating between human-written poems and generated poems?\n",
      " \n",
      "The mean accuracy rate of human evaluators in differentiating between human-written poems and generated poems was 65.8%.\n",
      "Question : for the text We manually select 25 generated poems from vernacular Chinese translations and pair each one with its corresponding human written poem. We then present the 25 pairs to human evaluators and ask them to differentiate which poem is generated by human poet..As demonstrated in Table TABREF29, although the general meanings in human poems and generated poems seem to be the same, the wordings they employ are quite different. This explains the low BLEU scores in Section 4.3. According to the test results in Table TABREF30, human evaluators only achieved 65.8% in mean accuracy. This indicates the best generated poems are somewhat comparable to poems written by amateur poets..We interviewed evaluators who achieved higher than 80% accuracy on their differentiation strategies. Most interviewed evaluators state they realize the sentences in a human written poem are usually well organized to highlight a theme or to build a poetic image, while the correlation between sentences in a generated poem does not seem strong. As demonstrated in Table TABREF29, the last two sentences in both human poems (marked as red) echo each other well, while the sentences in machine-generated poems seem more independent. This gives us hints on the weakness of generated poems: While neural models may generate poems that resemble human poems lexically and syntactically, it's still hard for them to compete with human beings in building up good structures. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "4\n",
      "What was the highest scoring literature form in the experiment?\n",
      "\n",
      "The highest scoring literature form in the experiment was Song lyric, which was not surprising as both Song lyric and quatrain poems are written in classical Chinese, while the other three literature forms are all in vernacular.\n",
      "Question : for the text Chinese literature is not only featured for classical poems, but also various other literature forms. Song lyricUTF8gbsn(宋词), or ci also gained tremendous popularity in its palmy days, standing out in classical Chinese literature. Modern prose, modern poems and pop song lyrics have won extensive praise among Chinese people in modern days. The goal of this experiment is to transfer texts of other literature forms into quatrain poems. We expect the generated poems to not only keep the semantic of the original text, but also demonstrate terseness, rhythm and other characteristics of ancient poems. Specifically, we chose 20 famous fragments from four types of Chinese literature (5 fragments for each of modern prose, modern poems, pop song lyrics and Song lyrics). We try to As no ground truth is available, we resorted to human evaluation with the same grading standard in Section SECREF23..Comparing the scores of different literature forms, we observe Song lyric achieves higher scores than the other three forms of modern literature. It is not surprising as both Song lyric and quatrain poems are written in classical Chinese, while the other three literature forms are all in vernacular..Comparing the scores within the same literature form, we observe the scores of poems generated from different paragraphs tends to vary. After carefully studying the generated poems as well as their scores, we have the following observation:.1) In classical Chinese poems, poetic images UTF8gbsn(意象) were widely used to express emotions and to build artistic conception. A certain poetic image usually has some fixed implications. For example, autumn is usually used to imply sadness and loneliness. However, with the change of time, poetic images and their implications have also changed. According to our observation, if a vernacular paragraph contains more poetic images used in classical literature, its generated poem usually achieves higher score. As illustrated in Table TABREF12, both paragraph 2 and 3 are generated from pop song lyrics, paragraph 2 uses many poetic images from classical literature (e.g. pear flowers, makeup), while paragraph 3 uses modern poetic images (e.g. sparrows on the utility pole). Obviously, compared with poem 2, sentences in poem 3 seems more confusing, as the poetic images in modern times may not fit well into the language model of classical poems..2) We also observed that poems generated from descriptive paragraphs achieve higher scores than from logical or philosophical paragraphs. For example, in Table TABREF12, both paragraph 4 (more descriptive) and paragraph 5 (more philosophical) were selected from famous modern prose. However, compared with poem 4, poem 5 seems semantically more confusing. We offer two explanations to the above phenomenon: i. Limited by the 28-character restriction, it is hard for quatrain poems to cover complex logical or philosophical explanation. ii. As vernacular paragraphs are more detailed and lengthy, some information in a vernacular paragraph may be lost when it is summarized into a classical poem. While losing some information may not change the general meaning of a descriptive paragraph, it could make a big difference in a logical or philosophical paragraph. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "5\n",
      "Question 1: Which transformer-based model showed the best performance in generating poems according to the experiment results?\n",
      "\n",
      "Answer 1: Anti OT&UT showed the best performance among transformer-based models in generating poems according to the experiment results. It demonstrated that alleviating under-translation and over-translation both helped generate better poems.\n",
      "Question : for the text As illustrated in Table TABREF12 (ID 1). Given the vernacular translation of each gold poem in test set, we generate five poems using our models. Intuitively, the more the generated poem resembles the gold poem, the better the model is. We report mean perplexity and BLEU scores in Table TABREF19 (Where +Anti OT refers to adding the reinforcement loss to mitigate over-fitting and +Anti UT refers to adding phrase segmentation-based padding to mitigate under-translation), human evaluation results in Table TABREF20..According to experiment results, perplexity, BLEU scores and total scores in human evaluation are consistent with each other. We observe all BLEU scores are fairly low, we believe it is reasonable as there could be multiple ways to compose a poem given a vernacular paragraph. Among transformer-based models, both +Anti OT and +Anti UT outperforms the naive transformer, while Anti OT&UT shows the best performance, this demonstrates alleviating under-translation and over-translation both helps generate better poems. Specifically, +Anti UT shows bigger improvement than +Anti OT. According to human evaluation, among the four perspectives, our Anti OT&UT brought most score improvement in Semantic preservability, this proves our improvement on semantic preservability was most obvious to human evaluators. All transformer-based models outperform LSTM. Note that the average length of the vernacular translation is over 70 characters, comparing with transformer-based models, LSTM may only keep the information in the beginning and end of the vernacular. We anticipated some score inconsistency between expert group and amateur group. However, after analyzing human evaluation results, we did not observed big divergence between two groups. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "6\n",
      "What are the two main problems faced during the preliminary experiments of classical Chinese poem generation from vernacular text?\n",
      "The two main problems faced during the preliminary experiments of classical Chinese poem generation from vernacular text are under-translation and over-translation.\n",
      "Question : for the text During thousands of years, millions of classical Chinese poems have been written. They contain ancient poets' emotions such as their appreciation for nature, desiring for freedom and concerns for their countries. Among various types of classical poetry, quatrain poems stand out. On the one hand, their aestheticism and terseness exhibit unique elegance. On the other hand, composing such poems is extremely challenging due to their phonological, tonal and structural restrictions..Most previous models for generating classical Chinese poems BIBREF0, BIBREF1 are based on limited keywords or characters at fixed positions (e.g., acrostic poems). Since users could only interfere with the semantic of generated poems using a few input words, models control the procedure of poem generation. In this paper, we proposed a novel model for classical Chinese poem generation. As illustrated in Figure FIGREF1, our model generates a classical Chinese poem based on a vernacular Chinese paragraph. Our objective is not only to make the model generate aesthetic and terse poems, but also keep rich semantic of the original vernacular paragraph. Therefore, our model gives users more control power over the semantic of generated poems by carefully writing the vernacular paragraph..Although a great number of classical poems and vernacular paragraphs are easily available, there exist only limited human-annotated pairs of poems and their corresponding vernacular translations. Thus, it is unlikely to train such poem generation model using supervised approaches. Inspired by unsupervised machine translation (UMT) BIBREF2, we treated our task as a translation problem, namely translating vernacular paragraphs to classical poems..However, our work is not just a straight-forward application of UMT. In a training example for UMT, the length difference of source and target languages are usually not large, but this is not true in our task. Classical poems tend to be more concise and abstract, while vernacular text tends to be detailed and lengthy. Based on our observation on gold-standard annotations, vernacular paragraphs usually contain more than twice as many Chinese characters as their corresponding classical poems. Therefore, such discrepancy leads to two main problems during our preliminary experiments: (1) Under-translation: when summarizing vernacular paragraphs to poems, some vernacular sentences are not translated and ignored by our model. Take the last two vernacular sentences in Figure FIGREF1 as examples, they are not covered in the generated poem. (2) Over-translation: when expanding poems to vernacular paragraphs, certain words are unnecessarily translated for multiple times. For example, the last sentence in the generated poem of Figure FIGREF1, as green as sapphire, is back-translated as as green as as as sapphire..Inspired by the phrase segmentation schema in classical poems BIBREF3, we proposed the method of phrase-segmentation-based padding to handle with under-translation. By padding poems based on the phrase segmentation custom of classical poems, our model better aligns poems with their corresponding vernacular paragraphs and meanwhile lowers the risk of under-translation. Inspired by Paulus2018ADR, we designed a reinforcement learning policy to penalize the model if it generates vernacular paragraphs with too many repeated words. Experiments show our method can effectively decrease the possibility of over-translation..The contributions of our work are threefold:.(1) We proposed a novel task for unsupervised Chinese poem generation from vernacular text..(2) We proposed using phrase-segmentation-based padding and reinforcement learning to address two important problems in this task, namely under-translation and over-translation..(3) Through extensive experiments, we proved the effectiveness of our models and explored how to write the input vernacular to inspire better poems. Human evaluation shows our models are able to generate high quality poems, which are comparable to amateur poems. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "7\n",
      "Question 1: What was the issue with applying the naive UMT framework to the task of translating classical Chinese poems?\n",
      "\n",
      "Answer 1: The issue with applying the naive UMT framework to the task of translating classical Chinese poems was that the huge gap in sequence length between the source language (classical Chinese) and the target language (vernacular translations) would induce over-translation and under-translation when training UMT models. Classical Chinese poems are featured for their terseness and abstractness, and usually focus on depicting broad poetic images rather than details. The average length of the poems in the dataset collected for this study was 32.0 characters, while for the vernacular translations it was 73.3.\n",
      "Question : for the text During our early experiments, we realize that the naive UMT framework is not readily applied to our task. Classical Chinese poems are featured for its terseness and abstractness. They usually focus on depicting broad poetic images rather than details. We collected a dataset of classical Chinese poems and their corresponding vernacular translations, the average length of the poems is $32.0$ characters, while for vernacular translations, it is $73.3$. The huge gap in sequence length between source and target language would induce over-translation and under-translation when training UMT models. In the following sections, we explain the two problems and introduce our improvements. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "8\n",
      "What is the repetition ratio of a paragraph in this proposed model?\n",
      "\n",
      "The repetition ratio of a paragraph in this proposed model is defined as the ratio of the number of repeated characters to the total number of characters in the paragraph.\n",
      "Question : for the text In NMT, when decoding is complete, the decoder would generate an <EOS>token, indicating it has reached the end of the output sequence. However, when expending a poem $T$ into a vernacular Chinese paragraph $S_T$, due to the conciseness nature of poems, after finishing translating every source character in $T$, the output sequence $S_T$ may still be much shorter than the expected length of a poem‘s vernacular translation. As a result, the decoder would believe it has not finished decoding. Instead of generating the <EOS>token, the decoder would continue to generate new output characters from previously translated source characters. This would cause the decoder to repetitively output a piece of text many times..To remedy this issue, in addition to minimizing the original loss function $\\mathcal {L}$, we propose to minimize a specific discrete metric, which is made possible with reinforcement learning..We define repetition ratio $RR(S)$ of a paragraph $S$ as:.where $vocab(S)$ refers to the number of distinctive characters in $S$, $len(S)$ refers the number of all characters in $S$. Obviously, if a generated sequence contains many repeated characters, it would have high repetition ratio. Following the self-critical policy gradient training BIBREF16, we define the following loss function:.where $\\tau $ is a manually set threshold. Intuitively, minimizing $\\mathcal {L}^{rl}$ is equivalent to maximizing the conditional likelihood of the sequence $S$ given $S_{T_S}$ if its repetition ratio is lower than the threshold $\\tau $. Following BIBREF17, we revise the composite loss as:.where $\\alpha _1, \\alpha _2, \\alpha _3$ are scaling factors. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "9\n",
      "Question 1: Why may the latter part of a vernacular paragraph not be covered in a summarized classical poem?\n",
      "\n",
      "Answer 1: Vernacular paragraphs are usually more detailed and lengthy than classical poems. Therefore, when summarizing a vernacular paragraph to a poem, the poem may not be long enough to cover all the information in the paragraph. This results in the latter part of the paragraph being left unmentioned in the poem.\n",
      "Question : for the text By nature, classical poems are more concise and abstract while vernaculars are more detailed and lengthy, to express the same meaning, a vernacular paragraph usually contains more characters than a classical poem. As a result, when summarizing a vernacular paragraph $S$ to a poem $T_S$, $T_S$ may not cover all information in $S$ due to its length limit. In real practice, we notice the generated poems usually only cover the information in the front part of the vernacular paragraph, while the latter part is unmentioned..To alleviate under-translation, we propose phrase segmentation-based padding. Specifically, we first segment each line in a classical poem into several sub-sequences, we then join these sub-sequences with the special padding tokens <p>. During training, the padded lines are used instead of the original poem lines. As illustrated in Figure FIGREF10, padding would create better alignments between a vernacular paragraph and a prolonged poem, making it more likely for the latter part of the vernacular paragraph to be covered in the poem. As we mentioned before, the length of the vernacular translation is about twice the length of its corresponding classical poem, so we pad each segmented line to twice its original length..According to Ye jia:1984, to present a stronger sense of rhythm, each type of poem has its unique phrase segmentation schema, for example, most seven-character quatrain poems adopt the 2-2-3 schema, i.e. each quatrain line contains 3 phrases, the first, second and third phrase contains 2, 2, 3 characters respectively. Inspired by this law, we segment lines in a poem according to the corresponding phrase segmentation schema. In this way, we could avoid characters within the scope of a phrase to be cut apart, thus best preserve the semantic of each phrase.BIBREF15 generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "10\n",
      "Q: What is the purpose of the parameter initialization procedure in the model?\n",
      "A: The purpose of the parameter initialization procedure is to initialize the character embeddings of both vernacular and classical poem languages in one common space, so that the same character in both languages share the same embedding. This helps associate characters with their plausible translations in the other language.\n",
      "Question : for the text We transform our poem generation task as an unsupervised machine translation problem. As illustrated in Figure FIGREF1, based on the recently proposed UMT framework BIBREF2, our model is composed of the following components:.Encoder $\\textbf {E}_s$ and decoder $\\textbf {D}_s$ for vernacular paragraph processing.Encoder $\\textbf {E}_t$ and decoder $\\textbf {D}_t$ for classical poem processing.where $\\textbf {E}_s$ (or $\\textbf {E}_t$) takes in a vernacular paragraph (or a classical poem) and converts it into a hidden representation, and $\\textbf {D}_s$ (or $\\textbf {D}_t$) takes in the hidden representation and converts it into a vernacular paragraph (or a poem). Our model relies on a vernacular texts corpus $\\textbf {\\emph {S}}$ and a poem corpus $\\textbf {\\emph {T}}$. We denote $S$ and $T$ as instances in $\\textbf {\\emph {S}}$ and $\\textbf {\\emph {T}}$ respectively..The training of our model relies on three procedures, namely parameter initialization, language modeling and back-translation. We will give detailed introduction to each procedure..Parameter initialization As both vernacular and classical poem use Chinese characters, we initialize the character embedding of both languages in one common space, the same character in two languages shares the same embedding. This initialization helps associate characters with their plausible translations in the other language..Language modeling It helps the model generate texts that conform to a certain language. A well-trained language model is able to detect and correct minor lexical and syntactic errors. We train the language models for both vernacular and classical poem by minimizing the following loss:.where $S_N$ (or $T_N$) is generated by adding noise (drop, swap or blank a few words) in $S$ (or $T$)..Back-translation Based on a vernacular paragraph $S$, we generate a poem $T_S$ using $\\textbf {E}_s$ and $\\textbf {D}_t$, we then translate $T_S$ back into a vernacular paragraph $S_{T_S} = \\textbf {D}_s(\\textbf {E}_t(T_S))$. Here, $S$ could be used as gold standard for the back-translated paragraph $S_{T_s}$. In this way, we could turn the unsupervised translation into a supervised task by maximizing the similarity between $S$ and $S_{T_S}$. The same also applies to using poem $T$ as gold standard for its corresponding back-translation $T_{S_T}$. We define the following loss:.Note that $\\mathcal {L}^{bt}$ does not back propagate through the generation of $T_S$ and $S_T$ as we observe no improvement in doing so. When training the model, we minimize the composite loss:.where $\\alpha _1$ and $\\alpha _2$ are scaling factors. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "11\n",
      "What is the focus of previous works in classical Chinese poem generation?\n",
      "The focus of previous works in classical Chinese poem generation is to improve the semantic coherence of generated poems.\n",
      "Question : for the text Classical Chinese Poem Generation Most previous works in classical Chinese poem generation focus on improving the semantic coherence of generated poems. Based on LSTM, Zhang and Lapata Zhang2014ChinesePG purposed generating poem lines incrementally by taking into account the history of what has been generated so far. Yan Yan2016iPA proposed a polishing generation schema, each poem line is generated incrementally and iteratively by refining each line one-by-one. Wang et al. Wang2016ChinesePG and Yi et al. Yi2018ChinesePG proposed models to keep the generated poems coherent and semantically consistent with the user's intent. There are also researches that focus on other aspects of poem generation. (Yang et al. Yang2018StylisticCP explored increasing the diversity of generated poems using an unsupervised approach. Xu et al. Xu2018HowII explored generating Chinese poems from images. While most previous works generate poems based on topic words, our work targets at a novel task: generating poems from vernacular Chinese paragraphs..Unsupervised Machine Translation Compared with supervised machine translation approaches BIBREF4, BIBREF5, unsupervised machine translation BIBREF6, BIBREF2 does not rely on human-labeled parallel corpora for training. This technique is proved to greatly improve the performance of low-resource languages translation systems. (e.g. English-Urdu translation). The unsupervised machine translation framework is also applied to various other tasks, e.g. image captioning BIBREF7, text style transfer BIBREF8, speech to text translation BIBREF9 and clinical text simplification BIBREF10. The UMT framework makes it possible to apply neural models to tasks where limited human labeled data is available. However, in previous tasks that adopt the UMT framework, the abstraction levels of source and target language are the same. This is not the case for our task..Under-Translation & Over-Translation Both are troublesome problems for neural sequence-to-sequence models. Most previous related researches adopt the coverage mechanism BIBREF11, BIBREF12, BIBREF13. However, as far as we know, there were no successful attempt applying coverage mechanism to transformer-based models BIBREF14. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "12\n",
      "Question 1: What funding supported this work?\n",
      "Answer 1: This work was supported by the National Natural Science Foundation of China (NSFC) via grant numbers 61976072, 61632011, and 61772153.\n",
      "Question : for the text We thank the anonymous reviewers for their helpful comments and suggestions. This work was supported by the National Natural Science Foundation of China (NSFC) via grant 61976072, 61632011 and 61772153. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "13\n",
      "What is the purpose of the proposed framework for improving entities consistency?\n",
      "The purpose of the proposed framework is to improve entities consistency by querying KB in two steps.\n",
      "Question : for the text In this paper, we propose a novel framework to improve entities consistency by querying KB in two steps. In the first step, inspired by the observation that a response can usually be supported by a single KB row, we introduce the KB retriever to return the most relevant KB row, which is used to filter the irrelevant KB entities and encourage consistent generation. In the second step, we further perform attention mechanism to select the most relevant KB column. Experimental results show the effectiveness of our method. Extensive analysis further confirms the observation and reveal the correlation between the success of KB query and the success of task-oriented dialogue generation. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "14\n",
      "Question 1: What is the input and output of the end-to-end task-oriented dialogue system?\n",
      "Answer 1: The input of the end-to-end task-oriented dialogue system is the user's utterance or dialogue act, while the output is the system's response or dialogue act.\n",
      "Question : for the text In this section, we will describe the input and output of the end-to-end task-oriented dialogue system, and the definition of Seq2Seq task-oriented dialogue generation. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "15\n",
      "Question 1: How do we represent a dialogue between a user and a system?\n",
      "\n",
      "Answer 1: We represent the $k$-turned dialogue utterances as $\\lbrace (u_{1}, s_{1} ), (u_{2} , s_{2} ), ... , (u_{k}, s_{k})\\rbrace $, and at the $i^{\\text{th}}$ turn of the dialogue, we aggregate dialogue context and use $\\mathbf {x} = (x_{1}, x_{2}, ..., x_{m})$ to denote the whole dialogue history word by word, where $m$ is the number of tokens in the dialogue history according to eric:2017:SIGDial.\n",
      "Question : for the text Given a dialogue between a user ($u$) and a system ($s$), we follow eric:2017:SIGDial and represent the $k$-turned dialogue utterances as $\\lbrace (u_{1}, s_{1} ), (u_{2} , s_{2} ), ... , (u_{k}, s_{k})\\rbrace $. At the $i^{\\text{th}}$ turn of the dialogue, we aggregate dialogue context which consists of the tokens of $(u_{1}, s_{1}, ..., s_{i-1}, u_{i})$ and use $\\mathbf {x} = (x_{1}, x_{2}, ..., x_{m})$ to denote the whole dialogue history word by word, where $m$ is the number of tokens in the dialogue history. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "16\n",
      "Question 1: What is the value of entity in the $5^{\\text{th}}$ row and the $7^{\\text{th}}$ column in the relational-database-like KB $B$?\n",
      "\n",
      "Answer 1: The value of entity in the $5^{\\text{th}}$ row and the $7^{\\text{th}}$ column in the KB $B$ is noted as $v_{5,7}$.\n",
      "Question : for the text In this paper, we assume to have the access to a relational-database-like KB $B$, which consists of $|\\mathcal {R}|$ rows and $|\\mathcal {C}|$ columns. The value of entity in the $j^{\\text{th}}$ row and the $i^{\\text{th}}$ column is noted as $v_{j, i}$. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "17\n",
      "Question 1: What is the Seq2Seq task-oriented dialogue generation?\n",
      "\n",
      "Answer 1: The Seq2Seq task-oriented dialogue generation is the process of finding the most likely response according to the input dialogue history and the knowledge base (KB). It is defined as the probability of a response, where $y_t$ represents an output token.\n",
      "Question : for the text We define the Seq2Seq task-oriented dialogue generation as finding the most likely response $\\mathbf {y}$ according to the input dialogue history $\\mathbf {x}$ and KB $B$. Formally, the probability of a response is defined as.where $y_t$ represents an output token. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "18\n",
      "What is the proposed framework for achieving entity-consistent generation in task-oriented dialogue systems?\n",
      "The proposed framework involves a two-step process; the first step being the use of a retrieval module called KB-retriever, which uses a memory network to select the most relevant KB row given the dialogue history and a set of KB rows. The second step involves using attention mechanism to address the most correlated KB column and adopting the copy mechanism to incorporate the retrieved KB entity.\n",
      "Question : for the text Task-oriented dialogue system, which helps users to achieve specific goals with natural language, is attracting more and more research attention. With the success of the sequence-to-sequence (Seq2Seq) models in text generation BIBREF0, BIBREF1, BIBREF2, BIBREF3, BIBREF4, several works tried to model the task-oriented dialogue as the Seq2Seq generation of response from the dialogue history BIBREF5, BIBREF6, BIBREF7. This kind of modeling scheme frees the task-oriented dialogue system from the manually designed pipeline modules and heavy annotation labor for these modules..Different from typical text generation, the successful conversations for task-oriented dialogue system heavily depend on accurate knowledge base (KB) queries. Taking the dialogue in Figure FIGREF1 as an example, to answer the driver's query on the gas station, the dialogue system is required to retrieve the entities like “200 Alester Ave” and “Valero”. For the task-oriented system based on Seq2Seq generation, there is a trend in recent study towards modeling the KB query as an attention network over the entire KB entity representations, hoping to learn a model to pay more attention to the relevant entities BIBREF6, BIBREF7, BIBREF8, BIBREF9. Though achieving good end-to-end dialogue generation with over-the-entire-KB attention mechanism, these methods do not guarantee the generation consistency regarding KB entities and sometimes yield responses with conflict entities, like “Valero is located at 899 Ames Ct” for the gas station query (as shown in Figure FIGREF1). In fact, the correct address for Valero is 200 Alester Ave. A consistent response is relatively easy to achieve for the conventional pipeline systems because they query the KB by issuing API calls BIBREF10, BIBREF11, BIBREF12, and the returned entities, which typically come from a single KB row, are consistently related to the object (like the “gas station”) that serves the user's request. This indicates that a response can usually be supported by a single KB row. It's promising to incorporate such observation into the Seq2Seq dialogue generation model, since it encourages KB relevant generation and avoids the model from producing responses with conflict entities..To achieve entity-consistent generation in the Seq2Seq task-oriented dialogue system, we propose a novel framework which query the KB in two steps. In the first step, we introduce a retrieval module — KB-retriever to explicitly query the KB. Inspired by the observation that a single KB row usually supports a response, given the dialogue history and a set of KB rows, the KB-retriever uses a memory network BIBREF13 to select the most relevant row. The retrieval result is then fed into a Seq2Seq dialogue generation model to filter the irrelevant KB entities and improve the consistency within the generated entities. In the second step, we further perform attention mechanism to address the most correlated KB column. Finally, we adopt the copy mechanism to incorporate the retrieved KB entity..Since dialogue dataset is not typically annotated with the retrieval results, training the KB-retriever is non-trivial. To make the training feasible, we propose two methods: 1) we use a set of heuristics to derive the training data and train the retriever in a distant supervised fashion; 2) we use Gumbel-Softmax BIBREF14 as an approximation of the non-differentiable selecting process and train the retriever along with the Seq2Seq dialogue generation model. Experiments on two publicly available datasets (Camrest BIBREF11 and InCar Assistant BIBREF6) confirm the effectiveness of the KB-retriever. Both the retrievers trained with distant-supervision and Gumbel-Softmax technique outperform the compared systems in the automatic and human evaluations. Analysis empirically verifies our assumption that more than 80% responses in the dataset can be supported by a single KB row and better retrieval results lead to better task-oriented dialogue generation performance. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "19\n",
      "Question 1: What are the two major components of the framework for end-to-end task-oriented dialogues described in this section?\n",
      "\n",
      "Answer 1: The two major components of the framework are the memory network-based retriever and the seq2seq dialogue generation with KB Retriever.\n",
      "Question : for the text In this section, we describe our framework for end-to-end task-oriented dialogues. The architecture of our framework is demonstrated in Figure FIGREF3, which consists of two major components including an memory network-based retriever and the seq2seq dialogue generation with KB Retriever. Our framework first uses the KB-retriever to select the most relevant KB row and further filter the irrelevant entities in a Seq2Seq response generation model to improve the consistency among the output entities. While in decoding, we further perform the attention mechanism to choose the most probable KB column. We will present the details of our framework in the following sections. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "20\n",
      "Question 1: What is the encoder in the text referring to?\n",
      "Answer 1: The encoder refers to the bidirectional LSTM used to encode the dialogue history in the text.\n",
      "Question : for the text In our encoder, we adopt the bidirectional LSTM BIBREF15 to encode the dialogue history $\\mathbf {x}$, which captures temporal relationships within the sequence. The encoder first map the tokens in $\\mathbf {x}$ to vectors with embedding function $\\phi ^{\\text{emb}}$, and then the BiLSTM read the vector forwardly and backwardly to produce context-sensitive hidden states $(\\mathbf {h}_{1}, \\mathbf {h}_2, ..., \\mathbf {h}_{m})$ by repeatedly applying the recurrence $\\mathbf {h}_{i}=\\text{BiLSTM}\\left( \\phi ^{\\text{emb}}\\left( x_{i}\\right) , \\mathbf {h}_{i-1}\\right)$. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "21\n",
      "Question 1: How does the model improve the entity consistency when generating tokens?\n",
      "\n",
      "Answer 1: The model improves the entity consistency by querying the KB explicitly in two steps. First, the KB-retriever selects the most relevant KB row, and the generation of KB entities is constrained to the entities within the row, thus improving entity generation consistency. Next, the column attention selects the most probable KB column. Finally, the copy mechanism is used to incorporate the retrieved entity while decoding.\n",
      "Question : for the text As shown in section SECREF7, we can see that the generation of tokens are just based on the dialogue history attention, which makes the model ignorant to the KB entities. In this section, we present how to query the KB explicitly in two steps for improving the entity consistence, which first adopt the KB-retriever to select the most relevant KB row and the generation of KB entities from the entities-augmented decoder is constrained to the entities within the most probable row, thus improve the entity generation consistency. Next, we perform the column attention to select the most probable KB column. Finally, we show how to use the copy mechanism to incorporate the retrieved entity while decoding. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "22\n",
      "Question 1: How is the final retrieved KB entity score defined in the text?\n",
      "\n",
      "Answer 1: The final retrieved KB entity score is defined as the element-wise dot between the row retriever result and the column selection score, which can be calculated as shown in the text.\n",
      "Question : for the text After the row selection and column selection, we can define the final retrieved KB entity score as the element-wise dot between the row retriever result and the column selection score, which can be calculated as.where the $v^{t}$ indicates the final KB retrieved entity score. Finally, we follow eric:2017:SIGDial to use copy mechanism to incorporate the retrieved entity, which can be defined as.where $\\mathbf {o}_t$’s dimensionality is $ |\\mathcal {V}|$ +$|\\mathcal {E}|$. In $\\mathbf {v}^t$ , lower $ |\\mathcal {V}|$ is zero and the rest$|\\mathcal {E}|$ is retrieved entity scores. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "23\n",
      "Question 1: How does the model select the probable KB column?\n",
      "\n",
      "Answer 1: After getting the retrieved row result indicating the most relevant KB row, the model performs column attention in decoding time to select the probable KB column. Attention score is computed with the embedding of column attribute name using the decoder hidden state. This attention score becomes the logits of the column to be selected.\n",
      "Question : for the text After getting the retrieved row result that indicates which KB row is the most relevant to the generation, we further perform column attention in decoding time to select the probable KB column. For our KB column selection, following the eric:2017:SIGDial we use the decoder hidden state $(\\tilde{\\mathbf {h}}_{1}, \\tilde{\\mathbf {h}}_2, ...,\\tilde{\\mathbf {h}}_t)$ to compute an attention score with the embedding of column attribute name. The attention score $\\mathbf {c}\\in R^{|\\mathcal {E}|}$ then become the logits of the column be selected, which can be calculated as.where $\\mathbf {c}_j$ is the attention score of the $j^{\\text{th}}$ KB column, $\\mathbf {k}_j$ is represented with the embedding of word embedding of KB column name. $W^{^{\\prime }}_{1}$, $W^{^{\\prime }}_{2}$ and $\\mathbf {t}^{T}$ are trainable parameters of the model. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "24\n",
      "Question 1: What is the role of the KB-retriever in the framework?\n",
      "\n",
      "Answer 1: The KB-retriever takes the dialogue history and KB rows as inputs and selects the most relevant row, resembling the task of selecting one word from the inputs to answer questions. It uses a memory network to model this process.\n",
      "Question : for the text In our framework, our KB-retriever takes the dialogue history and KB rows as inputs and selects the most relevant row. This selection process resembles the task of selecting one word from the inputs to answer questions BIBREF13, and we use a memory network to model this process. In the following sections, we will first describe how to represent the inputs, then we will talk about our memory network-based retriever generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "25\n",
      "Question 1: How is the dialogue history encoded in the model?\n",
      "\n",
      "Answer 1: The dialogue history is encoded using the neural bag-of-words (BoW) method, following the original paper BIBREF13. Each token in the dialogue history is mapped into a vector using the $\\phi ^{\\text{emb}^{\\prime }}(x)$ embedding function, and the dialogue history representation, $\\mathbf {q}$, is obtained by summing these vectors.\n",
      "Question : for the text We encode the dialogue history by adopting the neural bag-of-words (BoW) followed the original paper BIBREF13. Each token in the dialogue history is mapped into a vector by another embedding function $\\phi ^{\\text{emb}^{\\prime }}(x)$ and the dialogue history representation $\\mathbf {q}$ is computed as the sum of these vectors: $\\mathbf {q} = \\sum ^{m}_{i=1} \\phi ^{\\text{emb}^{\\prime }} (x_{i}) $. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "26\n",
      "Question 1: How is a KB cell represented in this model?\n",
      "Answer 1: Each KB cell is represented by the cell value embedding as $\\mathbf {c}_{j,k} = \\phi ^{\\text{value}}(v_{j,k})$, and is further used to represent a KB row using neural BoW as $\\mathbf {r}_{j} = \\sum _{k=1}^{|\\mathcal {C}|} \\mathbf {c}_{j,k}$.\n",
      "Question : for the text In this section, we describe how to encode the KB row. Each KB cell is represented as the cell value $v$ embedding as $\\mathbf {c}_{j, k} = \\phi ^{\\text{value}}(v_{j, k})$, and the neural BoW is also used to represent a KB row $\\mathbf {r}_{j}$ as $\\mathbf {r}_{j} = \\sum _{k=1}^{|\\mathcal {C}|} \\mathbf {c}_{j,k}$. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "27\n",
      "Question 1: What is the purpose of using a memory network in KB retrieval?\n",
      "\n",
      "Answer 1: The memory network is used to select the row that most-likely supports the response generation in KB retrieval. It keeps a set of input matrices and query and computes the probability of selecting each input. The retrieval results are represented as a 0-1 matrix, which is further flattened to a 0-1 vector.\n",
      "Question : for the text We model the KB retrieval process as selecting the row that most-likely supports the response generation. Memory network BIBREF13 has shown to be effective to model this kind of selection. For a $n$-hop memory network, the model keeps a set of input matrices $\\lbrace R^{1}, R^{2}, ..., R^{n+1}\\rbrace $, where each $R^{i}$ is a stack of $|\\mathcal {R}|$ inputs $(\\mathbf {r}^{i}_1, \\mathbf {r}^{i}_2, ..., \\mathbf {r}^{i}_{|\\mathcal {R}|})$. The model also keeps query $\\mathbf {q}^{1}$ as the input. A single hop memory network computes the probability $\\mathbf {a}_j$ of selecting the $j^{\\text{th}}$ input as.For the multi-hop cases, layers of single hop memory network are stacked and the query of the $(i+1)^{\\text{th}}$ layer network is computed as.and the output of the last layer is used as the output of the whole network. For more details about memory network, please refer to the original paper BIBREF13..After getting $\\mathbf {a}$, we represent the retrieval results as a 0-1 matrix $T \\in \\lbrace 0, 1\\rbrace ^{|\\mathcal {R}|\\times \\mathcal {|C|}}$, where each element in $T$ is calculated as.In the retrieval result, $T_{j, k}$ indicates whether the entity in the $j^{\\text{th}}$ row and the $k^{\\text{th}}$ column is relevant to the final generation of the response. In this paper, we further flatten T to a 0-1 vector $\\mathbf {t} \\in \\lbrace 0, 1\\rbrace ^{|\\mathcal {E}|}$ (where $|\\mathcal {E}|$ equals $|\\mathcal {R}|\\times \\mathcal {|C|}$) as our retrieval row results. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "28\n",
      "To generate the response word by word, we adopted the attention-based decoder as explained in eric:2017:SIGDial. We used LSTM to represent the partially generated output sequence $(y_{1}, y_2, ...,y_{t-1})$ as $(\\tilde{\\mathbf {h}}_{1}, \\tilde{\\mathbf {h}}_2, ...,\\tilde{\\mathbf {h}}_t)$. For generating the next token $y_t$, we first calculate an attentive representation $\\tilde{\\mathbf {h}}^{^{\\prime }}_t$ of the dialogue history. Then, we project the concatenation of the hidden representation of the partially outputted sequence $\\tilde{\\mathbf {h}}_t$ and the attentive dialogue history representation $\\tilde{\\mathbf {h}}^{^{\\prime }}_t$ to the vocabulary space $\\mathcal {V}$ by $U$ to calculate the score (logit) for the next token generation. Finally, we calculate the probability of the next token $y_t$. As an example, we present a question and answer generated by our model below:\n",
      "\n",
      "Question 1: What is your favorite color?\n",
      "\n",
      "Answer 1: My favorite color is blue.\n",
      "Question : for the text Here, we follow eric:2017:SIGDial to adopt the attention-based decoder to generation the response word by word. LSTM is also used to represent the partially generated output sequence $(y_{1}, y_2, ...,y_{t-1})$ as $(\\tilde{\\mathbf {h}}_{1}, \\tilde{\\mathbf {h}}_2, ...,\\tilde{\\mathbf {h}}_t)$. For the generation of next token $y_t$, their model first calculates an attentive representation $\\tilde{\\mathbf {h}}^{^{\\prime }}_t$ of the dialogue history as.Then, the concatenation of the hidden representation of the partially outputted sequence $\\tilde{\\mathbf {h}}_t$ and the attentive dialogue history representation $\\tilde{\\mathbf {h}}^{^{\\prime }}_t$ are projected to the vocabulary space $\\mathcal {V}$ by $U$ as.to calculate the score (logit) for the next token generation. The probability of next token $y_t$ is finally calculated as generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "29\n",
      "Question 1: What is the most popular way of modeling KB query in text generation using Seq2Seq models?\n",
      "\n",
      "Answer 1: The most popular way of modeling KB query is treating it as an attention network over the entire KB entities, and the return can be a fuzzy summation of the entity representations.\n",
      "Question : for the text Sequence-to-sequence (Seq2Seq) models in text generation BIBREF0, BIBREF1, BIBREF2, BIBREF3, BIBREF4 has gained more popular and they are applied for the open-domain dialogs BIBREF24, BIBREF25 in the end-to-end training method. Recently, the Seq2Seq can be used for learning task oriented dialogs and how to query the structured KB is the remaining challenges..Properly querying the KB has long been a challenge in the task-oriented dialogue system. In the pipeline system, the KB query is strongly correlated with the design of language understanding, state tracking, and policy management. Typically, after obtaining the dialogue state, the policy management module issues an API call accordingly to query the KB. With the development of neural network in natural language processing, efforts have been made to replacing the discrete and pre-defined dialogue state with the distributed representation BIBREF10, BIBREF11, BIBREF12, BIBREF26. In our framework, our retrieval result can be treated as a numeric representation of the API call return..Instead of interacting with the KB via API calls, more and more recent works tried to incorporate KB query as a part of the model. The most popular way of modeling KB query is treating it as an attention network over the entire KB entities BIBREF6, BIBREF27, BIBREF8, BIBREF28, BIBREF29 and the return can be a fuzzy summation of the entity representations. madotto2018mem2seq's practice of modeling the KB query with memory network can also be considered as learning an attentive preference over these entities. wen2018sequence propose the implicit dialogue state representation to query the KB and achieve the promising performance. Different from their modes, we propose the KB-retriever to explicitly query the KB, and the query result is used to filter the irrelevant entities in the dialogue generation to improve the consistency among the output entities. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "30\n",
      "What models were used to evaluate the performance of the framework?\n",
      "The models used to evaluate the performance of the framework were BLEU and the Micro Entity F1.\n",
      "Question : for the text Follow the prior works BIBREF6, BIBREF7, BIBREF9, we adopt the BLEU and the Micro Entity F1 to evaluate our model performance. The experimental results are illustrated in Table TABREF30..In the first block of Table TABREF30, we show the Human, rule-based and KV Net (with*) result which are reported from eric:2017:SIGDial. We argue that their results are not directly comparable because their work uses the entities in thier canonicalized forms, which are not calculated based on real entity value. It's noticing that our framework with two methods still outperform KV Net in InCar dataset on whole BLEU and Entity F metrics, which demonstrates the effectiveness of our framework..In the second block of Table TABREF30, we can see that our framework trained with both the distant supervision and the Gumbel-Softmax beats all existing models on two datasets. Our model outperforms each baseline on both BLEU and F1 metrics. In InCar dataset, Our model with Gumbel-Softmax has the highest BLEU compared with baselines, which which shows that our framework can generate more fluent response. Especially, our framework has achieved 2.5% improvement on navigate domain, 1.8% improvement on weather domain and 3.5% improvement on calendar domain on F1 metric. It indicates that the effectiveness of our KB-retriever module and our framework can retrieve more correct entity from KB. In CamRest dataset, the same trend of improvement has been witnessed, which further show the effectiveness of our framework..Besides, we observe that the model trained with Gumbel-Softmax outperforms with distant supervision method. We attribute this to the fact that the KB-retriever and the Seq2Seq module are fine-tuned in an end-to-end fashion, which can refine the KB-retriever and further promote the dialogue generation. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "31\n",
      "Question 1: What is the correlation between the number of KB rows and generation consistency in the conducted experiment? \n",
      "Answer 1: The experiment showed that as the number of KB rows increased, there was a decrease in generation consistency, indicating that irrelevant information negatively impacts dialogue generation consistency.\n",
      "Question : for the text To further explore the correlation between the number of KB rows and generation consistency, we conduct experiments with distant manner to study the correlation between the number of KB rows and generation consistency..We choose KBs with different number of rows on a scale from 1 to 5 for the generation. From Figure FIGREF32, as the number of KB rows increase, we can see a decrease in generation consistency. This indicates that irrelevant information would harm the dialogue generation consistency. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "32\n",
      "Question 1: What is the purpose of computing consistency recall in the model?\n",
      "\n",
      " Answer 1: The purpose of computing consistency recall in the model is to verify the consistent generation from the model. It measures the consistency of the utterances that have multiple entities, and checks whether they belong to the same row annotated with distant supervision. Incorporating a retriever in dialogue generation has been shown to improve consistency, as indicated in the results presented in Table TABREF37.\n",
      "Question : for the text In this paper, we expect the consistent generation from our model. To verify this, we compute the consistency recall of the utterances that have multiple entities. An utterance is considered as consistent if it has multiple entities and these entities belong to the same row which we annotated with distant supervision..The consistency result is shown in Table TABREF37. From this table, we can see that incorporating retriever in the dialogue generation improves the consistency. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "33\n",
      "Question 1: How does the evaluation process for the framework work?\n",
      "\n",
      "Answer 1: The evaluation process includes human experts judging the quality of responses based on correctness, fluency, and humanlikeness on a scale from 1 to 5. They are presented with the dialogue history, output from the system, and a gold response. Our model outperformed other baseline models on all metrics, with the most significant improvement in correctness.\n",
      "Question : for the text We provide human evaluation on our framework and the compared models. These responses are based on distinct dialogue history. We hire several human experts and ask them to judge the quality of the responses according to correctness, fluency, and humanlikeness on a scale from 1 to 5. In each judgment, the expert is presented with the dialogue history, an output of a system with the name anonymized, and the gold response..The evaluation results are illustrated in Table TABREF37. Our framework outperforms other baseline models on all metrics according to Table TABREF37. The most significant improvement is from correctness, indicating that our model can retrieve accurate entity from KB and generate more informative information that the users want to know. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "34\n",
      "Question 1: What is the proportion of responses that can be supported by a single row in the navigation domain?\n",
      "\n",
      "Answer 1: The proportion of responses that can be supported by a single row in the navigation domain is 95%.\n",
      "Question : for the text In this section, we verify our assumption by examining the proportion of responses that can be supported by a single row..We define a response being supported by the most relevant KB row as all the responded entities are included by that row. We study the proportion of these responses over the test set. The number is 95% for the navigation domain, 90% for the CamRest dataset and 80% for the weather domain. This confirms our assumption that most responses can be supported by the relevant KB row. Correctly retrieving the supporting row should be beneficial..We further study the weather domain to see the rest 20% exceptions. Instead of being supported by multiple rows, most of these exceptions cannot be supported by any KB row. For example, there is one case whose reference response is “It 's not rainy today”, and the related KB entity is sunny. These cases provide challenges beyond the scope of this paper. If we consider this kind of cases as being supported by a single row, such proportion in the weather domain is 99%. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "35\n",
      "Question 1: What visualization did the researchers use to gain insights into how the retriever module influences the KB score distribution?\n",
      "\n",
      "Answer 1: The researchers visualized the KB entity probability at the decoding position where they generated the entity 200_Alester_Ave.\n",
      "Question : for the text To gain more insights into how the our retriever module influences the whole KB score distribution, we visualized the KB entity probability at the decoding position where we generate the entity 200_Alester_Ave. From the example (Fig FIGREF38), we can see the $4^\\text{th}$ row and the $1^\\text{th}$ column has the highest probabilities for generating 200_Alester_Ave, which verify the effectiveness of firstly selecting the most relevant KB row and further selecting the most relevant KB column. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "36\n",
      "Question 1: How do the authors propose to train their KB-retriever in the Seq2Seq dialogue generation?\n",
      "\n",
      "Answer 1: The authors propose two methods for training their KB-retriever. The first method involves using heuristics to extract training data based on the similarity between the surface string of KB entries and reference responses. The second method involves training the KB-retriever along with the Seq2Seq dialogue generation and using Gumbel-Softmax as an approximation of the $\\operatornamewithlimits{argmax}$ during training to make the retrieval process differentiable.\n",
      "Question : for the text As mentioned in section SECREF9, we adopt the memory network to train our KB-retriever. However, in the Seq2Seq dialogue generation, the training data does not include the annotated KB row retrieval results, which makes supervised training the KB-retriever impossible. To tackle this problem, we propose two training methods for our KB-row-retriever. 1) In the first method, inspired by the recent success of distant supervision in information extraction BIBREF16, BIBREF17, BIBREF18, BIBREF19, we take advantage of the similarity between the surface string of KB entries and the reference response, and design a set of heuristics to extract training data for the KB-retriever. 2) In the second method, instead of training the KB-retriever as an independent component, we train it along with the training of the Seq2Seq dialogue generation. To make the retrieval process in Equation DISPLAY_FORM13 differentiable, we use Gumbel-Softmax BIBREF14 as an approximation of the $\\operatornamewithlimits{argmax}$ during training. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "37\n",
      "Question 1: What is DSR? \n",
      "\n",
      "Answer 1: DSR stands for Dialogue State Representation, a model that uses KB (Knowledge Base) retrieval and copying mechanisms while decoding to generate entities from the context. The results for DSR were obtained by running their code on CamRest dataset.\n",
      "Question : for the text We compare our model with several baselines including:.Attn seq2seq BIBREF22: A model with simple attention over the input context at each time step during decoding..Ptr-UNK BIBREF23: Ptr-UNK is the model which augments a sequence-to-sequence architecture with attention-based copy mechanism over the encoder context..KV Net BIBREF6: The model adopted and argumented decoder which decodes over the concatenation of vocabulary and KB entities, which allows the model to generate entities..Mem2Seq BIBREF7: Mem2Seq is the model that takes dialogue history and KB entities as input and uses a pointer gate to control either generating a vocabulary word or selecting an input as the output..DSR BIBREF9: DSR leveraged dialogue state representation to retrieve the KB implicitly and applied copying mechanism to retrieve entities from knowledge base while decoding..In InCar dataset, for the Attn seq2seq, Ptr-UNK and Mem2seq, we adopt the reported results from madotto2018mem2seq. In CamRest dataset, for the Mem2Seq, we adopt their open-sourced code to get the results while for the DSR, we run their code on the same dataset to obtain the results. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "38\n",
      "What datasets were used in the experiment?\n",
      "The InCar Assistant dataset and the CamRest dataset were used in the experiment.\n",
      "Question : for the text We choose the InCar Assistant dataset BIBREF6 including three distinct domains: navigation, weather and calendar domain. For weather domain, we follow wen2018sequence to separate the highest temperature, lowest temperature and weather attribute into three different columns. For calendar domain, there are some dialogues without a KB or incomplete KB. In this case, we padding a special token “-” in these incomplete KBs. Our framework is trained separately in these three domains, using the same train/validation/test split sets as eric:2017:SIGDial. To justify the generalization of the proposed model, we also use another public CamRest dataset BIBREF11 and partition the datasets into training, validation and testing set in the ratio 3:1:1. Especially, we hired some human experts to format the CamRest dataset by equipping the corresponding KB to every dialogues..All hyper-parameters are selected according to validation set. We use a three-hop memory network to model our KB-retriever. The dimensionalities of the embedding is selected from $\\lbrace 100, 200\\rbrace $ and LSTM hidden units is selected from $\\lbrace 50, 100, 150, 200, 350\\rbrace $. The dropout we use in our framework is selected from $\\lbrace 0.25, 0.5, 0.75\\rbrace $ and the batch size we adopt is selected from $\\lbrace 1,2\\rbrace $. L2 regularization is used on our model with a tension of $5\\times 10^{-6}$ for reducing overfitting. For training the retriever with distant supervision, we adopt the weight typing trick BIBREF20. We use Adam BIBREF21 to optimize the parameters in our model and adopt the suggested hyper-parameters for optimization..We adopt both the automatic and human evaluations in our experiments. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "39\n",
      "Question 1: How is the weakly labeled data obtained for the retriever in the KB-retriever system?\n",
      "Answer 1: The most relevant KB row is guessed from the reference response and then used to obtain weakly labeled data for the retriever. The largest number of entities mentioned in the whole dialogue, from the same topic, are used to support the utterance, and the similarity of each row is computed by matching the surface form of the entities in the row to the dialogue context. This retrieval result is then used as the input for the Seq2Seq generation during training.\n",
      "Question : for the text Although it's difficult to obtain the annotated retrieval data for the KB-retriever, we can “guess” the most relevant KB row from the reference response, and then obtain the weakly labeled data for the retriever. Intuitively, for the current utterance in the same dialogue which usually belongs to one topic and the KB row that contains the largest number of entities mentioned in the whole dialogue should support the utterance. In our training with distant supervision, we further simplify our assumption and assume that one dialogue which is usually belongs to one topic and can be supported by the most relevant KB row, which means for a $k$-turned dialogue, we construct $k$ pairs of training instances for the retriever and all the inputs $(u_{1}, s_{1}, ..., s_{i-1}, u_{i} \\mid i \\le k)$ are associated with the same weakly labeled KB retrieval result $T^*$..In this paper, we compute each row's similarity to the whole dialogue and choose the most similar row as $T^*$. We define the similarity of each row as the number of matched spans with the surface form of the entities in the row. Taking the dialogue in Figure FIGREF1 for an example, the similarity of the 4$^\\text{th}$ row equals to 4 with “200 Alester Ave”, “gas station”, “Valero”, and “road block nearby” matching the dialogue context; and the similarity of the 7$^\\text{th}$ row equals to 1 with only “road block nearby” matching..In our model with the distantly supervised retriever, the retrieval results serve as the input for the Seq2Seq generation. During training the Seq2Seq generation, we use the weakly labeled retrieval result $T^{*}$ as the input. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "40\n",
      "Question 1: How is the KB-retriever trained in an end-to-end fashion?\n",
      "Answer 1: The KB-retriever is trained in an end-to-end fashion by using the Gumbel-Softmax technique, which approximates the discrete retrieval result and allows for passing the training signal from the Seq2Seq generation model. The parameters are pre-trained with distant supervision and fine-tuned for optimal performance.\n",
      "Question : for the text In addition to treating the row retrieval result as an input to the generation model, and training the kb-row-retriever independently, we can train it along with the training of the Seq2Seq dialogue generation in an end-to-end fashion. The major difficulty of such a training scheme is that the discrete retrieval result is not differentiable and the training signal from the generation model cannot be passed to the parameters of the retriever. Gumbel-softmax technique BIBREF14 has been shown an effective approximation to the discrete variable and proved to work in sentence representation. In this paper, we adopt the Gumbel-Softmax technique to train the KB retriever. We use.as the approximation of $T$, where $\\mathbf {g}_{j}$ are i.i.d samples drawn from $\\text{Gumbel}(0,1)$ and $\\tau $ is a constant that controls the smoothness of the distribution. $T^{\\text{approx}}_{j}$ replaces $T^{\\text{}}_{j}$ in equation DISPLAY_FORM13 and goes through the same flattening and expanding process as $\\mathbf {V}$ to get $\\mathbf {v}^{\\mathbf {t}^{\\text{approx}^{\\prime }}}$ and the training signal from Seq2Seq generation is passed via the logit.To make training with Gumbel-Softmax more stable, we first initialize the parameters by pre-training the KB-retriever with distant supervision and further fine-tuning our framework. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "41\n",
      "Question 1: What features are used by the audio encoder?\n",
      "\n",
      "Answer 1: The audio encoder uses the VGGish features provided by the avsd challenge organizers.\n",
      "Question : for the text The audio encoder is structurally similar to the video encoder. We use the VGGish features provided by the avsd challenge organizers. Also similar to the video encoder, when not using the FiLM blocks, we use the VGGish features and encode them with the lstm directly, followed by the attention step. The audio encoder is depicted in Figure FIGREF4 . generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "42\n",
      "Question 1: How did modelname perform on the AVSD test set?\n",
      "\n",
      "Answer 1: According to the text, modelname achieved a relative improvement of more than 16% over the baseline model on BLEU-4 and more than 33% on CIDEr when evaluated on the official AVSD test set.\n",
      "Question : for the text We presented modelname, a state-of-the-art dialogue model for conversations about videos. We evaluated the model on the official AVSD test set, where it achieves a relative improvement of more than 16% over the baseline model on BLEU-4 and more than 33% on CIDEr. The challenging aspect of multi-modal dialogue is fusing modalities with varying information density. On AVSD, it is easiest to learn from the input text, while video features remain largely opaque to the decoder. modelname uses a generalization of FiLM to video that conditions video feature extraction on a question. However, similar to related work, absolute improvements of incorporating video features into dialogue are consistent but small. Thus, while our results indicate the suitability of our FiLM generalization, they also highlight that applications at the intersection between language and video are currently constrained by the quality of video features, and emphasizes the need for larger datasets. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "43\n",
      "Question 1: How does the answer decoder work?\n",
      "Answer 1: The answer decoder is a single-layer recurrent lstm network that generates the answer to the last question utterance. It is provided with the dialogue-level state at each time-step and produces a softmax output over a vector corresponding to vocabulary words. The decoding process stops when 30 words are produced or an end of sentence token is encountered.\n",
      "Question : for the text The answer decoder consists of a single-layer recurrent lstm network and generates the answer to the last question utterance. At each time-step, it is provided with the dialogue-level state and produces a softmax over a vector corresponding to vocabulary words and stops when 30 words were produced or an end of sentence token is encountered..The auxiliary decoder is functionally similar to the answer decoder. The decoded sentence is the caption and/or description of the video. We use the Video Encoder state instead of the Dialogue-level Encoder state as input since with this module we want to learn a better video representation capable of decoding the description. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "44\n",
      "Question 1: What is the input for the description encoder and how does it make use of the utterance-level encoder?\n",
      "\n",
      "Answer 1: The input for the description encoder is the caption and/or summary of the video provided with the dataset. It makes use of the last hidden state of the utterance-level encoder to generate an attention map over the hidden states of its own lstm, and then produces the attention-weighted sum of those hidden states as its final output.\n",
      "Question : for the text Similar to the utterance-level encoder, the description encoder is also a single-layer lstm recurrent neural network. Its word embeddings are also initialized with GloVe and then fine-tuned during training. For the description, we use the caption and/or the summary for the video provided with the dataset. The description encoder also has access to the last hidden state of the utterance-level encoder, which it uses to generate an attention map over the hidden states of its lstm. The final output of this module is the attention-weighted sum of the lstm hidden states. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "45\n",
      "Question 1: What are the AVSD challenge tasks addressed in the text? \n",
      "\n",
      "Answer 1: The AVSD challenge tasks addressed in the text are Task 1.a and Task 2.a, and the results of the model named modelname are presented in Table TABREF9.\n",
      "Question : for the text The avsd challenge tasks we address here are:.We train our modelname model for Task 1.a and Task 2.a of the challenge and we present the results in Table TABREF9 . Our model outperforms the baseline model released by BIBREF11 on all of these tasks. The scores for the winning team have been released to challenge participants and are also included. Their approach, however, is not public as of yet. We observe the following for our models:.Since the official test set has not been released publicly, results reported on the official test set have been provided by the challenge organizers. For the prototype test set and for the ablation study presented in Table TABREF24 , we use the same code for evaluation metrics as used by BIBREF11 for fairness and comparability. We attribute the significant performance gain of our model over the baseline to a combination of several factors as described below:.Our primary architectural differences over the baseline model are: not concatenating the question, answer pairs before encoding them, the auxiliary decoder module, and using the Time-Extended FiLM module for feature extraction. These, combined with using scheduled sampling and running hyperparameter optimization over the validation set to select hyperparameters, give us the observed performance boost..We observe that our models generate fairly relevant responses to questions in the dialogues, and models with audio-visual inputs respond to audio-visual questions (e.g. “is there any voices or music ?”) correctly more often..We conduct an ablation study on the effectiveness of different components (eg., text, video and audio) and present it in Table TABREF24 . Our experiments show that: generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "46\n",
      "Question 1: What method did the researchers find to be the most effective in combining the hidden states of diverse modalities for the dialogue-level encoder?\n",
      "\n",
      "Answer 1: The researchers found concatenation to perform better on the validation set than averaging or the Hadamard product in combining the hidden states of diverse modalities for the dialogue-level encoder.\n",
      "Question : for the text The outputs of the encoders for past utterances, descriptions, video, and audio together form the dialogue context INLINEFORM0 which is the input of the decoder. We first combine past utterances using a dialogue-level encoder which is a single-layer lstm recurrent neural network. The input to this encoder are the final hidden states of the utterance-level lstm. To combine the hidden states of these diverse modalities, we found concatenation to perform better on the validation set than averaging or the Hadamard product. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "47\n",
      "Question 1: What is the visdial task?\n",
      "Answer 1: The visdial task provides an image and dialogue where each turn is a QA pair, and the task is to train a model to answer these questions within the dialogue.\n",
      "Question : for the text Deep neural networks have been successfully applied to several computer vision tasks such as image classification BIBREF0 , object detection BIBREF1 , video action classification BIBREF2 , etc. They have also been successfully applied to natural language processing tasks such as machine translation BIBREF3 , machine reading comprehension BIBREF4 , etc. There has also been an explosion of interest in tasks which combine multiple modalities such as audio, vision, and language together. Some popular multi-modal tasks combining these three modalities, and their differences are highlighted in Table TABREF1 ..Given an image and a question related to the image, the vqa challenge BIBREF5 tasked users with selecting an answer to the question. BIBREF6 identified several sources of bias in the vqa dataset, which led to deep neural models answering several questions superficially. They found that in several instances, deep architectures exploited the statistics of the dataset to select answers ignoring the provided image. This prompted the release of vqa 2.0 BIBREF7 which attempts to balance the original dataset. In it, each question is paired to two similar images which have different answers. Due to the complexity of vqa, understanding the failures of deep neural architectures for this task has been a challenge. It is not easy to interpret whether the system failed in understanding the question or in understanding the image or in reasoning over it. The CLEVR dataset BIBREF8 was hence proposed as a useful benchmark to evaluate such systems on the task of visual reasoning. Extending question answering over images to videos, BIBREF9 have proposed MovieQA, where the task is to select the correct answer to a provided question given the movie clip on which it is based..Intelligent systems that can interact with human users for a useful purpose are highly valuable. To this end, there has been a recent push towards moving from single-turn qa to multi-turn dialogue, which is a natural and intuitive setting for humans. Among multi-modal dialogue tasks, visdial BIBREF10 provides an image and dialogue where each turn is a qa pair. The task is to train a model to answer these questions within the dialogue. The avsd challenge extends the visdial task from images to the audio-visual domain..We present our modelname model for the avsd task. modelname combines a hred for encoding and generating qa-dialogue with a novel FiLM-based audio-visual feature extractor for videos and an auxiliary multi-task learning-based decoder for decoding a summary of the video. It outperforms the baseline results for the avsd dataset BIBREF11 and was ranked 2nd overall among the dstc7 avsd challenge participants..In Section SECREF2 , we discuss existing literature on end-to-end dialogue systems with a special focus on multi-modal dialogue systems. Section SECREF3 describes the avsd dataset. In Section SECREF4 , we present the architecture of our modelname model. We describe our evaluation and experimental setup in Section SECREF5 and then conclude in Section SECREF6 . generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "48\n",
      "Question 1: What algorithm is used to optimize the model in this context embedding scenario?\n",
      "\n",
      "Answer 1: The AMSGrad algorithm is used to optimize the model in this context embedding scenario.\n",
      "Question : for the text For a given context embedding INLINEFORM0 at dialogue turn INLINEFORM1 , we minimize the negative log-likelihood of the answer word INLINEFORM2 (vocabulary size), normalized by the number of words INLINEFORM3 in the ground truth response INLINEFORM4 , L(Ct, r) = -1Mm=1MiV( [rt,m=i] INLINEFORM5 ) , where the probabilities INLINEFORM6 are given by the decoder LSTM output, r*t,m-1 ={ll rt,m-1 ; s>0.2, sU(0, 1).v INLINEFORM0 ; else . is given by scheduled sampling BIBREF32 , and INLINEFORM1 is a symbol denoting the start of a sequence. We optimize the model using the AMSGrad algorithm BIBREF33 and use a per-condition random search to determine hyperparameters. We train the model using the BLEU-4 score on the validation set as our stopping citerion. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "49\n",
      "Question 1: What is the framework used for modelling dialogue systems in our model?\n",
      "\n",
      "Answer 1: Our model is based on the hred framework for modelling dialogue systems.\n",
      "Question : for the text Our modelname model is based on the hred framework for modelling dialogue systems. In our model, an utterance-level recurrent lstm encoder encodes utterances and a dialogue-level recurrent lstm encoder encodes the final hidden states of the utterance-level encoders, thus maintaining the dialogue state and dialogue coherence. We use the final hidden states of the utterance-level encoders in the attention mechanism that is applied to the outputs of the description, video, and audio encoders. The attended features from these encoders are fused with the dialogue-level encoder's hidden states. An utterance-level decoder decodes the response for each such dialogue state following a question. We also add an auxiliary decoding module which is similar to the response decoder except that it tries to generate the caption and/or the summary of the video. We present our model in Figure FIGREF2 and describe the individual components in detail below. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "50\n",
      "Question 1: What are some challenges in moving from image-based dialogue to video-based dialogue?\n",
      "Answer 1: Limited availability of data, the complexity of extracting relevant features from videos, the need to incorporate audio into answering questions, and the difficulty in encoding the temporal nature of videos are all challenges in moving from image-based dialogue to video-based dialogue.\n",
      "Question : for the text With the availability of large conversational corpora from sources like Reddit and Twitter, there has been a lot of recent work on end-to-end modelling of dialogue for open domains. BIBREF12 treated dialogue as a machine translation problem where they translate from the stimulus to the response. They observed this to be more challenging than machine translation tasks due the larger diversity of possible responses. Among approaches that just use the previous utterance to generate the current response, BIBREF13 proposed a response generation model based on the encoder decoder framework. BIBREF14 also proposed an encoder-decoder based neural network architecture that uses the previous two utterances to generate the current response. Among discriminative methods (i.e. methods that produce a score for utterances from a set and then rank them), BIBREF15 proposed a neural architecture to select the best next response from a list of responses by measuring their similarity to the dialogue context. BIBREF16 extended prior work on encoder-decoder-based models to multi-turn conversations. They trained a hierarchical model called hred for generating dialogue utterances where a recurrent neural network encoder encodes each utterance. A higher-level recurrent neural network maintains the dialogue state by further encoding the individual utterance encodings. This dialogue state is then decoded by another recurrent decoder to generate the response at that point in time. In followup work, BIBREF17 used a latent stochastic variable to condition the generation process which aided their model in producing longer coherent outputs that better retain the context..Datasets and tasks BIBREF10 , BIBREF18 , BIBREF19 have also been released recently to study visual-input based conversations. BIBREF10 train several generative and discriminative deep neural models for the visdial task. They observe that on this task, discriminative models outperform generative models and that models making better use of the dialogue history do better than models that do not use dialogue history at all. Unexpectedly, the performance between models that use the image features and models that do no use these features is not significantly different. As we discussed in Section SECREF1 , this is similar to the issues vqa models faced initially due to the imbalanced nature of the dataset, which leads us to believe that language is a strong prior on the visdial dataset too. BIBREF20 train two separate agents to play a cooperative game where one agent has to answer the other agent's questions, which in turn has to predict the fc7 features of the Image obtained from VGGNet. Both agents are based on hred models and they show that agents fine-tuned with rl outperform agents trained solely with supervised learning. BIBREF18 train both generative and discriminative deep neural models on the igc dataset, where the task is to generate questions and answers to carry on a meaningful conversation. BIBREF19 train hred-based models on GuessWhat?! dataset in which agents have to play a guessing game where one player has to find an object in the picture which the other player knows about and can answer questions about them..Moving from image-based dialogue to video-based dialogue adds further complexity and challenges. Limited availability of such data is one of the challenges. Apart from the avsd dataset, there does not exist a video dialogue dataset to the best of our knowledge and the avsd data itself is fairly limited in size. Extracting relevant features from videos also contains the inherent complexity of extracting features from individual frames and additionally requires understanding their temporal interaction. The temporal nature of videos also makes it important to be able to focus on a varying-length subset of video frames as the action which is being asked about might be happening within them. There is also the need to encode the additional modality of audio which would be required for answering questions that rely on the audio track. With limited size of publicly available datasets based on the visual modality, learning useful features from high dimensional visual data has been a challenge even for the visdial dataset, and we anticipate this to be an even more significant challenge on the avsd dataset as it involves videos..On the avsd task, BIBREF11 train an attention-based audio-visual scene-aware dialogue model which we use as the baseline model for this paper. They divide each video into multiple equal-duration segments and, from each of them, extract video features using an I3D BIBREF21 model, and audio features using a VGGish BIBREF22 model. The I3D model was pre-trained on Kinetics BIBREF23 dataset and the VGGish model was pre-trained on Audio Set BIBREF24 . The baseline encodes the current utterance's question with a lstm BIBREF25 and uses the encoding to attend to the audio and video features from all the video segments and to fuse them together. The dialogue history is modelled with a hierarchical recurrent lstm encoder where the input to the lower level encoder is a concatenation of question-answer pairs. The fused feature representation is concatenated with the question encoding and the dialogue history encoding and the resulting vector is used to decode the current answer using an lstm decoder. Similar to the visdial models, the performance difference between the best model that uses text and the best model that uses both text and video features is small. This indicates that the language is a stronger prior here and the baseline model is unable to make good use of the highly relevant video..Automated evaluation of both task-oriented and non-task-oriented dialogue systems has been a challenge BIBREF26 , BIBREF27 too. Most such dialogue systems are evaluated using per-turn evaluation metrics since there is no suitable per-dialogue metric as conversations do not need to happen in a deterministic ordering of turns. These per-turn evaluation metrics are mostly word-overlap-based metrics such as BLEU, METEOR, ROUGE, and CIDEr, borrowed from the machine translation literature. Due to the diverse nature of possible responses, world-overlap metrics are not highly suitable for evaluating these tasks. Human evaluation of generated responses is considered the most reliable metric for such tasks but it is cost prohibitive and hence the dialogue system literature continues to rely widely on word-overlap-based metrics. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "1\n",
      "Question 1: What is the avsd dataset and how is it collected?\n",
      "Answer 1: The avsd dataset consists of dialogues collected via amt and is associated with a video from the Charades dataset. Each dialogue turn consists of a question and answer pair, with one amt worker assuming the role of questioner and the other amt worker assuming the role of answerer. After 10 such QA turns, the questioner wraps up by writing a summary of the video based on the conversation.\n",
      "Question : for the text The avsd dataset BIBREF28 consists of dialogues collected via amt. Each dialogue is associated with a video from the Charades BIBREF29 dataset and has conversations between two amt workers related to the video. The Charades dataset has multi-action short videos and it provides text descriptions for these videos, which the avsd challenge also distributes as the caption. The avsd dataset has been collected using similar methodology as the visdial dataset. In avsd, each dialogue turn consists of a question and answer pair. One of the amt workers assumes the role of questioner while the other amt worker assumes the role of answerer. The questioner sees three static frames from the video and has to ask questions. The answerer sees the video and answers the questions asked by the questioner. After 10 such qa turns, the questioner wraps up by writing a summary of the video based on the conversation..Dataset statistics such as the number of dialogues, turns, and words for the avsd dataset are presented in Table TABREF5 . For the initially released prototype dataset, the training set of the avsd dataset corresponds to videos taken from the training set of the Charades dataset while the validation and test sets of the avsd dataset correspond to videos taken from the validation set of the Charades dataset. For the official dataset, training, validation and test sets are drawn from the corresponding Charades sets..The Charades dataset also provides additional annotations for the videos such as action, scene, and object annotations, which are considered to be external data sources by the avsd challenge, for which there is a special sub-task in the challenge. The action annotations also include the start and end time of the action in the video. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "2\n",
      "Question 1: What is the input to the utterance-level encoder?\n",
      "\n",
      "Answer 1: The input to the utterance-level encoder is word embeddings for each word in the utterance, concatenated with a special symbol <eos> marking the end of the sequence. The word embeddings are initialized using 300-dimensional GloVe BIBREF30 and fine-tuned during training. For words not present in the GloVe vocabulary, their word embeddings are initialized from a random uniform distribution.\n",
      "Question : for the text The utterance-level encoder is a recurrent neural network consisting of a single layer of lstm cells. The input to the lstm are word embeddings for each word in the utterance. The utterance is concatenated with a special symbol <eos> marking the end of the sequence. We initialize our word embeddings using 300-dimensional GloVe BIBREF30 and then fine-tune them during training. For words not present in the GloVe vocabulary, we initialize their word embeddings from a random uniform distribution. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "3\n",
      "What is the purpose of the FiLM blocks in the video encoder?\n",
      "The FiLM blocks apply a conditional feature-wise affine transformation on the features input to it, leading to the extraction of more relevant features.\n",
      "Question : for the text For the video encoder, we use an I3D model pre-trained on the Kinetics dataset BIBREF23 and extract the output of its Mixed_7c layer for INLINEFORM0 (30 for our models) equi-distant segments of the video. Over these features, we add INLINEFORM1 (2 for our models) FiLM BIBREF31 blocks which have been highly successful in visual reasoning problems. Each FiLM block applies a conditional (on the utterance encoding) feature-wise affine transformation on the features input to it, ultimately leading to the extraction of more relevant features. The FiLM blocks are followed by fully connected layers which are further encoded by a single layer recurrent lstm network. The last hidden state of the utterance-level encoder then generates an attention map over the hidden states of its lstm, which is multiplied by the hidden states to provide the output of this module. We also experimented with using convolutional Mixed_5c features to capture spatial information but on the limited avsd dataset they did not yield any improvement. When not using the FiLM blocks, we use the final layer I3D features (provided by the avsd organizers) and encode them with the lstm directly, followed by the attention step. We present the video encoder in Figure FIGREF3 . generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "4\n",
      "Question 1: What traditional classification techniques were used for stage 1, and which one was chosen as the classification technique?\n",
      "\n",
      "Answer 1: The two traditional classification techniques used for stage 1 were SVM and NB. SVM outperformed NB by around INLINEFORM1 and became the choice of classification technique for stage one.\n",
      "Question : for the text The results of 10-fold cross-validation performed for stage one are shown in table TABREF20 , under the label “Stage 1”. In table TABREF20 , For “Stage 1” of classification, F-score obtained using SVM classifier is INLINEFORM0 as shown in row 2, column 2. We also provide the system with sample tweets in real time and assess its ability to detect the emergency, and classify it accordingly. The classification training for Stage 1 was performed using two traditional classification techniques SVM and NB. SVM outperformed NB by around INLINEFORM1 and became the choice of classification technique for stage one..Some false positives obtained during manual evaluation are, “I am sooooo so drunk right nowwwwwwww” and “fire in my office , the boss is angry”. These occurrences show the need of more labeled gold data for our classifiers, and some other features, like Part-of-Speech tags, Named Entity recognition, Bigrams, Trigrams etc. to perform better..The results of 10-fold cross-validation performed for stage two classfication model are also shown in table TABREF20 , under the label “Stage 2”. The training for stage two was also performed using both SVM and NB, but NB outperformed SVM by around INLINEFORM0 to become a choice for stage two classification model..We also perform attribute evaluation for the classification model, and create a word cloud based on the output values, shown in figure FIGREF24 . It shows that our classifier model is trained on appropriate words, which are very close to the emergency situations viz. “fire”, “earthquake”, “accident”, “break” (Unigram representation here, but possibly occurs in a bigram phrase with “fire”) etc. In figure FIGREF24 , the word cloud represents the word “respond” as the most frequently occurring word as people need urgent help, and quick response from the assistance teams. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "5\n",
      "What type of emergencies can Civique detect and visualize?\n",
      "Civique can detect and visualize a range of urban emergencies including earthquakes, cyclones, fire breakouts, accidents, and other related events through the data collected from Twitter.\n",
      "Question : for the text Civique is a system which detects urban emergencies like earthquakes, cyclones, fire break out, accidents etc. and visualizes them on both on a browsable web interface and an Android application. We collect data from the popular micro-blogging site Twitter and use language processing modules to sanitize the input. We use this data as input to train a two step classification system, which indicates whether a tweet is related to an emergency or not, and if it is, then what category of emergency it belongs to. We display such positively classified tweets along with their type and location on a Google map, and notify our users to inform the concerned authorities, and possibly evacuate the area, if his location matches the affected area. We believe such a system can help the disaster management machinery, and government bodies like Fire department, Police department, etc., to act swiftly, thus minimizing the loss of life..Twitter users use slang, profanity, misspellings and neologisms. We, use standard cleaning methods, and combine NLP with Machine Learning (ML) to further our cause of tweet classification. At the current stage, we also have an Android application ready for our system, which shows the improvised, mobile-viewable web interface..In the future, we aim to develop detection of emergency categories on the fly, obscure emergencies like “airplane hijacking” should also be detected by our system. We plan to analyze the temporal sequence of the tweet set from a single location to determine whether multiple problems on the same location are the result of a single event, or relate to multiple events. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "6\n",
      "Question 1: How many tweets were manually labeled with the emergency label in the first stage of data collection?\n",
      "\n",
      "Answer 1: 1313 tweets were manually labeled with the emergency label in the first stage of data collection.\n",
      "Question : for the text We collect data by using the Twitter API for saved data, available for public use. For our experiments we collect 3200 tweets filtered by keywords like “fire”, “earthquake”, “theft”, “robbery”, “drunk driving”, “drunk driving accident” etc. Later, we manually label tweets with <emergency>and <non-emergency>labels for classification as stage one. Our dataset contains 1313 tweet with positive label <emergency>and 1887 tweets with a negative label <non-emergency>. We create another dataset with the positively labeled tweets and provide them with category labels like “fire”, “accident”, “earthquake” etc.. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "7\n",
      "Question 1: What is the purpose of the Tweet Detection and Classification showcase in Civique?\n",
      "\n",
      "Answer 1: The purpose of the Tweet Detection and Classification showcase in Civique is to demonstrate how the system can detect related tweets and classify them into appropriate categories using a list of filter words. This allows users to see how the system captures emergency tweets and notifies them through the web-based user interface and Android application interface.\n",
      "Question : for the text Users interact with Civique through its Web-based user interface and Android based application interface. The features underlying Civique are demonstrated through the following two show cases:.Show case 1: Tweet Detection and Classification.This showcase aims at detecting related tweets, and classifying them into appropriate categories. For this, we have created a list of filter words, which are used to filter tweets from the Twitter streaming API. These set of words help us filter the tweets related to any incident. We will tweet, and users are able to see how our system captures such tweets and classifies them. Users should be able to see the tweet emerge as an incident on the web-interface, as shown in figure FIGREF26 and the on the android application, as shown in figure FIGREF27 . Figure FIGREF27 demonstrates how a notification is generated when our system detects an emergency tweet. When a user clicks the emerged spot, the system should be able to display the sanitized version / extracted spatio-temporal data from the tweet. We test the system in a realtime environment, and validate our experiments. We also report the false positives generated during the process in section SECREF25 above..Show case 2: User Notification and Contact Info..Civique includes a set of local contacts for civic authorities who are to be / who can be contacted in case of various emergencies. Users can see how Civique detects an emergency and classifies it. They can also watch how the system generates a notification on the web interface and the Android interface, requesting them to contact the authorities for emergencies. Users can change their preferences on the mobile device anytime and can also opt not to receive notifications. Users should be able to contact the authorities online using the application, but in case the online contact is not responsive, or in case of a sudden loss of connectivity, we provide the user with the offline contact information of the concerned civic authorities along with the notifications. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "8\n",
      "Question 1: What classification model did they choose for stage one of their classification process?\n",
      "\n",
      "Answer 1: They chose SVM for stage one of their classification process due to better F-score compared to using NB.\n",
      "Question : for the text The first classifier model acts as a filter for the second stage of classification. We use both SVM and NB to compare the results and choose SVM later for stage one classification model, owing to a better F-score. The training is performed on tweets labeled with classes <emergency>, and <non-emergency> based on unigrams as features. We create word vectors of strings in the tweet using a filter available in the WEKA API BIBREF9 , and perform cross validation using standard classification techniques. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "9\n",
      "Question 1: What evaluation techniques were used for the system?\n",
      "Answer 1: The system was evaluated using both automated and manual evaluation techniques, and 10-fold cross validation was performed to obtain the F-scores for the classification systems.\n",
      "Question : for the text We evaluate our system using automated, and manual evaluation techniques. We perform 10-fold cross validation to obtain the F-scores for our classification systems. We use the following technique for dataset creation. We test the system in realtime environments, and tweet about fires at random locations in our city, using test accounts. Our system was able to detect such tweets and detect them with locations shown on the map. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "10\n",
      "What is the focus of the system described in the text?\n",
      "\n",
      "The focus of the system described in the text is on detecting urban emergencies as events from Twitter messages and classifying them into appropriate categories, as well as providing notifications to users containing the contacts of specifically concerned authorities.\n",
      "Question : for the text With the surge in the use of social media, micro-blogging sites like Twitter, Facebook, and Foursquare have become household words. Growing ubiquity of mobile phones in highly populated developing nations has spurred an exponential rise in social media usage. The heavy volume of social media posts tagged with users' location information on micro-blogging website Twitter presents a unique opportunity to scan these posts. These Short texts (e.g. \"tweets\") on social media contain information about various events happening around the globe, as people post about events and incidents alike. Conventional web outlets provide emergency phone numbers (i.e. 100, 911), etc., and are fast and accurate. Our system, on the other hand, connects its users through a relatively newer platform i.e. social media, and provides an alternative to these conventional methods. In case of their failure or when such means are busy/occupied, an alternative could prove to be life saving..These real life events are reported on Twitter with different perspectives, opinions, and sentiment. Every day, people discuss events thousands of times across social media sites. We would like to detect such events in case of an emergency. Some previous studies BIBREF0 investigate the use of features such as keywords in the tweet, number of words, and context to devise a classifier for event detection. BIBREF1 discusses various techniques researchers have used previously to detect events from Twitter. BIBREF2 describe a system to automatically detect events about known entities from Twitter. This work is highly specific to detection of events only related to known entities. BIBREF3 discuss a system that returns a ranked list of relevant events given a user query..Several research efforts have focused on identifying events in real time( BIBREF4 BIBREF5 BIBREF6 BIBREF0 ). These include systems to detect emergent topics from Twitter in real time ( BIBREF4 BIBREF7 ), an online clustering technique for identifying tweets in real time BIBREF5 , a system to detect localized events and also track evolution of such events over a period of time BIBREF6 . Our focus is on detecting urban emergencies as events from Twitter messages. We classify events ranging from natural disasters to fire break outs, and accidents. Our system detects whether a tweet, which contains a keyword from a pre-decided list, is related to an actual emergency or not. It also classifies the event into its appropriate category, and visualizes the possible location of the emergency event on the map. We also support notifications to our users, containing the contacts of specifically concerned authorities, as per the category of their tweet..The rest of the paper is as follows: Section SECREF2 provides the motivation for our work, and the challenges in building such a system. Section SECREF3 describes the step by step details of our work, and its results. We evaluate our system and present the results in Section SECREF4 . Section SECREF5 showcases our demonstrations in detail, and Section SECREF6 concludes the paper by briefly describing the overall contribution, implementation and demonstration. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "11\n",
      "Question 1: How does the system display the possible location of a tweet?\n",
      "\n",
      "Answer 1: The system uses Google Maps Geocoding API to display the possible location of the tweet origin based on longitude and latitude. The visualizer presents the user with a map and pinpoints the location with custom icons for earthquake, cyclone, fire accident, and other emergencies.\n",
      "Question : for the text We use Google Maps Geocoding API to display the possible location of the tweet origin based on longitude and latitude. Our visualizer presents the user with a map and pinpoints the location with custom icons for earthquake, cyclone, fire accident etc. Since we currently collect tweets with a location filter for the city of \"Mumbai\", we display its map location on the interface. The possible occurrences of such incidents are displayed on the map as soon as our system is able to detect it..We also display the same on an Android device using the WebView functionality available to developers, thus solving the issue of portability. Our system displays visualization of the various emergencies detected on both web browsers and mobile devices. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "12\n",
      "What motivated the development of a social media system to minimize losses during natural disasters in India?\n",
      "\n",
      "Answer 1: The development of a social media system to minimize losses during natural disasters in India was motivated by the high percentage of deaths caused by accidents and accidental fires, as well as the frequent earthquakes in the region. Additionally, the accessibility of social media through mobile devices and the potential to quickly connect affected individuals with authorities were considered important factors.\n",
      "Question : for the text In 2015, INLINEFORM0 of all unnatural deaths in India were caused by accidents, and INLINEFORM1 by accidental fires. Moreover, the Indian subcontinent suffered seven earthquakes in 2015, with the recent Nepal earthquake alone killing more than 9000 people and injuring INLINEFORM2 . We believe we can harness the current social media activity on the web to minimize losses by quickly connecting affected people and the concerned authorities. Our work is motivated by the following factors, (a) Social media is very accessible in the current scenario. (The “Digital India” initiative by the Government of India promotes internet activity, and thus a pro-active social media.) (b) As per the Internet trends reported in 2014, about 117 million Indians are connected to the Internet through mobile devices. (c) A system such as ours can point out or visualize the affected areas precisely and help inform the authorities in a timely fashion. (d) Such a system can be used on a global scale to reduce the effect of natural calamities and prevent loss of life..There are several challenges in building such an application: (a) Such a system expects a tweet to be location tagged. Otherwise, event detection techniques to extract the spatio-temporal data from the tweet can be vague, and lead to false alarms. (b) Such a system should also be able to verify the user's credibility as pranksters may raise false alarms. (c) Tweets are usually written in a very informal language, which requires a sophisticated language processing component to sanitize the tweet input before event detection. (d) A channel with the concerned authorities should be established for them to take serious action, on alarms raised by such a system. (e) An urban emergency such as a natural disaster could affect communications severely, in case of an earthquake or a cyclone, communications channels like Internet connectivity may get disrupted easily. In such cases, our system may not be of help, as it requires the user to be connected to the internet. We address the above challenges and present our approach in the next section. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "13\n",
      "Question 1: What techniques are used for training the classifiers in the proposed software architecture for Emergency detection and visualization?\n",
      "\n",
      "Answer 1: Traditional classification techniques such as Support Vector Machines(SVM), and Naive Bayes(NB) are used for training the classifiers. The classifiers are trained using manually labeled data and 10-fold cross validation is performed to obtain f-scores.\n",
      "Question : for the text We propose a software architecture for Emergency detection and visualization as shown in figure FIGREF9 . We collect data using Twitter API, and perform language pre-processing before applying a classification model. Tweets are labelled manually with <emergency>and <non-emergency>labels, and later classified manually to provide labels according to the type of emergency they indicate. We use the manually labeled data for training our classifiers..We use traditional classification techniques such as Support Vector Machines(SVM), and Naive Bayes(NB) for training, and perform 10-fold cross validation to obtain f-scores. Later, in real time, our system uses the Twitter streaming APIs to get data, pre-processes it using the same modules, and detects emergencies using the classifiers built above. The tweets related to emergencies are displayed on the web interface along with the location and information for the concerned authorities. The pre-processing of Twitter data obtained is needed as it usually contains ad-hoc abbreviations, phonetic substitutions, URLs, hashtags, and a lot of misspelled words. We use the following language processing modules for such corrections. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "14\n",
      "What is text normalization and how is it implemented in the system?\n",
      "\n",
      "Text normalization is the process of translating ad-hoc abbreviations, typographical errors, phonetic substitutions, and ungrammatical structures used in text messaging to plain English. In the system, the normalization module implemented by BIBREF8 is used for text normalization. The training process requires a Language Model of the target language and a parallel corpora containing aligned un-normalized and normalized word pairs. The system's language model consists of 15,000 English words taken from various sources on the web, and the parallel corpora was collected from various sources including Stanford Normalization Corpora and crowd-sourcing.\n",
      "Question : for the text We implement a cleaning module to automate the cleaning of tweets obtained from the Twitter API. We remove URLs, special symbols like @ along with the user mentions, Hashtags and any associated text. We also replace special symbols by blank spaces, and inculcate the module as shown in figure FIGREF9 ..An example of such a sample tweet cleaning is shown in table TABREF10 ..While tweeting, users often express their emotions by stressing over a few characters in the word. For example, usage of words like hellpppp, fiiiiiireeee, ruuuuunnnnn, druuuuuunnnkkk, soooooooo actually corresponds to help, fire, run, drunk, so etc. We use the compression module implemented by BIBREF8 for converting terms like “pleeeeeeeaaaaaassseeee” to “please”..It is unlikely for an English word to contain the same character consecutively for three or more times. We, hence, compress all the repeated windows of character length greater than two, to two characters. For example “pleeeeeaaaassee” is converted to “pleeaassee”. Each window now contains two characters of the same alphabet in cases of repetition. Let n be the number of windows, obtained from the previous step. We, then, apply brute force search over INLINEFORM0 possibilities to select a valid dictionary word..Table TABREF13 contains sanitized sample output from our compression module for further processing..Text Normalization is the process of translating ad-hoc abbreviations, typographical errors, phonetic substitution and ungrammatical structures used in text messaging (Tweets and SMS) to plain English. Use of such language (often referred as Chatting Language) induces noise which poses additional processing challenges..We use the normalization module implemented by BIBREF8 for text normalization. Training process requires a Language Model of the target language and a parallel corpora containing aligned un-normalized and normalized word pairs. Our language model consists of 15000 English words taken from various sources on the web..Parallel corpora was collected from the following sources:.Stanford Normalization Corpora which consists of 9122 pairs of un-normalized and normalized words / phrases..The above corpora, however, lacked acronyms and short hand texts like 2mrw, l8r, b4, hlp, flor which are frequently used in chatting. We collected 215 pairs un-normalized to normalized word/phrase mappings via crowd-sourcing..Table TABREF16 contains input and normalized output from our module..Users often make spelling mistakes while tweeting. A spell checker makes sure that a valid English word is sent to the classification system. We take this problem into account by introducing a spell checker as a pre-processing module by using the JAVA API of Jazzy spell checker for handling spelling mistakes..An example of correction provided by the Spell Checker module is given below:-.Input: building INLINEFORM0 flor, help.Output: building INLINEFORM0 floor, help.Please note that, our current system performs compression, normalization and spell-checking if the language used is English. The classifier training and detection process are described below. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "15\n",
      "Question 1: What type of technique is used as the second stage classification mechanism in the text?\n",
      "\n",
      "Answer 1: The second stage classification mechanism in the text employs a multi-class Naive Bayes classifier.\n",
      "Question : for the text We employ a multi-class Naive Bayes classifier as the second stage classification mechanism, for categorizing tweets appropriately, depending on the type of emergencies they indicate. This multi-class classifier is trained on data manually labeled with classes. We tokenize the training data using “NgramTokenizer” and then, apply a filter to create word vectors of strings before training. We use “trigrams” as features to build a model which, later, classifies tweets into appropriate categories, in real time. We then perform cross validation using standard techniques to calculate the results, which are shown under the label “Stage 2”, in table TABREF20 . generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "16\n",
      "What organizations provided financial support for this work?\n",
      "\n",
      "The JST AIP-PRISM Grant Number JPMJCR18Y1 and JSPS KAKENHI Grant Number JP18H03284 provided financial support for this work.\n",
      "Question : for the text This work was partially supported by JST AIP- PRISM Grant Number JPMJCR18Y1, Japan, and JSPS KAKENHI Grant Number JP18H03284, Japan. We thank our three anonymous reviewers for helpful suggestions. We are also grateful to Koki Washio, Masashi Yoshikawa, and Thomas McLachlan for helpful discussion. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "17\n",
      "What is the most common predicted label for the dataset?\n",
      "\n",
      "The most common predicted label for the dataset is non-entailment, which was determined based on regarding the prediction label contradiction as non-entailment.\n",
      "Question : for the text To test the difficulty of our dataset, we checked the majority class label and the accuracies of five state-of-the-art NLI models adopting different approaches: BiMPM (Bilateral Multi-Perspective Matching Model; BIBREF31 , BIBREF31 ), ESIM (Enhanced Sequential Inference Model; BIBREF32 , BIBREF32 ), Decomposable Attention Model BIBREF33 , KIM (Knowledge-based Inference Model; BIBREF34 , BIBREF34 ), and BERT (Bidirectional Encoder Representations from Transformers model; BIBREF35 , BIBREF35 ). Regarding BERT, we checked the performance of a model pretrained on Wikipedia and BookCorpus for language modeling and trained with SNLI and MultiNLI. For other models, we checked the performance trained with SNLI. In agreement with our dataset, we regarded the prediction label contradiction as non-entailment..Table 6 shows that the accuracies of all models were better on upward inferences, in accordance with the reported results of the GLUE leaderboard. The overall accuracy of each model was low. In particular, all models underperformed the majority baseline on downward inferences, despite some models having rich lexical knowledge from a knowledge base (KIM) or pretraining (BERT). This indicates that downward inferences are difficult to perform even with the expansion of lexical knowledge. In addition, it is interesting to see that if a model performed better on upward inferences, it performed worse on downward inferences. We will investigate these results in detail below. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "18\n",
      "What is the relationship between accuracy on upward and downward inferences in NLI models tested on the MED dataset?\n",
      "\n",
      "Answer 1: The accuracy on downward inferences was found to be inversely proportional to the accuracy on upward inferences for all state-of-the-art NLI models tested on the MED dataset.\n",
      "Question : for the text We introduced a large monotonicity entailment dataset, called MED. To illustrate the usefulness of MED, we tested state-of-the-art NLI models, and found that performance on the new test set was substantially worse for all state-of-the-art NLI models. In addition, the accuracy on downward inferences was inversely proportional to the one on upward inferences..An experiment with the data augmentation technique showed that accuracy on upward and downward inferences depends on the proportion of upward and downward inferences in the training set. This indicates that current neural models might have limitations on their generalization ability in monotonicity reasoning. We hope that the MED will be valuable for future research on more advanced models that are capable of monotonicity reasoning in a proper way. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "19\n",
      "What is HELP?\n",
      "HELP is an automatically generated monotonicity dataset containing 36K monotonicity inference examples, which includes upward examples, downward examples, and non-monotone examples.\n",
      "Question : for the text To explore whether the performance of models on monotonicity reasoning depends on the training set or the model themselves, we conducted further analysis performed by data augmentation with the automatically generated monotonicity dataset HELP BIBREF11 . HELP contains 36K monotonicity inference examples (7,784 upward examples, 21,192 downward examples, and 1,105 non-monotone examples). The size of the HELP word vocabulary is 15K, and the overlap ratio of vocabulary between HELP and MED is 15.2%..We trained BERT on MultiNLI only and on MultiNLI augmented with HELP, and compared their performance. Following BIBREF3 , we also checked the performance of a hypothesis-only model trained with each training set to test whether our test set contains undesired biases..Table 7 shows that the performance of BERT with the hypothesis-only training set dropped around 10-40% as compared with the one with the premise-hypothesis training set, even if we use the data augmentation technique. This indicates that the MED test set does not allow models to predict from hypotheses alone. Data augmentation by HELP improved the overall accuracy to 71.6%, but there is still room for improvement. In addition, while adding HELP increased the accuracy on downward inferences, it slightly decreased accuracy on upward inferences. The size of downward examples in HELP is much larger than that of upward examples. This might improve accuracy on downward inferences, but might decrease accuracy on upward inferences..To investigate the relationship between accuracy on upward inferences and downward inferences, we checked the performance throughout training BERT with only upward and downward inference examples in HELP (Figure 2 (i), (ii)). These two figures show that, as the size of the upward training set increased, BERT performed better on upward inferences but worse on downward inferences, and vice versa..Figure 2 (iii) shows performance on a different ratio of upward and downward inference training sets. When downward inference examples constitute more than half of the training set, accuracies on upward and downward inferences were reversed. As the ratio of downward inferences increased, BERT performed much worse on upward inferences. This indicates that a training set in one direction (upward or downward entailing) of monotonicity might be harmful to models when learning the opposite direction of monotonicity..Previous work using HELP BIBREF11 reported that the BERT trained with MultiNLI and HELP containing both upward and downward inferences improved accuracy on both directions of monotonicity. MultiNLI rarely comes from downward inferences (see Section \"Discussion\" ), and its size is large enough to be immune to the side-effects of downward inference examples in HELP. This indicates that MultiNLI might act as a buffer against side-effects of the monotonicity-driven data augmentation technique..Table 8 shows the evaluation results by genre. This result shows that inference problems collected from linguistics publications are more challenging than crowdsourced inference problems, even if we add HELP to training sets. As shown in Figure 2 , the change in performance on problems from linguistics publications is milder than that on problems from crowdsourcing. This result also indicates the difficulty of problems from linguistics publications. Regarding non-monotone problems collected via crowdsourcing, there are very few non-monotone problems, so accuracy is 100%. Adding non-monotone problems to our test set is left for future work..Table 9 shows the evaluation results by type of linguistic phenomenon. While accuracy on problems involving NPIs and conditionals was improved on both upward and downward inferences, accuracy on problems involving conjunction and disjunction was improved on only one direction. In addition, it is interesting to see that the change in accuracy on conjunction was opposite to that on disjunction. Downward inference examples involving disjunction are similar to upward inference ones; that is, inferences from a sentence to a shorter sentence are valid (e.g., Not many campers have had a sunburn or caught a cold $\\Rightarrow $ Not many campers have caught a cold). Thus, these results were also caused by addition of downward inference examples. Also, accuracy on problems annotated with reverse tags was apparently better without HELP because all examples are upward inferences embedded in a downward environment twice..Table 9 also shows that accuracy on conditionals was better on upward inferences than that on downward inferences. This indicates that BERT might fail to capture the monotonicity property that conditionals create a downward entailing context in their scope while they create an upward entailing context out of their scope..Regarding lexical knowledge, the data augmentation technique improved the performance much better on downward inferences which do not require lexical knowledge. However, among the 394 problems for which all models provided wrong answers, 244 problems are non-lexical inference problems. This indicates that some non-lexical inference problems are more difficult than lexical inference problems, though accuracy on non-lexical inference problems was better than that on lexical inference problems. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "20\n",
      "What is one possible reason why there are few downward inferences in naturally-occurring texts?\n",
      "One possible reason is certain pragmatic factors that can block people from drawing a downward inference, such as the salience of added disjuncts in the context.\n",
      "Question : for the text One of our findings is that there is a type of downward inferences to which every model fails to provide correct answers. One such example is concerned with the contrast between few and a few. Among 394 problems for which all models provided wrong answers, 148 downward inference problems were problems involving the downward monotonicity operator few such as in the following example:. $P$ : Few of the books had typical or marginal readers $H$ : Few of the books had some typical readers We transformed these downward inference problems to upward inference problems in two ways: (i) by replacing the downward operator few with the upward operator a few, and (ii) by removing the downward operator few. We tested BERT using these transformed test sets. The results showed that BERT predicted the same answers for the transformed test sets. This suggests that BERT does not understand the difference between the downward operator few and the upward operator a few..The results of crowdsourcing tasks in Section 3.1.3 showed that some downward inferences can naturally be performed in human reasoning. However, we also found that the MultiNLI training set BIBREF10 , which is one of the dataset created from naturally-occurring texts, contains only 77 downward inference problems, including the following one.. $P$ : No racin' on the Range $H$ : No horse racing is allowed on the Range .One possible reason why there are few downward inferences is that certain pragmatic factors can block people to draw a downward inference. For instance, in the case of the inference problem in ( \"Discussion\" ), unless the added disjunct in $H$ , i.e., a small cat with green eyes, is salient in the context, it would be difficult to draw the conclusion $H$ from the premise $P$ .. $P$ : I saw a dog $H$ : I saw a dog or a small cat with green eyes .Such pragmatic factors would be one of the reasons why it is difficult to obtain downward inferences in naturally occurring texts. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "21\n",
      "Question 1: How were the tasks for creating a monotonicity test set performed via crowdsourcing?\n",
      "\n",
      "Answer 1: The tasks for creating a monotonicity test set were performed via crowdsourcing in two steps. The first step was to create a hypothesis by making some polarized part of an original sentence more specific. The second step was to annotate an entailment label for the premise-hypothesis pair generated in the first step. Workers were asked to rewrite only a relatively short sentence by replacing the underlined part with more specific phrases in three different ways. The gold label of each premise-hypothesis pair created in the first step was automatically determined by monotonicity calculus. Workers were paid for each set of substitutions and each question, and each set and question was assigned to three workers.\n",
      "Question : for the text To create monotonicity inference problems, we should satisfy three requirements: (a) detect the monotonicity operators and their arguments; (b) based on the syntactic structure, induce the polarity of the argument positions; and (c) replace the phrase in the argument position with a more general or specific phrase in natural and various ways (e.g., by using lexical knowledge or logical connectives). For (a) and (b), we first conduct polarity computation on a syntactic structure for each sentence, and then select premises involving upward/downward expressions..For (c), we use crowdsourcing to narrow or broaden the arguments. The motivation for using crowdsourcing is to collect naturally alike monotonicity inference problems that include various expressions. One problem here is that it is unclear how to instruct workers to create monotonicity inference problems without knowledge of natural language syntax and semantics. We must make tasks simple for workers to comprehend and provide sound judgements. Moreover, recent studies BIBREF12 , BIBREF3 , BIBREF13 point out that previous crowdsourced datasets, such as SNLI BIBREF14 and MultiNLI BIBREF10 , include hidden biases. As these previous datasets are motivated by approximated entailments, workers are asked to freely write hypotheses given a premise, which does not strictly restrict them to creating logically complex inferences..Taking these concerns into consideration, we designed two-step tasks to be performed via crowdsourcing for creating a monotonicity test set; (i) a hypothesis creation task and (ii) a validation task. The task (i) is to create a hypothesis by making some polarized part of an original sentence more specific. Instead of writing a complete sentence from scratch, workers are asked to rewrite only a relatively short sentence. By restricting workers to rewrite only a polarized part, we can effectively collect monotonicity inference examples. The task (ii) is to annotate an entailment label for the premise-hypothesis pair generated in (i). Figure 1 summarizes the overview of our human-oriented dataset creation. We used the crowdsourcing platform Figure Eight for both tasks..As a resource, we use declarative sentences with more than five tokens from the Parallel Meaning Bank (PMB) BIBREF15 . The PMB contains syntactically correct sentences annotated with its syntactic category in Combinatory Categorial Grammar (CCG; BIBREF16 , BIBREF16 ) format, which is suitable for our purpose. To get a whole CCG derivation tree, we parse each sentence by the state-of-the-art CCG parser, depccg BIBREF17 . Then, we add a polarity to every constituent of the CCG tree by the polarity computation system ccg2mono BIBREF18 and make the polarized part a blank field..We ran a trial rephrasing task on 500 examples and detected 17 expressions that were too general and thus difficult to rephrase them in a natural way (e.g., every one, no time). We removed examples involving such expressions. To collect more downward inference examples, we select examples involving determiners in Table 1 and downward operators in Table 2 . As a result, we selected 1,485 examples involving expressions having arguments with upward monotonicity and 1,982 examples involving expressions having arguments with downward monotonicity..We present crowdworkers with a sentence whose polarized part is underlined, and ask them to replace the underlined part with more specific phrases in three different ways. In the instructions, we showed examples rephrased in various ways: by adding modifiers, by adding conjunction phrases, and by replacing a word with its hyponyms..Workers were paid US$0.05 for each set of substitutions, and each set was assigned to three workers. To remove low-quality examples, we set the minimum time it should take to complete each set to 200 seconds. The entry in our task was restricted to workers from native speaking English countries. 128 workers contributed to the task, and we created 15,339 hypotheses (7,179 upward examples and 8,160 downward examples)..The gold label of each premise-hypothesis pair created in the previous task is automatically determined by monotonicity calculus. That is, a downward inference pair is labeled as entailment, while an upward inference pair is labeled as non-entailment..However, workers sometimes provided some ungrammatical or unnatural sentences such as the case where a rephrased phrase does not satisfy the selectional restrictions (e.g., original: Tom doesn't live in Boston, rephrased: Tom doesn't live in yes), making it difficult to judge their entailment relations. Thus, we performed an annotation task to ensure accurate labeling of gold labels. We asked workers about the entailment relation of each premise-hypothesis pair as well as how natural it is..Worker comprehension of an entailment relation directly affects the quality of inference problems. To avoid worker misunderstandings, we showed workers the following definitions of labels and five examples for each label:.entailment: the case where the hypothesis is true under any situation that the premise describes..non-entailment: the case where the hypothesis is not always true under a situation that the premise describes..unnatural: the case where either the premise and/or the hypothesis is ungrammatical or does not make sense..Workers were paid US$0.04 for each question, and each question was assigned to three workers. To collect high-quality annotation results, we imposed ten test questions on each worker, and removed workers who gave more than three wrong answers. We also set the minimum time it should take to complete each question to 200 seconds. 1,237 workers contributed to this task, and we annotated gold labels of 15,339 premise-hypothesis pairs..Table 3 shows the numbers of cases where answers matched gold labels automatically determined by monotonicity calculus. This table shows that there exist inference pairs whose labels are difficult even for humans to determine; there are 3,354 premise-hypothesis pairs whose gold labels as annotated by polarity computations match with those answered by all workers. We selected these naturalistic monotonicity inference pairs for the candidates of the final test set..To make the distribution of gold labels symmetric, we checked these pairs to determine if we can swap the premise and the hypothesis, reverse their gold labels, and create another monotonicity inference pair. In some cases, shown below, the gold label cannot be reversed if we swap the premise and the hypothesis..In ( UID15 ), child and kid are not hyponyms but synonyms, and the premise $P$ and the hypothesis $H$ are paraphrases.. $P$ : Tom is no longer a child $H$ : Tom is no longer a kid .These cases are not strict downward inference problems, in the sense that a phrase is not replaced by its hyponym/hypernym..Consider the example ( UID16 ).. $P$ : The moon has no atmosphere $H$ : The moon has no atmosphere, and the gravity force is too low .The hypothesis $H$ was created by asking workers to make atmosphere in the premise $P$ more specific. However, the additional phrase and the gravity force is too low does not form constituents with atmosphere. Thus, such examples are not strict downward monotone inferences..In such cases as (a) and (b), we do not swap the premise and the hypothesis. In the end, we collected 4,068 examples from crowdsourced datasets. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "22\n",
      "What is the purpose of the Natural Language Inference (NLI) task?\n",
      "The purpose of the NLI task is to determine whether a premise semantically entails a hypothesis, and it has been proposed as a benchmark task for natural language understanding.\n",
      "Question : for the text Natural language inference (NLI), also known as recognizing textual entailment (RTE), has been proposed as a benchmark task for natural language understanding. Given a premise $P$ and a hypothesis $H$ , the task is to determine whether the premise semantically entails the hypothesis BIBREF0 . A number of recent works attempt to test and analyze what type of inferences an NLI model may be performing, focusing on various types of lexical inferences BIBREF1 , BIBREF2 , BIBREF3 and logical inferences BIBREF4 , BIBREF5 ..Concerning logical inferences, monotonicity reasoning BIBREF6 , BIBREF7 , which is a type of reasoning based on word replacement, requires the ability to capture the interaction between lexical and syntactic structures. Consider examples in ( \"Introduction\" ) and ( \"Introduction\" )..All [ workers $\\leavevmode {\\color {blue!80!black}\\downarrow }$ ] [joined for a French dinner $\\leavevmode {\\color {red!80!black}\\uparrow }$ ] All workers joined for a dinner All new workers joined for a French dinner Not all [new workers $\\leavevmode {\\color {red!80!black}\\uparrow }$ ] joined for a dinner Not all workers joined for a dinner .A context is upward entailing (shown by [... $\\leavevmode {\\color {red!80!black}\\uparrow }$ ]) that allows an inference from ( \"Introduction\" ) to ( \"Introduction\" ), where French dinner is replaced by a more general concept dinner. On the other hand, a downward entailing context (shown by [... $\\leavevmode {\\color {blue!80!black}\\downarrow }$ ]) allows an inference from ( \"Introduction\" ) to ( \"Introduction\" ), where workers is replaced by a more specific concept new workers. Interestingly, the direction of monotonicity can be reversed again by embedding yet another downward entailing context (e.g., not in ( \"Introduction\" )), as witness the fact that ( \"Introduction\" ) entails ( \"Introduction\" ). To properly handle both directions of monotonicity, NLI models must detect monotonicity operators (e.g., all, not) and their arguments from the syntactic structure..For previous datasets containing monotonicity inference problems, FraCaS BIBREF8 and the GLUE diagnostic dataset BIBREF9 are manually-curated datasets for testing a wide range of linguistic phenomena. However, monotonicity problems are limited to very small sizes (FraCaS: 37/346 examples and GLUE: 93/1650 examples). The limited syntactic patterns and vocabularies in previous test sets are obstacles in accurately evaluating NLI models on monotonicity reasoning..To tackle this issue, we present a new evaluation dataset that covers a wide range of monotonicity reasoning that was created by crowdsourcing and collected from linguistics publications (Section \"Dataset\" ). Compared with manual or automatic construction, we can collect naturally-occurring examples by crowdsourcing and well-designed ones from linguistics publications. To enable the evaluation of skills required for monotonicity reasoning, we annotate each example in our dataset with linguistic tags associated with monotonicity reasoning..We measure the performance of state-of-the-art NLI models on monotonicity reasoning and investigate their generalization ability in upward and downward reasoning (Section \"Results and Discussion\" ). The results show that all models trained with SNLI BIBREF4 and MultiNLI BIBREF10 perform worse on downward inferences than on upward inferences..In addition, we analyzed the performance of models trained with an automatically created monotonicity dataset, HELP BIBREF11 . The analysis with monotonicity data augmentation shows that models tend to perform better in the same direction of monotonicity with the training set, while they perform worse in the opposite direction. This indicates that the accuracy on monotonicity reasoning depends solely on the majority direction in the training set, and models might lack the ability to capture the structural relations between monotonicity operators and their arguments. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "23\n",
      "What are the types of monotone reasoning used in the collected examples?\n",
      "\n",
      "The collected examples were reclassified into three types of monotone reasoning: upward, downward, and non-monotone, based on whether they included the target monotonicity operator in both the premise and the hypothesis and if they had phrase replacement in their argument position.\n",
      "Question : for the text We also collect monotonicity inference problems from previous manually curated datasets and linguistics publications. The motivation is that previous linguistics publications related to monotonicity reasoning are expected to contain well-designed inference problems, which might be challenging problems for NLI models..We collected 1,184 examples from 11 linguistics publications BIBREF19 , BIBREF20 , BIBREF21 , BIBREF22 , BIBREF23 , BIBREF24 , BIBREF25 , BIBREF26 , BIBREF27 , BIBREF28 , BIBREF29 . Regarding previous manually-curated datasets, we collected 93 examples for monotonicity reasoning from the GLUE diagnostic dataset, and 37 single-premise problems from FraCaS..Both the GLUE diagnostic dataset and FraCaS categorize problems by their types of monotonicity reasoning, but we found that each dataset has different classification criteria. Thus, following GLUE, we reclassified problems into three types of monotone reasoning (upward, downward, and non-monotone) by checking if they include (i) the target monotonicity operator in both the premise and the hypothesis and (ii) the phrase replacement in its argument position. In the GLUE diagnostic dataset, there are several problems whose gold labels are contradiction. We regard them as non-entailment in that the premise does not semantically entail the hypothesis. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "24\n",
      "What are some ways in which the term \"person\" can be made more specific in the given example?\n",
      "The term \"person\" can be made more specific in the given example by adding modifiers such as \"young\", replacing it with its hyponym \"spectator\", or adding conjunction such as \"person and alien\".\n",
      "Question : for the text As an example of a monotonicity inference, consider the example with the determiner every in ( \"Monotonicity\" ); here the premise $P$ entails the hypothesis $H$ .. $P$ : Every [ $_{\\scriptsize \\mathsf {NP}}$ person $\\leavevmode {\\color {blue!80!black}\\downarrow }$ ] [ $_{\\scriptsize \\mathsf {VP}}$ bought a movie ticket $\\leavevmode {\\color {red!80!black}\\uparrow }$ ] $H$ : Every young person bought a ticket .Every is downward entailing in the first argument ( $\\mathsf {NP}$ ) and upward entailing in the second argument ( $\\mathsf {VP}$ ), and thus the term person can be more specific by adding modifiers (person $\\sqsupseteq $ young person), replacing it with its hyponym (person $\\sqsupseteq $ spectator), or adding conjunction (person $\\sqsupseteq $ person and alien). On the other hand, the term buy a ticket can be more general by removing modifiers (bought a movie ticket $\\sqsubseteq $ bought a ticket), replacing it with its hypernym (bought a movie ticket $\\sqsubseteq $ bought a show ticket), or adding disjunction (bought a movie ticket $\\sqsubseteq $ bought or sold a movie ticket). Table 1 shows determiners modeled as binary operators and their polarities with respect to the first and second arguments..There are various types of downward operators, not limited to determiners (see Table 2 ). As shown in ( \"Monotonicity\" ), if a propositional object is embedded in a downward monotonic context (e.g., when), the polarity of words over its scope can be reversed.. $P$ : When [every [ $_{\\scriptsize \\mathsf {NP}}$ young person $\\leavevmode {\\color {red!80!black}\\uparrow }$ ] [ $_{\\scriptsize \\mathsf {VP}}$ bought a ticket $\\leavevmode {\\color {blue!80!black}\\downarrow }$ ]], [that shop was open] $H$ : When [every [ $_{\\scriptsize \\mathsf {NP}}$ person] [ $_{\\scriptsize \\mathsf {VP}}$ bought a movie ticket]], [that shop was open] .Thus, the polarity ( $\\leavevmode {\\color {red!80!black}\\uparrow }$ and $\\leavevmode {\\color {blue!80!black}\\downarrow }$ ), where the replacement with more general (specific) phrases licenses entailment, needs to be determined by the interaction of monotonicity properties and syntactic structures; polarity of each constituent is calculated based on a monotonicity operator of functional expressions (e.g., every, when) and their function-term relations. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "25\n",
      "Question 1: What is the MED dataset and how was it created?\n",
      "\n",
      "Answer 1: The MED dataset is a monotonicity entailment dataset that was created by merging a human-oriented dataset created via crowdsourcing and a linguistics-oriented dataset created from linguistics publications. The dataset contains various phrase replacements and includes 5,382 premise-hypothesis pairs. A set of annotation tags for linguistic phenomena was assigned to each example in the test set, which allows for the analysis of model performance on each linguistic phenomenon related to monotonicity reasoning.\n",
      "Question : for the text We merged the human-oriented dataset created via crowdsourcing and the linguistics-oriented dataset created from linguistics publications to create the current version of the monotonicity entailment dataset (MED). Table 4 shows some examples from the MED dataset. We can see that our dataset contains various phrase replacements (e.g., conjunction, relative clauses, and comparatives). Table 5 reports the statistics of the MED dataset, including 5,382 premise-hypothesis pairs (1,820 upward examples, 3,270 downward examples, and 292 non-monotone examples). Regarding non-monotone problems, gold labels are always non-entailment, whether a hypothesis is more specific or general than its premise, and thus almost all non-monotone problems are labeled as non-entailment. The size of the word vocabulary in the MED dataset is 4,023, and overlap ratios of vocabulary with previous standard NLI datasets is 95% with MultiNLI and 90% with SNLI..We assigned a set of annotation tags for linguistic phenomena to each example in the test set. These tags allow us to analyze how well models perform on each linguistic phenomenon related to monotonicity reasoning. We defined 6 tags (see Table 4 for examples):.lexical knowledge (2,073 examples): inference problems that require lexical relations (i.e., hypernyms, hyponyms, or synonyms).reverse (240 examples): inference problems where a propositional object is embedded in a downward environment more than once.conjunction (283 examples): inference problems that include the phrase replacement by adding conjunction (and) to the hypothesis.disjunction (254 examples): inference problems that include the phrase replacement by adding disjunction (or) to the hypothesis.conditionals (149 examples): inference problems that include conditionals (e.g., if, when, unless) in the hypothesis .negative polarity items (NPIs) (338 examples): inference problems that include NPIs (e.g., any, ever, at all, anything, anyone, anymore, anyhow, anywhere) in the hypothesis generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "26\n",
      "Question 1: What is the funding source for the research project mentioned?\n",
      "\n",
      "Answer 1: The funding source for the research project mentioned is the European Union's Horizon 2020 research and innovation programme under the Marie Sklodowska Curie grant agreement No 642667 (SECURE).\n",
      "Question : for the text We would like to acknowledge funding from the European Union's Horizon 2020 research and innovation programme under the Marie Sklodowska Curie grant agreement No 642667 (SECURE). generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "27\n",
      "Question 1: What are the two emotion taxonomies mentioned in the text?\n",
      "\n",
      "Answer 1: The two emotion taxonomies mentioned in the text are Discrete Emotion Categories (DEC) and Fine-grained Dimensional Basis of Emotion States (DBE).\n",
      "Question : for the text There are two emotion taxonomies: (1) discrete emotion categories (DEC) and (2) fined-grained dimensional basis of emotion states (DBE). The DECs are Joy, Sadness, Fear, Surprise, Disgust, Anger and Neutral; identified by Ekman et al. ekman1987universalemos. The DBE of the emotion is usually elicited from two or three dimensions BIBREF1, BIBREF11, BIBREF12. A two-dimensional model is commonly used with Valence and Arousal (also called activation), and in the three-dimensional model, the third dimension is Dominance. IEMOCAP is annotated with all DECs and two additional emotion classes, Frustration and Excited. IEMOCAP is also annotated with three DBE, that includes Valance, Arousal and Dominance BIBREF6. MELD BIBREF8, which is an evolved version of the Emotionlines dataset developed by BIBREF13, is annotated with exactly 7 DECs and sentiments (positive, negative and neutral). generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "28\n",
      "Question 1: What is the purpose of the Dialogue Act Markup in Several Layers (DAMSL) tag set? \n",
      "\n",
      "Answer 1: The purpose of the DAMSL tag set is to classify dialogue acts according to their forward-looking and backwards-looking functions, providing a fine-grained classification of utterances in conversation. It includes segmented-utterance labelling and distinguishes different types of questions based on their syntactic formulation and expected answers. The SwDA Corpus, which is annotated with the DAMSL tag set, has been used for reporting and benchmarking state-of-the-art results in dialogue act recognition tasks.\n",
      "Question : for the text There have been many taxonomies for dialogue acts: speech acts BIBREF14 refer to the utterance, not only to present information but to the action at is performed. Speech acts were later modified into five classes (Assertive, Directive, Commissive, Expressive, Declarative) BIBREF15. There are many such standard taxonomies and schemes to annotate conversational data, and most of them follow the discourse compositionality. These schemes have proven their importance for discourse or conversational analysis BIBREF16. During the increased development of dialogue systems and discourse analysis, the standard taxonomy was introduced in recent decades, called Dialogue Act Markup in Several Layers (DAMSL) tag set. According to DAMSL, each DA has a forward-looking function (such as Statement, Info-request, Thanking) and a backwards-looking function (such as Accept, Reject, Answer) BIBREF17..The DAMSL annotation includes not only the utterance-level but also segmented-utterance labelling. However, in the emotion datasets, the utterances are not segmented, as we can see in Figure FIGREF2 first or fourth utterances are not segmented as two separate. The fourth utterance, it could be segmented to have two dialogue act labels, for example, a statement (sd) and a question (qy). That provides very fine-grained DA classes and follows the concept of discourse compositionality. DAMSL distinguishes wh-question (qw), yes-no question (qy), open-ended (qo), and or-question (qr) classes, not just because these questions are syntactically distinct, but also because they have different forward functions BIBREF18. For example, yes-no question is more likely to get a “yes\" answer than a wh-question (qw). This also gives an intuition that the answers follow the syntactic formulation of question, providing a context. For example, qy is used for a question that, from a discourse perspective, expects a Yes (ny) or No (nn) answer..We have investigated the annotation method and trained our neural models with the Switchboard Dialogue Act (SwDA) Corpus BIBREF9, BIBREF10. SwDA Corpus is annotated with the DAMSL tag set and it is been used for reporting and bench-marking state-of-the-art results in dialogue act recognition tasks BIBREF19, BIBREF20, BIBREF21 which makes it ideal for our use case. The Switchboard DAMSL Coders Manual can be followed for knowing more about the dialogue act labels. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "29\n",
      "What is the percentage of labels that are taken based on context models in the annotation process?\n",
      "About 47% of the labels are taken based on the context models.\n",
      "Question : for the text First preference is given to the labels that are perfectly matching in all the neural annotators. In Table TABREF11, we can see that both datasets have about 40% of exactly matching labels over all models (AM). Then priority is given to the context-based models to check if the label in all context models is matching perfectly. In case two out of three context models are correct, then it is being checked if that label is also produced by at least one of the non-context models. Then, we allow labels to rely on these at least two context models. As a result, about 47% of the labels are taken based on the context models (CM). When we see that none of the context models is producing the same results, then we rank the labels with their respective confidence values produced as a probability distribution using the $softmax$ function. The labels are sorted in descending order according to confidence values. Then we check if the first three (case when one context model and both non-context models produce the same label) or at least two labels are matching, then we allow to pick that one. There are about 3% in IEMOCAP and 5% in MELD (BM)..Finally, when none the above conditions are fulfilled, we leave out the label with an unknown category. This unknown category of the dialogue act is labeled with `xx' in the final annotations, and they are about 7% in IEMOCAP and 11% in MELD (NM). The statistics of the EDAs is reported in Table TABREF13 for both datasets. Total utterances in MELD includes training, validation and test datasets. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "30\n",
      "What are the five neural annotators used in the Discourse-Wizard tool?\n",
      "\n",
      "The five neural annotators used in the Discourse-Wizard tool are Utt-level 1 Dialogue Act Neural Annotator (DANA), Context 1 DANA, Utt-level 2 DANA, Context 2 DANA, and Context 3 DANA.\n",
      "Question : for the text We adopted the neural architectures based on Bothe et al. bothe2018discourse where two variants are: non-context model (classifying at utterance level) and context model (recognizing the dialogue act of the current utterance given a few preceding utterances). From conversational analysis using dialogue acts in Bothe et al. bothe2018interspeech, we learned that the preceding two utterances contribute significantly to recognizing the dialogue act of the current utterance. Hence, we adapt this setting for the context model and create a pool of annotators using recurrent neural networks (RNNs). RNNs can model the contextual information in the sequence of words of an utterance and in the sequence of utterances of a dialogue. Each word in an utterance is represented with a word embedding vector of dimension 1024. We use the word embedding vectors from pre-trained ELMo (Embeddings from Language Models) embeddings BIBREF22. We have a pool of five neural annotators as shown in Figure FIGREF6. Our online tool called Discourse-Wizard is available to practice automated dialogue act labeling. In this tool we use the same neural architectures but model-trained embeddings (while, in this work we use pre-trained ELMo embeddings, as they are better performant but computationally and size-wise expensive to be hosted in the online tool). The annotators are:.Utt-level 1 Dialogue Act Neural Annotator (DANA) is an utterance-level classifier that uses word embeddings ($w$) as an input to an RNN layer, attention mechanism and computes the probability of dialogue acts ($da$) using the softmax function (see in Figure FIGREF10, dotted line utt-l1). This model achieved 75.13% accuracy on the SwDA corpus test set..Context 1 DANA is a context model that uses 2 preceding utterances while recognizing the dialogue act of the current utterance (see context model with con1 line in Figure FIGREF10). It uses a hierarchical RNN with the first RNN layer to encode the utterance from word embeddings ($w$) and the second RNN layer is provided with three utterances ($u$) (current and two preceding) composed from the first layer followed by the attention mechanism ($a$), where $\\sum _{n=0}^{n} a_{t-n} = 1$. Finally, the softmax function is used to compute the probability distribution. This model achieved 77.55% accuracy on the SwDA corpus test set..Utt-level 2 DANA is another utterance-level classifier which takes an average of the word embeddings in the input utterance and uses a feedforward neural network hidden layer (see utt-l2 line in Figure FIGREF10, where $mean$ passed to $softmax$ directly). Similar to the previous model, it computes the probability of dialogue acts using the softmax function. This model achieved 72.59% accuracy on the test set of the SwDA corpus..Context 2 DANA is another context model that uses three utterances similar to the Context 1 DANA model, but the utterances are composed as the mean of the word embeddings over each utterance, similar to the Utt-level 2 model ($mean$ passed to context model in Figure FIGREF10 with con2 line). Hence, the Context 2 DANA model is composed of one RNN layer with three input vectors, finally topped with the softmax function for computing the probability distribution of the dialogue acts. This model achieved 75.97% accuracy on the test set of the SwDA corpus..Context 3 DANA is a context model that uses three utterances similar to the previous models, but the utterance representations combine both features from the Context 1 and Context 2 models (con1 and con2 together in Figure FIGREF10). Hence, the Context 3 DANA model combines features of almost all the previous four models to provide the recognition of the dialogue acts. This model achieves 75.91% accuracy on the SwDA corpus test set. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "31\n",
      "Question 1: What is Krippendorff's Alpha and how is it used in emotion annotation?\n",
      "\n",
      "Answer 1: Krippendorff's Alpha is a reliability coefficient used to measure the agreement among observers, annotators, and raters. It is often used in emotion annotation and is computed by comparing the observed disagreement to the disagreement that is expected by chance. A score of 1 indicates perfect agreement while a score of 0 indicates no agreement. In this study, Krippendorff's Alpha was applied on the five neural annotators at the nominal level of measurement of dialogue act categories in datasets IEMOCAP and MELD, producing a significant inter-neural annotator agreement of 0.553 and 0.494, respectively.\n",
      "Question : for the text The pool of neural annotators provides a fair range of annotations, and we checked the reliability with the following metrics BIBREF23. Krippendorff's Alpha ($\\alpha $) is a reliability coefficient developed to measure the agreement among observers, annotators, and raters, and is often used in emotion annotation BIBREF24. We apply it on the five neural annotators at the nominal level of measurement of dialogue act categories. $\\alpha $ is computed as follows:.where $D_{o}$ is the observed disagreement and $D_{e}$ is the disagreement that is expected by chance. $\\alpha =1$ means all annotators produce the same label, while $\\alpha =0$ would mean none agreed on any label. As we can see in Table TABREF20, both datasets IEMOCAP and MELD produce significant inter-neural annotator agreement, 0.553 and 0.494, respectively..A very popular inter-annotator metric is Fleiss' Kappa score, also reported in Table TABREF20, which determines consistency in the ratings. The kappa $k$ can be defined as,.where the denominator $1 -\\bar{P}_e$ elicits the degree of agreement that is attainable above chance, and the numerator $\\bar{P} -\\bar{P}_e$ provides the degree of the agreement actually achieved above chance. Hence, $k = 1$ if the raters agree completely, and $k = 0$ when none reach any agreement. We got 0.556 and 0.502 for IEOMOCAP and MELD respectively with our five neural annotators. This indicated that the annotators are labeling the dialogue acts reliably and consistently. We also report the Spearman's correlation between context-based models (Context1 and Context2), and it shows a strong correlation between them (Table TABREF20). While using the labels we checked the absolute match between all context-based models and hence their strong correlation indicates their robustness. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "32\n",
      "Question 1: What were the two emotion datasets that were labeled with dialogue acts?\n",
      "\n",
      "Answer 1: The two emotion datasets that were labeled with dialogue acts were IEMOCAP and MELD.\n",
      "Question : for the text In this work, we presented a method to extend conversational multi-modal emotion datasets with dialogue act labels. We successfully show this on two well-established emotion datasets: IEMOCAP and MELD, which we labeled with dialogue acts and made publicly available for further study and research. As a first insight, we found that many of the dialogue acts and emotion labels follow certain relations. These relations can be useful to learn about the emotional behaviours with dialogue acts to build a natural dialogue system and for deeper conversational analysis. The conversational agent might benefit in generating an appropriate response when considering both emotional states and dialogue acts in the utterances..In future work, we foresee the human in the loop for the annotation process along with a pool of automated neural annotators. Robust annotations can be achieved with very little human effort and supervision, for example, observing and correcting the final labels produced by ensemble output labels from the neural annotators. The human-annotator might also help to achieve segmented-utterance labelling of the dialogue acts. We also plan to use these datasets for conversational analysis to infer interactive behaviours of the emotional states with respect to dialogue acts. In our recent work, where we used dialogue acts to build a dialogue system for a social robot, we find this study and dataset very helpful. For example, we can extend our robotic conversational system to consider emotion as an added linguistic feature to produce natural interaction. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "33\n",
      "Question 1: What emotion labels are frequently associated with Acknowledgements (b) dialogue acts?\n",
      "\n",
      "Answer 1: Acknowledgements (b) are mostly associated with positive or neutral emotion labels, but Appreciation (ba) and Rhetorical (bh) backchannels often occur with greater frequency in the emotions of 'Surprise', 'Joy', and/or 'Excited'.\n",
      "Question : for the text We can see emotional dialogue act co-occurrences with respect to emotion labels in Figure FIGREF12 for both datasets. There are sets of three bars per dialogue act in the figure, the first and second bar represent emotion labels of IEMOCAP (IE) and MELD (ME), and the third bar is for MELD sentiment (MS) labels. MELD emotion and sentiment statistics are interesting as they are strongly correlated to each other. The bars contain the normalized number of utterances for emotion labels with respect to the total number of utterances for that particular dialogue act category. The statements without-opinion (sd) and with-opinion (sv) contain utterances with almost all emotions. Many neutral utterances are spanning over all the dialogue acts..Quotation (⌃q) dialogue acts, on the other hand, are mostly used with `Anger' and `Frustration' (in case of IEMOCAP), however, some utterances with `Joy' or `Sadness' as well (see examples in Table TABREF21). Action Directive (ad) dialogue act utterances, which are usually orders, frequently occur with `Anger' or `Frustration' although many with `Happy' emotion in case of the MELD dataset. Acknowledgements (b) are mostly with positive or neutral, however, Appreciation (ba) and Rhetorical (bh) backchannels often occur with a greater number in `Surprise', `Joy' and/or with `Excited' (in case of IEMOCAP). Questions (qh, qw, qy and qy⌃d) are mostly asked with emotions `Surprise', `Excited', `Frustration' or `Disgust' (in case of MELD) and many are neutral. No-answers (nn) are mostly `Sad' or `Frustrated' as compared to yes-answers (ny). Forward-functions such as Apology (fa) are mostly with `Sadness' whereas Thanking (ft) and Conventional-closing or -opening (fc or fp) are usually with `Joy' or `Excited'..We also noticed that both datasets exhibit a similar relation between dialogue act and emotion. It is important to notice that the dialogue act annotation is based on the given transcripts, however, the emotional expressions are better perceived with audio or video BIBREF6. We report some examples where we mark the utterances with an determined label (xx) in the last row of Table TABREF21. They are skipped from the final annotation because of not fulfilling the conditions explained in Section SECREF14 It is also interesting to see the previous utterance dialogue acts (P-DA) of those skipped utterances, and the sequence of the labels can be followed from Figure FIGREF6 (utt-l1, utt-l2, con1, con2, con3)..In the first example, the previous utterance was b, and three DANA models produced labels of the current utterance as b, but it is skipped because the confidence values were not sufficient to bring it as a final label. The second utterance can be challenging even for humans to perceive with any of the dialogue acts. However, the third and fourth utterances are followed by a yes-no question (qy), and hence, we can see in the third example, that context models tried their best to at least perceive it as an answer (ng, ny, nn). The last utterance, “I'm so sorry!\", has been completely disagreed by all the five annotators. Similar apology phrases are mostly found with `Sadness' emotion label's, and the correct dialogue act is Apology (fa). However, they are placed either in the sd or in ba dialogue act category. We believe that with human annotator's help those labels of the utterances can be corrected with very limited efforts. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "34\n",
      "Question 1: What are some potential applications of emotion recognition in conversational agents?\n",
      "\n",
      "Answer 1: Some potential applications include response recommendation or generation, emotion-based text-to-speech, personalization, and producing natural interactions in dialogue systems.\n",
      "Question : for the text With the growing demand for human-computer/robot interaction systems, detecting the emotional state of the user can heavily benefit a conversational agent to respond at an appropriate emotional level. Emotion recognition in conversations has proven important for potential applications such as response recommendation or generation, emotion-based text-to-speech, personalisation, etc. Human emotional states can be expressed verbally and non-verbally BIBREF0, BIBREF1, however, while building an interactive dialogue system, the interface needs dialogue acts. A typical dialogue system consists of a language understanding module which requires to determine the meaning of and intention in the human input utterances BIBREF2, BIBREF3. Also, in discourse or conversational analysis, dialogue acts are the main linguistic features to consider BIBREF4. A dialogue act provides an intention and performative function in an utterance of the dialogue. For example, it can infer a user's intention by distinguishing Question, Answer, Request, Agree/Reject, etc. and performative functions such as Acknowledgement, Conversational-opening or -closing, Thanking, etc. The dialogue act information together with emotional states can be very useful for a spoken dialogue system to produce natural interaction BIBREF5..The research in emotion recognition is growing very rapidly and many datasets are available, such as text-based, speech- or vision-level, and multimodal emotion data. Emotion expression recognition is a challenging task and hence multimodality is crucial BIBREF0. However, few conversational multi-modal emotion recognition datasets are available, for example, IEMOCAP BIBREF6, SEMAINE BIBREF7, MELD BIBREF8. They are multi-modal dyadic conversational datasets containing audio-visual and conversational transcripts. Every utterance in these datasets is labeled with an emotion label..In this work, we apply an automated neural ensemble annotation process for dialogue act labeling. Several neural models are trained with the Switchboard Dialogue Act (SwDA) Corpus BIBREF9, BIBREF10 and used for inferring dialogue acts on the emotion datasets. We ensemble five model output labels by checking majority occurrences (most of the model labels are the same) and ranking confidence values of the models. We have annotated two potential multi-modal conversation datasets for emotion recognition: IEMOCAP (Interactive Emotional dyadic MOtion CAPture database) BIBREF6 and MELD (Multimodal EmotionLines Dataset) BIBREF8. Figure FIGREF2, shows an example of dialogue acts with emotion and sentiment labels from the MELD dataset. We confirmed the reliability of annotations with inter-annotator metrics. We analysed the co-occurrences of the dialogue act and emotion labels and discovered a key relationship between them; certain dialogue acts of the utterances show significant and useful association with respective emotional states. For example, Accept/Agree dialogue act often occurs with the Joy emotion while Reject with Anger, Acknowledgements with Surprise, Thanking with Joy, and Apology with Sadness, etc. The detailed analysis of the emotional dialogue acts (EDAs) and annotated datasets are being made available at the SECURE EU Project website. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "35\n",
      "Question 1: What is the grant number that supported the project?\n",
      "Answer 1: The grant number that supported the project is EP/P02338X/1, which is under the EPSRC Healthcare Partnerships Programme.\n",
      "Question : for the text Supported by EPSRC Healthcare Partnerships Programme grant number EP/P02338X/1 (Ultrax2020). generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "36\n",
      "Question 1: How does SyncNet synchronize speech audio with lip videos?\n",
      "\n",
      "Answer 1: SyncNet uses a two-stream neural network and self-supervision to learn cross-modal embeddings, which are then used to synchronize audio with lip videos. It achieves near perfect accuracy using manual evaluation where lip-sync error is not detectable to a human.\n",
      "Question : for the text Speech audio is generated by articulatory movement and is therefore fundamentally correlated with other manifestations of this movement, such as lip or tongue videos BIBREF10 . An alternative to the hardware approach is to exploit this correlation to find the offset. Previous approaches have investigated the effects of using different representations and feature extraction techniques on finding dimensions of high correlation BIBREF11 , BIBREF12 , BIBREF13 . More recently, neural networks, which learn features directly from input, have been employed for the task. SyncNet BIBREF4 uses a two-stream neural network and self-supervision to learn cross-modal embeddings, which are then used to synchronise audio with lip videos. It achieves near perfect accuracy ( INLINEFORM0 99 INLINEFORM1 ) using manual evaluation where lip-sync error is not detectable to a human. It has since been extended to use different sample creation methods for self-supervision BIBREF5 , BIBREF14 and different training objectives BIBREF14 . We adopt the original approach BIBREF4 , as it is both simpler and significantly less expensive to train than the more recent variants. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "37\n",
      "What is the main reason for hardware synchronisation to fail in ultrasound tongue videos and audio recordings?\n",
      "\n",
      "The main reason for hardware synchronisation to fail in ultrasound tongue videos and audio recordings is incorrect use of the external synchroniser device or missing the pulse signal, which can cause synchronisation to fail for entire therapy sessions. Additionally, low-quality sound cards can report an approximate sample rate, leading to errors in the offset calculation. There is currently no recovery mechanism for when synchronisation fails.\n",
      "Question : for the text Ultrasound and audio are recorded using separate components, and hardware synchronisation is achieved by translating information from the visual signal into audio at recording time. Specifically, for every ultrasound frame recorded, the ultrasound beam-forming unit releases a pulse signal, which is translated by an external hardware synchroniser into an audio pulse signal and captured by the sound card BIBREF6 , BIBREF7 . Synchronisation is achieved by aligning the ultrasound frames with the audio pulse signal, which is already time-aligned with the speech audio BIBREF8 ..Hardware synchronisation can fail for a number of reasons. The synchroniser is an external device which needs to be correctly connected and operated by therapists. Incorrect use can lead to missing the pulse signal, which would cause synchronisation to fail for entire therapy sessions BIBREF9 . Furthermore, low-quality sound cards report an approximate, rather than the exact, sample rate which leads to errors in the offset calculation BIBREF8 . There is currently no recovery mechanism for when synchronisation fails, and to the best of our knowledge, there has been no prior work on automatically correcting the synchronisation error between ultrasound tongue videos and audio. There is, however, some prior work on synchronising lip movement with audio which we describe next. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "38\n",
      "Question 1: What is the future direction for the model discussed in the text?\n",
      "\n",
      "Answer 1: The future direction for the model discussed in the text includes integrating the model and synchronisation offset prediction process into speech therapy software and using the learned embeddings for other tasks such as active speaker detection.\n",
      "Question : for the text We have shown how a two-stream neural network originally designed to synchronise lip videos with audio can be used to synchronise UTI data with audio. Our model exploits the correlation between the modalities to learn cross-model embeddings which are used to find the synchronisation offset. It generalises well to held-out data, allowing us to correctly synchronise the majority of test utterances. The model is best-suited to utterances which contain natural variation in speech and least suited to those containing isolated phones, with the exception of stop consonants. Future directions include integrating the model and synchronisation offset prediction process into speech therapy software BIBREF6 , BIBREF7 , and using the learned embeddings for other tasks such as active speaker detection BIBREF4 . generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "39\n",
      "What are positive and negative training pairs used for in the model training? \n",
      "\n",
      "Positive and negative training pairs are used to train the model. Positive samples are pairs of ultrasound windows and the corresponding MFCC frames, while negative samples are generated by randomizing pairings of ultrasound windows to MFCC frames within the same utterance to achieve a balanced dataset.\n",
      "Question : for the text To train our model we need positive and negative training pairs. The model ingests short clips from each modality of INLINEFORM0 200ms long, calculated as INLINEFORM1 , where INLINEFORM2 is the time window, INLINEFORM3 is the number of ultrasound frames per window (5 in our case), and INLINEFORM4 is the ultrasound frame rate of the utterance ( INLINEFORM5 24.3 fps). For each recording, we split the ultrasound into non-overlapping windows of 5 frames each. We extract MFCC features (13 cepstral coefficients) from the audio using a window length of INLINEFORM6 20ms, calculated as INLINEFORM7 , and a step size of INLINEFORM8 10ms, calculated as INLINEFORM9 . This give us the input sizes shown in Figure FIGREF1 ..Positive samples are pairs of ultrasound windows and the corresponding MFCC frames. To create negative samples, we randomise pairings of ultrasound windows to MFCC frames within the same utterance, generating as many negative as positive samples to achieve a balanced dataset. We obtain 243,764 samples for UXTD (13.5hrs), 333,526 for UXSSD (18.5hrs), and 572,078 for UPX (31.8 hrs), or a total 1,149,368 samples (63.9hrs) which we divide into training, validation and test sets. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "40\n",
      "Question 1: What is UltraSuite and how many spoken utterances does it contain?\n",
      "\n",
      "Answer 1: UltraSuite is a repository of ultrasound and acoustic data gathered from child speech therapy sessions. It contains 13,815 spoken utterances from 86 speakers, corresponding to 35.9 hours of recordings.\n",
      "Question : for the text For our experiments, we select a dataset whose utterances have been correctly synchronised at recording time. This allows us to control how the model is trained and verify its performance using ground truth synchronisation offsets. We use UltraSuite: a repository of ultrasound and acoustic data gathered from child speech therapy sessions BIBREF15 . We used all three datasets from the repository: UXTD (recorded with typically developing children), and UXSSD and UPX (recorded with children with speech sound disorders). In total, the dataset contains 13,815 spoken utterances from 86 speakers, corresponding to 35.9 hours of recordings. The utterances have been categorised by the type of task the child was given, and are labelled as: Words (A), Non-words (B), Sentence (C), Articulatory (D), Non-speech (E), or Conversations (F). See BIBREF15 for details..Each utterance consists of 3 files: audio, ultrasound, and parameter. The audio file is a RIFF wave file, sampled at 22.05 KHz, containing the speech of the child and therapist. The ultrasound file consists of a sequence of ultrasound frames capturing the midsagittal view of the child's tongue. A single ultrasound frame is recorded as a 2D matrix where each column represents the ultrasound reflection intensities along a single scan line. Each ultrasound frame consists of 63 scan lines of 412 data points each, and is sampled at a rate of INLINEFORM0 121.5 fps. Raw ultrasound frames can be visualised as greyscale images and can thus be interpreted as videos. The parameter file contains the synchronisation offset value (in milliseconds), determined using hardware synchronisation at recording time and confirmed by the therapists to be correct for this dataset. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "41\n",
      "Question 1: What is the selection process for creating the training, validation, and testing datasets?\n",
      "Answer 1: The selection process involves selecting a group of speakers from each dataset and holding out their data either for validation or testing. Additionally, one entire session from each remaining speaker is held out for validation or testing. The aim is to reserve approximately 80% of the created samples for training, 10% for validation, and 10% for testing. The result is 909,858 (pooled) samples for training, 128,414 for validation, and 111,096 for testing.\n",
      "Question : for the text We aim to test whether our model generalises to data from new speakers, and to data from new sessions recorded with known speakers. To simulate this, we select a group of speakers from each dataset, and hold out all of their data either for validation or for testing. Additionally, we hold out one entire session from each of the remaining speakers, and use the rest of their data for training. We aim to reserve approximately 80% of the created samples for training, 10% for validation, and 10% for testing, and select speakers and sessions on this basis..Each speaker in UXTD recorded 1 session, but sessions are of different durations. We reserve 45 speakers for training, 5 for validation, and 8 for testing. UXSSD and UPX contain fewer speakers, but each recorded multiple sessions. We hold out 1 speaker for validation and 1 for testing from each of the two datasets. We also hold out a session from the first half of the remaining speakers for validation, and a session from the second half of the remaining speakers for testing. This selection process results in 909,858 (pooled) samples for training (50.5hrs), 128,414 for validation (7.1hrs) and 111,096 for testing (6.2hrs). From the training set, we create shuffled batches which are balanced in the number of positive and negative samples. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "42\n",
      "Question 1: What is the accuracy of the model in predicting synchronisation offset for test utterances?\n",
      "\n",
      "Answer 1: The model achieves 82.9% accuracy in predicting synchronisation offset for test utterances with a mean and standard deviation discrepancy of 32 and 223ms respectively.\n",
      "Question : for the text We select the hyper-parameters of our model empirically by tuning on the validation set (Table ). Hyper-parameter exploration is guided by BIBREF24 . We train our model using the Adam optimiser BIBREF25 with a learning rate of 0.001, a batch size of 64 samples, and for 20 epochs. We implement learning rate scheduling which reduces the learning rate by a factor of 0.1 when the validation loss plateaus for 2 epochs..Upon convergence, the model achieves 0.193 training loss, 0.215 validation loss, and 0.213 test loss. By placing a threshold of 0.5 on predicted distances, the model achieves 69.9% binary classification accuracy on training samples, 64.7% on validation samples, and 65.3% on test samples..Synchronisation offset prediction: Section SECREF3 described briefly how to use our model to predict the synchronisation offset for test utterances. To obtain a discretised set of offset candidates, we retrieve the true offsets of the training utterances, and find that they fall in the range [0, 179] ms. We discretise this range taking 45ms steps and rendering 40 candidate values (45ms is the smaller of the absolute values of the detectability boundaries, INLINEFORM0 125 and INLINEFORM1 45 ms). We bin the true offsets in the candidate set and discard empty bins, reducing the set from 40 to 24 values. We consider all 24 candidates for each test utterance. We do this by aligning the two signals according to the given candidate, then producing the non-overlapping windows of ultrasound and MFCC pairs, as we did when preparing the data. We then use our model to predict the Euclidean distance for each pair, and average the distances. Finally, we select the offset with the smallest average distance as our prediction..Evaluation: Because the true offsets are known, we evaluate the performance of our model by computing the discrepancy between the predicted and the true offset for each utterance. If the discrepancy falls within the minimum detectability range ( INLINEFORM0 125 INLINEFORM1 INLINEFORM2 INLINEFORM3 INLINEFORM4 45) then the prediction is correct. Random prediction (averaged over 1000 runs) yields 14.6% accuracy with a mean and standard deviation discrepancy of 328 INLINEFORM5 518ms. We achieve 82.9% accuracy with a mean and standard deviation discrepancy of 32 INLINEFORM6 223ms. SyncNet reports INLINEFORM7 99% accuracy on lip video synchronisation using a manual evaluation where the lip error is not detectable to a human observer BIBREF4 . However, we argue that our data is more challenging (Section SECREF4 )..Analysis: We analyse the performance of our model across different conditions. Table shows the model accuracy broken down by utterance type. The model achieves 91.2% accuracy on utterances containing words, sentences, and conversations, all of which exhibit natural variation in speech. The model is less successful with Articulatory utterances, which contain isolated phones occurring once or repeated (e.g., “sh sh sh\"). Such utterances contain subtle tongue movement, making it more challenging to correlate the visual signal with the audio. And indeed, the model finds the correct offset for only 55.9% of Articulatory utterances. A further analysis shows that 84.4% (N INLINEFORM0 90) of stop consonants (e.g., “t”), which are relied upon by therapists as the most salient audiovisual synchronisation cues BIBREF3 , are correctly synchronised by our model, compared to 48.6% (N INLINEFORM1 140) of vowels, which contain less distinct movement and are also more challenging for therapists to synchronise..Table shows accuracy broken down by test set. The model performs better on test sets containing entirely new speakers compared with test sets containing new sessions from previously seen speakers. This is contrary to expectation but could be due to the UTI challenges (described in Section SECREF4 ) affecting different subsets to different degrees. Table shows that the model performs considerably worse on UXTD compared to other test sets (64.8% accuracy). However, a further breakdown of the results in Table by test set and utterance type explains this poor performance; the majority of UXTD utterances (71%) are Articulatory utterances which the model struggles to correctly synchronise. In fact, for other utterance types (where there is a large enough sample, such as Words) performance on UXTD is on par with other test sets. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "43\n",
      "What is the importance of correctly synchronizing ultrasound tongue imaging (UTI) and audio in instrumental speech therapy?\n",
      "\n",
      "Correctly synchronizing UTI and audio in instrumental speech therapy is crucial for providing accurate diagnoses, designing effective treatments, and measuring therapy progress. However, synchronization errors beyond a certain range can render the data unusable, making it essential to find a way to automatically correct these errors.\n",
      "Question : for the text Ultrasound tongue imaging (UTI) is a non-invasive way of observing the vocal tract during speech production BIBREF0 . Instrumental speech therapy relies on capturing ultrasound videos of the patient's tongue simultaneously with their speech audio in order to provide a diagnosis, design treatments, and measure therapy progress BIBREF1 . The two modalities must be correctly synchronised, with a minimum shift of INLINEFORM0 45ms if the audio leads and INLINEFORM1 125ms if the audio lags, based on synchronisation standards for broadcast audiovisual signals BIBREF2 . Errors beyond this range can render the data unusable – indeed, synchronisation errors do occur, resulting in significant wasted effort if not corrected. No mechanism currently exists to automatically correct these errors, and although manual synchronisation is possible in the presence of certain audiovisual cues such as stop consonants BIBREF3 , it is time consuming and tedious..In this work, we exploit the correlation between the two modalities to synchronise them. We utilise a two-stream neural network architecture for the task BIBREF4 , using as our only source of supervision pairs of ultrasound and audio segments which have been automatically generated and labelled as positive (correctly synchronised) or negative (randomly desynchronised); a process known as self-supervision BIBREF5 . We demonstrate how this approach enables us to correctly synchronise the majority of utterances in our test set, and in particular, those exhibiting natural variation in speech..Section SECREF2 reviews existing approaches for audiovisual synchronisation, and describes the challenges specifically associated with UTI data, compared with lip videos for which automatic synchronisation has been previously attempted. Section SECREF3 describes our approach. Section SECREF4 describes the data we use, including data preprocessing and positive and negative sample creation using a self-supervision strategy. Section SECREF5 describes our experiments, followed by an analysis of the results. We conclude with a summary and future directions in Section SECREF6 . generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "44\n",
      "What are some challenges specifically associated with UTI data compared to lip videos?\n",
      "Some of the challenges associated with UTI data include poor image quality, variation in probe placement, inter-speaker variation, limited amount of data, and uncorrelated segments due to interactions between therapist and patient.\n",
      "Question : for the text Videos of lip movement can be obtained from various sources including TV, films, and YouTube, and are often cropped to include only the lips BIBREF4 . UTI data, on the other hand, is recorded in clinics by trained therapists BIBREF15 . An ultrasound probe placed under the chin of the patient captures the midsaggital view of their oral cavity as they speak. UTI data consists of sequences of 2D matrices of raw ultrasound reflection data, which can be interpreted as greyscale images BIBREF15 . There are several challenges specifically associated with UTI data compared with lip videos, which can potentially lower the performance of models relative to results reported on lip video data. These include:.Poor image quality: Ultrasound data is noisy, containing arbitrary high-contrast edges, speckle noise, artefacts, and interruptions to the tongue's surface BIBREF0 , BIBREF16 , BIBREF17 . The oral cavity is not entirely visible, missing the lips, the palate, and the pharyngeal wall, and visually interpreting the data requires specialised training. In contrast, videos of lip movement are of much higher quality and suffer from none of these issues..Probe placement variation: Surfaces that are orthogonal to the ultrasound beam image better than those at an angle. Small shifts in probe placement during recording lead to high variation between otherwise similar tongue shapes BIBREF0 , BIBREF18 , BIBREF17 . In contrast, while the scaling and rotations of lip videos lead to variation, they do not lead to a degradation in image quality..Inter-speaker variation: Age and physiology affect the quality of ultrasound data, and subjects with smaller vocal tracts and less tissue fat image better BIBREF0 , BIBREF17 . Dryness in the mouth, as a result of nervousness during speech therapy, leads to poor imaging. While inter-speaker variation is expected in lip videos, again, the variation does not lead to quality degradation..Limited amount of data: Existing UTI datasets are considerably smaller than lip movement datasets. Consider for example VoxCeleb and VoxCeleb2 used to train SyncNet BIBREF4 , BIBREF14 , which together contain 1 million utterances from 7,363 identities BIBREF19 , BIBREF20 . In contrast, the UltraSuite repository (used in this work) contains 13,815 spoken utterances from 86 identities..Uncorrelated segments: Speech therapy data contains interactions between the therapist and patient. The audio therefore contains speech from both speakers, while the ultrasound captures only the patient's tongue BIBREF15 . As a result, parts of the recordings will consist of completely uncorrelated audio and ultrasound. This issue is similar to that of dubbed voices in lip videos BIBREF4 , but is more prevalent in speech therapy data. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "45\n",
      "What is the objective of UltraSync model?\n",
      "\n",
      "The objective of UltraSync model is to learn a mapping from high-dimensional inputs of ultrasound and audio to a pair of low-dimensional vectors of the same length, such that the Euclidean distance between the two vectors is small when they correlate and large otherwise.\n",
      "Question : for the text We adopt the approach in BIBREF4 , modifying it to synchronise audio with UTI data. Our model, UltraSync, consists of two streams: the first takes as input a short segment of ultrasound and the second takes as input the corresponding audio. Both inputs are high-dimensional and are of different sizes. The objective is to learn a mapping from the inputs to a pair of low-dimensional vectors of the same length, such that the Euclidean distance between the two vectors is small when they correlate and large otherwise BIBREF21 , BIBREF22 . This model can be viewed as an extension of a siamese neural network BIBREF23 but with two asymmetrical streams and no shared parameters. Figure FIGREF1 illustrates the main architecture. The visual data INLINEFORM0 (ultrasound) and audio data INLINEFORM1 (MFCC), which have different shapes, are mapped to low dimensional embeddings INLINEFORM2 (visual) and INLINEFORM3 (audio) of the same size: DISPLAYFORM0 .The model is trained using a contrastive loss function BIBREF21 , BIBREF22 , INLINEFORM0 , which minimises the Euclidean distance INLINEFORM1 between INLINEFORM2 and INLINEFORM3 for positive pairs ( INLINEFORM4 ), and maximises it for negative pairs ( INLINEFORM5 ), for a number of training samples INLINEFORM6 : DISPLAYFORM0 .Given a pair of ultrasound and audio segments we can calculate the distance between them using our model. To predict the synchronisation offset for an utterance, we consider a discretised set of candidate offsets, calculate the average distance for each across utterance segments, and select the one with the minimum average distance. The candidate set is independent of the model, and is chosen based on task knowledge (Section SECREF5 ). generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "46\n",
      "Q: Why are Non-speech utterances excluded from the training data?\n",
      "\n",
      "A: Non-speech utterances, such as coughs or swallowing motions, are excluded as they rarely contain audible content and are therefore not relevant to the task.\n",
      "Question : for the text First, we exclude utterances of type “Non-speech\" (E) from our training data (and statistics). These are coughs recorded to obtain additional tongue shapes, or swallowing motions recorded to capture a trace of the hard palate. Both of these rarely contain audible content and are therefore not relevant to our task. Next, we apply the offset, which should be positive if the audio leads and negative if the audio lags. In this dataset, the offset is always positive. We apply it by cropping the leading audio and trimming the end of the longer signal to match the duration..To process the ultrasound more efficiently, we first reduce the frame rate from INLINEFORM0 121.5 fps to INLINEFORM1 24.3 fps by retaining 1 out of every 5 frames. We then downsample by a factor of (1, 3), shrinking the frame size from 63x412 to 63x138 using max pixel value. This retains the number of ultrasound vectors (63), but reduces the number of pixels per vector (from 412 to 138)..The final pre-preprocessing step is to remove empty regions. UltraSuite was previously anonymised by zero-ing segments of audio which contained personally identifiable information. As a preprocessing step, we remove the zero regions from audio and corresponding ultrasound. We additionally experimented with removing regions of silence using voice activity detection, but obtained a higher performance by retaining them. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "47\n",
      "What is the purpose of integrating a higher-order GBN for sentences into the first layer of the CNN?\n",
      "The purpose of integrating a higher-order GBN for sentences into the first layer of the CNN is to pre-train the input layer using a dynamic GBN and generate a new set of sentences containing high ML motifs for pre-training the deep neural network prior to training with the complete dataset. This helps increase the weights of the corresponding causal features among words and concepts extracted using Gaussian Bayesian networks.\n",
      "Question : for the text In this work, we integrate a higher-order GBN for sentences into the first layer of the CNN. The GBN layer of connections INLINEFORM0 is learned using maximum likelihood approach on the BOW model of the training data. The input sequence of sentences INLINEFORM1 are parsed through this layer prior to training the CNN. Only sentences or groups of sentences containing high ML motifs are then used to train the CNN. Hence, motifs are convolved with the input sentences to generate a new set of sentences for pre-training. DISPLAYFORM0 .where INLINEFORM0 is the number of high ML motifs and INLINEFORM1 is the training set of sentences in a particular class..Fig. FIGREF28 illustrates the state space of Bayesian CNN where the input layer is pre-trained using a dynamic GBN with up-to two time point delays shown for three sentences in a review on iPhone. The dashed lines correspond to second-order edges among the words learned using BOW. Each hidden layer does convolution followed by pooling across the length of the sentence. To preserve the order of words we adopt kernels of increasing sizes..Since, the number of possible words in the vocabulary is very large, we consider only the top subjectivity clue words to learn the GBN layer. Lastly, In-order to preserve the context of words in conceptual phrases such as `touchscreen'; we consider additional nodes in the Bayesian network for phrases with subjectivity clues. Further, the word embeddings in the CNN are initialized using the log-bilinear language model (LBL) where the INLINEFORM0 dimensional vector representation of each word INLINEFORM1 in ( EQREF10 ) is given by : DISPLAYFORM0 .where INLINEFORM0 are the INLINEFORM1 co-occurrence or context matrices computed from the data..The time series of sentences is used to generate a sub-set of sentences containing high ML motifs using ( EQREF27 ). The frequency of a sentence in the new dataset will also correspond to the corresponding number of high ML motifs in the sentence. In this way, we are able to increase the weights of the corresponding causal features among words and concepts extracted using Gaussian Bayesian networks..The new set of sentences is used to pre-train the deep neural network prior to training with the complete dataset. Each sentence can be divided into chunks or phrases using POS taggers. The phrases have hierarchical structures and combine in distinct ways to form sentences. The INLINEFORM0 -gram kernels learned in the first layer hence correspond to a chunk in the sentence. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "48\n",
      "Question 1: Who funded the work mentioned in the text? \n",
      "Answer 1: The Complexity Institute, Nanyang Technological University funded the work mentioned in the text.\n",
      "Question : for the text This work was funded by Complexity Institute, Nanyang Technological University. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "49\n",
      "Question 1: Where is the School of Computer Science and Engineering located?\n",
      "\n",
      "Answer 1: The School of Computer Science and Engineering is located in Nanyang Technological University, Singapore.\n",
      "Question : for the text School of Computer Science and Engineering, Nanyang Technological University, Singapore generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "50\n",
      "What features were used in the aspect term extraction framework?\n",
      "\n",
      "The aspect term extraction framework used word embeddings and part of speech tags as features. Additionally, linguistic patterns derived from SenticNet were used to improve recall, particularly Rules 1 and 3. The framework also used a deep CNN and did not require any feature engineering.\n",
      "Question : for the text In order to train the CNN for aspect extraction, instead, we used a special training algorithm suitable for sequential data, proposed by BIBREF52 . We will summarize it here, mainly following BIBREF59 . The algorithm trains the neural network by back-propagation in order to maximize the likelihood over training sentences. Consider the network parameter INLINEFORM0 . We say that INLINEFORM1 is the output score for the likelihood of an input INLINEFORM2 to have the tag INLINEFORM3 . Then, the probability to assign the label INLINEFORM4 to INLINEFORM5 is calculated as DISPLAYFORM0 . Define the logadd operation as DISPLAYFORM0 . then for a training example, the log-likelihood becomes DISPLAYFORM0 . In aspect term extraction, the terms can be organized as chunks and are also often surrounded by opinion terms. Hence, it is important to consider sentence structure on a whole in order to obtain additional clues. Let it be given that there are INLINEFORM0 tokens in a sentence and INLINEFORM1 is the tag sequence while INLINEFORM2 is the network score for the INLINEFORM3 -th tag having INLINEFORM4 -th tag. We introduce INLINEFORM5 transition score from moving tag INLINEFORM6 to tag INLINEFORM7 . Then, the score tag for the sentence INLINEFORM8 to have the tag path INLINEFORM9 is defined by: DISPLAYFORM0 . This formula represents the tag path probability over all possible paths. Now, from ( EQREF32 ) we can write the log-likelihood DISPLAYFORM0 . The number of tag paths has exponential growth. However, using dynamic programming techniques, one can compute in polynomial time the score for all paths that end in a given tag BIBREF52 . Let INLINEFORM0 denote all paths that end with the tag INLINEFORM1 at the token INLINEFORM2 . Then, using recursion, we obtain DISPLAYFORM0 . For the sake of brevity, we shall not delve into details of the recursive procedure, which can be found in BIBREF52 . The next equation gives the log-add for all the paths to the token INLINEFORM0 : DISPLAYFORM0 .Using these equations, we can maximize the likelihood of ( EQREF35 ) over all training pairs. For inference, we need to find the best tag path using the Viterbi algorithm; e.g., we need to find the best tag path that minimizes the sentence score ( EQREF34 )..The features of an aspect term depend on its surrounding words. Thus, we used a window of 5 words around each word in a sentence, i.e., INLINEFORM0 words. We formed the local features of that window and considered them to be features of the middle word. Then, the feature vector was fed to a CNN..The network contained one input layer, two convolution layers, two max-pool layers, and a fully connected layer with softmax output. The first convolution layer consisted of 100 feature maps with filter size 2. The second convolution layer had 50 feature maps with filter size 3. The stride in each convolution layer is 1 as we wanted to tag each word. A max-pooling layer followed each convolution layer. The pool size we use in the max-pool layers was 2. We used regularization with dropout on the penultimate layer with a constraint on L2-norms of the weight vectors, with 30 epochs. The output of each convolution layer was computed using a non-linear function; in our case we used INLINEFORM0 ..As features, we used word embeddings trained on two different corpora. We also used some additional features and rules to boost the accuracy; see Section UID49 . The CNN produces local features around each word in a sentence and then combines these features into a global feature vector. Since the kernel size for the two convolution layers was different, the dimensionality INLINEFORM0 mentioned in Section SECREF16 was INLINEFORM1 and INLINEFORM2 , respectively. The input layer was INLINEFORM3 , where 65 was the maximum number of words in a sentence, and 300 the dimensionality of the word embeddings used, per each word..The process was performed for each word in a sentence. Unlike traditional max-likelihood leaning scheme, we trained the system using propagation after convolving all tokens in the sentence. Namely, we stored the weights, biases, and features for each token after convolution and only back-propagated the error in order to correct them once all tokens were processed using the training scheme as explained in Section SECREF30 ..If a training instance INLINEFORM0 had INLINEFORM1 words, then we represented the input vector for that instance as INLINEFORM2 . Here, INLINEFORM3 is a INLINEFORM4 -dimensional feature vector for the word INLINEFORM5 . We found that this network architecture produced good results on both of our benchmark datasets. Adding extra layers or changing the pooling size and window size did not contribute to the accuracy much, and instead, only served to increase computational cost..In this subsection, we present the data used in our experiments.. BIBREF64 presented two different neural network models for creating word embeddings. The models were log-linear in nature, trained on large corpora. One of them is a bag-of-words based model called CBOW; it uses word context in order to obtain the embeddings. The other one is called skip-gram model; it predicts the word embeddings of surrounding words given the current word. Those authors made a dataset called word2vec publicly available. These 300-dimensional vectors were trained on a 100-billion-word corpus from Google News using the CBOW architecture..We trained the CBOW architecture proposed by BIBREF64 on a large Amazon product review dataset developed by BIBREF65 . This dataset consists of 34,686,770 reviews (4.7 billion words) of 2,441,053 Amazon products from June 1995 to March 2013. We kept the word embeddings 300-dimensional (http://sentic.net/AmazonWE.zip). Due to the nature of the text used to train this model, this includes opinionated/affective information, which is not present in ordinary texts such as the Google News corpus..For training and evaluation of the proposed approach, we used two corpora:.Aspect-based sentiment analysis dataset developed by BIBREF66 ; and.SemEval 2014 dataset. The dataset consists of training and test sets from two domains, Laptop and Restaurant; see Table TABREF52 ..The annotations in both corpora were encoded according to IOB2, a widely used coding scheme for representing sequences. In this encoding, the first word of each chunk starts with a “B-Type” tag, “I-Type” is the continuation of the chunk and “O” is used to tag a word which is out of the chunk. In our case, we are interested to determine whether a word or chunk is an aspect, so we only have “B–A”, “I–A” and “O” tags for the words..Here is an example of IOB2 tags:.also/O excellent/O operating/B-A system/I-A ,/O size/B-A and/O weight/B-A for/O optimal/O mobility/B-A excellent/O durability/B-A of/O the/O battery/B-A the/O functions/O provided/O by/O the/O trackpad/B-A is/O unmatched/O by/O any/O other/O brand/O.In this section, we present the features, the representation of the text, and linguistic rules used in our experiments..We used the following the features:.Word Embeddings We used the word embeddings described earlier as features for the network. This way, each word was encoded as 300-dimensional vector, which was fed to the network..Part of speech tags Most of the aspect terms are either nouns or noun chunk. This justifies the importance of POS features. We used the POS tag of the word as its additional feature. We used 6 basic parts of speech (noun, verb, adjective, adverb, preposition, conjunction) encoded as a 6- dimensional binary vector. We used Stanford Tagger as a POS tagger..These two features vectors were concatenated and fed to CNN..So, for each word the final feature vector is 306 dimensional..In some of our experiments, we used a set of linguistic patterns (LPs) derived from sentic patterns (LP) BIBREF11 , a linguistic framework based on SenticNet BIBREF22 . SenticNet is a concept-level knowledge base for sentiment analysis built by means of sentic computing BIBREF67 , a multi-disciplinary approach to natural language processing and understanding at the crossroads between affective computing, information extraction, and commonsense reasoning, which exploits both computer and human sciences to better interpret and process social information on the Web. In particular, we used the following linguistic rules:.Let a noun h be a subject of a word t, which has an adverbial or adjective modifier present in a large sentiment lexicon, SenticNet. Then mark h as an aspect..Except when the sentence has an auxiliary verb, such as is, was, would, should, could, etc., we apply:.If the verb t is modified by an adjective or adverb or is in adverbial clause modifier relation with another token, then mark h as an aspect. E.g., in “The battery lasts little”,.battery is the subject of lasts, which is modified by an adjective modifier little, so battery is marked as an aspect..If t has a direct object, a noun n, not found in SenticNet, then mark n an aspect, as, e.g., in “I like the lens of this camera”..If a noun h is a complement of a couplar verb, then mark h as an explicit aspect. E.g., in “The camera is nice”, camera is marked as an aspect..If a term marked as an aspect by the CNN or the other rules is in a noun-noun compound relationship with another word, then instead form one aspect term composed of both of them. E.g., if in “battery life”, “battery” or “life” is marked as an aspect, then the whole expression is marked as an aspect..The above rules 1–4 improve recall by discovering more aspect terms. However, to improve precision, we apply some heuristics: e.g., we remove stop-words such as of, the, a, etc., even if they were marked as aspect terms by the CNN or the other rules..We used the Stanford parser to determine syntactic relations in the sentences..We combined LPs with the CNN as follows: both LPs and CNN-based classifier are run on the text; then all terms marked by any of the two classifiers are reported as aspect terms, except for those unmarked by the last rule..Table TABREF63 shows the accuracy of our aspect term extraction framework in laptop and restaurant domains. The framework gave better accuracy on restaurant domain reviews, because of the lower variety of aspect available terms than in laptop domain. However, in both cases recall was lower than precision..Table TABREF63 shows improvement in terms of both precision and recall when the POS feature is used. Pre-trained word embeddings performed better than randomized features (each word's vector initialized randomly); see Table TABREF62 . Amazon embeddings performed better than Google word2vec embeddings. This supports our claim that the former contains opinion-specific information which helped it to outperform the accuracy of Google embeddings trained on more formal text—the Google news corpus. Because of this, in the sequel we only show the performance using Amazon embeddings, which we denote simply as WE (word embeddings)..In both domains, CNN suffered from low recall, i.e., it missed some valid aspect terms. Linguistic analysis of the syntactic structure of the sentences substantially helped to overcome some drawbacks of machine learning-based analysis. Our experiments showed good improvement in both precision and recall when LPs were used together with CNN; see Table TABREF64 ..As to the LPs, the removal of stop-words, Rule 1, and Rule 3 were most beneficial. Figure FIGREF66 shows a visualization for the Table TABREF64 . Table TABREF65 and Figure FIGREF61 shows the comparison between the proposed method and the state of the art on the Semeval dataset. It is noted that about 36.55% aspect terms present in the laptop domain corpus are phrase and restaurant corpus consists of 24.56% aspect terms. The performance of detecting aspect phrases are lower than single word aspect tokens in both domains. This shows that the sequential tagging is indeed a tough task to do. Lack of sufficient training data for aspect phrases is also one of the reasons to get lower accuracy in this case..In particular, we got 79.20% and 83.55% F-score to detect aspect phrases in laptop and restaurant domain respectively. We observed some cases where only 1 term in an aspect phrase is detected as aspect term. In those cases Rule 4 of the LPs helped to correctly detect the aspect phrases. We also carried out experiments on the aspect dataset originally developed by BIBREF66 . This is to date the largest comprehensive aspect-based sentiment analysis dataset. The best accuracy on this dataset was obtained when word embedding features were used together with the POS features. This shows that while the word embedding features are most useful, the POS feature also plays a major role in aspect extraction..As on the SemEval dataset, LPs together with CNN increased the overall accuracy. However, LPs have performed much better on this dataset than on the SemEval dataset. This supports the observation made previously BIBREF66 that on this dataset LPs are more useful. One of the possible reasons for this is that most of the sentences in this dataset are grammatically correct and contain only one aspect term. Here we combined LPs and a CNN to achieve even better results than the approach of by BIBREF66 based only on LPs. Our experimental results showed that this ensemble algorithm (CNN+LP) can better understand the semantics of the text than BIBREF66 's pure LP-based algorithm, and thus extracts more salient aspect terms. Table TABREF69 and Figure FIGREF68 shows the performance and comparisons of different frameworks..Figure FIGREF70 compares the proposed method with the state of the art. We believe that there are two key reasons for our framework to outperform state-of-the-art approaches. First, a deep CNN, which is non-linear in nature, better fits the data than linear models such as CRF. Second, the pre-trained word embedding features help our framework to outperform state-of-the-art methods that do not use word embeddings. The main advantage of our framework is that it does not need any feature engineering. This minimizes development cost and time. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "1\n",
      "What is the difference between explicit and implicit aspects in opinion mining? \n",
      "Answer 1: Explicit aspects are directly mentioned in the text, while implicit aspects require interpretation by the reader to infer their presence. The distinction between the two was introduced by BIBREF7 in the study of aspect extraction from opinions.\n",
      "Question : for the text Aspect extraction from opinions was first studied by BIBREF7 . They introduced the distinction between explicit and implicit aspects. However, the authors only dealt with explicit aspects and used a set of rules based on statistical observations. Hu and Liu's method was later improved by BIBREF32 and by BIBREF33 . BIBREF32 assumed the product class is known in advance. Their algorithm detects whether a noun or noun phrase is a product feature by computing the point-wise mutual information between the noun phrase and the product class.. BIBREF34 presented a method that uses language model to identify product features. They assumed that product features are more frequent in product reviews than in a general natural language text. However, their method seems to have low precision since retrieved aspects are affected by noise. Some methods treated the aspect term extraction as sequence labeling and used CRF for that. Such methods have performed very well on the datasets even in cross-domain experiments BIBREF9 , BIBREF10 ..Topic modeling has been widely used as a basis to perform extraction and grouping of aspects BIBREF35 , BIBREF36 . Two models were considered: pLSA BIBREF37 and LDA BIBREF38 . Both models introduce a latent variable “topic” between the observable variables “document” and “word” to analyze the semantic topic distribution of documents. In topic models, each document is represented as a random mixture over latent topics, where each topic is characterized by a distribution over words..Such methods have been gaining popularity in social media analysis like emerging political topic detection in Twitter BIBREF39 . The LDA model defines a Dirichlet probabilistic generative process for document-topic distribution; in each document, a latent aspect is chosen according to a multinomial distribution, controlled by a Dirichlet prior INLINEFORM0 . Then, given an aspect, a word is extracted according to another multinomial distribution, controlled by another Dirichlet prior INLINEFORM1 . Among existing works employing these models are the extraction of global aspects ( such as the brand of a product) and local aspects (such as the property of a product BIBREF40 ), the extraction of key phrases BIBREF41 , the rating of multi-aspects BIBREF42 , and the summarization of aspects and sentiments BIBREF43 . BIBREF44 employed the maximum entropy method to train a switch variable based on POS tags of words and used it to separate aspect and sentiment words.. BIBREF45 added user feedback to LDA as a response-variable related to each document. BIBREF46 proposed a semi-supervised model. DF-LDA BIBREF47 also represents a semi-supervised model, which allows the user to set must-link and cannot-link constraints. A must-link constraint means that two terms must be in the same topic, while a cannot-link constraint means that two terms cannot be in the same topic. BIBREF48 integrated commonsense in the calculation of word distributions in the LDA algorithm, thus enabling the shift from syntax to semantics in aspect-based sentiment analysis. BIBREF49 proposed two semi-supervised models for product aspect extraction based on the use of seeding aspects. In the category of supervised methods, BIBREF50 employed seed words to guide topic models to learn topics of specific interest to a user, while BIBREF42 and BIBREF51 employed seeding words to extract related product aspects from product reviews. On the other hand, recent approaches using deep CNNs BIBREF52 , BIBREF53 showed significant performance improvement over the state-of-the-art methods on a range of NLP tasks. BIBREF52 fed word embeddings to a CNN to solve standard NLP problems such as named entity recognition (NER), part-of-speech (POS) tagging and semantic role labeling. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "2\n",
      "Question 1: How did the ensemble approach proposed in the chapter perform compared to state-of-the-art techniques in sentiment analysis?\n",
      "\n",
      "Answer 1: The ensemble approach proposed in the chapter, which used a combination of deep learning and linguistic rules, gave a significant improvement in performance over state-of-the-art techniques in sentiment analysis. The approach paved the way for a more multifaceted and multidisciplinary approach to the complex problem of sentiment analysis.\n",
      "Question : for the text In this chapter, we tackled the two basic tasks of sentiment analysis in social media: subjectivity detection and aspect extraction. We used an ensemble of deep learning and linguistics to collect opinionated information and, hence, perform fine-grained (aspect-based) sentiment analysis. In particular, we proposed a Bayesian deep convolutional belief network to classify a sequence of sentences as either subjective or objective and used a convolutional neural network for aspect extraction. Coupled with some linguistic rules, this ensemble approach gave a significant improvement in performance over state-of-the-art techniques and paved the way for a more multifaceted (i.e., covering more NLP subtasks) and multidisciplinary (i.e., integrating techniques from linguistics and other disciplines) approach to the complex problem of sentiment analysis. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "3\n",
      "What is a deep belief network?\n",
      "\n",
      "A deep belief network (DBN) is a type of deep neural network that consists of simple unsupervised models such as restricted Boltzmann machines (RBMs). The hidden layer of each RBM serves as the visible layer for the next RBM. The weights of an RBM are computed using the maximum likelihood contrastive divergence approach, which involves approximating the average over the distribution with an average over a sample obtained by Markov chain Monte Carlo.\n",
      "Question : for the text A deep belief network (DBN) is a type of deep neural network that can be viewed as a composite of simple, unsupervised models such as restricted Boltzmann machines (RBMs) where each RBMs hidden layer serves as the visible layer for the next RBM BIBREF56 . RBM is a bipartite graph comprising two layers of neurons: a visible and a hidden layer; it is restricted such that the connections among neurons in the same layer are not allowed. To compute the weights INLINEFORM0 of an RBM, we assume that the probability distribution over the input vector INLINEFORM1 is given as: DISPLAYFORM0 .where INLINEFORM0 is a normalisation constant. Computing the maximum likelihood is difficult as it involves solving the normalisation constant, which is a sum of an exponential number of terms. The standard approach is to approximate the average over the distribution with an average over a sample from INLINEFORM1 , obtained by Markov chain Monte Carlo until convergence..To train such a multi-layer system, we must compute the gradient of the total energy function INLINEFORM0 with respect to weights in all the layers. To learn these weights and maximize the global energy function, the approximate maximum likelihood contrastive divergence (CD) approach can be used. This method employs each training sample to initialize the visible layer. Next, it uses the Gibbs sampling algorithm to update the hidden layer and then reconstruct the visible layer consecutively, until convergence BIBREF57 . As an example, here we use a logistic regression model to learn the binary hidden neurons and each visible unit is assumed to be a sample from a normal distribution BIBREF58 ..The continuous state INLINEFORM0 of the hidden neuron INLINEFORM1 , with bias INLINEFORM2 , is a weighted sum over all continuous visible nodes INLINEFORM3 and is given by: DISPLAYFORM0 .where INLINEFORM0 is the connection weight to hidden neuron INLINEFORM1 from visible node INLINEFORM2 . The binary state INLINEFORM3 of the hidden neuron can be defined by a sigmoid activation function: DISPLAYFORM0 .Similarly, in the next iteration, the binary state of each visible node is reconstructed and labeled as INLINEFORM0 . Here, we determine the value to the visible node INLINEFORM1 , with bias INLINEFORM2 , as a random sample from the normal distribution where the mean is a weighted sum over all binary hidden neurons and is given by: DISPLAYFORM0 .where INLINEFORM0 is the connection weight to hidden neuron INLINEFORM1 from visible node INLINEFORM2 . The continuous state INLINEFORM3 is a random sample from INLINEFORM4 , where INLINEFORM5 is the variance of all visible nodes. Lastly, the weights are updated as the difference between the original and reconstructed visible layer using: DISPLAYFORM0 .where INLINEFORM0 is the learning rate and INLINEFORM1 is the expected frequency with which visible unit INLINEFORM2 and hidden unit INLINEFORM3 are active together when the visible vectors are sampled from the training set and the hidden units are determined by ( EQREF21 ). Finally, the energy of a DNN can be determined in the final layer using INLINEFORM4 ..To extend the deep belief networks to convolution deep belief network (CDBN) we simply partition the hidden layer into INLINEFORM0 groups. Each of the INLINEFORM1 groups is associated with a INLINEFORM2 filter where INLINEFORM3 is the width of the kernel and INLINEFORM4 is the number of dimensions in the word vector. Let us assume that the input layer has dimension INLINEFORM5 where INLINEFORM6 is the length of the sentence. Then the convolution operation given by ( EQREF17 ) will result in a hidden layer of INLINEFORM7 groups each of dimension INLINEFORM8 . These learned kernel weights are shared among all hidden units in a particular group. The energy function is now a sum over the energy of individual blocks given by: DISPLAYFORM0 .The CNN sentence model preserve the order of words by adopting convolution kernels of gradually increasing sizes that span an increasing number of words and ultimately the entire sentence BIBREF31 . However, several word dependencies may occur across sentences hence, in this work we propose a Bayesian CNN model that uses dynamic Bayesian networks to model a sequence of sentences. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "4\n",
      "What is the purpose of max pooling in convolutional neural networks?\n",
      "Max pooling is used to obtain the maximum value of each feature map generated by the convolution operation, which provides a summary of the most significant features in the input. This helps to reduce the dimensionality of the feature map while retaining important information for further processing.\n",
      "Question : for the text The idea behind convolution is to take the dot product of a vector of INLINEFORM0 weights INLINEFORM1 also known as kernel vector with each INLINEFORM2 -gram in the sentence INLINEFORM3 to obtain another sequence of features INLINEFORM4 . DISPLAYFORM0 .We then apply a max pooling operation over the feature map and take the maximum value INLINEFORM0 as the feature corresponding to this particular kernel vector. Similarly, varying kernel vectors and window sizes are used to obtain multiple features BIBREF23 ..For each word INLINEFORM0 in the vocabulary, an INLINEFORM1 dimensional vector representation is given in a look up table that is learned from the data BIBREF30 . The vector representation of a sentence is hence a concatenation of vectors for individual words. Similarly, we can have look up tables for other features. One might want to provide features other than words if these features are suspected to be helpful. Now, the convolution kernels are applied to word vectors instead of individual words..We use these features to train higher layers of the CNN that can represent bigger groups of words in sentences. We denote the feature learned at hidden neuron INLINEFORM0 in layer INLINEFORM1 as INLINEFORM2 . Multiple features may be learned in parallel in the same CNN layer. The features learned in each layer are used to train the next layer DISPLAYFORM0 .where * indicates convolution and INLINEFORM0 is a weight kernel for hidden neuron INLINEFORM1 and INLINEFORM2 is the total number of hidden neurons. Training a CNN becomes difficult as the number of layers increases, as the Hessian matrix of second-order derivatives often does not exist. Recently, deep learning has been used to improve the scalability of a model that has inherent parallel computation. This is because hierarchies of modules can provide a compact representation in the form of input-output pairs. Each layer tries to minimize the error between the original state of the input nodes and the state of the input nodes predicted by the hidden neurons..This results in a downward coupling between modules. The more abstract representation at the output of a higher layer module is combined with the less abstract representation at the internal nodes from the module in the layer below. In the next section, we describe deep CNN that can have arbitrary number of layers. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "5\n",
      "Question 1: What is the difference between the semantic sentiment analysis of Twitter data and the Twitter microblog sentiment analysis?\n",
      "Answer 1: The semantic sentiment analysis of Twitter data focuses on understanding the meaning and context of the language used in tweets, while the Twitter microblog sentiment analysis primarily looks at keywords and patterns in the text to identify sentiment.\n",
      "Question : for the text Sentiment Quantification of User-Generated Content, 110170 Semantic Sentiment Analysis of Twitter Data, 110167 Twitter Microblog Sentiment Analysis, 265 generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "6\n",
      "Question 1: What is aspect extraction in sentiment analysis?\n",
      "\n",
      "Answer 1: Aspect extraction is a subtask in sentiment analysis that involves identifying the specific areas or features of a product or service that an opinion holder is either praising or complaining about. Its main goal is to detect the opinion targets in opinionated text.\n",
      "Question : for the text Subjectivity detection is the task of identifying objective and subjective sentences. Objective sentences are those which do not exhibit any sentiment. So, it is desired for a sentiment analysis engine to find and separate the objective sentences for further analysis e.g., polarity detection. In subjective sentences, opinions can often be expressed on one or multiple topics. Aspect extraction is a subtask of sentiment analysis that consists in identifying opinion targets in opinionated text, i.e., in detecting the specific aspects of a product or service the opinion holder is either praising or complaining about. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "7\n",
      "Question 1: What is one potential way to improve the visualization of hierarchies learned through deep learning?\n",
      "\n",
      "Answer 1: In the future, we can consider visualizing the hierarchies of features learned through deep learning and potentially fusing this data with other modalities such as YouTube videos for more comprehensive insights.\n",
      "Question : for the text In the future we will try to visualize the hierarchies of features learned via deep learning. We can also consider fusion with other modalities such as YouTube videos. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "8\n",
      "What is a Bayesian network and how is it used in modeling sequences of sentences?\n",
      "\n",
      "A Bayesian network is a graphical model that represents a joint multivariate probability distribution for a set of random variables. It is used to model sequences of sentences by decomposing the likelihood of node expressions into a product of conditional probabilities, assuming independence of non-descendant nodes given their parents. This enables the network to capture causal dependencies and feedback relationships among words and sentences over one or more time instants. The likelihood of the data is obtained by assuming that the nodes are multivariate Gaussian, and the joint probability of the network can be the product of a set of conditional probability distributions. This approach produces discourse vector representations that are sensitive to the structure of the discourse and promise to capture subtle aspects of discourse comprehension.\n",
      "Question : for the text In tasks where one is concerned with a specific sentence within the context of the previous discourse, capturing the order of the sequences preceding the one at hand may be particularly crucial..We take as given a sequence of sentences INLINEFORM0 , each in turn being a sequence of words so that INLINEFORM1 , where INLINEFORM2 is the length of sentence INLINEFORM3 . Thus, the probability of a word INLINEFORM4 follows the distribution : DISPLAYFORM0 .A Bayesian network is a graphical model that represents a joint multivariate probability distribution for a set of random variables BIBREF54 . It is a directed acyclic graph INLINEFORM0 with a set of parameters INLINEFORM1 that represents the strengths of connections by conditional probabilities..The BN decomposes the likelihood of node expressions into a product of conditional probabilities by assuming independence of non-descendant nodes, given their parents. DISPLAYFORM0 .where INLINEFORM0 denotes the conditional probability of node expression INLINEFORM1 given its parent node expressions INLINEFORM2 , and INLINEFORM3 denotes the maximum likelihood(ML) estimate of the conditional probabilities..Figure FIGREF11 (a) illustrates the state space of a Gaussian Bayesian network (GBN) at time instant INLINEFORM0 where each node INLINEFORM1 is a word in the sentence INLINEFORM2 . The connections represent causal dependencies over one or more time instants. The observed state vector of variable INLINEFORM3 is denoted as INLINEFORM4 and the conditional probability of variable INLINEFORM5 given variable INLINEFORM6 is INLINEFORM7 . The optimal Gaussian network INLINEFORM8 is obtained by maximizing the posterior probability of INLINEFORM9 given the data INLINEFORM10 . From Bayes theorem, the optimal Gaussian network INLINEFORM11 is given by: DISPLAYFORM0 .where INLINEFORM0 is the probability of the Gaussian network and INLINEFORM1 is the likelihood of the expression data given the Gaussian network..Given the set of conditional distributions with parameters INLINEFORM0 , the likelihood of the data is given by DISPLAYFORM0 .To find the likelihood in ( EQREF14 ), and to obtain the optimal Gaussian network as in ( EQREF13 ), Gaussian BN assumes that the nodes are multivariate Gaussian. That is, expression of node INLINEFORM0 can be described with mean INLINEFORM1 and covariance matrix INLINEFORM2 of size INLINEFORM3 . The joint probability of the network can be the product of a set of conditional probability distributions given by: DISPLAYFORM0 .where INLINEFORM0 and INLINEFORM1 denotes the regression coefficient matrix, INLINEFORM2 is the conditional variance of INLINEFORM3 given its parent set INLINEFORM4 , INLINEFORM5 is the covariance between observations of INLINEFORM6 and the variables in INLINEFORM7 , and INLINEFORM8 is the covariance matrix of INLINEFORM9 . The acyclic condition of BN does not allow feedback among nodes, and feedback is an essential characteristic of real world GN..Therefore, dynamic Bayesian networks have recently become popular in building GN with time delays mainly due to their ability to model causal interactions as well as feedback regulations BIBREF55 . A first-order dynamic BN is defined by a transition network of interactions between a pair of Gaussian networks connecting nodes at time instants INLINEFORM0 and INLINEFORM1 . In time instant INLINEFORM2 , the parents of nodes are those specified in the time instant INLINEFORM3 . Similarly, the Gaussian network of a INLINEFORM4 -order dynamic system is represented by a Gaussian network comprising INLINEFORM5 consecutive time points and INLINEFORM6 nodes, or a graph of INLINEFORM7 nodes. In practice, the sentence data is transformed to a BOW model where each sentence is a vector of frequencies for each word in the vocabulary. Figure FIGREF11 (b) illustrates the state space of a first-order Dynamic GBN models transition networks among words in sentences INLINEFORM8 and INLINEFORM9 in consecutive time points, the lines correspond to first-order edges among the words learned using BOW..Hence, a sequence of sentences results in a time series of word frequencies. It can be seen that such a discourse model produces compelling discourse vector representations that are sensitive to the structure of the discourse and promise to capture subtle aspects of discourse comprehension, especially when coupled to further semantic data and unsupervised pre-training. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "9\n",
      "Question 1: What is the relationship between BOW and NLP?\n",
      "Answer 1: BOW is a technique used in NLP where words are represented as a bag of independent words without considering the context, which is useful in text classification and sentiment analysis.\n",
      "Question : for the text Aspect : Feature related to an opinion target.Convolution : features made of consecutive words.BOW : Bag of Words.NLP : Natural Language Processing.CNN : Convolutional Neural Network.LDA : Latent Dirichlet Allocation generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "10\n",
      "Question 1: What method was proposed between 2002 and 2009 to improve subjectivity identification in resource-deficient languages like Spanish? \n",
      "Answer 1: Deep neural networks were proposed to automatically learn a dictionary of features that is portable to new languages.\n",
      "Question : for the text Traditional methods prior to 2001 used hand-crafted templates to identify subjectivity and did not generalize well for resource-deficient languages such as Spanish. Later works published between 2002 and 2009 proposed the use of deep neural networks to automatically learn a dictionary of features (in the form of convolution kernels) that is portable to new languages. Recently, recurrent deep neural networks are being used to model alternating subjective and objective sentences within a single review. Such networks are difficult to train for a large vocabulary of words due to the problem of vanishing gradients. Hence, in this chapter we consider use of heuristics to learn dynamic Gaussian networks to select significant word dependencies between sentences in a single review..Further, in order to relation between opinion targets and the corresponding polarity in a review, aspect based opinion mining is used. Explicit aspects were models by several authors using statistical observations such mutual information between noun phrase and the product class. However this method was unable to detect implicit aspects due to high level of noise in the data. Hence, topic modeling was widely used to extract and group aspects, where the latent variable 'topic' is introduced between the observed variables 'document' and 'word'. In this chapter, we demonstrate the use of 'common sense reasoning' when computing word distributions that enable shifting from a syntactic word model to a semantic concept model. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "11\n",
      "What are the two basic issues associated with sentiment analysis on the Web that require the use of subjectivity detection and aspect extraction?\n",
      "\n",
      "The two basic issues associated with sentiment analysis on the Web are that a lot of factual or non-opinionated information needs to be filtered out, and opinions are most times about different aspects of the same product or service rather than on the whole item. Subjectivity detection ensures that factual information is filtered out, and only opinionated information is passed on to the polarity classifier, while aspect extraction enables the correct distribution of polarity among the different features of the opinion target.\n",
      "Question : for the text While sentiment analysis research has become very popular in the past ten years, most companies and researchers still approach it simply as a polarity detection problem. In reality, sentiment analysis is a `suitcase problem' that requires tackling many natural language processing (NLP) subtasks, including microtext analysis, sarcasm detection, anaphora resolution, subjectivity detection and aspect extraction. In this chapter, we focus on the last two subtasks as they are key for ensuring a minimum level of accuracy in the detection of polarity from social media..The two basic issues associated with sentiment analysis on the Web, in fact, are that (1) a lot of factual or non-opinionated information needs to be filtered out and (2) opinions are most times about different aspects of the same product or service rather than on the whole item and reviewers tend to praise some and criticize others. Subjectivity detection, hence, ensures that factual information is filtered out and only opinionated information is passed on to the polarity classifier and aspect extraction enables the correct distribution of polarity among the different features of the opinion target (in stead of having one unique, averaged polarity assigned to it). In this chapter, we offer some insights about each task and apply an ensemble of deep learning and linguistics to tackle both..The opportunity to capture the opinion of the general public about social events, political movements, company strategies, marketing campaigns, and product preferences has raised increasing interest of both the scientific community (because of the exciting open challenges) and the business world (because of the remarkable benefits for marketing and financial market prediction). Today, sentiment analysis research has its applications in several different scenarios. There are a good number of companies, both large- and small-scale, that focus on the analysis of opinions and sentiments as part of their mission BIBREF0 . Opinion mining techniques can be used for the creation and automated upkeep of review and opinion aggregation websites, in which opinions are continuously gathered from the Web and not restricted to just product reviews, but also to broader topics such as political issues and brand perception. Sentiment analysis also has a great potential as a sub-component technology for other systems. It can enhance the capabilities of customer relationship management and recommendation systems; for example, allowing users to find out which features customers are particularly interested in or to exclude items that have received overtly negative feedback from recommendation lists. Similarly, it can be used in social communication for troll filtering and to enhance anti-spam systems. Business intelligence is also one of the main factors behind corporate interest in the field of sentiment analysis BIBREF1 ..Sentiment analysis is a `suitcase' research problem that requires tackling many NLP sub-tasks, including semantic parsing BIBREF2 , named entity recognition BIBREF3 , sarcasm detection BIBREF4 , subjectivity detection and aspect extraction. In opinion mining, different levels of analysis granularity have been proposed, each one having its own advantages and drawbacks BIBREF5 , BIBREF6 . Aspect-based opinion mining BIBREF7 , BIBREF8 focuses on the relations between aspects and document polarity. An aspect, also known as an opinion target, is a concept in which the opinion is expressed in the given document. For example, in the sentence, “The screen of my phone is really nice and its resolution is superb” for a phone review contains positive polarity, i.e., the author likes the phone. However, more specifically, the positive opinion is about its screen and resolution; these concepts are thus called opinion targets, or aspects, of this opinion. The task of identifying the aspects in a given opinionated text is called aspect extraction. There are two types of aspects defined in aspect-based opinion mining: explicit aspects and implicit aspects. Explicit aspects are words in the opinionated document that explicitly denote the opinion target. For instance, in the above example, the opinion targets screen and resolution are explicitly mentioned in the text. In contrast, an implicit aspect is a concept that represents the opinion target of an opinionated document but which is not specified explicitly in the text. One can infer that the sentence, “This camera is sleek and very affordable” implicitly contains a positive opinion of the aspects appearance and price of the entity camera. These same aspects would be explicit in an equivalent sentence: “The appearance of this camera is sleek and its price is very affordable.”.Most of the previous works in aspect term extraction have either used conditional random fields (CRFs) BIBREF9 , BIBREF10 or linguistic patterns BIBREF7 , BIBREF11 . Both of these approaches have their own limitations: CRF is a linear model, so it needs a large number of features to work well; linguistic patterns need to be crafted by hand, and they crucially depend on the grammatical accuracy of the sentences. In this chapter, we apply an ensemble of deep learning and linguistics to tackle both the problem of aspect extraction and subjectivity detection..The remainder of this chapter is organized as follows: Section SECREF3 and SECREF4 propose some introductory explanation and some literature for the tasks of subjectivity detection and aspect extraction, respectively; Section SECREF5 illustrates the basic concepts of deep learning adopted in this work; Section SECREF6 describes in detail the proposed algorithm; Section SECREF7 shows evaluation results; finally, Section SECREF9 concludes the chapter. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "12\n",
      "Question 1: How can subjectivity detection be useful in multi-perspective question answering summarization systems?\n",
      "\n",
      "Answer 1: Subjectivity detection can prevent the sentiment classifier from considering irrelevant or potentially misleading text and provide clearer visibility into the emotions of people. This is particularly useful in multi-perspective question answering summarization systems that need to summarize different opinions and perspectives and present multiple answers to the user based on opinions derived from different sources.\n",
      "Question : for the text Subjectivity detection can prevent the sentiment classifier from considering irrelevant or potentially misleading text. This is particularly useful in multi-perspective question answering summarization systems that need to summarize different opinions and perspectives and present multiple answers to the user based on opinions derived from different sources. It is also useful to analysts in government, commercial and political domains who need to determine the response of the people to different crisis events. After filtering of subjective sentences, aspect mining can be used to provide clearer visibility into the emotions of people by connecting different polarities to the corresponding target attribute. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "13\n",
      "Question 1: How do the authors model temporal dynamics in product reviews using deep convolutional neural networks?\n",
      "\n",
      "Answer 1: The authors pre-train the deep CNN using dynamic Gaussian Bayesian networks to model temporal dynamics in product reviews.\n",
      "Question : for the text We consider deep convolutional neural networks where each layer is learned independent of the others resulting in low complexity..We model temporal dynamics in product reviews by pre-training the deep CNN using dynamic Gaussian Bayesian networks..We combine linguistic aspect mining with CNN features for effective sentiment detection. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "14\n",
      "What is maximum likelihood estimation in dynamic Gaussian Bayesian networks?\n",
      "\n",
      "Maximum likelihood estimation is a technique used to estimate the edges in dynamic Gaussian Bayesian networks where each node corresponds to a word in a sentence. It involves determining the probability distribution that best explains the data by finding the parameter values that maximize the likelihood of the data given the model.\n",
      "Question : for the text In this section, we briefly review the theoretical concepts necessary to comprehend the present work. We begin with a description of maximum likelihood estimation of edges in dynamic Gaussian Bayesian networks where each node is a word in a sentence. Next, we show that weights in the CNN can be learned by minimizing a global error function that corresponds to an exponential distribution over a linear combination of input sequence of word features..Notations : Consider a Gaussian network (GN) with time delays which comprises a set of INLINEFORM0 nodes and observations gathered over INLINEFORM1 instances for all the nodes. Nodes can take real values from a multivariate distribution determined by the parent set. Let the dataset of samples be INLINEFORM2 , where INLINEFORM3 represents the sample value of the INLINEFORM4 random variable in instance INLINEFORM5 . Lastly, let INLINEFORM6 be the set of parent variables regulating variable INLINEFORM7 . generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "15\n",
      "What method was used to determine the part-of-speech for each word in a sentence during the data pre-processing stage?\n",
      "\n",
      "A POS tagger was used to determine the part-of-speech for each word in a sentence during the data pre-processing stage.\n",
      "Question : for the text We use the MPQA corpus BIBREF20 , a collection of 535 English news articles from a variety of sources manually annotated with subjectivity flag. From the total of 9,700 sentences in this corpus, 55 INLINEFORM0 of the sentences are labeled as subjective while the rest are objective. We also compare with the Movie Review (MR) benchmark dataset BIBREF28 , that contains 5000 subjective movie review snippets from Rotten Tomatoes website and another 5000 objective sentences from plot summaries available from the Internet Movies Database. All sentences are at least ten words long and drawn from reviews or plot summaries of movies released post 2001..The data pre-processing included removing top 50 stop words and punctuation marks from the sentences. Next, we used a POS tagger to determine the part-of-speech for each word in a sentence. Subjectivity clues dataset BIBREF19 contains a list of over 8,000 clues identified manually as well as automatically using both annotated and unannotated data. Each clue is a word and the corresponding part of speech..The frequency of each clue was computed in both subjective and objective sentences of the MPQA corpus. Here we consider the top 50 clue words with highest frequency of occurrence in the subjective sentences. We also extracted 25 top concepts containing the top clue words using the method described in BIBREF11 . The CNN is collectively pre-trained with both subjective and objective sentences that contain high ML word and concept motifs. The word vectors are initialized using the LBL model and a context window of size 5 and 30 features. Each sentence is wrapped to a window of 50 words to reduce the number of parameters and hence the over-fitting of the model. A CNN with three hidden layers of 100 neurons and kernels of size INLINEFORM0 is used. The output layer corresponds to two neurons for each class of sentiments..We used 10 fold cross validation to determine the accuracy of classifying new sentences using the trained CNN classifier. A comparison is done with classifying the time series data using baseline classifiers such as Naive Bayes SVM (NBSVM) BIBREF60 , Multichannel CNN (CNN-MC) BIBREF61 , Subjectivity Word Sense Disambiguation (SWSD) BIBREF62 and Unsupervised-WSD (UWSD) BIBREF63 . Table TABREF41 shows that BCDBN outperforms previous methods by INLINEFORM0 in accuracy on both datasets. Almost INLINEFORM1 improvement is observed over NBSVM on the movie review dataset. In addition, we only consider word vectors of 30 features instead of the 300 features used by CNN-MC and hence are 10 times faster. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "16\n",
      "What is subjectivity detection and why is it important?\n",
      "\n",
      "Answer 1: Subjectivity detection is a crucial subtask of sentiment analysis that can prevent a sentiment classifier from considering irrelevant or potentially misleading text in online social platforms such as Twitter and Facebook. It allows analysts to determine the response of people to different crisis events and helps to construct and maintain sentiment lexicons. Subjectivity detection has progressed from syntactic to semantic methods, and deep neural networks such as convolutional neural networks are being used for high accuracy results.\n",
      "Question : for the text Subjectivity detection is an important subtask of sentiment analysis that can prevent a sentiment classifier from considering irrelevant or potentially misleading text in online social platforms such as Twitter and Facebook. Subjective extraction can reduce the amount of review data to only 60 INLINEFORM0 and still produce the same polarity results as full text classification BIBREF12 . This allows analysts in government, commercial and political domains who need to determine the response of people to different crisis events BIBREF12 , BIBREF13 , BIBREF14 . Similarly, online reviews need to be summarized in a manner that allows comparison of opinions, so that a user can clearly see the advantages and weaknesses of each product merely with a single glance, both in unimodal BIBREF15 and multimodal BIBREF16 , BIBREF17 contexts. Further, we can do in-depth opinion assessment, such as finding reasons or aspects BIBREF18 in opinion-bearing texts. For example, INLINEFORM1 , which makes the film INLINEFORM2 . Several works have explored sentiment composition through careful engineering of features or polarity shifting rules on syntactic structures. However, sentiment accuracies for classifying a sentence as positive/negative/neutral has not exceeded 60 INLINEFORM3 ..Early attempts used general subjectivity clues to generate training data from un-annotated text BIBREF19 . Next, bag-of-words (BOW) classifiers were introduced that represent a document as a multi set of its words disregarding grammar and word order. These methods did not work well on short tweets. Co-occurrence matrices also were unable to capture difference in antonyms such as `good/bad' that have similar distributions. Subjectivity detection hence progressed from syntactic to semantic methods in BIBREF19 , where the authors used extraction pattern to represent subjective expressions. For example, the pattern `hijacking' of INLINEFORM0 , looks for the noun `hijacking' and the object of the preposition INLINEFORM1 . Extracted features are used to train machine-learning classifiers such as SVM BIBREF20 and ELM BIBREF21 . Subjectivity detection is also useful for constructing and maintaining sentiment lexicons, as objective words or concepts need to be omitted from them BIBREF22 ..Since, subjective sentences tend to be longer than neutral sentences, recursive neural networks were proposed where the sentiment class at each node in the parse tree was captured using matrix multiplication of parent nodes BIBREF23 , BIBREF24 . However, the number of possible parent composition functions is exponential, hence in BIBREF25 recursive neural tensor network was introduced that use a single tensor composition function to define multiple bilinear dependencies between words. In BIBREF26 , the authors used logistic regression predictor that defines a hyperplane in the word vector space where a word vectors positive sentiment probability depends on where it lies with respect to this hyperplane. However, it was found that while incorporating words that are more subjective can generally yield better results, the performance gain by employing extra neutral words is less significant BIBREF27 . Another class of probabilistic models called Latent Dirichlet Allocation assumes each document is a mixture of latent topics. Lastly, sentence-level subjectivity detection was integrated into document-level sentiment detection using graphs where each node is a sentence. The contextual constraints between sentences in a graph led to significant improvement in polarity classification BIBREF28 ..Similarly, in BIBREF29 the authors take advantage of the sequence encoding method for trees and treat them as sequence kernels for sentences. Templates are not suitable for semantic role labeling, because relevant context might be very far away. Hence, deep neural networks have become popular to process text. In word2vec, for example, a word's meaning is simply a signal that helps to classify larger entities such as documents. Every word is mapped to a unique vector, represented by a column in a weight matrix. The concatenation or sum of the vectors is then used as features for prediction of the next word in a sentence BIBREF30 . Related words appear next to each other in a INLINEFORM0 dimensional vector space. Vectorizing them allows us to measure their similarities and cluster them. For semantic role labeling, we need to know the relative position of verbs, hence the features can include prefix, suffix, distance from verbs in the sentence etc. However, each feature has a corresponding vector representation in INLINEFORM1 dimensional space learned from the training data..Recently, convolutional neural network (CNN) is being used for subjectivity detection. In particular, BIBREF31 used recurrent CNNs. These show high accuracy on certain datasets such as Twitter we are also concerned with a specific sentence within the context of the previous discussion, the order of the sentences preceding the one at hand results in a sequence of sentences also known as a time series of sentences BIBREF31 . However, their model suffers from overfitting, hence in this work we consider deep convolutional neural networks, where temporal information is modeled via dynamic Gaussian Bayesian networks. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "17\n",
      "Question 1: What is sentiment analysis?\n",
      "Answer 1: Sentiment analysis is the process of analyzing and categorizing text based on the opinions, emotions, and attitudes conveyed within it. This can be used to understand customer feedback, public opinion, and brand reputation.\n",
      "Question : for the text Sentiment Analysis, Subjectivity Detection, Deep Learning Aspect Extraction, Polarity Distribution, Convolutional Neural Network. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "18\n",
      "Question 1: What funding did this work receive?\n",
      "Answer 1: This work was partially supported by the UK EPSRC Grant No. EP/K017896/1 uComp and by the European Union under Grant Agreements No. 611233 PHEME.\n",
      "Question : for the text This work was partially supported by the UK EPSRC Grant No. EP/K017896/1 uComp and by the European Union under Grant Agreements No. 611233 PHEME. The authors wish to thank the CS&L reviewers for their helpful and constructive feedback. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "19\n",
      "Question 1: What is the best approach to generalizing NER systems from training to testing data?\n",
      "\n",
      "Answer 1: According to the study, SENNA achieves consistently the highest performance across most corpora and has the best approach to generalizing from training to testing data, which can mostly be attributed to its use of word embeddings trained with deep convolutional neural nets.\n",
      "Question : for the text This paper investigated the ability of modern NER systems to generalise effectively over a variety of genres. Firstly, by analysing different corpora, we demonstrated that datasets differ widely in many regards: in terms of size; balance of entity classes; proportion of NEs; and how often NEs and tokens are repeated. The most balanced corpus in terms of NE classes is the CoNLL corpus, which, incidentally, is also the most widely used NERC corpus, both for method tuning of off-the-shelf NERC systems (e.g. Stanford NER, SENNA), as well as for comparative evaluation. Corpora, traditionally viewed as noisy, i.e. the Twitter and Web corpora, were found to have a low repetition of NEs and tokens. More surprisingly, however, so does the CoNLL corpus, which indicates that it is well balanced in terms of stories. Newswire corpora have a large proportion of NEs as percentage of all tokens, which indicates high information density. Web, Twitter and telephone conversation corpora, on the other hand, have low information density..Our second set of findings relates to the NERC approaches studied. Overall, SENNA achieves consistently the highest performance across most corpora, and thus has the best approach to generalising from training to testing data. This can mostly be attributed to SENNA's use of word embeddings, trained with deep convolutional neural nets. The default parameters of SENNA achieve a balanced precision and recall, while for Stanford NER and CRFSuite, precision is almost twice as high as recall..Our experiments also confirmed the correlation between NERC performance and training corpus size, although size alone is not an absolute predictor. In particular, the biggest NE-annotated corpus amongst those studied is OntoNotes NW – almost twice the size of CoNLL in terms of number of NEs. Nevertheless, the average F1 for CoNLL is the highest of all corpora and, in particular, SENNA has 11 points higher F1 on CoNLL than on OntoNotes NW..Studying NERC on size-normalised corpora, it becomes clear that there is also a big difference in performance on corpora from the same genre. When normalising training data by size, diverse corpora, such as Web and social media, still yield lower F1 than newswire corpora. This indicates that annotating more training examples for diverse genres would likely lead to a dramatic increase in F1..What is found to be a good predictor of F1 is a memorisation baseline, which picks the most frequent NE label for each token sequence in the test corpus as observed in the training corpus. This supported our hypothesis that entity diversity plays an important role, being negatively correlated with F1. Studying proportions of unseen entity surface forms, experiments showed corpora with a large proportion of unseen NEs tend to yield lower F1, due to much lower performance on unseen than seen NEs (about 17 points lower averaged over all NERC methods and corpora). This finally explains why the performance is highest for the benchmark CoNLL newswire corpus – it contains the lowest proportion of unseen NEs. It also explains the difference in performance between NERC on other corpora. Out of all the possible indicators for high NER F1 studied, this is found to be the most reliable one. This directly supports our hypothesis that generalising for unseen named entities is both difficult and important..Also studied is the proportion of unseen features per unseen and seen NE portions of different corpora. However, this is found to not be very helpful. The proportion of seen features is higher for seen NEs, as it should be. However, within the seen and unseen NE splits, there is no clear trend indicating if having more seen features helps..We also showed that hand-annotating more training examples is a straight-forward and reliable way of improving NERC performance. However, this is costly, which is why it can be useful to study if using different, larger corpora for training might be helpful. Indeed, substituting in-domain training corpora with other training corpora for the same genre created at the same time improves performance, and studying how such corpora can be combined with transfer learning or domain adaptation strategies might improve performance even further. However, for most corpora, there is a significant drop in performance for out-of-domain training. What is again found to be reliable is to check the memorisation baseline: if results for the out-of-domain memorisation baseline are higher than for in-domain memorisation, than using the out-of-domain corpus for training is likely to be helpful..Across a broad range of corpora and genres, characterised in different ways, we have examined how named entities are embedded and presented. While there is great variation in the range and class of entities found, it is consistent that the more varied texts are harder to do named entity recognition in. This connection with variation occurs to such an extent that, in fact, performance when memorising lexical forms stably predicts system accuracy. The result of this is that systems are not sufficiently effective at generalising beyond the entity surface forms and contexts found in training data. To close this gap and advance NER systems, and cope with the modern reality of streamed NER, as opposed to the prior generation of batch-learning based systems with static evaluation sets being used as research benchmarks, future work needs to address named entity generalisation and out-of-vocabulary lexical forms. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "20\n",
      "What is the goal of the study, and what types of NER corpora are included?\n",
      "The goal of the study is to compare NER performance on corpora from diverse domains and genres, and seven benchmark NER corpora are included, spanning newswire, broadcast conversation, Web content, and social media. These datasets were chosen such that they have been annotated with the same or very similar entity classes, in particular, names of people, locations, and organizations.\n",
      "Question : for the text Since the goal of this study is to compare NER performance on corpora from diverse domains and genres, seven benchmark NER corpora are included, spanning newswire, broadcast conversation, Web content, and social media (see Table 1 for details). These datasets were chosen such that they have been annotated with the same or very similar entity classes, in particular, names of people, locations, and organisations. Thus corpora including only domain-specific entities (e.g. biomedical corpora) were excluded. The choice of corpora was also motivated by their chronological age; we wanted to ensure a good temporal spread, in order to study possible effects of entity drift over time..A note is required about terminology. This paper refers to text genre and also text domain. These are two dimensions by which a document or corpus can be described. Genre here accounts the general characteristics of the text, measurable with things like register, tone, reading ease, sentence length, vocabulary and so on. Domain describes the dominant subject matter of text, which might give specialised vocabulary or specific, unusal word senses. For example, “broadcast news\" is a genre, describing the manner of use of language, whereas “financial text\" or “popular culture\" are domains, describing the topic. One notable exception to this terminology is social media, which tends to be a blend of myriad domains and genres, with huge variation in both these dimensions BIBREF38 , BIBREF39 ; for simplicity, we also refer to this as a genre here..In chronological order, the first corpus included here is MUC 7, which is the last of the MUC challenges BIBREF31 . This is an important corpus, since the Message Understanding Conference (MUC) was the first one to introduce the NER task in 1995 BIBREF3 , with focus on recognising persons, locations and organisations in newswire text..A subsequent evaluation campaign was the CoNLL 2003 NER shared task BIBREF40 , which created gold standard data for newswire in Spanish, Dutch, English and German. The corpus of this evaluation effort is now one of the most popular gold standards for NER, with new NER approaches and methods often reporting performance on that..Later evaluation campaigns began addressing NER for genres other than newswire, specifically ACE BIBREF30 and OntoNotes BIBREF29 . Both of those contain subcorpora in several genres, namely newswire, broadcast news, broadcast conversation, weblogs, and conversational telephone speech. ACE, in addition, contains a subcorpus with usenet newsgroups. Like CoNLL 2003, the OntoNotes corpus is also a popular benchmark dataset for NER. The languages covered are English, Arabic and Chinese. A further difference between the ACE and OntoNotes corpora on one hand, and CoNLL and MUC on the other, is that they contain annotations not only for NER, but also for other tasks such as coreference resolution, relation and event extraction and word sense disambiguation. In this paper, however, we restrict ourselves purely to the English NER annotations, for consistency across datasets. The ACE corpus contains HEAD as well as EXTENT annotations for NE spans. For our experiments we use the EXTENT tags..With the emergence of social media, studying NER performance on this genre gained momentum. So far, there have been no big evaluation efforts, such as ACE and OntoNotes, resulting in substantial amounts of gold standard data. Instead, benchmark corpora were created as part of smaller challenges or individual projects. The first such corpus is the UMBC corpus for Twitter NER BIBREF33 , where researchers used crowdsourcing to obtain annotations for persons, locations and organisations. A further Twitter NER corpus was created by BIBREF21 , which, in contrast to other corpora, contains more fine-grained classes defined by the Freebase schema BIBREF41 . Next, the Making Sense of Microposts initiative BIBREF32 (MSM) provides single annotated data for named entity recognition on Twitter for persons, locations, organisations and miscellaneous. MSM initiatives from 2014 onwards in addition feature a named entity linking task, but since we only focus on NER here, we use the 2013 corpus..These corpora are diverse not only in terms of genres and time periods covered, but also in terms of NE classes and their definitions. In particular, the ACE and OntoNotes corpora try to model entity metonymy by introducing facilities and geo-political entities (GPEs). Since the rest of the benchmark datasets do not make this distinction, metonymous entities are mapped to a more common entity class (see below)..In order to ensure consistency across corpora, only Person (PER), Location (LOC) and Organisation (ORG) are used in our experiments, and other NE classes are mapped to O (no NE). For the Ritter corpus, the 10 entity classes are collapsed to three as in BIBREF21 . For the ACE and OntoNotes corpora, the following mapping is used: PERSON $\\rightarrow $ PER; LOCATION, FACILITY, GPE $\\rightarrow $ LOC; ORGANIZATION $\\rightarrow $ ORG; all other classes $\\rightarrow $ O..Tokens are annotated with BIO sequence tags, indicating that they are the beginning (B) or inside (I) of NE mentions, or outside of NE mentions (O). For the Ritter and ACE 2005 corpora, separate training and test corpora are not publicly available, so we randomly sample 1/3 for testing and use the rest for training. The resulting training and testing data sizes measured in number of NEs are listed in Table 2 . Separate models are then trained on the training parts of each corpus and evaluated on the development (if available) and test parts of the same corpus. If development parts are available, as they are for CoNLL (CoNLL Test A) and MUC (MUC 7 Dev), they are not merged with the training corpora for testing, as it was permitted to do in the context of those evaluation challenges..[t]. P, R and F1 of NERC with different models evaluated on different testing corpora, trained on corpora normalised by size.Table 1 shows which genres the different corpora belong to, the number of NEs and the proportions of NE classes per corpus. Sizes of NER corpora have increased over time, from MUC to OntoNotes..Further, the class distribution varies between corpora: while the CoNLL corpus is very balanced and contains about equal numbers of PER, LOC and ORG NEs, other corpora are not. The least balanced corpus is the MSM 2013 Test corpus, which contains 98 LOC NEs, but 1110 PER NEs. This makes it difficult to compare NER performance here, since performance partly depends on training data size. Since comparing NER performance as such is not the goal of this paper, we will illustrate the impact of training data size by using learning curves in the next section; illustrate NERC performance on trained corpora normalised by size in Table UID9 ; and then only use the original training data size for subsequent experiments..In order to compare corpus diversity across genres, we measure NE and token/type diversity (following e.g. BIBREF2 ). Note that types are the unique tokens, so the ratio can be understood as ratio of total tokens to unique ones. Table 4 shows the ratios between the number of NEs and the number of unique NEs per corpus, while Table 5 reports the token/type ratios. The lower those ratios are, the more diverse a corpus is. While token/type ratios also include tokens which are NEs, they are a good measure of broader linguistic diversity..Aside from these metrics, there are other factors which contribute to corpus diversity, including how big a corpus is and how well sampled it is, e.g. if a corpus is only about one story, it should not be surprising to see a high token/type ratio. Therefore, by experimenting on multiple corpora, from different genres and created through different methodologies, we aim to encompass these other aspects of corpus diversity..Since the original NE and token/type ratios do not account for corpus size, Tables 5 and 4 present also the normalised ratios. For those, a number of tokens equivalent to those in the corpus, e.g. 7037 for UMBC (Table 5 ) or, respectively, a number of NEs equivalent to those in the corpus (506 for UMBC) are selected (Table 4 )..An easy choice of sampling method would be to sample tokens and NEs randomly. However, this would not reflect the composition of corpora appropriately. Corpora consist of several documents, tweets or blog entries, which are likely to repeat the words or NEs since they are about one story. The difference between bigger and smaller corpora is then that bigger corpora consist of more of those documents, tweets, blog entries, interviews, etc. Therefore, when we downsample, we take the first $n$ tokens for the token/type ratios or the first $n$ NEs for the NEs/Unique NEs ratios..Looking at the normalised diversity metrics, the lowest NE/Unique NE ratios $<= 1.5$ (in bold, Table 4 ) are observed on the Twitter and CoNLL Test corpora. Seeing this for Twitter is not surprising since one would expect noise in social media text (e.g. spelling variations or mistakes) to also have an impact on how often the same NEs are seen. Observing this in the latter, though, is less intuitive and suggests that the CoNLL corpora are well balanced in terms of stories. Low NE/Unique ratios ( $<= 1.7$ ) can also be observed for ACE WL, ACE UN and OntoNotes TC. Similar to social media text, content from weblogs, usenet dicussions and telephone conversations also contains a larger amount of noise compared to the traditionally-studied newswire genre, so this is not a surprising result. Corpora bearing high NE/Unique NE ratios ( $> 2.5$ ) are ACE CTS, OntoNotes MZ and OntoNotes BN. These results are also not surprising. The telephone conversations in ACE CTS are all about the same story, and newswire and broadcast news tend to contain longer stories (reducing variety in any fixed-size set) and are more regular due to editing..The token/type ratios reflect similar trends (Table 5 ). Low token/type ratios $<= 2.8$ (in bold) are observed for the Twitter corpora (Ritter and UMBC), as well as for the CoNLL Test corpus. Token/type ratios are also low ( $<= 3.2$ ) for CoNLL Train and ACE WL. Interestingly, ACE UN and MSM Train and Test do not have low token/type ratios although they have low NE/Unique ratios. That is, many diverse persons, locations and organisations are mentioned in those corpora, but similar context vocabulary is used. Token/type ratios are high ( $>= 4.4$ ) for MUC7 Dev, ACE BC, ACE CTS, ACE UN and OntoNotes TC. Telephone conversations (TC) having high token/type ratios can be attributed to the high amount filler words (e.g. “uh”, “you know”). NE corpora are generally expected to have regular language use – for ACE, at least, in this instance..Furthermore, it is worth pointing out that, especially for the larger corpora (e.g. OntoNotes NW), size normalisation makes a big difference. The normalised NE/Unique NE ratios drop by almost a half compared to the un-normalised ratios, and normalised Token/Type ratios drop by up to 85%. This strengthens our argument for size normalisation and also poses the question of low NERC performance for diverse genres being mostly due to the lack of large training corpora. This is examined in Section \"RQ2: NER performance in Different Genres\" ..Lastly, Table 6 reports tag density (percentage of tokens tagged as part of a NE), which is another useful metric of corpus diversity that can be interpreted as the information density of a corpus. What can be observed here is that the NW corpora have the highest tag density and generally tend to have higher tag density than corpora of other genres; that is, newswire bears a lot of entities. Corpora with especially low tag density $<= 0.06$ (in bold) are the TC corpora, Ritter, OntoNotes WB, ACE UN, ACE BN and ACE BC. As already mentioned, conversational corpora, to which ACE BC also belong, tend to have many filler words, thus it is not surprising that they have a low tag density. There are only minor differences between the tag density and the normalised tag density, since corpus size as such does not impact tag density. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "21\n",
      "What is the main objective of the paper?\n",
      "\n",
      "The main objective of the paper is to investigate the impact of named entity (NE) diversity and context diversity on performance of several different approaches to statistical named entity recognition (NER), and to study the relationship between out-of-vocabulary NEs and features and F1 score.\n",
      "Question : for the text Named entity recognition and classification (NERC, short NER), the task of recognising and assigning a class to mentions of proper names (named entities, NEs) in text, has attracted many years of research BIBREF0 , BIBREF1 , analyses BIBREF2 , starting from the first MUC challenge in 1995 BIBREF3 . Recognising entities is key to many applications, including text summarisation BIBREF4 , search BIBREF5 , the semantic web BIBREF6 , topic modelling BIBREF7 , and machine translation BIBREF8 , BIBREF9 ..As NER is being applied to increasingly diverse and challenging text genres BIBREF10 , BIBREF11 , BIBREF12 , this has lead to a noisier, sparser feature space, which in turn requires regularisation BIBREF13 and the avoidance of overfitting. This has been the case even for large corpora all of the same genre and with the same entity classification scheme, such as ACE BIBREF14 . Recall, in particular, has been a persistent problem, as named entities often seem to have unusual surface forms, e.g. unusual character sequences for the given language (e.g. Szeged in an English-language document) or words that individually are typically not NEs, unless they are combined together (e.g. the White House)..Indeed, the move from ACE and MUC to broader kinds of corpora has presented existing NER systems and resources with a great deal of difficulty BIBREF15 , which some researchers have tried to address through domain adaptation, specifically with entity recognition in mind BIBREF16 , BIBREF17 , BIBREF18 , BIBREF19 , BIBREF20 . However, more recent performance comparisons of NER methods over different corpora showed that older tools tend to simply fail to adapt, even when given a fair amount of in-domain data and resources BIBREF21 , BIBREF11 . Simultaneously, the value of NER in non-newswire data BIBREF21 , BIBREF22 , BIBREF23 , BIBREF24 , BIBREF25 has rocketed: for example, social media now provides us with a sample of all human discourse, unmolested by editors, publishing guidelines and the like, and all in digital format – leading to, for example, whole new fields of research opening in computational social science BIBREF26 , BIBREF27 , BIBREF28 ..The prevailing assumption has been that this lower NER performance is due to domain differences arising from using newswire (NW) as training data, as well as from the irregular, noisy nature of new media (e.g. BIBREF21 ). Existing studies BIBREF11 further suggest that named entity diversity, discrepancy between named entities in the training set and the test set (entity drift over time in particular), and diverse context, are the likely reasons behind the significantly lower NER performance on social media corpora, as compared to newswire..No prior studies, however, have investigated these hypotheses quantitatively. For example, it is not yet established whether this performance drop is really due to a higher proportion of unseen NEs in the social media, or is it instead due to NEs being situated in different kinds of linguistic context..Accordingly, the contributions of this paper lie in investigating the following open research questions:.In particular, the paper carries out a comparative analyses of the performance of several different approaches to statistical NER over multiple text genres, with varying NE and lexical diversity. In line with prior analyses of NER performance BIBREF2 , BIBREF11 , we carry out corpus analysis and introduce briefly the NER methods used for experimentation. Unlike prior efforts, however, our main objectives are to uncover the impact of NE diversity and context diversity on performance (measured primarily by F1 score), and also to study the relationship between OOV NEs and features and F1. See Section \"Experiments\" for details..To ensure representativeness and comprehensiveness, our experimental findings are based on key benchmark NER corpora spanning multiple genres, time periods, and corpus annotation methodologies and guidelines. As detailed in Section \"Datasets\" , the corpora studied are OntoNotes BIBREF29 , ACE BIBREF30 , MUC 7 BIBREF31 , the Ritter NER corpus BIBREF21 , the MSM 2013 corpus BIBREF32 , and the UMBC Twitter corpus BIBREF33 . To eliminate potential bias from the choice of statistical NER approach, experiments are carried out with three differently-principled NER approaches, namely Stanford NER BIBREF34 , SENNA BIBREF35 and CRFSuite BIBREF36 (see Section \"NER Models and Features\" for details). generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "22\n",
      "What are the three supervised statistical approaches to NER used in the experiments?\n",
      "\n",
      "The three supervised statistical approaches to NER used in the experiments are Stanford NER, SENNA, and CRFSuite.\n",
      "Question : for the text To avoid system-specific bias in our experiments, three widely-used supervised statistical approaches to NER are included: Stanford NER, SENNA, and CRFSuite. These systems each have contrasting notable attributes..Stanford NER BIBREF34 is the most popular of the three, deployed widely in both research and commerce. The system has been developed in terms of both generalising the underlying technology and also specific additions for certain languages. The majority of openly-available additions to Stanford NER, in terms of models, gazetteers, prefix/suffix handling and so on, have been created for newswire-style text. Named entity recognition and classification is modelled as a sequence labelling task with first-order conditional random fields (CRFs) BIBREF43 ..SENNA BIBREF35 is a more recent system for named entity extraction and other NLP tasks. Using word representations and deep learning with deep convolutional neural networks, the general principle for SENNA is to avoid task-specific engineering while also doing well on multiple benchmarks. The approach taken to fit these desiderata is to use representations induced from large unlabelled datasets, including LM2 (introduced in the paper itself) and Brown clusters BIBREF44 , BIBREF45 . The outcome is a flexible system that is readily adaptable, given training data. Although the system is more flexible in general, it relies on learning language models from unlabelled data, which might take a long time to gather and retrain. For the setup in BIBREF35 language models are trained for seven weeks on the English Wikipedia, Reuters RCV1 BIBREF46 and parts of the Wall Street Journal, and results are reported over the CoNLL 2003 NER dataset. Reuters RCV1 is chosen as unlabelled data because the English CoNLL 2003 corpus is created from the Reuters RCV1 corpus. For this paper, we use the original language models distributed with SENNA and evaluate SENNA with the DeepNL framework BIBREF47 . As such, it is to some degree also biased towards the CoNLL 2003 benchmark data..Finally, we use the classical NER approach from CRFsuite BIBREF36 , which also uses first-order CRFs. This frames NER as a structured sequence prediction task, using features derived directly from the training text. Unlike the other systems, no external knowledge (e.g. gazetteers and unsupervised representations) are used. This provides a strong basic supervised system, and – unlike Stanford NER and SENNA – has not been tuned for any particular domain, giving potential to reveal more challenging domains without any intrinsic bias..We use the feature extractors natively distributed with the NER frameworks. For Stanford NER we use the feature set “chris2009” without distributional similarity, which has been tuned for the CoNLL 2003 data. This feature was tuned to handle OOV words through word shape, i.e. capitalisation of constituent characters. The goal is to reduce feature sparsity – the basic problem behind OOV named entities – by reducing the complexity of word shapes for long words, while retaining word shape resolution for shorter words. In addition, word clusters, neighbouring n-grams, label sequences and quasi-Newton minima search are included. SENNA uses word embedding features and gazetteer features; for the training configuration see https://github.com/attardi/deepnl#benchmarks. Finally, for CRFSuite, we use the provided feature extractor without POS or chunking features, which leaves unigram and bigram word features of the mention and in a window of 2 to the left and the right of the mention, character shape, prefixes and suffixes of tokens..These systems are compared against a simple surface form memorisation tagger. The memorisation baseline picks the most frequent NE label for each token sequence as observed in the training corpus. There are two kinds of ambiguity: one is overlapping sequences, e.g. if both “New York City” and “New York” are memorised as a location. In that case the longest-matching sequence is labelled with the corresponding NE class. The second, class ambiguity, occurs when the same textual label refers to different NE classes, e.g. “Google” could either refer to the name of a company, in which case it would be labelled as ORG, or to the company's search engine, which would be labelled as O (no NE). generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "23\n",
      "What NERC method achieves the highest F1 results according to the study?\n",
      "\n",
      "According to the study, the NERC method that achieves the highest F1 results is SENNA, followed by Stanford NER and CRFSuite.\n",
      "Question : for the text [t]. P, R and F1 of NERC with different models trained on original corpora.[t]. F1 per NE type with different models trained on original corpora.Our first research question is how NERC performance differs for corpora between approaches. In order to answer this, Precision (P), Recall (R) and F1 metrics are reported on size-normalised corpora (Table UID9 ) and original corpora (Tables \"RQ1: NER performance with Different Approaches\" and \"RQ1: NER performance with Different Approaches\" ). The reason for size normalisation is to make results comparable across corpora. For size normalisation, the training corpora are downsampled to include the same number of NEs as the smallest corpus, UMBC. For that, sentences are selected from the beginning of the train part of the corpora so that they include the same number of NEs as UMBC. Other ways of downsampling the corpora would be to select the first $n$ sentences or the first $n$ tokens, where $n$ is the number of sentences in the smallest corpus. The reason that the number of NEs, which represent the number of positive training examples, is chosen for downsampling the corpora is that the number of positive training examples have a much bigger impact on learning than the number of negative training examples. For instance, BIBREF48 , among others, study topic classification performance for small corpora and sample from the Reuters corpus. They find that adding more negative training data gives little to no improvement, whereas adding positive examples drastically improves performance..Table UID9 shows results with size normalised precision (P), recall (R), and F1-Score (F1). The five lowest P, R and F1 values per method (CRFSuite, Stanford NER, SENNA) are in bold to highlight underperformers. Results for all corpora are summed with macro average..Comparing the different methods, the highest F1 results are achieved with SENNA, followed by Stanford NER and CRFSuite. SENNA has a balanced P and R, which can be explained by the use of word embeddings as features, which help with the unseen word problem. For Stanford NER as well as CRFSuite, which do not make use of embeddings, recall is about half of precision. These findings are in line with other work reporting the usefulness of word embeddings and deep learning for a variety of NLP tasks and domains BIBREF49 , BIBREF50 , BIBREF51 . With respect to individual corpora, the ones where SENNA outperforms other methods by a large margin ( $>=$ 13 points in F1) are CoNLL Test A, ACE CTS and OntoNotes TC. The first success can be attributed to being from the same the domain SENNA was originally tuned for. The second is more unexpected and could be due to those corpora containing a disproportional amount of PER and LOC NEs (which are easier to tag correctly) compared to ORG NEs, as can be seen in Table \"RQ1: NER performance with Different Approaches\" , where F1 of NERC methods is reported on the original training data..Our analysis of CRFSuite here is that it is less tuned for NW corpora and might therefore have a more balanced performance across genres does not hold. Results with CRFSuite for every corpus are worse than the results for that corpus with Stanford NER, which is also CRF-based..To summarise, our findings are:.[noitemsep].F1 is highest with SENNA, followed by Stanford NER and CRFSuite.SENNA outperforms other methods by a large margin (e.g. $>=$ 13 points in F1) for CoNLL Test A, ACE CTS and OntoNotes TC.Our hypothesis that CRFSuite is less tuned for NW corpora and will therefore have a more balanced performance across genres does not hold, as results for CRFSuite for every corpus are worse than with Stanford NER generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "24\n",
      "Question 1: Is there a correlation between tag density and NERC performance?\n",
      "Answer 1: Yes, there is a strong positive correlation between high F1 and high tag density. However, tag density is not an absolute predictor for NERC performance as other factors such as in-domain training data and entity drift can also impact performance.\n",
      "Question : for the text Our second research question is whether existing NER approaches generalise well over corpora in different genres. To do this we study again Precision (P), Recall (R) and F1 metrics on size-normalised corpora (Table UID9 ), on original corpora (Tables \"RQ1: NER performance with Different Approaches\" and \"RQ1: NER performance with Different Approaches\" ), and we further test performance per genre in a separate table (Table 3 )..F1 scores over size-normalised corpora vary widely (Table UID9 ). For example, the SENNA scores range from 9.35% F1 (ACE UN) to 71.48% (CoNLL Test A). Lowest results are consistently observed for the ACE subcorpora, UMBC, and OntoNotes BC and WB. The ACE corpora are large and so may be more prone to non-uniformities emerging during downsampling; they also have special rules for some kinds of organisation which can skew results (as described in Section UID9 ). The highest results are on the CoNLL Test A corpus, OntoNotes BN and MUC 7 Dev. This moderately supports our hypothesis that NER systems perform better on NW than on other genres, probably due to extra fitting from many researchers using them as benchmarks for tuning their approaches. Looking at the Twitter (TWI) corpora present the most challenge due to increased diversity, the trends are unstable. Although results for UMBC are among the lowest, results for MSM 2013 and Ritter are in the same range or even higher than those on NW datasets. This begs the question whether low results for Twitter corpora reported previously were due to the lack of sufficient in-genre training data..Comparing results on normalised to non-normalised data, Twitter results are lower than those for most OntoNotes corpora and CoNLL test corpora, mostly due to low recall. Other difficult corpora having low performance are ACE UN and WEB corpora. We further explicitly examine results on size normalised corpora grouped by corpus type, shown in Table 3 . It becomes clear that, on average, newswire corpora and OntoNotes MZ are the easiest corpora and ACE UN, WEB and TWI are harder. This confirms our hypothesis that social media and Web corpora are challenging for NERC..The CoNLL results, on the other hand, are the highest across all corpora irrespective of the NERC method. What is very interesting to see is that they are much higher than the results on the biggest training corpus, OntoNotes NW. For instance, SENNA has an F1 of 78.04 on OntoNotes, compared to an F1 of 92.39 and 86.44 for CoNLL Test A and Test B respectively. So even though OntoNotes NW is more than twice the size of CoNLL in terms of NEs (see Table 4 ), NERC performance is much higher on CoNLL. NERC performance with respect to training corpus size is represented in Figure 1 . The latter figure confirms that although there is some correlation between corpus size and F1, the variance between results on comparably sized corpora is big. This strengthens our argument that there is a need for experimental studies, such as those reported below, to find out what, apart from corpus size, impacts NERC performance..Another set of results presented in Table \"RQ1: NER performance with Different Approaches\" are those of the simple NERC memorisation baseline. It can be observed that corpora with a low F1 for NERC methods, such as UMBC and ACE UN, also have a low memorisation performance. Memorisation is discussed in more depth in Section \"RQ5: Out-Of-Domain NER Performance and Memorisation\" ..When NERC results are compared to the corpus diversity statistics, i.e. NE/Unique NE ratios (Table 4 ), token/type ratios (Table 5 ), and tag density (Table 6 ), the strongest predictor for F1 is tag density, as can be evidenced by the R correlation values between the ratios and F1 scores with the Stanford NER system, shown in the respective tables..There is a positive correlation between high F1 and high tag density (R of 0.57 and R of 0.62 with normalised tag density), a weak positive correlation for NE/unique ratios (R of 0.20 and R of 0.15 for normalised ratio), whereas for token/type ratios, no such clear correlation can be observed (R of 0.25 and R of -0.07 for normalised ratio)..However, tag density is also not an absolute predictor for NERC performance. While NW corpora have both high NERC performance and high tag density, this high density is not necessarily an indicator of high performance. For example, systems might not find high tag density corpora of other genres necessarily so easy..One factor that can explain the difference in genre performance between e.g. newswire and social media is entity drift – the change in observed entity terms over time. In this case, it is evident from the differing surface forms and contexts for a given entity class. For example, the concept of “location\" that NER systems try to learn might be frequently represented in English newswire from 1991 with terms like Iraq or Kuwait, but more with Atlanta, Bosnia and Kabul in the same language and genre from 1996. Informally, drift on Twitter is often characterised as both high-frequency and high-magnitude; that is, the changes are both rapid and correspond to a large amount of surface form occurrences (e.g. BIBREF12 , BIBREF52 )..We examined the impact of drift in newswire and Twitter corpora, taking datasets based in different timeframes. The goal is to gauge how much diversity is due to new entities appearing over time. To do this, we used just the surface lexicalisations of entities as the entity representation. The overlap of surface forms was measured across different corpora of the same genre and language. We used an additional corpus based on recent data – that from the W-NUT 2015 challenge BIBREF25 . This is measured in terms of occurrences, rather than distinct surface forms, so that the magnitude of the drift is shown instead of having skew in results from the the noisy long tail. Results are given in Table 7 for newswire and Table 8 for Twitter corpora..It is evident that the within-class commonalities in surface forms are much higher in newswire than in Twitter. That is to say, observations of entity texts in one newswire corpus are more helpful in labelling other newswire corpora, than if the same technique is used to label other twitter corpora..This indicates that drift is lower in newswire than in tweets. Certainly, the proportion of entity mentions in most recent corpora (the rightmost-columns) are consistently low compared to entity forms available in earlier data. These reflect the raised OOV and drift rates found in previous work BIBREF12 , BIBREF53 . Another explanation is that there is higher noise in variation, and that the drift is not longitudinal, but rather general. This is partially addressed by RQ3, which we will address next, in Section \"RQ3: Impact of NE Diversity\" ..To summarise, our findings are:.[noitemsep].Overall, F1 scores vary widely across corpora..Trends can be marked in some genres. On average, newswire corpora and OntoNotes MZ are the easiest corpora and ACE UN, WEB and TWI are the hardest corpora for NER methods to reach good performance on..Normalising corpora by size results in more noisy data such as TWI and WEB data achieving similar results to NW corpora..Increasing the amount of available in-domain training data will likely result in improved NERC performance..There is a strong positive correlation between high F1 and high tag density, a weak positive correlation for NE/unique ratios and no clear correlation between token/type ratios and F1.Temporal NE drift is lower in newswire than in tweets.The next section will take a closer look at the impact of seen and unseen NEs on NER performance. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "25\n",
      "What is the difference between seen and unseen NEs and how does it affect NERC performance?\n",
      "\n",
      "Answer 1: Seen NEs are those encountered in both the test and training data, while unseen NEs are those with surface forms present only in the test, but not in training data. The F1 scores on unseen NEs are significantly lower than on seen NEs for all three NERC approaches, mostly due to lower recall on unseen NEs. This suggests poor generalization and memorization in existing systems. Additionally, there are still significant differences in labelling seen NEs in different corpora, which indicates that if NEs are seen or unseen does not account for all the differences in F1 scores between corpora of different genres.\n",
      "Question : for the text Unseen NEs are those with surface forms present only in the test, but not training data, whereas seen NEs are those also encountered in the training data. As discussed previously, the ratio between those two measures is an indicator of corpus NE diversity..Table 9 shows how the number of unseen NEs per test corpus relates to the total number of NEs per corpus. The proportion of unseen forms varies widely by corpus, ranging from 0.351 (ACE NW) to 0.931 (UMBC). As expected there is a correlation between corpus size and percentage of unseen NEs, i.e. smaller corpora such as MUC and UMBC tend to contain a larger proportion of unseen NEs than bigger corpora such as ACE NW. In addition, similar to the token/type ratios listed in Table 5 , we observe that TWI and WEB corpora have a higher proportion of unseen entities..As can be seen from Table \"RQ1: NER performance with Different Approaches\" , corpora with a low percentage of unseen NEs (e.g. CoNLL Test A and OntoNotes NW) tend to have high NERC performance, whereas corpora with high percentage of unseen NEs (e.g. UMBC) tend to have low NERC performance. This suggests that systems struggle to recognise and classify unseen NEs correctly..To check this seen/unseen performance split, next we examine NERC performance for unseen and seen NEs separately; results are given in Table 10 . The “All\" column group represents an averaged performance result. What becomes clear from the macro averages is that F1 on unseen NEs is significantly lower than F1 on seen NEs for all three NERC approaches. This is mostly due to recall on unseen NEs being lower than that on seen NEs, and suggests some memorisation and poor generalisation in existing systems. In particular, Stanford NER and CRFSuite have almost 50% lower recall on unseen NEs compared to seen NEs. One outlier is ACE UN, for which the average seen F1 is 1.01 and the average unseen F1 is 1.52, though both are miniscule and the different negligible..Of the three approaches, SENNA exhibits the narrowest F1 difference between seen and unseen NEs. In fact it performs below Stanford NER for seen NEs on many corpora. This may be because SENNA has but a few features, based on word embeddings, which reduces feature sparsity; intuitively, the simplicity of the representation is likely to help with unseen NEs, at the cost of slightly reduced performance on seen NEs through slower fitting. Although SENNA appears to be better at generalising than Stanford NER and our CRFSuite baseline, the difference between its performance on seen NEs and unseen NEs is still noticeable. This is 21.77 for SENNA (macro average), whereas it is 29.41 for CRFSuite and 35.68 for Stanford NER..The fact that performance over unseen entities is significantly lower than on seen NEs partly explains what we observed in the previous section; i.e., that corpora with a high proportion of unseen entities, such as the ACE WL corpus, are harder to label than corpora of a similar size from other genres, such as the ACE BC corpus (e.g. systems reach F1 of $\\sim $ 30 compared to $\\sim $ 50; Table \"RQ1: NER performance with Different Approaches\" )..However, even though performance on seen NEs is higher than on unseen, there is also a difference between seen NEs in corpora of different sizes and genres. For instance, performance on seen NEs in ACE WL is 70.86 (averaged over the three different approaches), whereas performance on seen NEs in the less-diverse ACE BC corpus is higher at 76.42; the less diverse data is, on average, easier to tag. Interestingly, average F1 on seen NEs in the Twitter corpora (MSM and Ritter) is around 80, whereas average F1 on the ACE corpora, which are of similar size, is lower, at around 70..To summarise, our findings are:.[noitemsep].F1 on unseen NEs is significantly lower than F1 on seen NEs for all three NERC approaches, which is mostly due to recall on unseen NEs being lower than that on seen NEs..Performance on seen NEs is significantly and consistently higher than that of unseen NEs in different corpora, with the lower scores mostly attributable to lower recall..However, there are still significant differences at labelling seen NEs in different corpora, which means that if NEs are seen or unseen does not account for all of the difference of F1 between corpora of different genres. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "26\n",
      "What is the difference between unseen features and unseen words in NER? \n",
      "\n",
      "Unseen features are those observed in the test data but not in the training data, while unseen words are those that have not been encountered by the system at the time of labeling. Unseen features are the difference in representation, not surface form, and may still have shared features with seen NEs.\n",
      "Question : for the text Having examined the impact of seen/unseen NEs on NERC performance in RQ3, and touched upon surface form drift in RQ2, we now turn our attention towards establishing the impact of seen features, i.e. features appearing in the test set that are observed also in the training set. While feature sparsity can help to explain low F1, it is not a good predictor of performance across methods: sparse features can be good if mixed with high-frequency ones. For instance, Stanford NER often outperforms CRFSuite (see Table \"RQ1: NER performance with Different Approaches\" ) despite having a lower proportion of seen features (i.e. those that occur both in test data and during training). Also, some approaches such as SENNA use a small number of features and base their features almost entirely on the NEs and not on their context..Subsequently, we want to measure F1 for unseens and seen NEs, as in Section \"RQ3: Impact of NE Diversity\" , but also examine how the proportion of seen features impacts on the result. We define seen features as those observed in the test data and also the training data. In turn, unseen features are those observed in the test data but not in the training data. That is, they have not been previously encountered by the system at the time of labeling. Unseen features are different from unseen words in that they are the difference in representation, not surface form. For example, the entity “Xoxarle\" may be an unseen entity not found in training data This entity could reasonably have “shape:Xxxxxxx\" and “last-letter:e\" as part of its feature representation. If the training data contains entities “Kenneth\" and “Simone\", each of this will have generated these two features respectively. Thus, these example features will not be unseen features in this case, despite coming from an unseen entity. Conversely, continuing this example, if the training data contains no feature “first-letter:X\" – which applies to the unseen entity in question – then this will be an unseen feature..We therefore measure the proportion of unseen features per unseen and seen proportion of different corpora. An analysis of this with Stanford NER is shown in Figure 2 . Each data point represents a corpus. The blue squares are data points for seen NEs and the red circles are data points for unseen NEs. The figure shows a negative correlation between F1 and percentage of unseen features, i.e. the lower the percentage of unseen features, the higher the F1. Seen and unseen performance and features separate into two groups, with only two outlier points. The figure shows that novel, previously unseen NEs have more unseen features and that systems score a lower F1 on them. This suggests that despite the presence of feature extractors for tackling unseen NEs, the features generated often do not overlap with those from seen NEs. However, one would expect individual features to give different generalisation power for other sets of entities, and for systems use these features in different ways. That is, machine learning approaches to the NER task do not seem to learn clear-cut decision boundaries based on a small set of features. This is reflected in the softness of the correlation..Finally, the proportion of seen features is higher for seen NEs. The two outlier points are ACE UN (low F1 for seen NEs despite low percentage of unseen features) and UMBC (high F1 for seen NEs despite high percentage of unseen features). An error analysis shows that the ACE UN corpus suffers from the problem that the seen NEs are ambiguous, meaning even if they have been seen in the training corpus, a majority of the time they have been observed with a different NE label. For the UMBC corpus, the opposite is true: seen NEs are unambiguous. This kind of metonymy is a known and challenging issue in NER, and the results on these corpora highlight the impact is still has on modern systems..For all approaches the proportion of observed features for seen NEs is bigger than the proportion of observed features for unseen NEs, as it should be. However, within the seen and unseen testing instances, there is no clear trend indicating whether having more observed features overall increases F1 performance. One trend that is observable is that the smaller the token/type ratio is (Table 5 ), the bigger the variance between the smallest and biggest $n$ for each corpus, or, in other words, the smaller the token/type ratio is, the more diverse the features..To summarise, our findings are:.[noitemsep].Seen NEs have more unseen features and systems score a lower F1 on them..Outliers are due to low/high ambiguity of seen NEs..The proportion of observed features for seen NEs is bigger than the proportion of observed features for unseen NEs.Within the seen and unseen testing instances, there is no clear trend indicating whether having more observed features overall increases F1 performance..The smaller the token/type ratio is, the more diverse the features. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "27\n",
      "What is a consistent predictor of Named Entity Recognition (NER) performance both within and across domains?\n",
      "\n",
      "Answer 1: Memorisation is a consistent predictor of NER performance, both within and across genres or domains.\n",
      "Question : for the text This section explores baseline out-of-domain NERC performance without domain adaptation; what percentage of NEs are seen if there is a difference between the the training and the testing domains; and how the difference in performance on unseen and seen NEs compares to in-domain performance..As demonstrated by the above experiments, and in line with related work, NERC performance varies across domains while also being influenced by the size of the available in-domain training data. Prior work on transfer learning and domain adaptation (e.g. BIBREF16 ) has aimed at increasing performance in domains where only small amounts of training data are available. This is achieved by adding out-of domain data from domains where larger amounts of training data exist. For domain adaptation to be successful, however, the seed domain needs to be similar to the target domain, i.e. if there is no or very little overlap in terms of contexts of the training and testing instances, the model does not learn any additional helpful weights. As a confounding factor, Twitter and other social media generally consist of many (thousands-millions) of micro-domains, with each author BIBREF54 community BIBREF55 and even conversation BIBREF56 having its own style, which makes it hard to adapt to it as a single, monolithic genre; accordingly, adding out-of-domain NER data gives bad results in this situation BIBREF21 . And even if recognised perfectly, entities that occur just once cause problems beyond NER, e.g. in co-reference BIBREF57 ..In particular, BIBREF58 has reported improving F1 by around 6% through adaptation from the CoNLL to the ACE dataset. However, transfer learning becomes more difficult if the target domain is very noisy or, as mentioned already, too different from the seed domain. For example, BIBREF59 unsuccessfully tried to adapt the CoNLL 2003 corpus to a Twitter corpus spanning several topics. They found that hand-annotating a Twitter corpus consisting of 24,000 tokens performs better on new Twitter data than their transfer learning efforts with the CoNLL 2003 corpus..The seed domain for the experiments here is newswire, where we use the classifier trained on the biggest NW corpus investigated in this study, i.e. OntoNotes NW. That classifier is then applied to all other corpora. The rationale is to test how suitable such a big corpus would be for improving Twitter NER, for which only small training corpora are available..Results for out-of-domain performance are reported in Table 11 . The highest F1 performance is on the OntoNotes BC corpus, with similar results to the in-domain task. This is unsurprising as it belongs to a similar domain as the training corpus (broadcast conversation) the data was collected in the same time period, and it was annotated using the same guidelines. In contrast, out-of-domain results are much lower than in-domain results for the CoNLL corpora, even though they belong to the same genre as OntoNotes NW. Memorisation recall performance on CoNLL TestA and TestB with OntoNotes NW test suggest that this is partly due to the relatively low overlap in NEs between the two datasets. This could be attributed to the CoNLL corpus having been collected in a different time period to the OntoNotes corpus, when other entities were popular in the news; an example of drift BIBREF37 . Conversely, Stanford NER does better on these corpora than it does on other news data, e.g. ACE NW. This indicates that Stanford NER is capable of some degree of generalisation and can detect novel entity surface forms; however, recall is still lower than precision here, achieving roughly the same scores across these three (from 44.11 to 44.96), showing difficulty in picking up novel entities in novel settings..In addition, there are differences in annotation guidelines between the two datasets. If the CoNLL annotation guidelines were more inclusive than the Ontonotes ones, then even a memorisation evaluation over the same dataset would yield this result. This is, in fact, the case: OntoNotes divides entities into more classes, not all of which can be readily mapped to PER/LOC/ORG. For example, OntoNotes includes PRODUCT, EVENT, and WORK OF ART classes, which are not represented in the CoNLL data. It also includes the NORP class, which blends nationalities, religious and political groups. This has some overlap with ORG, but also includes terms such as “muslims\" and “Danes\", which are too broad for the ACE-related definition of ORGANIZATION. Full details can be found in the OntoNotes 5.0 release notes and the (brief) CoNLL 2003 annotation categories. Notice how the CoNLL guidelines are much more terse, being generally non-prose, but also manage to cram in fairly comprehensive lists of sub-kinds of entities in each case. This is likely to make the CoNLL classes include a diverse range of entities, with the many suggestions acting as generative material for the annotator, and therefore providing a broader range of annotations from which to generalise from – i.e., slightly easier to tag..The lowest F1 of 0 is “achieved\" on ACE BN. An examination of that corpus reveals the NEs contained in that corpus are all lower case, whereas those in OntoNotes NW have initial capital letters..Results on unseen NEs for the out-of-domain setting are in Table 12 . The last section's observation of NERC performance being lower for unseen NEs also generally holds true in this out-of-domain setting. The macro average over F1 for the in-domain setting is 76.74% for seen NEs vs. 53.76 for unseen NEs, whereas for the out-of-domain setting the F1 is 56.10% for seen NEs and 47.73% for unseen NEs..Corpora with a particularly big F1 difference between seen and unseen NEs ( $<=$ 20% averaged over all NERC methods) are ACE NW, ACE BC, ACE UN, OntoNotes BN and OntoNotes MZ. For some corpora (CoNLL Test A and B, MSM and Ritter), out-of-domain F1 (macro average over all methods) of unseen NEs is better than for seen NEs. We suspect that this is due to the out-of-domain evaluation setting encouraging better generalisation, as well as the regularity in entity context observed in the fairly limited CoNLL news data – for example, this corpus contains a large proportion of cricket score reports and many cricketer names, occurring in linguistically similar contexts. Others have also noted that the CoNLL datasets are low-diversity compared to OntoNotes, in the context of named entity recognition BIBREF60 . In each of the exceptions except MSM, the difference is relatively small. We note that the MSM test corpus is one of the smallest datasets used in the evaluation, also based on a noisier genre than most others, and so regard this discrepancy as an outlier..Corpora for which out-of-domain F1 is better than in-domain F1 for at least one of the NERC methods are: MUC7 Test, ACE WL, ACE UN, OntoNotes WB, OntoNotes TC and UMBC. Most of those corpora are small, with combined training and testing bearing fewer than 1,000 NEs (MUC7 Test, ACE UN, UMBC). In such cases, it appears beneficial to have a larger amount of training data, even if it is from a different domain and/or time period. The remaining 3 corpora contain weblogs (ACE WL, ACE WB) and online Usenet discussions (ACE UN). Those three are diverse corpora, as can be observed by the relatively low NEs/Unique NEs ratios (Table 4 ). However, NE/Unique NEs ratios are not an absolute predictor for better out-of-domain than in-domain performance: there are corpora with lower NEs/Unique NEs ratios than ACE WB which have better in-domain than out-of-domain performance. As for the other Twitter corpora, MSM 2013 and Ritter, performance is very low, especially for the memorisation system. This reflects that, as well as surface form variation, the context or other information represented by features shifts significantly more in Twitter than across different samples of newswire, and that the generalisations that can be drawn from newswire by modern NER systems are not sufficient to give any useful performance in this natural, unconstrained kind of text..In fact, it is interesting to see that the memorisation baseline is so effective with many genres, including broadcast news, weblog and newswire. This indicates that there is low variation in the topics discussed by these sources – only a few named entities are mentioned by each. When named entities are seen as micro-topics, each indicating a grounded and small topic of interest, this reflects the nature of news having low topic variation, focusing on a few specific issues – e.g., location referred to tend to be big; persons tend to be politically or financially significant; and organisations rich or governmental BIBREF61 . In contrast, social media users also discuss local locations like restaurants, organisations such as music band and sports clubs, and are content to discuss people that are not necessarily mentioned in Wikipedia. The low overlap and memorisation scores on tweets, when taking entity lexica based on newswire, are therefore symptomatic of the lack of variation in newswire text, which has a limited authorship demographic BIBREF62 and often has to comply to editorial guidelines..The other genre that was particularly difficult for the systems was ACE Usenet. This is a form of user-generated content, not intended for publication but rather discussion among communities. In this sense, it is social media, and so it is not surprising that system performance on ACE UN resembles performance on social media more than other genres..Crucially, the computationally-cheap memorisation method actually acts as a reasonable predictor of the performance of other methods. This suggests that high entity diversity predicts difficulty for current NER systems. As we know that social media tends to have high entity diversity – certainly higher than other genres examined – this offers an explanation for why NER systems perform so poorly when taken outside the relatively conservative newswire domain. Indeed, if memorisation offers a consistent prediction of performance, then it is reasonable to say that memorisation and memorisation-like behaviour accounts for a large proportion of NER system performance..To conclude regarding memorisation and out-of-domain performance, there are multiple issues to consider: is the corpus a sub-corpus of the same corpus as the training corpus, does it belong to the same genre, is it collected in the same time period, and was it created with similar annotation guidelines. Yet it is very difficult to explain high/low out-of-domain performance compared to in-domain performance with those factors..A consistent trend is that, if out-of-domain memorisation is better in-domain memorisation, out-of-domain NERC performance with supervised learning is better than in-domain NERC performance with supervised learning too. This reinforces discussions in previous sections: an overlap in NEs is a good predictor for NERC performance. This is useful when a suitable training corpus has to be identified for a new domain. It can be time-consuming to engineer features or study and compare machine learning methods for different domains, while memorisation performance can be checked quickly..Indeed, memorisation consistently predicts NER performance. The prediction applies both within and across domains. This has implications for the focus of future work in NER: the ability to generalise well enough to recognise unseen entities is a significant and still-open problem..To summarise, our findings are:.[noitemsep].What time period an out of domain corpus is collected in plays an important role in NER performance..The context or other information represented by features shifts significantly more in Twitter than across different samples of newswire..The generalisations that can be drawn from newswire by modern NER systems are not sufficient to give any useful performance in this varied kind of text..Memorisation consistently predicts NER performance, both inside and outside genres or domains. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "28\n",
      "Question 1: What is the reason for the performance plateau in the experiment with increasing the number of negative samples?\n",
      "\n",
      "Answer 1: The performance plateau in the experiment with increasing the number of negative samples is suspected to be because the training signal from the positive samples decreases as the number of negative samples increases.\n",
      "Question : for the text In this section we analyze some of the design choices we made for . We pre-train on the 80 hour subset of clean Librispeech and evaluate on TIMIT. Table shows that increasing the number of negative samples only helps up to ten samples. Thereafter, performance plateaus while training time increases. We suspect that this is because the training signal from the positive samples decreases as the number of negative samples increases. In this experiment, everything is kept equal except for the number of negative samples..Next, we analyze the effect of data augmentation through cropping audio sequences (§ SECREF11 ). When creating batches we crop sequences to a pre-defined maximum length. Table shows that a crop size of 150K frames results in the best performance. Not restricting the maximum length (None) gives an average sequence length of about 207K frames and results in the worst accuracy. This is most likely because the setting provides the least amount of data augmentation..Table shows that predicting more than 12 steps ahead in the future does not result in better performance and increasing the number of steps increases training time. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "29\n",
      "Question 1: Who provided the convolutional language models for the experiments? \n",
      "Answer 1: Tatiana Likhomanenko provided the convolutional language models for the experiments.\n",
      "Question : for the text We thank the Speech team at FAIR, especially Jacob Kahn, Vineel Pratap and Qiantong Xu for help with wav2letter++ experiments, and Tatiana Likhomanenko for providing convolutional language models for our experiments. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "30\n",
      "Question 1: What is the baseline model for the WSJ benchmark in the text?\n",
      "\n",
      "Answer 1: The baseline model for the WSJ benchmark in the text is the wav2letter++ setup described in BIBREF29, which is a 17 layer model with gated convolutions.\n",
      "Question : for the text We use the wav2letter++ toolkit for training and evaluation of acoustic models BIBREF28 . For the TIMIT task, we follow the character-based wav2letter++ setup of BIBREF24 which uses seven consecutive blocks of convolutions (kernel size 5 with 1,000 channels), followed by a PReLU nonlinearity and a dropout rate of 0.7. The final representation is projected to a 39-dimensional phoneme probability. The model is trained using the Auto Segmentation Criterion (ASG; Collobert et al., 2016)) using SGD with momentum..Our baseline for the WSJ benchmark is the wav2letter++ setup described in BIBREF29 which is a 17 layer model with gated convolutions BIBREF30 . The model predicts probabilities for 31 graphemes, including the standard English alphabet, the apostrophe and period, two repetition characters (e.g. the word ann is transcribed as an1), and a silence token (|) used as word boundary..All acoustic models are trained on 8 Nvidia V100 GPUs using the distributed training implementations of fairseq and wav2letter++. When training acoustic models on WSJ, we use plain SGD with learning rate 5.6 as well as gradient clipping BIBREF29 and train for 1,000 epochs with a total batch size of 64 audio sequences. We use early stopping and choose models based on validation WER after evaluating checkpoints with a 4-gram language model. For TIMIT we use learning rate 0.12, momentum of 0.9 and train for 1,000 epochs on 8 GPUs with a batch size of 16 audio sequences. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "31\n",
      "Question 1: What is the WER achieved by the approach introduced in the text?\n",
      "\n",
      "Answer 1: The approach introduced in the text achieved a WER of 2.78 on the test set of WSJ, outperforming the next best known character-based speech recognition model in the literature while using three orders of magnitude less transcribed training data.\n",
      "Question : for the text We introduce , the first application of unsupervised pre-training to speech recognition with a fully convolutional model. Our approach achieves 2.78 WER on the test set of WSJ, a result that outperforms the next best known character-based speech recognition model in the literature BIBREF1 while using three orders of magnitude less transcribed training data. We show that more data for pre-training improves performance and that this approach not only improves resource-poor setups, but also settings where all WSJ training data is used. In future work, we will investigate different architectures and fine-tuning which is likely to further improve performance. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "32\n",
      "Question 1: How many hours of transcribed audio data does the Wall Street Journal (WSJ) corpus comprise?\n",
      "\n",
      "Answer 1: The WSJ corpus comprises about 81 hours of transcribed audio data.\n",
      "Question : for the text We consider the following corpora: For phoneme recognition on TIMIT BIBREF26 we use the standard train, dev and test split where the training data contains just over three hours of audio data. Wall Street Journal (WSJ; Woodland et al., 1994) comprises about 81 hours of transcribed audio data. We train on si284, validate on nov93dev and test on nov92. Librispeech BIBREF27 contains a total of 960 hours of clean and noisy speech for training. For pre-training, we use either the full 81 hours of the WSJ corpus, an 80 hour subset of clean Librispeech, the full 960 hour Librispeech training set, or a combination of all of them..To train the baseline acoustic model we compute 80 log-mel filterbank coefficients for a 25ms sliding window with stride 10ms. Final models are evaluated in terms of both word error rate (WER) and letter error rate (LER). generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "33\n",
      "Question 1: What are the three types of language models used in decoding the emissions from the acoustic model?\n",
      "\n",
      "Answer 1: The three types of language models used in decoding the emissions from the acoustic model are a 4-gram KenLM language model, a word-based convolutional language model, and a character-based convolutional language model.\n",
      "Question : for the text For decoding the emissions from the acoustic model we use a lexicon as well as a separate language model trained on the WSJ language modeling data only. We consider a 4-gram KenLM language model BIBREF31 , a word-based convolutional language model BIBREF29 , and a character based convolutional language model BIBREF32 . We decode the word sequence INLINEFORM0 from the output of the context network INLINEFORM1 or log-mel filterbanks using the beam search decoder of BIBREF29 by maximizing DISPLAYFORM0 .where INLINEFORM0 is the acoustic model, INLINEFORM1 is the language model, INLINEFORM2 are the characters of INLINEFORM3 . Hyper-parameters INLINEFORM4 , INLINEFORM5 and INLINEFORM6 are weights for the language model, the word penalty, and the silence penalty..For decoding WSJ, we tune the hyperparameters INLINEFORM0 , INLINEFORM1 and INLINEFORM2 using a random search. Finally, we decode the emissions from the acoustic model with the best parameter setting for INLINEFORM3 , INLINEFORM4 and INLINEFORM5 , and a beam size of 4000 and beam score threshold of 250. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "34\n",
      "What is the key idea of pre-training in neural networks for speech recognition?\n",
      "The key idea of pre-training in neural networks for speech recognition is to learn general representations using large amounts of labeled or unlabeled data and to leverage these representations to improve performance on downstream tasks for which the amount of data is limited. This is particularly useful for tasks such as speech recognition where obtaining labeled data can be difficult and time-consuming.\n",
      "Question : for the text Current state of the art models for speech recognition require large amounts of transcribed audio data to attain good performance BIBREF1 . Recently, pre-training of neural networks has emerged as an effective technique for settings where labeled data is scarce. The key idea is to learn general representations in a setup where substantial amounts of labeled or unlabeled data is available and to leverage the learned representations to improve performance on a downstream task for which the amount of data is limited. This is particularly interesting for tasks where substantial effort is required to obtain labeled data, such as speech recognition..In computer vision, representations for ImageNet BIBREF2 and COCO BIBREF3 have proven to be useful to initialize models for tasks such as image captioning BIBREF4 or pose estimation BIBREF5 . Unsupervised pre-training for computer vision has also shown promise BIBREF6 . In natural language processing (NLP), unsupervised pre-training of language models BIBREF7 , BIBREF8 , BIBREF9 improved many tasks such as text classification, phrase structure parsing and machine translation BIBREF10 , BIBREF11 . In speech processing, pre-training has focused on emotion recogniton BIBREF12 , speaker identification BIBREF13 , phoneme discrimination BIBREF14 , BIBREF15 as well as transferring ASR representations from one language to another BIBREF16 . There has been work on unsupervised learning for speech but the resulting representations have not been applied to improve supervised speech recognition BIBREF17 , BIBREF18 , BIBREF19 , BIBREF20 , BIBREF21 ..In this paper, we apply unsupervised pre-training to improve supervised speech recognition. This enables exploiting unlabeled audio data which is much easier to collect than labeled data. Our model, , is a convolutional neural network that takes raw audio as input and computes a general representation that can be input to a speech recognition system. The objective is a contrastive loss that requires distinguishing a true future audio sample from negatives BIBREF22 , BIBREF23 , BIBREF15 . Different to previous work BIBREF15 , we move beyond frame-wise phoneme classification and apply the learned representations to improve strong supervised ASR systems. relies on a fully convolutional architecture which can be easily parallelized over time on modern hardware compared to recurrent autoregressive models used in previous work (§ SECREF2 )..Our experimental results on the WSJ benchmark demonstrate that pre-trained representations estimated on about 1,000 hours of unlabeled speech can substantially improve a character-based ASR system and outperform the best character-based result in the literature, Deep Speech 2. On the TIMIT task, pre-training enables us to match the best reported result in the literature. In a simulated low-resource setup with only eight hours of transcriped audio data, reduces WER by up to 32% compared to a baseline model that relies on labeled data only (§ SECREF3 & § SECREF4 ). generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "35\n",
      "What is the purpose of the encoder network in the audio model?\n",
      "The purpose of the encoder network is to embed the audio signal in latent space.\n",
      "Question : for the text Our model takes raw audio signal as input and then applies two networks. The encoder network embeds the audio signal in latent space and the context network combines multiple time-steps of the encoder to obtain contextualized representations (Figure FIGREF2 ). Both networks are then used to compute the objective function (§ SECREF4 )..Given raw audio samples INLINEFORM0 , we apply the encoder network INLINEFORM1 which we parameterize as a five-layer convolutional network similar to BIBREF15 . Alternatively, one could use other architectures such as the trainable frontend of BIBREF24 amongst others. The encoder layers have kernel sizes INLINEFORM2 and strides INLINEFORM3 . The output of the encoder is a low frequency feature representation INLINEFORM4 which encodes about 30ms of 16KHz of audio and the striding results in representation INLINEFORM5 every 10ms..Next, we apply the context network INLINEFORM0 to the output of the encoder network to mix multiple latent representations INLINEFORM1 into a single contextualized tensor INLINEFORM2 for a receptive field size INLINEFORM3 . The context network has seven layers and each layer has kernel size three and stride one. The total receptive field of the context network is about 180ms..The layers of both networks consist of a causal convolution with 512 channels, a group normalization layer and a ReLU nonlinearity. We normalize both across the feature and temporal dimension for each sample which is equivalent to group normalization with a single normalization group BIBREF25 . We found it important to choose a normalization scheme that is invariant to the scaling and the offset of the input data. This choice resulted in representations that generalize well across datasets. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "36\n",
      "Question 1: What is the proposal distribution used to train the model?\n",
      "\n",
      "Answer 1: The proposal distribution used to train the model is referred to as INLINEFORM2 in the text. It is used to draw distractor samples for the contrastive loss, while the model is trained to distinguish samples INLINEFORM0 that are k steps in the future.\n",
      "Question : for the text We train the model to distinguish a sample INLINEFORM0 that is k steps in the future from distractor samples INLINEFORM1 drawn from a proposal distribution INLINEFORM2 , by minimizing the contrastive loss for each step INLINEFORM3 : DISPLAYFORM0 .where we denote the sigmoid INLINEFORM0 , and where INLINEFORM1 is the probability of INLINEFORM2 being the true sample. We consider a step-specific affine transformation INLINEFORM3 for each step INLINEFORM4 , that is applied to INLINEFORM5 BIBREF15 . We optimize the loss INLINEFORM6 , summing ( EQREF5 ) over different step sizes. In practice, we approximate the expectation by sampling ten negatives examples by uniformly choosing distractors from each audio sequence, i.e., INLINEFORM7 , where INLINEFORM8 is the sequence length and we set INLINEFORM9 to the number of negatives..After training, we input the representations produced by the context network INLINEFORM0 to the acoustic model instead of log-mel filterbank features. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "37\n",
      "Question 1: Why do we encode raw speech samples into a feature representation?\n",
      "\n",
      "Answer 1: We encode raw speech samples into a feature representation to overcome the challenge of accurately modeling the data distribution. By converting the signal into a lower temporal frequency feature representation, we can implicitly model a density function and avoid the need to model the data distribution accurately.\n",
      "Question : for the text Given an audio signal as input, we optimize our model (§ SECREF3 ) to predict future samples from a given signal context. A common problem with these approaches is the requirement to accurately model the data distribution INLINEFORM0 , which is challenging. We avoid this problem by first encoding raw speech samples INLINEFORM1 into a feature representation INLINEFORM2 at a lower temporal frequency and then implicitly model a density function INLINEFORM3 similar to BIBREF15 . generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "38\n",
      "Question 1: What is the learning rate schedule used for optimizing the pre-training models?\n",
      "\n",
      "Answer 1: The pre-training models are optimized with Adam and a cosine learning rate schedule annealed over 40K update steps for both WSJ and the clean Librispeech training datasets. The learning rate is gradually warmed up for 500 updates up to 0.005 and then decayed following the cosine curve up to 1e-6.\n",
      "Question : for the text The pre-training models are implemented in PyTorch in the fairseq toolkit BIBREF0 . We optimize them with Adam BIBREF33 and a cosine learning rate schedule BIBREF34 annealed over 40K update steps for both WSJ and the clean Librispeech training datasets. We start with a learning rate of 1e-7, and the gradually warm it up for 500 updates up to 0.005 and then decay it following the cosine curve up to 1e-6. We train for 400K steps for full Librispeech. To compute the objective, we sample ten negatives and we use INLINEFORM0 tasks..We train on 8 GPUs and put a variable number of audio sequences on each GPU, up to a pre-defined limit of 1.5M frames per GPU. Sequences are grouped by length and we crop them to a maximum size of 150K frames each, or the length of the shortest sequence in the batch, whichever is smaller. Cropping removes speech signal from either the beginning or end of the sequence and we randomly decide the cropping offsets for each sample; we re-sample every epoch. This is a form of data augmentation but also ensures equal length of all sequences on a GPU and removes on average 25% of the training data. After cropping the total effective batch size across GPUs is about 556 seconds of speech signal (for a variable number of audio sequences). generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "39\n",
      "Question 1: What model is used for the TIMIT task and what is its accuracy compared to the state of the art?\n",
      "\n",
      "Answer 1: The TIMIT task uses a 7-layer wav2letter++ model with high dropout. When pre-trained on Librispeech and WSJ audio data, the accuracy matches the state of the art. Accuracy steadily increases with more data for pre-training and the best accuracy is achieved when the largest amount of data is used for pre-training.\n",
      "Question : for the text On the TIMIT task we use a 7-layer wav2letter++ model with high dropout (§ SECREF3 ; Synnaeve et al., 2016). Table shows that we can match the state of the art when we pre-train on Librispeech and WSJ audio data. Accuracy steadily increases with more data for pre-training and the best accuracy is achieved when we use the largest amount of data for pre-training. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "40\n",
      "What is the impact of pre-trained representations with less transcribed data?\n",
      "\n",
      "Pre-training can reduce WER by 32% on nov93dev when only about eight hours of transcribed data is available, according to Figure.\n",
      "Question : for the text We consider pre-training on the audio data (without labels) of WSJ, part of clean Librispeech (about 80h) and full Librispeech as well as a combination of all datasets (§ SECREF7 ). For the pre-training experiments we feed the output of the context network to the acoustic model, instead of log-mel filterbank features..Table shows that pre-training on more data leads to better accuracy on the WSJ benchmark. Pre-trained representations can substantially improve performance over our character-based baseline which is trained on log-mel filterbank features. This shows that pre-training on unlabeled audio data can improve over the best character-based approach, Deep Speech 2 BIBREF1 , by 0.3 WER on nov92. Our best pre-training model performs as well as the phoneme-based model of BIBREF35 . BIBREF36 is a phoneme-based approach that pre-trains on the transcribed Libirspeech data and then fine-tunes on WSJ. In comparison, our method requires only unlabeled audio data and BIBREF36 also rely on a stronger baseline model than our setup..What is the impact of pre-trained representations with less transcribed data? In order to get a better understanding of this, we train acoustic models with different amounts of labeled training data and measure accuracy with and without pre-trained representations (log-mel filterbanks). The pre-trained representations are trained on the full Librispeech corpus and we measure accuracy in terms of WER when decoding with a 4-gram language model. Figure shows that pre-training reduces WER by 32% on nov93dev when only about eight hours of transcribed data is available. Pre-training only on the audio data of WSJ ( WSJ) performs worse compared to the much larger Librispeech ( Libri). This further confirms that pre-training on more data is crucial to good performance. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "41\n",
      "Question 1: What downstream tasks were the pre-trained representations evaluated on?\n",
      "\n",
      "Answer 1: The pre-trained representations were evaluated on speech recognition tasks, including the WSJ benchmark and the TIMIT phoneme recognition task. Various low resource setups were also simulated for the WSJ benchmark.\n",
      "Question : for the text Different to BIBREF15 , we evaluate the pre-trained representations directly on downstream speech recognition tasks. We measure speech recognition performance on the WSJ benchmark and simulate various low resource setups (§ SECREF12 ). We also evaluate on the TIMIT phoneme recognition task (§ SECREF13 ) and ablate various modeling choices (§ SECREF14 ). generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "42\n",
      "in except\n",
      "42\n",
      "Question 1: How many unique morphological tags are listed for each language in tab:num-tags?\n",
      "\n",
      "Answer 1: The number of unique morphological tags listed for each language in tab:num-tags serves as an approximate measure of the morphological complexity each language exhibits.\n",
      "Question : for the text We use the morphological tagging datasets provided by the Universal Dependencies (UD) treebanks (the concatenation of the $4^\\text{th}$ and $6^\\text{th}$ columns of the file format) BIBREF13 . We list the size of the training, development and test splits of the UD treebanks we used in tab:lang-size. Also, we list the number of unique morphological tags in each language in tab:num-tags, which serves as an approximate measure of the morphological complexity each language exhibits. Crucially, the data are annotated in a cross-linguistically consistent manner, such that words in the different languages that have the same syntacto-semantic function have the same bundle of tags (see sec:morpho-tagging for a discussion). Potentially, further gains would be possible by using a more universal scheme, e.g., the UniMorph scheme. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "1\n",
      "What metric do the authors use to evaluate their models?\n",
      "\n",
      "The authors use average per token accuracy and per feature F1 to evaluate their models.\n",
      "Question : for the text We train our models with the following conditions..We evaluate using average per token accuracy, as is standard for both POS tagging and morphological tagging, and per feature $F_1$ as employed in buys-botha:2016:P16-1. The per feature $F_1$ calculates a key $F^k_1$ for each key in the target language's tags by asking if the key-attribute pair $k_i$ $=$ $v_i$ is in the predicted tag. Then, the key-specific $F^k_1$ values are averaged equally. Note that $F_1$ is a more flexible metric as it gives partial credit for getting some of the attributes in the bundle correct, where accuracy does not..[author=Ryan,color=purple!40,size=,fancyline,caption=,]Georg needs to check. Taken from: http://www.dfki.de/ neumann/publications/new-ps/BigNLP2016.pdf Our networks are four layers deep (two LSTM layers for the character embedder, i.e., to compute ${v_i}$ and two LSTM layers for the tagger, i.e., to compute ${e_i}$ ) and we use an embedding size of 128 for the character input vector size and hidden layers of 256 nodes in all other cases. All networks are trained with the stochastic gradient method RMSProp BIBREF22 , with a fixed initial learning rate and a learning rate decay that is adjusted for the other languages according to the amount of training data. The batch size is always 16. Furthermore, we use dropout BIBREF23 . The dropout probability is set to 0.2. We used Torch 7 BIBREF24 to configure the computation graphs implementing the network architectures. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "2\n",
      "Question 1: Which language families are being experimented with?\n",
      "Answer 1: The language families being experimented with are Romance (Indo-European), Northern Germanic (Indo-European), Slavic (Indo-European), and Uralic.\n",
      "Question : for the text We experiment with the language families: Romance (Indo-European), Northern Germanic (Indo-European), Slavic (Indo-European) and Uralic. In the Romance sub-grouping of the wider Indo-European family, we experiment on Catalan (ca), French (fr), Italian (it), Portuguese (pt), Romanian (ro) and Spanish (es). In the Northern Germanic family, we experiment on Danish (da), Norwegian (no) and Swedish (sv). In the Slavic family, we experiment on Bulgarian (bg), Czech (bg), Polish (pl), Russian (ru), Slovak (sk) and Ukrainian (uk). Finally, in the Uralic family we experiment on Estonian (et), Finnish (fi) and Hungarian (hu). generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "3\n",
      "Question 1: Which specific aspect of transfer learning with morphological tagging models is being evaluated in the study?\n",
      "Answer 1: The study evaluates the effectiveness of transferring morphological tagging models from high-resource languages to low-resource languages, specifically in terms of the amount of annotated data needed and the level of language similarity required for successful transfer.\n",
      "Question : for the text Empirically, we ask three questions of our architectures. i) How well can we transfer morphological tagging models from high-resource languages to low-resource languages in each architecture? (Does one of the three outperform the others?) ii) How many annotated data in the low-resource language do we need? iii) How closely related do the languages need to be to get good transfer? generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "4\n",
      "What is the approach used in this work for low-resource language tagging?\n",
      "The approach used in this work for low-resource language tagging is transfer learning, where a recurrent neural tagger for a low-resource language is trained jointly with a tagger for a related high-resource language, allowing for the training of high-quality tools from smaller amounts of data.\n",
      "Question : for the text State-of-the-art morphological taggers require thousands of annotated sentences to train. For the majority of the world's languages, however, sufficient, large-scale annotation is not available and obtaining it would often be infeasible. Accordingly, an important road forward in low-resource NLP is the development of methods that allow for the training of high-quality tools from smaller amounts of data. In this work, we focus on transfer learning—we train a recurrent neural tagger for a low-resource language jointly with a tagger for a related high-resource language. Forcing the models to share character-level features among the languages allows large gains in accuracy when tagging the low-resource languages, while maintaining (or even improving) accuracy on the high-resource language..Recurrent neural networks constitute the state of the art for a myriad of tasks in NLP, e.g., multi-lingual part-of-speech tagging BIBREF0 , syntactic parsing BIBREF1 , BIBREF2 , morphological paradigm completion BIBREF3 , BIBREF4 and language modeling BIBREF5 , BIBREF6 ; recently, such models have also improved morphological tagging BIBREF7 , BIBREF8 . In addition to increased performance over classical approaches, neural networks also offer a second advantage: they admit a clean paradigm for multi-task learning. If the learned representations for all of the tasks are embedded jointly into a shared vector space, the various tasks reap benefits from each other and often performance improves for all BIBREF9 . We exploit this idea for language-to-language transfer to develop an approach for cross-lingual morphological tagging..We experiment on 18 languages taken from four different language families. Using the Universal Dependencies treebanks, we emulate a low-resource setting for our experiments, e.g., we attempt to train a morphological tagger for Catalan using primarily data from a related language like Spanish. Our results demonstrate the successful transfer of morphological knowledge from the high-resource languages to the low-resource languages without relying on an externally acquired bilingual lexicon or bitext. We consider both the single- and multi-source transfer case and explore how similar two languages must be in order to enable high-quality transfer of morphological taggers. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "5\n",
      "What is the goal of morphological tagging?\n",
      "The goal of morphological tagging is to label each word in its sentential context with the appropriate morphological tag, which is a bundle of key-attribute pairs representing the syntactic function and form of the word.\n",
      "Question : for the text Many languages in the world exhibit rich inflectional morphology: the form of individual words mutates to reflect the syntactic function. For example, the Spanish verb soñar will appear as sueño in the first person present singular, but soñáis in the second person present plural, depending on the bundle of syntaco-semantic attributes associated with the given form (in a sentential context). For concreteness, we list a more complete table of Spanish verbal inflections in tab:paradigm. [author=Ryan,color=purple!40,size=,fancyline,caption=,]Notation in table is different. Note that some languages, e.g. the Northeastern Caucasian language Archi, display a veritable cornucopia of potential forms with the size of the verbal paradigm exceeding 10,000 BIBREF10 ..Standard NLP annotation, e.g., the scheme in sylakglassman-EtAl:2015:ACL-IJCNLP, marks forms in terms of universal key–attribute pairs, e.g., the first person present singular is represented as $\\left[\\right.$ pos=V, per=1, num=sg, tns=pres $\\left.\\right]$ . This bundle of key–attributes pairs is typically termed a morphological tag and we may view the goal of morphological tagging to label each word in its sentential context with the appropriate tag BIBREF11 , BIBREF12 . As the part-of-speech (POS) is a component of the tag, we may view morphological tagging as a strict generalization of POS tagging, where we have significantly refined the set of available tags. All of the experiments in this paper make use of the universal morphological tag set available in the Universal Dependencies (UD) BIBREF13 . As an example, we have provided a Russian sentence with its UD tagging in fig:russian-sentence. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "6\n",
      "What is the undergirding philosophical motivation of the proposal?\n",
      "The undergirding philosophical motivation of the proposal is to attack low-resource NLP through multi-task transfer learning.\n",
      "Question : for the text In terms of methodology, however, our proposal bears similarity to recent work in speech and machine translation–we discuss each in turn. In speech recognition, heigold2013multilingual train a cross-lingual neural acoustic model on five Romance languages. The architecture bears similarity to our multi-language softmax approach. Dependency parsing benefits from cross-lingual learning in a similar fashion BIBREF35 , BIBREF20 ..In neural machine translation BIBREF36 , BIBREF37 , recent work BIBREF38 , BIBREF39 , BIBREF40 has explored the possibility of jointly train translation models for a wide variety of languages. Our work addresses a different task, but the undergirding philosophical motivation is similar, i.e., attack low-resource NLP through multi-task transfer learning. kann-cotterell-schutze:2017:ACL2017 offer a similar method for cross-lingual transfer in morphological inflection generation. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "7\n",
      "Question 1: Why does the author divide the discussion of related work topically?\n",
      "\n",
      "Answer 1: The author divides the discussion of related work topically for ease of intellectual digestion.\n",
      "Question : for the text We divide the discussion of related work topically into three parts for ease of intellectual digestion. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "8\n",
      "What are the two general trends found in the study?\n",
      "\n",
      "The first trend found in the study is that genetically closer languages yield better source languages. The second trend is that the multi-softmax architecture is the best in terms of transferability, as shown by the results in tab:results.\n",
      "Question : for the text [author=Ryan,color=purple!40,size=,fancyline,caption=,]Needs to be updated! We report our results in two tables. First, we report a detailed cross-lingual evaluation in tab:results. Secondly, we report a comparison against two baselines in tab:baseline-table1 (accuracy) and tab:baseline-table2 ( $F_1$ ). We see two general trends of the data. First, we find that genetically closer languages yield better source languages. Second, we find that the multi-softmax architecture is the best in terms of transfer ability, as evinced by the results in tab:results. We find a wider gap between our model and the baselines under the accuracy than under $F_1$ . We attribute this to the fact that $F_1$ is a softer metric in that it assigns credit to partially correct guesses. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "9\n",
      "Question 1: Who did the authors thank in their paper and why?\n",
      "Answer 1: The authors thanked Mo Yu for sharing their ACE05 English data split and anonymous reviewers for their valuable comments in their paper.\n",
      "Question : for the text We thank Mo Yu for sharing their ACE05 English data split and the anonymous reviewers for their valuable comments. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "10\n",
      "Question 1: What are the resource requirements for the neural cross-lingual RE model transfer approach developed in the paper?\n",
      "\n",
      "Answer 1: The neural cross-lingual RE model transfer approach developed in the paper has very low resource requirements, which include a small bilingual dictionary with 1K word pairs.\n",
      "Question : for the text In this paper, we developed a simple yet effective neural cross-lingual RE model transfer approach, which has very low resource requirements (a small bilingual dictionary with 1K word pairs) and can be easily extended to a new language. Extensive experiments for 7 target languages across a variety of language families on both in-house and open datasets show that the proposed approach achieves very good performance (up to $79\\%$ of the accuracy of the supervised target-language RE model), which provides a strong baseline for building cross-lingual RE models with minimal resources. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "11\n",
      "Question 1: What is the purpose of a monolingual word embedding model?\n",
      "\n",
      "Answer 1: The purpose of a monolingual word embedding model is to map words in the vocabulary of a language to real-valued vectors in a smaller vector space with the aim of capturing semantic similarities between the words based on their distributional properties in large samples of monolingual data.\n",
      "Question : for the text In recent years, vector representations of words, known as word embeddings, become ubiquitous for many NLP applications BIBREF12, BIBREF13, BIBREF14..A monolingual word embedding model maps words in the vocabulary $\\mathcal {V}$ of a language to real-valued vectors in $\\mathbb {R}^{d\\times 1}$. The dimension of the vector space $d$ is normally much smaller than the size of the vocabulary $V=|\\mathcal {V}|$ for efficient representation. It also aims to capture semantic similarities between the words based on their distributional properties in large samples of monolingual data..Cross-lingual word embedding models try to build word embeddings across multiple languages BIBREF15, BIBREF16. One approach builds monolingual word embeddings separately and then maps them to the same vector space using a bilingual dictionary BIBREF17, BIBREF18. Another approach builds multilingual word embeddings in a shared vector space simultaneously, by generating mixed language corpora using aligned sentences BIBREF19, BIBREF20..In this paper, we adopt the technique in BIBREF17 because it only requires a small bilingual dictionary of aligned word pairs, and does not require parallel corpora of aligned sentences which could be more difficult to obtain. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "12\n",
      "Question 1: What is the purpose of finding a linear mapping between the vector spaces of different languages?\n",
      "\n",
      "Answer 1: The purpose of finding a linear mapping between the vector spaces of different languages is to approximate the word embeddings of one language using the word embeddings of another language, allowing for cross-lingual applications such as machine translation, cross-lingual information retrieval, and cross-lingual sentiment analysis.\n",
      "Question : for the text BIBREF17 observed that word embeddings of different languages often have similar geometric arrangements, and suggested to learn a linear mapping between the vector spaces..Let $\\mathcal {D}$ be a bilingual dictionary with aligned word pairs ($w_i, v_i)_{i=1,...,D}$ between a source language $s$ and a target language $t$, where $w_i$ is a source-language word and $v_i$ is the translation of $w_i$ in the target language. Let $\\mathbf {x}_i \\in \\mathbb {R}^{d \\times 1}$ be the word embedding of the source-language word $w_i$, $\\mathbf {y}_i \\in \\mathbb {R}^{d \\times 1}$ be the word embedding of the target-language word $v_i$..We find a linear mapping (matrix) $\\mathbf {M}_{t\\rightarrow s}$ such that $\\mathbf {M}_{t\\rightarrow s}\\mathbf {y}_i$ approximates $\\mathbf {x}_i$, by solving the following least squares problem using the dictionary as the training set:.Using $\\mathbf {M}_{t\\rightarrow s}$, for any target-language word $v$ with word embedding $\\mathbf {y}$, we can project it into the source-language embedding space as $\\mathbf {M}_{t\\rightarrow s}\\mathbf {y}$. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "13\n",
      "Q: What are the methods used to ensure that all training instances in the dictionary contribute equally to the optimization objective in (DISPLAY_FORM14)?\n",
      "\n",
      "A: Length normalization and orthogonal transformation are the methods used to ensure that all training instances in the dictionary contribute equally to the optimization objective in (DISPLAY_FORM14). First, the source-language and target-language word embeddings are normalized to be unit vectors. Next, an orthogonality constraint is added to (DISPLAY_FORM14) such that $\\mathbf {M}$ is an orthogonal matrix. $\\mathbf {M}^{O} _{t\\rightarrow s}$ can be computed using singular-value decomposition (SVD).\n",
      "Question : for the text To ensure that all the training instances in the dictionary $\\mathcal {D}$ contribute equally to the optimization objective in (DISPLAY_FORM14) and to preserve vector norms after projection, we have tried length normalization and orthogonal transformation for learning the bilingual mapping as in BIBREF22, BIBREF23, BIBREF24..First, we normalize the source-language and target-language word embeddings to be unit vectors: $\\mathbf {x}^{\\prime }=\\frac{\\mathbf {x}}{||\\mathbf {x}||}$ for each source-language word embedding $\\mathbf {x}$, and $\\mathbf {y}^{\\prime }= \\frac{\\mathbf {y}}{||\\mathbf {y}||}$ for each target-language word embedding $\\mathbf {y}$..Next, we add an orthogonality constraint to (DISPLAY_FORM14) such that $\\mathbf {M}$ is an orthogonal matrix, i.e., $\\mathbf {M}^\\mathrm {T}\\mathbf {M} = \\mathbf {I}$ where $\\mathbf {I}$ denotes the identity matrix:.$\\mathbf {M}^{O} _{t\\rightarrow s}$ can be computed using singular-value decomposition (SVD). generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "14\n",
      "Question 1: What is the self-learning procedure proposed in BIBREF25?\n",
      "\n",
      "Answer 1: BIBREF25 proposed a self-learning procedure that can be combined with a dictionary-based mapping technique. It starts with a small seed dictionary and iteratively learns a mapping using the current dictionary and computes a new dictionary using the learned mapping.\n",
      "Question : for the text The mapping learned in (DISPLAY_FORM14) or (DISPLAY_FORM16) requires a seed dictionary. To relax this requirement, BIBREF25 proposed a self-learning procedure that can be combined with a dictionary-based mapping technique. Starting with a small seed dictionary, the procedure iteratively 1) learns a mapping using the current dictionary; and 2) computes a new dictionary using the learned mapping..BIBREF26 proposed an unsupervised method to learn the bilingual mapping without using a seed dictionary. The method first uses a heuristic to build an initial dictionary that aligns the vocabularies of two languages, and then applies a robust self-learning procedure to iteratively improve the mapping. Another unsupervised method based on adversarial training was proposed in BIBREF27..We compare the performance of different mappings for cross-lingual RE model transfer in Section SECREF45. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "15\n",
      "Question 1: What modifications are made to the standard CBOW model in the variant used to build monolingual word embeddings in this text?\n",
      "\n",
      "Answer 1: In the variant used to build monolingual word embeddings, a separate input word matrix is used for a context word at each position, and weights are employed that decay with the distances of the context words to the target word.\n",
      "Question : for the text To build monolingual word embeddings for the source and target languages, we use a variant of the Continuous Bag-of-Words (CBOW) word2vec model BIBREF13..The standard CBOW model has two matrices, the input word matrix $\\tilde{\\mathbf {X}} \\in \\mathbb {R}^{d\\times V}$ and the output word matrix $\\mathbf {X} \\in \\mathbb {R}^{d\\times V}$. For the $i$th word $w_i$ in $\\mathcal {V}$, let $\\mathbf {e}(w_i) \\in \\mathbb {R}^{V \\times 1}$ be a one-hot vector with 1 at index $i$ and 0s at other indexes, so that $\\tilde{\\mathbf {x}}_i = \\tilde{\\mathbf {X}}\\mathbf {e}(w_i)$ (the $i$th column of $\\tilde{\\mathbf {X}}$) is the input vector representation of word $w_i$, and $\\mathbf {x}_i = \\mathbf {X}\\mathbf {e}(w_i)$ (the $i$th column of $\\mathbf {X}$) is the output vector representation (i.e., word embedding) of word $w_i$..Given a sequence of training words $w_1, w_2, ..., w_N$, the CBOW model seeks to predict a target word $w_t$ using a window of $2c$ context words surrounding $w_t$, by maximizing the following objective function:.The conditional probability is calculated using a softmax function:.where $\\mathbf {x}_t=\\mathbf {X}\\mathbf {e}(w_t)$ is the output vector representation of word $w_t$, and.is the sum of the input vector representations of the context words..In our variant of the CBOW model, we use a separate input word matrix $\\tilde{\\mathbf {X}}_j$ for a context word at position $j, -c \\le j \\le c, j\\ne 0$. In addition, we employ weights that decay with the distances of the context words to the target word. Under these modifications, we have.We use the variant to build monolingual word embeddings because experiments on named entity recognition and word similarity tasks showed this variant leads to small improvements over the standard CBOW model BIBREF21. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "16\n",
      "Question 1: Which datasets were utilized to evaluate the performance of the cross-lingual RE approach?\n",
      "Answer 1: The proposed cross-lingual RE approach was evaluated on both an in-house dataset and the ACE 2005 multilingual dataset.\n",
      "Question : for the text In this section, we evaluate the performance of the proposed cross-lingual RE approach on both in-house dataset and the ACE (Automatic Content Extraction) 2005 multilingual dataset BIBREF11. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "17\n",
      "Question 1: Which languages did the researchers apply the English RE models to?\n",
      "Answer 1: The researchers applied the English RE models to 7 target languages from a variety of language families.\n",
      "Question : for the text We apply the English RE models to the 7 target languages across a variety of language families. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "18\n",
      "What mapping method achieved the best average accuracy over the target languages for the cross-lingual RE task?\n",
      "The regular mapping achieved the best average accuracy over the target languages using a dictionary with only 1K word pairs, and hence it was adopted for the cross-lingual RE task.\n",
      "Question : for the text We compare the performance of cross-lingual RE model transfer under the following bilingual word embedding mappings:.Regular-1K: the regular mapping learned in (DISPLAY_FORM14) using 1K word pairs;.Orthogonal-1K: the orthogonal mapping with length normalization learned in (DISPLAY_FORM16) using 1K word pairs (in this case we train the English RE models with the normalized English word embeddings);.Semi-Supervised-1K: the mapping learned with 1K word pairs and improved by the self-learning method in BIBREF25;.Unsupervised: the mapping learned by the unsupervised method in BIBREF26..The results are summarized in Table TABREF46. The regular mapping outperforms the orthogonal mapping consistently across the target languages. While the orthogonal mapping was shown to work better than the regular mapping for the word translation task BIBREF22, BIBREF23, BIBREF24, our cross-lingual RE approach directly maps target-language word embeddings to the English embedding space without conducting word translations. Moreover, the orthogonal mapping requires length normalization, but we observed that length normalization adversely affects the performance of the English RE models (about 2.0 $F_1$ points drop)..We apply the vecmap toolkit to obtain the semi-supervised and unsupervised mappings. The unsupervised mapping has the lowest average accuracy over the target languages, but it does not require a seed dictionary. Among all the mappings, the regular mapping achieves the best average accuracy over the target languages using a dictionary with only 1K word pairs, and hence we adopt it for the cross-lingual RE task. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "19\n",
      "Question 1: What is the selected dictionary size for the cross-lingual RE approach?\n",
      "\n",
      "Answer 1: The selected dictionary size for the cross-lingual RE approach is 1K, which was found to be sufficient for most target languages after evaluating the performance with an increasing dictionary size.\n",
      "Question : for the text The bilingual dictionary includes the most frequent target-language words and their translations in English. To determine how many word pairs are needed to learn an effective bilingual word embedding mapping for cross-lingual RE, we first evaluate the performance ($F_1$ score) of our cross-lingual RE approach on the target-language development sets with an increasing dictionary size, as plotted in Figure FIGREF35..We found that for most target languages, once the dictionary size reaches 1K, further increasing the dictionary size may not improve the transfer performance. Therefore, we select the dictionary size to be 1K. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "20\n",
      "What language families does our approach work better for?\n",
      "Our approach works better for languages that belong to the SVO (Subject, Verb, Object) language family, such as English, Spanish, Italian, Portuguese, German, and Chinese.\n",
      "Question : for the text Since our approach projects the target-language word embeddings to the source-language embedding space preserving the word order, it is expected to work better for a target language that has more similar word order as the source language. This has been verified by our experiments. The source language, English, belongs to the SVO (Subject, Verb, Object) language family where in a sentence the subject comes first, the verb second, and the object third. Spanish, Italian, Portuguese, German (in conventional typology) and Chinese also belong to the SVO language family, and our approach achieves over $70\\%$ relative accuracy for these languages. On the other hand, Japanese belongs to the SOV (Subject, Object, Verb) language family and Arabic belongs to the VSO (Verb, Subject, Object) language family, and our approach achieves lower relative accuracy for these two languages. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "21\n",
      "What is the performance comparison between the Bi-LSTM model and the CNN model for cross-lingual RE?\n",
      "The Bi-LSTM model achieves a better cross-lingual RE performance than the CNN model for 6 out of the 7 target languages, with over $40.0$ $F_1$ scores for German, Spanish, Portuguese, and Chinese. It also reaches over $75\\%$ of the accuracy of the supervised target-language RE model for German, Spanish, Italian, and Portuguese, and achieves $55\\%$ and $52\\%$ of the accuracy of the supervised Japanese and Arabic RE model, respectively, without using any manually annotated RE data in Japanese/Arabic.\n",
      "Question : for the text The cross-lingual RE model transfer results for the in-house test data are summarized in Table TABREF52 and the results for the ACE05 test data are summarized in Table TABREF53, using the regular mapping learned with a bilingual dictionary of size 1K. In the tables, we also provide the performance of the supervised RE model (Bi-LSTM) for each target language, which is trained with a few hundred thousand tokens of manually annotated RE data in the target-language, and may serve as an upper bound for the cross-lingual model transfer performance..Among the 2 neural network models, the Bi-LSTM model achieves a better cross-lingual RE performance than the CNN model for 6 out of the 7 target languages. In terms of absolute performance, the Bi-LSTM model achieves over $40.0$ $F_1$ scores for German, Spanish, Portuguese and Chinese. In terms of relative performance, it reaches over $75\\%$ of the accuracy of the supervised target-language RE model for German, Spanish, Italian and Portuguese. While Japanese and Arabic appear to be more difficult to transfer, it still achieves $55\\%$ and $52\\%$ of the accuracy of the supervised Japanese and Arabic RE model, respectively, without using any manually annotated RE data in Japanese/Arabic..We apply model ensemble to further improve the accuracy of the Bi-LSTM model. We train 5 Bi-LSTM English RE models initiated with different random seeds, apply the 5 models on the target languages, and combine the outputs by selecting the relation type labels with the highest probabilities among the 5 models. This Ensemble approach improves the single model by 0.6-1.9 $F_1$ points, except for Arabic. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "22\n",
      "Question 1: What languages are included in the in-house dataset for manually annotated RE data?\n",
      "\n",
      "Answer 1: The in-house dataset includes manually annotated RE data for 6 languages: English, German, Spanish, Italian, Japanese, and Portuguese.\n",
      "Question : for the text Our in-house dataset includes manually annotated RE data for 6 languages: English, German, Spanish, Italian, Japanese and Portuguese. It defines 56 entity types (e.g., Person, Organization, Geo-Political Entity, Location, Facility, Time, Event_Violence, etc.) and 53 relation types between the entities (e.g., AgentOf, LocatedAt, PartOf, TimeOf, AffectedBy, etc.)..The ACE05 dataset includes manually annotated RE data for 3 languages: English, Arabic and Chinese. It defines 7 entity types (Person, Organization, Geo-Political Entity, Location, Facility, Weapon, Vehicle) and 6 relation types between the entities (Agent-Artifact, General-Affiliation, ORG-Affiliation, Part-Whole, Personal-Social, Physical)..For both datasets, we create a class label “O\" to denote that the two entities under consideration do not have a relationship belonging to one of the relation types of interest. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "23\n",
      "What are the three neural network English RE models built under the described architecture?\n",
      "The three neural network English RE models built under the described architecture are Pass-Through, Bi-LSTM, and CNN.\n",
      "Question : for the text We build 3 neural network English RE models under the architecture described in Section SECREF4:.The first neural network RE model does not have a context layer and the word embeddings are directly passed to the summarization layer. We call it Pass-Through for short..The second neural network RE model has a Bi-LSTM context layer. We call it Bi-LSTM for short..The third neural network model has a CNN context layer with a window size 3. We call it CNN for short..First we compare our neural network English RE models with the state-of-the-art RE models on the ACE05 English data. The ACE05 English data can be divided to 6 different domains: broadcast conversation (bc), broadcast news (bn), telephone conversation (cts), newswire (nw), usenet (un) and webblogs (wl). We apply the same data split in BIBREF31, BIBREF30, BIBREF6, which uses news (the union of bn and nw) as the training set, a half of bc as the development set and the remaining data as the test set..We learn the model parameters using Adam BIBREF32. We apply dropout BIBREF33 to the hidden layers to reduce overfitting. The development set is used for tuning the model hyperparameters and for early stopping..In Table TABREF40 we compare our models with the best models in BIBREF30 and BIBREF6. Our Bi-LSTM model outperforms the best model (single or ensemble) in BIBREF30 and the best single model in BIBREF6, without using any language-specific resources such as dependency parsers..While the data split in the previous works was motivated by domain adaptation, the focus of this paper is on cross-lingual model transfer, and hence we apply a random data split as follows. For the source language English and each target language, we randomly select $80\\%$ of the data as the training set, $10\\%$ as the development set, and keep the remaining $10\\%$ as the test set. The sizes of the sets are summarized in Table TABREF41..We report the Precision, Recall and $F_1$ score of the 3 neural network English RE models in Table TABREF42. Note that adding an additional context layer with either Bi-LSTM or CNN significantly improves the performance of our English RE model, compared with the simple Pass-Through model. Therefore, we will focus on the Bi-LSTM model and the CNN model in the subsequent experiments. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "24\n",
      "What is relation extraction and what are its applications in NLP?\n",
      "Relation extraction is the task of detecting and classifying semantic relationships between entities such as persons, organizations, locations, and events. It is useful for applications such as knowledge base construction, text mining, and question answering.\n",
      "Question : for the text Relation extraction (RE) is an important information extraction task that seeks to detect and classify semantic relationships between entities like persons, organizations, geo-political entities, locations, and events. It provides useful information for many NLP applications such as knowledge base construction, text mining and question answering. For example, the entity Washington, D.C. and the entity United States have a CapitalOf relationship, and extraction of such relationships can help answer questions like “What is the capital city of the United States?\".Traditional RE models (e.g., BIBREF0, BIBREF1, BIBREF2) require careful feature engineering to derive and combine various lexical, syntactic and semantic features. Recently, neural network RE models (e.g., BIBREF3, BIBREF4, BIBREF5, BIBREF6) have become very successful. These models employ a certain level of automatic feature learning by using word embeddings, which significantly simplifies the feature engineering task while considerably improving the accuracy, achieving the state-of-the-art performance for relation extraction..All the above RE models are supervised machine learning models that need to be trained with large amounts of manually annotated RE data to achieve high accuracy. However, annotating RE data by human is expensive and time-consuming, and can be quite difficult for a new language. Moreover, most RE models require language-specific resources such as dependency parsers and part-of-speech (POS) taggers, which also makes it very challenging to transfer an RE model of a resource-rich language to a resource-poor language..There are a few existing weakly supervised cross-lingual RE approaches that require no human annotation in the target languages, e.g., BIBREF7, BIBREF8, BIBREF9, BIBREF10. However, the existing approaches require aligned parallel corpora or machine translation systems, which may not be readily available in practice..In this paper, we make the following contributions to cross-lingual RE:.We propose a new approach for direct cross-lingual RE model transfer based on bilingual word embedding mapping. It projects word embeddings from a target language to a source language (e.g., English), so that a well-trained source-language RE model can be directly applied to the target language, with no manually annotated RE data needed for the target language..We design a deep neural network architecture for the source-language (English) RE model that uses word embeddings and generic language-independent features as the input. The English RE model achieves the-state-of-the-art performance without using language-specific resources..We conduct extensive experiments which show that the proposed approach achieves very good performance (up to $79\\%$ of the accuracy of the supervised target-language RE model) for a number of target languages on both in-house and the ACE05 datasets BIBREF11, using a small bilingual dictionary with only 1K word pairs. To the best of our knowledge, this is the first work that includes empirical studies for cross-lingual RE on several languages across a variety of language families, without using aligned parallel corpora or machine translation systems..We organize the paper as follows. In Section 2 we provide an overview of our approach. In Section 3 we describe how to build monolingual word embeddings and learn a linear mapping between two languages. In Section 4 we present a neural network architecture for the source-language (English). In Section 5 we evaluate the performance of the proposed approach for a number of target languages. We discuss related work in Section 6 and conclude the paper in Section 7. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "25\n",
      "Question 1: Which type of models achieve state-of-the-art performance for relation extraction?\n",
      "\n",
      "Answer 1: Neural network RE models achieve state-of-the-art performance for relation extraction.\n",
      "Question : for the text For any two entities in a sentence, an RE model determines whether these two entities have a relationship, and if yes, classifies the relationship into one of the pre-defined relation types. We focus on neural network RE models since these models achieve the state-of-the-art performance for relation extraction. Most importantly, neural network RE models use word embeddings as the input, which are amenable to cross-lingual model transfer via cross-lingual word embeddings. In this paper, we use English as the source language..Our neural network architecture has four layers. The first layer is the embedding layer which maps input words in a sentence to word embeddings. The second layer is a context layer which transforms the word embeddings to context-aware vector representations using a recurrent or convolutional neural network layer. The third layer is a summarization layer which summarizes the vectors in a sentence by grouping and pooling. The final layer is the output layer which returns the classification label for the relation type. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "26\n",
      "Question 1: What is the goal of the context layer in text analysis?\n",
      "Answer 1: The goal of the context layer is to build a sentence-context-aware vector representation for each word in a sentence using word embeddings.\n",
      "Question : for the text Given the word embeddings $\\mathbf {x}_t$'s of the words in the sentence, the context layer tries to build a sentence-context-aware vector representation for each word. We consider two types of neural network layers that aim to achieve this. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "27\n",
      "Question 1: What are LSTM networks and how do they capture long-range dependencies in sequential data?\n",
      "\n",
      "Answer 1: LSTM networks are a type of recurrent neural network that have been invented to better capture long-range dependencies in sequential data. They consist of a set of recurrently connected blocks known as memory blocks, each containing a memory cell, and three gates: an input gate, a forget gate, and an output gate. By passing the word embeddings to a forward and a backward LSTM layer, the hidden state vectors can incorporate information from both left and right contextual information in the sentence, thereby better capturing long-range dependencies in the sequential data.\n",
      "Question : for the text The first type of context layer is based on Long Short-Term Memory (LSTM) type recurrent neural networks BIBREF28, BIBREF29. Recurrent neural networks (RNNs) are a class of neural networks that operate on sequential data such as sequences of words. LSTM networks are a type of RNNs that have been invented to better capture long-range dependencies in sequential data..We pass the word embeddings $\\mathbf {x}_t$'s to a forward and a backward LSTM layer. A forward or backward LSTM layer consists of a set of recurrently connected blocks known as memory blocks. The memory block at the $t$-th word in the forward LSTM layer contains a memory cell $\\overrightarrow{\\mathbf {c}}_t$ and three gates: an input gate $\\overrightarrow{\\mathbf {i}}_t$, a forget gate $\\overrightarrow{\\mathbf {f}}_t$ and an output gate $\\overrightarrow{\\mathbf {o}}_t$ ($\\overrightarrow{\\cdot }$ indicates the forward direction), which are updated as follows:.where $\\sigma $ is the element-wise sigmoid function and $\\odot $ is the element-wise multiplication..The hidden state vector $\\overrightarrow{\\mathbf {h}}_t$ in the forward LSTM layer incorporates information from the left (past) tokens of $w_t$ in the sentence. Similarly, we can compute the hidden state vector $\\overleftarrow{\\mathbf {h}}_t$ in the backward LSTM layer, which incorporates information from the right (future) tokens of $w_t$ in the sentence. The concatenation of the two vectors $\\mathbf {h}_t = [\\overrightarrow{\\mathbf {h}}_t, \\overleftarrow{\\mathbf {h}}_t]$ is a good representation of the word $w_t$ with both left and right contextual information in the sentence. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "28\n",
      "Question 1: What type of operation does the second context layer use on successive windows of size $k$ around each word in the sentence?\n",
      "\n",
      "Answer 1: The second context layer uses convolution-like operation on successive windows of size $k$ around each word in the sentence.\n",
      "Question : for the text The second type of context layer is based on Convolutional Neural Networks (CNNs) BIBREF3, BIBREF4, which applies convolution-like operation on successive windows of size $k$ around each word in the sentence. Let $\\mathbf {z}_t = [\\mathbf {x}_{t-(k-1)/2},...,\\mathbf {x}_{t+(k-1)/2}]$ be the concatenation of $k$ word embeddings around $w_t$. The convolutional layer computes a hidden state vector.for each word $w_t$, where $\\mathbf {W}$ is a weight matrix and $\\mathbf {b}$ is a bias vector, and $\\tanh (\\cdot )$ is the element-wise hyperbolic tangent function. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "29\n",
      "Question 1: Is language-specific resources like dependency parsers or POS taggers used in the models described?\n",
      "\n",
      "Answer 1: No, the models do not use language-specific resources such as dependency parsers or POS taggers because these resources might not be readily available for a target language.\n",
      "Question : for the text Given the word embeddings of a sequence of words in a target language $t$, $(\\mathbf {y}_1,...,\\mathbf {y}_n)$, we project them into the English embedding space by applying the linear mapping $\\mathbf {M}_{t\\rightarrow s}$ learned in Section SECREF13: $(\\mathbf {M}_{t\\rightarrow s}\\mathbf {y}_1, \\mathbf {M}_{t\\rightarrow s}\\mathbf {y}_2,...,\\mathbf {M}_{t\\rightarrow s}\\mathbf {y}_n)$. The neural network English RE model is then applied on the projected word embeddings and the entity label embeddings (which are language independent) to perform relationship classification..Note that our models do not use language-specific resources such as dependency parsers or POS taggers because these resources might not be readily available for a target language. Also our models do not use precise word position features since word positions in sentences can vary a lot across languages. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "30\n",
      "Question 1: What is the dimensionality of the word embedding and entity label embedding used in the English sentence embedding layer?\n",
      "\n",
      "Answer 1: The dimensionality of the word embedding used in the English sentence embedding layer is 300, and the dimensionality of the entity label embedding is 50.\n",
      "Question : for the text For an English sentence with $n$ words $\\mathbf {s}=(w_1,w_2,...,w_n)$, the embedding layer maps each word $w_t$ to a real-valued vector (word embedding) $\\mathbf {x}_t\\in \\mathbb {R}^{d \\times 1}$ using the English word embedding model (Section SECREF9). In addition, for each entity $m$ in the sentence, the embedding layer maps its entity type to a real-valued vector (entity label embedding) $\\mathbf {l}_m \\in \\mathbb {R}^{d_m \\times 1}$ (initialized randomly). In our experiments we use $d=300$ and $d_m = 50$. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "31\n",
      "Question 1: What does the output layer receive as inputs in the relation extraction model?\n",
      "\n",
      "Answer 1: The output layer in the relation extraction model receives inputs from the summarization vector, entity label embeddings for two entities, and returns a probability distribution over the relation type labels.\n",
      "Question : for the text The output layer receives inputs from the previous layers (the summarization vector $\\mathbf {h}_s$, the entity label embeddings $\\mathbf {l}_{m_1}$ and $\\mathbf {l}_{m_2}$ for the two entities under consideration) and returns a probability distribution over the relation type labels: generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "32\n",
      "Question 1: What is the purpose of the summarization layer in this model?\n",
      "\n",
      "Answer 1: The purpose of the summarization layer is to build a fixed-length vector that best summarizes the representations of the sentence and two entities for relation type classification. It does this by dividing the hidden state vectors into 5 groups based on the location of the entities in the sentence, performing element-wise max pooling among the vectors in each group, and concatenating the resulting vectors to generate a fixed-length vector.\n",
      "Question : for the text After the context layer, the sentence $(w_1,w_2,...,w_n)$ is represented by $(\\mathbf {h}_1,....,\\mathbf {h}_n)$. Suppose $m_1=(w_{b_1},..,w_{e_1})$ and $m_2=(w_{b_2},..,w_{e_2})$ are two entities in the sentence where $m_1$ is on the left of $m_2$ (i.e., $e_1 < b_2$). As different sentences and entities may have various lengths, the summarization layer tries to build a fixed-length vector that best summarizes the representations of the sentence and the two entities for relation type classification..We divide the hidden state vectors $\\mathbf {h}_t$'s into 5 groups:.$G_1=\\lbrace \\mathbf {h}_{1},..,\\mathbf {h}_{b_1-1}\\rbrace $ includes vectors that are left to the first entity $m_1$..$G_2=\\lbrace \\mathbf {h}_{b_1},..,\\mathbf {h}_{e_1}\\rbrace $ includes vectors that are in the first entity $m_1$..$G_3=\\lbrace \\mathbf {h}_{e_1+1},..,\\mathbf {h}_{b_2-1}\\rbrace $ includes vectors that are between the two entities..$G_4=\\lbrace \\mathbf {h}_{b_2},..,\\mathbf {h}_{e_2}\\rbrace $ includes vectors that are in the second entity $m_2$..$G_5=\\lbrace \\mathbf {h}_{e_2+1},..,\\mathbf {h}_{n}\\rbrace $ includes vectors that are right to the second entity $m_2$..We perform element-wise max pooling among the vectors in each group:.where $d_h$ is the dimension of the hidden state vectors. Concatenating the $\\mathbf {h}_{G_i}$'s we get a fixed-length vector $\\mathbf {h}_s=[\\mathbf {h}_{G_1},...,\\mathbf {h}_{G_5}]$. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "33\n",
      "Question 1: What is the purpose of building word embeddings separately for the source and target languages in the neural cross-lingual RE model transfer approach described in the text?\n",
      "\n",
      "Answer 1: The purpose of building word embeddings separately for the source and target languages is to enable the projection of target-language word embeddings into the source-language embedding space using a small bilingual dictionary, which is a key step in the neural cross-lingual RE model transfer approach.\n",
      "Question : for the text We summarize the main steps of our neural cross-lingual RE model transfer approach as follows..Build word embeddings for the source language and the target language separately using monolingual data..Learn a linear mapping that projects the target-language word embeddings into the source-language embedding space using a small bilingual dictionary..Build a neural network source-language RE model that uses word embeddings and generic language-independent features as the input..For a target-language sentence and any two entities in it, project the word embeddings of the words in the sentence to the source-language word embeddings using the linear mapping, and then apply the source-language RE model on the projected word embeddings to classify the relationship between the two entities. An example is shown in Figure FIGREF4, where the target language is Portuguese and the source language is English..We will describe each component of our approach in the subsequent sections. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "34\n",
      "Question 1: What is the approach proposed in the paper for cross-lingual relation extraction and how is it different from existing approaches?\n",
      "\n",
      "Answer 1: The approach proposed in the paper for cross-lingual relation extraction uses the bilingual word embedding mapping technique to enable cross-lingual model transfer. It is different from existing approaches as it does not require aligned parallel corpora or machine translation systems.\n",
      "Question : for the text There are a few weakly supervised cross-lingual RE approaches. BIBREF7 and BIBREF8 project annotated English RE data to Korean to create weakly labeled training data via aligned parallel corpora. BIBREF9 translates a target-language sentence into English, performs RE in English, and then projects the relation phrases back to the target-language sentence. BIBREF10 proposes an adversarial feature adaptation approach for cross-lingual relation classification, which uses a machine translation system to translate source-language sentences into target-language sentences. Unlike the existing approaches, our approach does not require aligned parallel corpora or machine translation systems. There are also several multilingual RE approaches, e.g., BIBREF34, BIBREF35, BIBREF36, where the focus is to improve monolingual RE by jointly modeling texts in multiple languages..Many cross-lingual word embedding models have been developed recently BIBREF15, BIBREF16. An important application of cross-lingual word embeddings is to enable cross-lingual model transfer. In this paper, we apply the bilingual word embedding mapping technique in BIBREF17 to cross-lingual RE model transfer. Similar approaches have been applied to other NLP tasks such as dependency parsing BIBREF37, POS tagging BIBREF38 and named entity recognition BIBREF21, BIBREF39. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "35\n",
      "Question 1: What are the three components that the authors decompose the problem into?\n",
      "\n",
      "Answer 1: The authors decompose the problem into three components: completing the query from text prefix and an image, estimating probabilities of objects based on the completed text, and segmenting and classifying all instances in the image.\n",
      "Question : for the text This work focuses on the problem of finding objects in an image based on natural language descriptions. Existing solutions take into account both the image and the query BIBREF0, BIBREF1, BIBREF2. In our problem formulation, rather than having the entire text, we are given only a prefix of the text which requires completing the text based on a language model and the image, and finding a relevant object in the image. We decompose the problem into three components: (i) completing the query from text prefix and an image; (ii) estimating probabilities of objects based on the completed text, and (iii) segmenting and classifying all instances in the image. We combine, extend, and modify state of the art components: (i) we extend a FactorCell LSTM BIBREF3, BIBREF4 which conditionally completes text to complete a query from both a text prefix and an image; (ii) we fine tune a BERT embedding to compute instance probabilities from a complete sentence, and (iii) we use Mask-RCNN BIBREF5 for instance segmentation..Recent natural language embeddings BIBREF6 have been trained with the objectives of predicting masked words and determining whether sentences follow each other, and are efficiently used across a dozen of natural language processing tasks. Sequence models have been conditioned to complete text from a prefix and index BIBREF3, however have not been extended to take into account an image. Deep neural networks have been trained to segment all instances in an image at very high quality BIBREF5, BIBREF7. We propose a novel method of natural language query auto-completion for estimating instance probabilities conditioned on the image and a user query prefix. Our system combines and modifies state of the art components used in query completion, language embedding, and masked instance segmentation. Estimating a broad set of instance probabilities enables selection which is agnostic to the segmentation procedure. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "36\n",
      "Question: What is the architecture of the approach described in the text?\n",
      "Answer: Figure FIGREF2 shows the architecture of the approach. It involves extracting image features with a pre-trained CNN, incorporating them into a modified FactorCell LSTM language model along with the user query prefix to complete the query. Then, the completed query is fed into a fine-tuned BERT embedding to estimate instance probabilities, which are used for instance selection.\n",
      "Question : for the text Figure FIGREF2 shows the architecture of our approach. First, we extract image features with a pre-trained CNN. We incorporate the image features into a modified FactorCell LSTM language model along with the user query prefix to complete the query. The completed query is then fed into a fine-tuned BERT embedding to estimate instance probabilities, which in turn are used for instance selection..We denote a set of objects $o_k \\in O$ where O is the entire set of recognizable object classes. The user inputs a prefix, $p$, an incomplete query on an image, $I$. Given $p$, we auto-complete the intended query $q$. We define the auto-completion query problem in equation DISPLAY_FORM3 as the maximization of the probability of a query conditioned on an image where $w_i \\in A$ is the word in position $i$...We pose our instance probability estimation problem given an auto-completed query $\\mathbf {q^*}$ as a multilabel problem where each class can independently exist. Let $O_{q*}$ be the set of instances referred to in $\\mathbf {q^*}$. Given $\\hat{p}_k$ is our estimate of $P(o_k \\in O_{q*})$ and $y_k = \\mathbb {1}[o_k \\in O_{q*}]$, the instance selection model minimizes the sigmoid cross-entropy loss function: generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "37\n",
      "Question 1: What type of data does the Visual Genome dataset contain?\n",
      "\n",
      "Answer 1: The Visual Genome dataset contains images, region descriptions, relationships, question-answers, attributes, and object instances.\n",
      "Question : for the text We use the Visual Genome (VG) BIBREF8 and ReferIt BIBREF9 datasets which are suitable for our purposes. The VG data contains images, region descriptions, relationships, question-answers, attributes, and object instances. The region descriptions provide a replacement for queries since they mention various objects in different regions of each image. However, while some region descriptions are referring phrases, some are more similar to descriptions (see examples in Table TABREF10). The large number of examples makes the Visual Genome dataset particularly useful for our task. The smaller ReferIt dataset consists of referring expressions attached to images which more closely resemble potential user queries of images. We train separate models using both datasets..For training, we aggregated (query, image) pairs using the region descriptions from the VG dataset and referring expressions from the ReferIt dataset. Our VG training set consists of 85% of the data: 16k images and 740k corresponding region descriptions. The Referit training data consists of 9k images and 54k referring expressions..The query completion models are trained using a 128 dimensional image representation, a rank $r=64$ personalized matrix, 24 dimensional character embeddings, 512 dimensional LSTM hidden units, and a max length of 50 characters per query, with Adam at a 5e-4 learning rate, and a batch size of 32 for 80K iterations. The instance selection model is trained using (region description, object set) pairs from the VG dataset resulting in a training set of approximately 1.73M samples. The remaining 300K samples are split into validation and testing. Our training procedure for the instance selection model fine tunes all 12 layers of BERT with 32 sample batch sizes for 250K iterations, using Adam and performing learning rate warm-up for the first 10% of iterations with a target 5e-5 learning rate. The entire training processes takes around a day on an NVIDIA Tesla P100 GPU. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "38\n",
      "Question 1: What is the architecture used to fine-tune BERT embeddings for the instance selection task? \n",
      "Answer 1: The architecture used is a 12-layer implementation of BERT which has been previously shown to perform well when fine-tuned for various tasks such as text classification, question answering, and named entity recognition. An additional dense layer with 10% dropout is added to the BERT architecture to map the last pooled layer to the object classes in our data for our specific task.\n",
      "Question : for the text We fine tune a pre-trained BERT embedding to perform transfer learning for our instance selection task. We use a 12-layer implementation which has been shown to generalize and perform well when fine-tuned for new tasks such as question answering, text classification, and named entity recognition. To apply the model to our task, we add an additional dense layer to the BERT architecture with 10% dropout, mapping the last pooled layer to the object classes in our data. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "39\n",
      "Question 1: How does the FactorCell adaptation of an LSTM with coupled input and forget gates assist in query autocompletion?\n",
      "\n",
      "Answer 1: The FactorCell is an LSTM with a context-dependent weight matrix that is formed by taking the product of the context with two basis tensors. In this case, the context is a low-dimensional image representation extracted from an input image using a CNN pretrained on ImageNet. By modifying each query completion to be personalized to a specific image representation, the FactorCell assists in achieving better query autocompletion. Beam search is used to choose the optimal completion from the sequence of predicted characters for the given prefix.\n",
      "Question : for the text We utilize the FactorCell (FC) adaptation of an LSTM with coupled input and forget gates BIBREF4 to autocomplete queries. The FactorCell is an LSTM with a context-dependent weight matrix $\\mathbf {W^{\\prime }} = \\mathbf {W} + \\mathbf {A}$ in place of $\\mathbf {W}$. Given a character embedding $w_t \\in \\mathbb {R}^e$, a previous hidden state $h_{t-1} \\in \\mathbb {R}^h$ , the adaptation matrix, $\\mathbf {A}$, is formed by taking the product of the context, c, with two basis tensors $\\mathbf {Z_L} \\in \\mathbb {R}^{m\\times (e+h)\\times r}$ and $\\mathbf {Z_R} \\in \\mathbb {R}^{r\\times h \\times m}$..To adapt the FactorCell BIBREF4 for our purposes, we replace user embeddings with a low-dimensional image representation. Thus, we are able to modify each query completion to be personalized to a specific image representation. We extract features from an input image using a CNN pretrained on ImageNet, retraining only the last two fully connected layers. The image feature vector is fed into the FactorCell through the adaptation matrix. We perform beam search over the sequence of predicted characters to chose the optimal completion for the given prefix. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "40\n",
      "Question 1: What is the F1-score achieved by the instance selection method?\n",
      "\n",
      "Answer 1: The F1-score achieved by the instance selection method is 0.7618 over all 2,909 instance classes.\n",
      "Question : for the text Figure 3 shows example results. We evaluate query completion by language perplexity and mean reciprocal rank (MRR) and evaluate instance selection by F1-score. We compare the perplexity on both sets of test queries using corresponding images vs. random noise as context. Table TABREF11 shows perplexity on the VG and ReferIt test queries with both corresponding images and random noise. The VG and ReferIt datasets have character vocabulary sizes of 89 and 77 respectively..Given the matching index $t$ of the true query in the top 10 completions we compute the MRR as $\\sum _{n}{\\frac{1}{t}}$ where we replace the reciprocal rank with 0 if the true query does not appear in the top ten completions. We evaluate the VG and ReferIt test queries with varying prefix sizes and compare performance with the corresponding image and random noise as context. MRR is influenced by the length of the query, as longer queries are more difficult to match. Therefore, as expected we observe better performance on the ReferIt dataset for all prefix lengths. Finally, our instance selection achieves an F1-score of 0.7618 over all 2,909 instance classes. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "41\n",
      "Question 1: What are the future goals for this project?\n",
      "\n",
      "Answer 1: The future goals for this project include extracting referring expressions using simple grammatical rules to differentiate between referring and non-referring region descriptions, and combining the VG and ReferIt datasets to train a single model and scale up the datasets to improve query completions.\n",
      "Question : for the text Our results demonstrate that auto-completion based on both language and vision performs better than by using only language, and that fine tuning a BERT embedding allows to efficiently rank instances in the image. In future work we would like to extract referring expressions using simple grammatical rules to differentiate between referring and non-referring region descriptions. We would also like to combine the VG and ReferIt datasets to train a single model and scale up our datasets to improve query completions. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "42\n",
      "Question 1: Who provided funds for the research mentioned in the text? \n",
      "Answer 1: The Toyota Research Institute (TRI) and Fondecyt grant 1181739, Conicyt, Chile provided funds for the research mentioned in the text. However, the opinions and conclusions presented in the paper solely reflect those of the authors, and not of TRI or any other Toyota entity.\n",
      "Question : for the text The Toyota Research Institute (TRI) provided funds to assist with this research, but this paper solely reflects the opinions and conclusions of its authors and not TRI or any other Toyota entity. This work is also partially funded by Fondecyt grant 1181739, Conicyt, Chile. The authors would also like to thank Gabriel Sepúlveda for his assistance with parts of this project. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "43\n",
      "What is the proposed model for interpreting navigation instructions?\n",
      "\n",
      "The proposed model for interpreting navigation instructions leverages recent advances in deep learning and builds on the sequence-to-sequence translation model to translate natural language instructions to a sequence of navigation behaviors in an end-to-end fashion. The model includes layers for embedding, encoding, attention, fully-connected, decoder, and output. It also uses a knowledge base in the form of a behavioral navigation graph to facilitate the grounding of navigation commands.\n",
      "Question : for the text We leverage recent advances in deep learning to translate natural language instructions to a sequence of navigation behaviors in an end-to-end fashion. Our proposed model builds on the sequence-to-sequence translation model of BIBREF23 , which computes a soft-alignment between a source sequence (natural language instructions in our case) and the corresponding target sequence (navigation behaviors)..As one of our main contributions, we augment the neural machine translation approach of BIBREF23 to take as input not only natural language instructions, but also the corresponding behavioral navigation graph INLINEFORM0 of the environment where navigation should take place. Specifically, at each step, the graph INLINEFORM1 operates as a knowledge base that the model can access to obtain information about path connectivity, facilitating the grounding of navigation commands..Figure FIGREF8 shows the structure of the proposed model for interpreting navigation instructions. The model consists of six layers:.Embed layer: The model first encodes each word and symbol in the input sequences INLINEFORM0 and INLINEFORM1 into fixed-length representations. The instructions INLINEFORM2 are embedded into a 100-dimensional pre-trained GloVe vector BIBREF24 . Each of the triplet components, INLINEFORM3 , INLINEFORM4 , and INLINEFORM5 of the graph INLINEFORM6 , are one-hot encoded into vectors of dimensionality INLINEFORM7 , where INLINEFORM8 and INLINEFORM9 are the number of nodes and edges in INLINEFORM10 , respectively..Encoder layer: The model then uses two bidirectional Gated Recurrent Units (GRUs) BIBREF25 to independently process the information from INLINEFORM0 and INLINEFORM1 , and incorporate contextual cues from the surrounding embeddings in each sequence. The outputs of the encoder layer are the matrix INLINEFORM2 for the navigational commands and the matrix INLINEFORM3 for the behavioral graph, where INLINEFORM4 is the hidden size of each GRU, INLINEFORM5 is the number of words in the instruction INLINEFORM6 , and INLINEFORM7 is the number of triplets in the graph INLINEFORM8 ..Attention layer: Matrices INLINEFORM0 and INLINEFORM1 generated by the encoder layer are combined using an attention mechanism. We use one-way attention because the graph contains information about the whole environment, while the instruction has (potentially incomplete) local information about the route of interest. The use of attention provides our model with a two-step strategy to interpret commands. This resembles the way people find paths on a map: first, relevant parts on the map are selected according to their affinity to each of the words in the input instruction (attention layer); second, the selected parts are connected to assemble a valid path (decoder layer). More formally, let INLINEFORM2 ( INLINEFORM3 ) be the INLINEFORM4 -th row of INLINEFORM5 , and INLINEFORM6 ( INLINEFORM7 ) the INLINEFORM8 -th row of INLINEFORM9 . We use each encoded triplet INLINEFORM10 in INLINEFORM11 to calculate its associated attention distribution INLINEFORM12 over all the atomic instructions INLINEFORM13 : DISPLAYFORM0 .where the matrix INLINEFORM0 serves to combine the different sources of information INLINEFORM1 and INLINEFORM2 . Each component INLINEFORM3 of the attention distributions INLINEFORM4 quantifies the affinity between the INLINEFORM5 -th triplet in INLINEFORM6 and the INLINEFORM7 -th word in the corresponding input INLINEFORM8 ..The model then uses each attention distribution INLINEFORM0 to obtain a weighted sum of the encodings of the words in INLINEFORM1 , according to their relevance to the corresponding triplet INLINEFORM2 . This results in L attention vectors INLINEFORM3 , INLINEFORM4 ..The final step in the attention layer concatenates each INLINEFORM0 with INLINEFORM1 to generate the outputs INLINEFORM2 , INLINEFORM3 . Following BIBREF8 , we include the encoded triplet INLINEFORM4 in the output tensor INLINEFORM5 of this layer to prevent early summaries of relevant map information..FC layer: The model reduces the dimensionality of each individual vector INLINEFORM0 from INLINEFORM1 to INLINEFORM2 with a fully-connected (FC) layer. The resulting L vectors are output to the next layer as columns of a context matrix INLINEFORM3 ..Decoder layer: After the FC layer, the model predicts likelihoods over the sequence of behaviors that correspond to the input instructions with a GRU network. Without loss of generality, consider the INLINEFORM0 -th recurrent cell in the GRU network. This cell takes two inputs: a hidden state vector INLINEFORM1 from the prior cell, and a one-hot embedding of the previous behavior INLINEFORM2 that was predicted by the model. Based on these inputs, the GRU cell outputs a new hidden state INLINEFORM3 to compute likelihoods for the next behavior. These likelihoods are estimated by combining the output state INLINEFORM4 with relevant information from the context INLINEFORM5 : DISPLAYFORM0 . where INLINEFORM0 , INLINEFORM1 , and INLINEFORM2 are trainable parameters. The attention vector INLINEFORM3 in Eq. () quantifies the affinity of INLINEFORM4 with respect to each of the columns INLINEFORM5 of INLINEFORM6 , where INLINEFORM7 . The attention vector also helps to estimate a dynamic contextual vector INLINEFORM8 that the INLINEFORM9 -th GRU cell uses to compute logits for the next behavior: DISPLAYFORM0 .with INLINEFORM0 trainable parameters. Note that INLINEFORM1 includes a value for each of the pre-defined behaviors in the graph INLINEFORM2 , as well as for a special “stop” symbol to identify the end of the output sequence..Output layer: The final layer of the model searches for a valid sequence of robot behaviors based on the robot's initial node, the connectivity of the graph INLINEFORM0 , and the output logits from the previous decoder layer. Again, without loss of generality, consider the INLINEFORM1 -th behavior INLINEFORM2 that is finally predicted by the model. The search for this behavior is implemented as: DISPLAYFORM0 .with INLINEFORM0 a masking function that takes as input the graph INLINEFORM1 and the node INLINEFORM2 that the robot reaches after following the sequence of behaviors INLINEFORM3 previously predicted by the model. The INLINEFORM4 function returns a vector of the same dimensionality as the logits INLINEFORM5 , but with zeros for the valid behaviors after the last location INLINEFORM6 and for the special stop symbol, and INLINEFORM7 for any invalid predictions according to the connectivity of the behavioral navigation graph. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "44\n",
      "What was the contribution of the authors in this work?\n",
      "\n",
      "The authors proposed an end-to-end system to translate user instructions to a high-level navigation plan using an attention mechanism to merge relevant information from the instructions with a behavioral graph of the environment. They also contributed a new dataset of 11,051 pairs of user instructions and navigation plans from 100 different environments and achieved the best performance in this dataset compared to baseline approaches. Their approach demonstrated a practical form of learning for a complex and useful task.\n",
      "Question : for the text This work introduced behavioral navigation through free-form natural language instructions as a challenging and a novel task that falls at the intersection of natural language processing and robotics. This problem has a range of interesting cross-domain applications, including information retrieval..We proposed an end-to-end system to translate user instructions to a high-level navigation plan. Our model utilized an attention mechanism to merge relevant information from the navigation instructions with a behavioral graph of the environment. The model then used a decoder to predict a sequence of navigation behaviors that matched the input commands..As part of this effort, we contributed a new dataset of 11,051 pairs of user instructions and navigation plans from 100 different environments. Our model achieved the best performance in this dataset in comparison to a two-step baseline approach for interpreting navigation instructions, and a sequence-to-sequence model that does not consider the behavioral graph. Our quantitative and qualitative results suggest that attention mechanisms can help leverage the behavioral graph as a relevant knowledge base to facilitate the translation of free-form navigation instructions. Overall, our approach demonstrated practical form of learning for a complex and useful task. In future work, we are interested in investigating mechanisms to improve generalization to new environments. For example, pointer and graph networks BIBREF30 , BIBREF31 are a promising direction to help supervise translation models and predict motion behaviors. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "45\n",
      "What is the total number of distinct navigation plans in the dataset? \n",
      "\n",
      "Answer 1: The dataset consists of 6064 distinct navigation plans collected from 88 unique simulated environments.\n",
      "Question : for the text We created a new dataset for the problem of following navigation instructions under the behavioral navigation framework of BIBREF5 . This dataset was created using Amazon Mechanical Turk and 100 maps of simulated indoor environments, each with 6 to 65 rooms. To the best of our knowledge, this is the first benchmark for comparing translation models in the context of behavioral robot navigation..As shown in Table TABREF16 , the dataset consists of 8066 pairs of free-form natural language instructions and navigation plans for training. This training data was collected from 88 unique simulated environments, totaling 6064 distinct navigation plans (2002 plans have two different navigation instructions each; the rest has one). The dataset contains two test set variants:.While the dataset was collected with simulated environments, no structure was imposed on the navigation instructions while crowd-sourcing data. Thus, many instructions in our dataset are ambiguous. Moreover, the order of the behaviors in the instructions is not always the same. For instance, a person said “turn right and advance” to describe part of a route, while another person said “go straight after turning right” in a similar situation. The high variability present in the natural language descriptions of our dataset makes the problem of decoding instructions into behaviors not trivial. See Appendix A of the supplementary material for additional details on our data collection effort. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "46\n",
      "Question 1: How are evaluation metrics computed in this study?\n",
      "\n",
      "Answer 1: Evaluation metrics are computed by treating each behavior in a route as a single token and comparing it to the ground truth plan. The metrics used include EM, harmonic average of precision and recall, edit distance, and GM.\n",
      "Question : for the text While computing evaluation metrics, we only consider the behaviors present in the route because they are sufficient to recover the high-level navigation plan from the graph. Our metrics treat each behavior as a single token. For example, the sample plan “R-1 oor C-1 cf C-1 lt C-0 cf C-0 iol O-3\" is considered to have 5 tokens, each corresponding to one of its behaviors (“oor\", “cf\", “lt\", “cf\", “iol\"). In this plan, “R-1\",“C-1\", “C-0\", and “O-3\" are symbols for locations (nodes) in the graph..We compare the performance of translation approaches based on four metrics:.[align=left,leftmargin=0em,labelsep=0.4em,font=].As in BIBREF20 , EM is 1 if a predicted plan matches exactly the ground truth; otherwise it is 0..The harmonic average of the precision and recall over all the test set BIBREF26 ..The minimum number of insertions, deletions or swap operations required to transform a predicted sequence of behaviors into the ground truth sequence BIBREF27 ..GM is 1 if a predicted plan reaches the ground truth destination (even if the full sequence of behaviors does not match exactly the ground truth). Otherwise, GM is 0. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "47\n",
      "Question 1: What types of results are presented in the evaluation of the natural language navigation approach?\n",
      "\n",
      "Answer 1: Both quantitative and qualitative results are provided in the evaluation of the proposed approach for interpreting navigation commands in natural language.\n",
      "Question : for the text This section describes our evaluation of the proposed approach for interpreting navigation commands in natural language. We provide both quantitative and qualitative results. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "48\n",
      "What pre-processing steps were taken to prepare the input instructions for the models in the experiment?\n",
      "\n",
      "The inputs were lowercased, tokenized, spell-checked, and lemmatized using WordNet. The maximum length for graphs and navigational instructions was set to 300 and 150, respectively. Only a very small percentage of the training and validation sets had longer inputs.\n",
      "Question : for the text We pre-processed the inputs to the various models that are considered in our experiment. In particular, we lowercased, tokenized, spell-checked and lemmatized the input instructions in text-form using WordNet BIBREF28 . We also truncated the graphs to a maximum of 300 triplets, and the navigational instructions to a maximum of 150 words. Only 6.4% (5.4%) of the unique graphs in the training (validation) set had more than 300 triplets, and less than 0.15% of the natural language instructions in these sets had more than 150 tokens..The dimensionality of the hidden state of the GRU networks was set to 128 in all the experiments. In general, we used 12.5% of the training set as validation for choosing models' hyper-parameters. In particular, we used dropout after the encoder and the fully-connected layers of the proposed model to reduce overfitting. Best performance was achieved with a dropout rate of 0.5 and batch size equal to 256. We also used scheduled sampling BIBREF29 at training time for all models except the baseline..We input the triplets from the graph to our proposed model in alphabetical order, and consider a modification where the triplets that surround the start location of the robot are provided first in the input graph sequence. We hypothesized that such rearrangement would help identify the starting location (node) of the robot in the graph. In turn, this could facilitate the prediction of correct output sequences. In the remaining of the paper, we refer to models that were provided a rearranged graph, beginning with the starting location of the robot, as models with “Ordered Triplets”. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "49\n",
      "What is the main contribution of this work?\n",
      "The main contribution of this work is a new end-to-end model for following directions in natural language under the behavioral navigation framework. Additionally, the work also contributes a new dataset of pairs of free-form natural language instructions and high-level navigation plans.\n",
      "Question : for the text Enabling robots to follow navigation instructions in natural language can facilitate human-robot interaction across a variety of applications. For instance, within the service robotics domain, robots can follow navigation instructions to help with mobile manipulation BIBREF0 and delivery tasks BIBREF1 ..Interpreting navigation instructions in natural language is difficult due to the high variability in the way people describe routes BIBREF2 . For example, there are a variety of ways to describe the route in Fig. FIGREF4 (a):.Each fragment of a sentence within these instructions can be mapped to one or more than one navigation behaviors. For instance, assume that a robot counts with a number of primitive, navigation behaviors, such as “enter the room on the left (or on right)” , “follow the corridor”, “cross the intersection”, etc. Then, the fragment “advance forward” in a navigation instruction could be interpreted as a “follow the corridor” behavior, or as a sequence of “follow the corridor” interspersed with “cross the intersection” behaviors depending on the topology of the environment. Resolving such ambiguities often requires reasoning about “common-sense” concepts, as well as interpreting spatial information and landmarks, e.g., in sentences such as “the room on the left right before the end of the corridor” and “the room which is in the middle of two vases”..In this work, we pose the problem of interpreting navigation instructions as finding a mapping (or grounding) of the commands into an executable navigation plan. While the plan is typically modeled as a formal specification of low-level motions BIBREF2 or a grammar BIBREF3 , BIBREF4 , we focus specifically on translating instructions to a high-level navigation plan based on a topological representation of the environment. This representation is a behavioral navigation graph, as recently proposed by BIBREF5 , designed to take advantage of the semantic structure typical of human environments. The nodes of the graph correspond to semantically meaningful locations for the navigation task, such as kitchens or entrances to rooms in corridors. The edges are parameterized, visuo-motor behaviors that allow a robot to navigate between neighboring nodes, as illustrated in Fig. FIGREF4 (b). Under this framework, complex navigation routes can be achieved by sequencing behaviors without an explicit metric representation of the world..We formulate the problem of following instructions under the framework of BIBREF5 as finding a path in the behavioral navigation graph that follows the desired route, given a known starting location. The edges (behaviors) along this path serve to reach the – sometimes implicit – destination requested by the user. As in BIBREF6 , our focus is on the problem of interpreting navigation directions. We assume that a robot can realize valid navigation plans according to the graph..We contribute a new end-to-end model for following directions in natural language under the behavioral navigation framework. Inspired by the information retrieval and question answering literature BIBREF7 , BIBREF8 , BIBREF9 , BIBREF10 , we propose to leverage the behavioral graph as a knowledge base to facilitate the interpretation of navigation commands. More specifically, the proposed model takes as input user directions in text form, the behavioral graph of the environment encoded as INLINEFORM0 node; edge; node INLINEFORM1 triplets, and the initial location of the robot in the graph. The model then predicts a set of behaviors to reach the desired destination according to the instructions and the map (Fig. FIGREF4 (c)). Our main insight is that using attention mechanisms to correlate navigation instructions with the topological map of the environment can facilitate predicting correct navigation plans..This work also contributes a new dataset of INLINEFORM0 pairs of free-form natural language instructions and high-level navigation plans. This dataset was collected through Mechanical Turk using 100 simulated environments with a corresponding topological map and, to the best of our knowledge, it is the first of its kind for behavioral navigation. The dataset opens up opportunities to explore data-driven methods for grounding navigation commands into high-level motion plans..We conduct extensive experiments to study the generalization capabilities of the proposed model for following natural language instructions. We investigate both generalization to new instructions in known and in new environments. We conclude this paper by discussing the benefits of the proposed approach as well as opportunities for future research based on our findings. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "50\n",
      "What is the difference between the proposed approach and the baseline approach for translating natural language instructions into a navigation plan?\n",
      "\n",
      "The proposed approach uses behavioral graphs as an extra input to the translation model, while the baseline approach divides the task into path generation and path verification, using a sequence-to-sequence model with attention for path generation and depth-first search for path verification.\n",
      "Question : for the text We compare the proposed approach for translating natural language instructions into a navigation plan against alternative deep-learning models:.[align=left,leftmargin=0em,labelsep=0.4em,font=].The baseline approach is based on BIBREF20 . It divides the task of interpreting commands for behavioral navigation into two steps: path generation, and path verification. For path generation, this baseline uses a standard sequence-to-sequence model augmented with an attention mechanism, similar to BIBREF23 , BIBREF6 . For path verification, the baseline uses depth-first search to find a route in the graph that matches the sequence of predicted behaviors. If no route matches perfectly, the baseline changes up to three behaviors in the predicted sequence to try to turn it into a valid path..To test the impact of using the behavioral graphs as an extra input to our translation model, we implemented a version of our approach that only takes natural language instructions as input. In this ablation model, the output of the bidirectional GRU that encodes the input instruction INLINEFORM0 is directly fed to the decoder layer. This model does not have the attention and FC layers described in Sec. SECREF4 , nor uses the masking function in the output layer..This model is the same as the previous Ablation model, but with the masking function in the output layer. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "1\n",
      "What is the prior knowledge used for the translation task?\n",
      "\n",
      "The prior knowledge used for the translation task is a topological map in the form of a behavioral navigation graph, where the nodes correspond to semantically-meaningful locations and its directed edges are visuo-motor behaviors for the robot.\n",
      "Question : for the text Our goal is to translate navigation instructions in text form into a sequence of behaviors that a robot can execute to reach a desired destination from a known start location. We frame this problem under a behavioral approach to indoor autonomous navigation BIBREF5 and assume that prior knowledge about the environment is available for the translation task. This prior knowledge is a topological map, in the form of a behavioral navigation graph (Fig. FIGREF4 (b)). The nodes of the graph correspond to semantically-meaningful locations for the navigation task, and its directed edges are visuo-motor behaviors that a robot can use to move between nodes. This formulation takes advantage of the rich semantic structure behind man-made environments, resulting in a compact route representation for robot navigation..Fig. FIGREF4 (c) provides a schematic view of the problem setting. The inputs are: (1) a navigation graph INLINEFORM0 , (2) the starting node INLINEFORM1 of the robot in INLINEFORM2 , and (3) a set of free-form navigation instructions INLINEFORM3 in natural language. The instructions describe a path in the graph to reach from INLINEFORM4 to a – potentially implicit – destination node INLINEFORM5 . Using this information, the objective is to predict a suitable sequence of robot behaviors INLINEFORM6 to navigate from INLINEFORM7 to INLINEFORM8 according to INLINEFORM9 . From a supervised learning perspective, the goal is then to estimate: DISPLAYFORM0 .based on a dataset of input-target pairs INLINEFORM0 , where INLINEFORM1 and INLINEFORM2 , respectively. The sequential execution of the behaviors INLINEFORM3 should replicate the route intended by the instructions INLINEFORM4 . We assume no prior linguistic knowledge. Thus, translation approaches have to cope with the semantics and syntax of the language by discovering corresponding patterns in the data. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "2\n",
      "Question 1: What does Fig FIGREF33 (b) show in terms of the proposed model's attention distribution?\n",
      "\n",
      "Answer 1: Fig FIGREF33 (b) shows the attention map for a correct prediction made by the proposed model. The attention distribution shown in each column of the array suggests that the decoder is paying attention to graph triplets associated with particular neighborhoods of the environment in each prediction step.\n",
      "Question : for the text This section discusses qualitative results to better understand how the proposed model uses the navigation graph..We analyze the evolution of the attention weights INLINEFORM0 in Eq. () to assess if the decoder layer of the proposed model is attending to the correct parts of the behavioral graph when making predictions. Fig FIGREF33 (b) shows an example of the resulting attention map for the case of a correct prediction. In the Figure, the attention map is depicted as a scaled and normalized 2D array of color codes. Each column in the array shows the attention distribution INLINEFORM1 used to generate the predicted output at step INLINEFORM2 . Consequently, each row in the array represents a triplet in the corresponding behavioral graph. This graph consists of 72 triplets for Fig FIGREF33 (b)..We observe a locality effect associated to the attention coefficients corresponding to high values (bright areas) in each column of Fig FIGREF33 (b). This suggests that the decoder is paying attention to graph triplets associated to particular neighborhoods of the environment in each prediction step. We include additional attention visualizations in the supplementary Appendix, including cases where the dynamics of the attention distribution are harder to interpret..All the routes in our dataset are the shortest paths from a start location to a given destination. Thus, we collected a few additional natural language instructions to check if our model was able to follow navigation instructions describing sub-optimal paths. One such example is shown in Fig. FIGREF37 , where the blue route (shortest path) and the red route (alternative path) are described by:.[leftmargin=*, labelsep=0.2em, itemsep=0em].“Go out the office and make a left. Turn right at the corner and go down the hall. Make a right at the next corner and enter the kitchen in front of table.”.“Exit the room 0 and turn right, go to the end of the corridor and turn left, go straight to the end of the corridor and turn left again. After passing bookshelf on your left and table on your right, Enter the kitchen on your right.”.For both routes, the proposed model was able to predict the correct sequence of navigation behaviors. This result suggests that the model is indeed using the input instructions and is not just approximating shortest paths in the behavioral graph. Other examples on the prediction of sub-obtimal paths are described in the Appendix. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "3\n",
      "Question 1: What is the impact of providing the behavioral navigation graph to the model in an end-to-end fashion?\n",
      "\n",
      "Answer 1: The final model \"Ours with Mask and Ordered Triplets\" outperforms the Baseline and Ablation models on all metrics in previously seen environments, with a significant increase in accuracy for Exact Match and Goal Match metrics. This suggests that providing the behavioral navigation graph to the model and allowing it to process this information as a knowledge base in an end-to-end fashion is beneficial.\n",
      "Question : for the text Table TABREF28 shows the performance of the models considered in our evaluation on both test sets. The next two sections discuss the results in detail..First, we can observe that the final model “Ours with Mask and Ordered Triplets” outperforms the Baseline and Ablation models on all metrics in previously seen environments. The difference in performance is particularly evident for the Exact Match and Goal Match metrics, with our model increasing accuracy by 35% and 25% in comparison to the Baseline and Ablation models, respectively. These results suggest that providing the behavioral navigation graph to the model and allowing it to process this information as a knowledge base in an end-to-end fashion is beneficial..We can also observe from Table TABREF28 that the masking function of Eq. ( EQREF12 ) tends to increase performance in the Test-Repeated Set by constraining the output sequence to a valid set of navigation behaviors. For the Ablation model, using the masking function leads to about INLINEFORM0 increase in EM and GM accuracy. For the proposed model (with or without reordering the graph triplets), the increase in accuracy is around INLINEFORM1 . Note that the impact of the masking function is less evident in terms of the F1 score because this metric considers if a predicted behavior exists in the ground truth navigation plan, irrespective of its specific position in the output sequence..The results in the last four rows of Table TABREF28 suggest that ordering the graph triplets can facilitate predicting correct navigation plans in previously seen environments. Providing the triplets that surround the starting location of the robot first to the model leads to a boost of INLINEFORM0 in EM and GM performance. The rearrangement of the graph triplets also helps to reduce ED and increase F1..Lastly, it is worth noting that our proposed model (last row of Table TABREF28 ) outperforms all other models in previously seen environments. In particular, we obtain over INLINEFORM0 increase in EM and GM between our model and the next best two models..The previous section evaluated model performance on new instructions (and corresponding navigation plans) for environments that were previously seen at training time. Here, we examine whether the trained models succeed on environments that are completely new..The evaluation on the Test-New Set helps understand the generalization capabilities of the models under consideration. This experiment is more challenging than the one in the previous section, as can be seen in performance drops in Table TABREF28 for the new environments. Nonetheless, the insights from the previous section still hold: masking in the output layer and reordering the graph triplets tend to increase performance..Even though the results in Table TABREF28 suggest that there is room for future work on decoding natural language instructions, our model still outperforms the baselines by a clear margin in new environments. For instance, the difference between our model and the second best model in the Test-New set is about INLINEFORM0 EM and GM. Note that the average number of actions in the ground truth output sequences is 7.07 for the Test-New set. Our model's predictions are just INLINEFORM1 edits off on average from the correct navigation plans. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "4\n",
      "What approach does the paper take to follow natural language navigation instructions?\n",
      "\n",
      "The paper takes a machine translation approach to interpret free-form natural language instructions using information from a high-level representation of the environment, and generates a high-level navigation plan that can be executed by a robot to reach a desired destination. The approach builds on prior work in statistical machine translation and end-to-end learning, and uses a sequence-to-sequence model with attention mechanisms to align different input modalities.\n",
      "Question : for the text This section reviews relevant prior work on following navigation instructions. Readers interested in an in-depth review of methods to interpret spatial natural language for robotics are encouraged to refer to BIBREF11 ..Typical approaches to follow navigation commands deal with the complexity of natural language by manually parsing commands, constraining language descriptions, or using statistical machine translation methods. While manually parsing commands is often impractical, the first type of approaches are foundational: they showed that it is possible to leverage the compositionality of semantic units to interpret spatial language BIBREF12 , BIBREF13 ..Constraining language descriptions can reduce the size of the input space to facilitate the interpretation of user commands. For example, BIBREF14 explored using structured, symbolic language phrases for navigation. As in this earlier work, we are also interested in navigation with a topological map of the environment. However, we do not process symbolic phrases. Our aim is to translate free-form natural language instructions to a navigation plan using information from a high-level representation of the environment. This translation problem requires dealing with missing actions in navigation instructions and actions with preconditions, such as “at the end of the corridor, turn right” BIBREF15 ..Statistical machine translation BIBREF16 is at the core of recent approaches to enable robots to follow navigation instructions. These methods aim to automatically discover translation rules from a corpus of data, and often leverage the fact that navigation directions are composed of sequential commands. For instance, BIBREF17 , BIBREF4 , BIBREF2 used statistical machine translation to map instructions to a formal language defined by a grammar. Likewise, BIBREF18 , BIBREF0 mapped commands to spatial description clauses based on the hierarchical structure of language in the navigation problem. Our approach to machine translation builds on insights from these prior efforts. In particular, we focus on end-to-end learning for statistical machine translation due to the recent success of Neural Networks in Natural Language Processing BIBREF19 ..Our work is inspired by methods that reduce the task of interpreting user commands to a sequential prediction problem BIBREF20 , BIBREF21 , BIBREF22 . Similar to BIBREF21 and BIBREF22 , we use a sequence-to-sequence model to enable a mobile agent to follow routes. But instead leveraging visual information to output low-level navigation commands, we focus on using a topological map of the environment to output a high-level navigation plan. This plan is a sequence of behaviors that can be executed by a robot to reach a desired destination BIBREF5 , BIBREF6 ..We explore machine translation from the perspective of automatic question answering. Following BIBREF8 , BIBREF9 , our approach uses attention mechanisms to learn alignments between different input modalities. In our case, the inputs to our model are navigation instructions, a topological environment map, and the start location of the robot (Fig. FIGREF4 (c)). Our results show that the map can serve as an effective source of contextual information for the translation task. Additionally, it is possible to leverage this kind of information in an end-to-end fashion. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "5\n",
      "Question 1: How many types of behaviors are considered in the navigation graph?\n",
      "\n",
      "Answer 1: 11 types of behaviors are considered in the navigation graph, as listed in Table TABREF7.\n",
      "Question : for the text We view the behavioral graph INLINEFORM0 as a knowledge base that encodes a set of navigational rules as triplets INLINEFORM1 , where INLINEFORM2 and INLINEFORM3 are adjacent nodes in the graph, and the edge INLINEFORM4 is an executable behavior to navigate from INLINEFORM5 to INLINEFORM6 . In general, each behaviors includes a list of relevant navigational attributes INLINEFORM7 that the robot might encounter when moving between nodes..We consider 7 types of semantic locations, 11 types of behaviors, and 20 different types of landmarks. A location in the navigation graph can be a room, a lab, an office, a kitchen, a hall, a corridor, or a bathroom. These places are labeled with unique tags, such as \"room-1\" or \"lab-2\", except for bathrooms and kitchens which people do not typically refer to by unique names when describing navigation routes..Table TABREF7 lists the navigation behaviors that we consider in this work. These behaviors can be described in reference to visual landmarks or objects, such as paintings, book shelfs, tables, etc. As in Fig. FIGREF4 , maps might contain multiple landmarks of the same type. Please see the supplementary material (Appendix A) for more details. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "6\n",
      "Question 1: What was the grant number for the study supported by the National Institute of Mental Health?\n",
      "\n",
      "Answer 1: The grant number for the study supported by the National Institute of Mental Health was 5R01MH109687.\n",
      "Question : for the text This work was supported by a grant from the National Institute of Mental Health (grant no. 5R01MH109687 to Mei-Hua Hall). We would also like to thank the LOUHI 2018 Workshop reviewers for their constructive and helpful comments..[9]NEO Five-Factor Inventory BIBREF30 [10]Positive and Negative Syndrome Scale BIBREF31 [11]Montgomery-Asperg Depression Rating Scale BIBREF32 [12]Young Mania Rating Scale BIBREF33  generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "7\n",
      "What was the purpose of removing the surrounding EHR context when annotating the paragraphs?\n",
      "\n",
      "Answer: The purpose of removing the surrounding EHR context when annotating the paragraphs was to ensure that annotators were not influenced by the additional contextual information when labeling the clinically relevant domains.\n",
      "Question : for the text In order to evaluate our models, we annotated 1,654 paragraphs selected from the 240,000 paragraphs extracted from Meditech with the clinically relevant domains described in Table TABREF3 . The annotation task was completed by three licensed clinicians. All paragraphs were removed from the surrounding EHR context to ensure annotators were not influenced by the additional contextual information. Our domain classification models consider each paragraph independently and thus we designed the annotation task to mirror the information available to the models..The annotators were instructed to label each paragraph with one or more of the seven risk factor domains. In instances where more than one domain was applicable, annotators assigned the domains in order of prevalence within the paragraph. An eighth label, `Other', was included if a paragraph was ambiguous, uninterpretable, or about a domain not included in the seven risk factor domains (e.g. non-psychiatric medical concerns and lab results). The annotations were then reviewed by a team of two clinicians who adjudicated collaboratively to create a gold standard. The gold standard and the clinician-identified keywords and MWEs have received IRB approval for release to the community. They are available as supplementary data to this paper. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "8\n",
      "Question 1: What is the OnTrackTM program at McLean Hospital and who are the patients in the target cohort?\n",
      "\n",
      "Answer 1: The OnTrackTM program at McLean Hospital is an outpatient program focusing on treating adults ages 18 to 30 who are experiencing their first episodes of psychosis. The target cohort consists of 220 patients in the program. The vast majority of these patients are dependents on a parental private health insurance plan.\n",
      "Question : for the text [2]The vast majority of patients in our target cohort are.dependents on a parental private health insurance plan..Our target data set consists of a corpus of discharge summaries, admission notes, individual encounter notes, and other clinical notes from 220 patients in the OnTrackTM program at McLean Hospital. OnTrackTM is an outpatient program, focusing on treating adults ages 18 to 30 who are experiencing their first episodes of psychosis. The length of time in the program varies depending on patient improvement and insurance coverage, with an average of two to three years. The program focuses primarily on early intervention via individual therapy, group therapy, medication evaluation, and medication management. See Table TABREF2 for a demographic breakdown of the 220 patients, for which we have so far extracted approximately 240,000 total EHR paragraphs spanning from 2011 to 2014 using Meditech, the software employed by McLean for storing and organizing EHR data..These patients are part of a larger research cohort of approximately 1,800 psychosis patients, which will allow us to connect the results of this EHR study with other ongoing research studies incorporating genetic, cognitive, neurobiological, and functional outcome data from this cohort..We also use an additional data set for training our vector space model, comprised of EHR texts queried from the Research Patient Data Registry (RPDR), a centralized regional data repository of clinical data from all institutions in the Partners HealthCare network. These records are highly comparable in style and vocabulary to our target data set. The corpus consists of discharge summaries, encounter notes, and visit notes from approximately 30,000 patients admitted to the system's hospitals with psychiatric diagnoses and symptoms. This breadth of data captures a wide range of clinical narratives, creating a comprehensive foundation for topic extraction..After using the RPDR query tool to extract EHR paragraphs from the RPDR database, we created a training corpus by categorizing the extracted paragraphs according to their risk factor domain using a lexicon of 120 keywords that were identified by the clinicians involved in this project. Certain domains – particularly those involving thoughts and other abstract concepts – are often identifiable by MWEs rather than single words. The same clinicians who identified the keywords manually examined the bigrams and trigrams with the highest TF-IDF scores for each domain in the categorized paragraphs, identifying those which are conceptually related to the given domain. We then used this lexicon of 775 keyphrases to identify more relevant training paragraphs in RPDR and treat them as (non-stemmed) unigrams when generating the matrix. By converting MWEs such as `shortened attention span', `unusual motor activity', `wide-ranging affect', or `linear thinking' to non-stemmed unigrams, the TF-IDF score (and therefore the predictive value) of these terms is magnified. In total, we constructed a corpus of roughly 100,000 paragraphs consisting of 7,000,000 tokens for training our model. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "9\n",
      "Question 1: What additional features does the study plan to factor in for training the machine learning classifier?\n",
      "\n",
      "Answer 1: The study plans to factor in a deeper analysis of clinical narratives in EHRs, distinguish between clinically positive and negative phenomena within each risk factor domain, clinical sentiment scores, brain based electrophysiological biomarkers, structural brain anatomy from MRI scans, social and role functioning assessments, personality assessment, and various symptom scales. Each feature will be evaluated for its contribution as a predictor of readmission.\n",
      "Question : for the text To achieve our goal of creating a framework for a readmission risk classifier, the present study performed necessary evaluation steps by updating and adding to our model iteratively. In the first stage of the project, we focused on collecting the data necessary for training and testing, and on the domain classification annotation task. At the same time, we began creating the tools necessary for automatically extracting domain relevance scores at the paragraph and document level from patient EHRs using several forms of vectorization and topic modeling. In future versions of our risk factor domain classification model we will explore increasing robustness through sequence modeling that considers more contextual information..Our current feature set for training a machine learning classifier is relatively small, consisting of paragraph domain scores, bag-of-words, length of stay, and number of previous admissions, but we intend to factor in many additional features that extend beyond the scope of the present study. These include a deeper analysis of clinical narratives in EHRs: our next task will be to extend our EHR data pipeline by distinguishing between clinically positive and negative phenomena within each risk factor domain. This will involve a series of annotation tasks that will allow us to generate lexicon-based and corpus-based sentiment analysis tools. We can then use these clinical sentiment scores to generate a gradient of patient improvement or deterioration over time..We will also take into account structured data that have been collected on the target cohort throughout the course of this study such as brain based electrophysiological (EEG) biomarkers, structural brain anatomy from MRI scans (gray matter volume, cortical thickness, cortical surface-area), social and role functioning assessments, personality assessment (NEO-FFI[9]), and various symptom scales (PANSS[10], MADRS[11], YMRS[12]). For each feature we consider adding, we will evaluate the performance of the classifier with and without the feature to determine its contribution as a predictor of readmission. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "10\n",
      "Question 1: What is the Fleiss's Kappa value in this study, and what does it indicate about the agreement among annotators?\n",
      "\n",
      "Answer 1: The Fleiss's Kappa value in this study is 0.575, which indicates a moderate to substantial agreement among annotators on the risk factor domains.\n",
      "Question : for the text Inter-annotator agreement (IAA) was assessed using a combination of Fleiss's Kappa (a variant of Scott's Pi that measures pairwise agreement for annotation tasks involving more than two annotators) BIBREF16 and Cohen's Multi-Kappa as proposed by Davies and Fleiss davies1982measuring. Table TABREF6 shows IAA calculations for both overall agreement and agreement on the first (most important) domain only. Following adjudication, accuracy scores were calculated for each annotator by evaluating their annotations against the gold standard..Overall agreement was generally good and aligned almost exactly with the IAA on the first domain only. Out of the 1,654 annotated paragraphs, 671 (41%) had total agreement across all three annotators. We defined total agreement for the task as a set-theoretic complete intersection of domains for a paragraph identified by all annotators. 98% of paragraphs in total agreement involved one domain. Only 35 paragraphs had total disagreement, which we defined as a set-theoretic null intersection between the three annotators. An analysis of the 35 paragraphs with total disagreement showed that nearly 30% included the term “blunted/restricted\". In clinical terminology, these terms can be used to refer to appearance, affect, mood, or emotion. Because the paragraphs being annotated were extracted from larger clinical narratives and examined independently of any surrounding context, it was difficult for the annotators to determine the most appropriate domain. This lack of contextual information resulted in each annotator using a different `default' label: Appearance, Mood, and Other. During adjudication, Other was decided as the most appropriate label unless the paragraph contained additional content that encompassed other domains, as it avoids making unnecessary assumptions. [3]Suicidal ideation [4]Homicidal ideation [5]Ethyl alcohol and ethanol.A Fleiss's Kappa of 0.575 lies on the boundary between `Moderate' and `Substantial' agreement as proposed by Landis and Koch landis1977measurement. This is a promising indication that our risk factor domains are adequately defined by our present guidelines and can be employed by clinicians involved in similar work at other institutions..The fourth column in Table TABREF6 , Mean Accuracy, was calculated by averaging the three annotator accuracies as evaluated against the gold standard. This provides us with an informative baseline of human parity on the domain classification task..[6]Rectified Linear Units, INLINEFORM0 BIBREF17 [7]Adaptive Moment Estimation BIBREF18  generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "11\n",
      "What are the challenges of topic extraction when dealing with clinical narratives in psychiatric EHRs?\n",
      "- The challenges include the highly varied and context-sensitive vocabulary used, multiword expressions in technical terms, variable narrative structures, and complex phenotypic assessment leading to differences in lexicon across patients and time. These factors make topic extraction a difficult task that cannot be accomplished by simple text-mining techniques or keyword search.\n",
      "Question : for the text Psychotic disorders typically emerge in late adolescence or early adulthood BIBREF0 , BIBREF1 and affect approximately 2.5-4% of the population BIBREF2 , BIBREF3 , making them one of the leading causes of disability worldwide BIBREF4 . A substantial proportion of psychiatric inpatients are readmitted after discharge BIBREF5 . Readmissions are disruptive for patients and families, and are a key driver of rising healthcare costs BIBREF6 , BIBREF7 . Reducing readmission risk is therefore a major unmet need of psychiatric care. Developing clinically implementable machine learning tools to enable accurate assessment of risk factors associated with readmission offers opportunities to inform the selection of treatment interventions and implement appropriate preventive measures..In psychiatry, traditional strategies to study readmission risk factors rely on clinical observation and manual retrospective chart review BIBREF8 , BIBREF9 . This approach, although benefitting from clinical expertise, does not scale well for large data sets, is effort-intensive, and lacks automation. An efficient, more robust, and cheaper NLP-based alternative approach has been developed and met with some success in other medical fields BIBREF10 . However, this approach has seldom been applied in psychiatry because of the unique characteristics of psychiatric medical record content..There are several challenges for topic extraction when dealing with clinical narratives in psychiatric EHRs. First, the vocabulary used is highly varied and context-sensitive. A patient may report “feeling `really great and excited'\" – symptoms of mania – without any explicit mention of keywords that differ from everyday vocabulary. Also, many technical terms in clinical narratives are multiword expressions (MWEs) such as `obsessive body image', `linear thinking', `short attention span', or `panic attack'. These phrasemes are comprised of words that in isolation do not impart much information in determining relatedness to a given topic but do in the context of the expression..Second, the narrative structure in psychiatric clinical narratives varies considerably in how the same phenomenon can be described. Hallucinations, for example, could be described as “the patient reports auditory hallucinations,\" or “the patient has been hearing voices for several months,\" amongst many other possibilities..Third, phenomena can be directly mentioned without necessarily being relevant to the patient specifically. Psychosis patient discharge summaries, for instance, can include future treatment plans (e.g. “Prevent relapse of a manic or major depressive episode.\", “Prevent recurrence of psychosis.\") containing vocabulary that at the word-level seem strongly correlated with readmission risk. Yet at the paragraph-level these do not indicate the presence of a readmission risk factor in the patient and in fact indicate the absence of a risk factor that was formerly present..Lastly, given the complexity of phenotypic assessment in psychiatric illnesses, patients with psychosis exhibit considerable differences in terms of illness and symptom presentation. The constellation of symptoms leads to various diagnoses and comorbidities that can change over time, including schizophrenia, schizoaffective disorder, bipolar disorder with psychosis, and substance use induced psychosis. Thus, the lexicon of words and phrases used in EHRs differs not only across diagnoses but also across patients and time..Taken together, these factors make topic extraction a difficult task that cannot be accomplished by keyword search or other simple text-mining techniques..To identify specific risk factors to focus on, we not only reviewed clinical literature of risk factors associated with readmission BIBREF11 , BIBREF12 , but also considered research related to functional remission BIBREF13 , forensic risk factors BIBREF14 , and consulted clinicians involved with this project. Seven risk factor domains – Appearance, Mood, Interpersonal, Occupation, Thought Content, Thought Process, and Substance – were chosen because they are clinically relevant, consistent with literature, replicable across data sets, explainable, and implementable in NLP algorithms..In our present study, we evaluate multiple approaches to automatically identify which risk factor domains are associated with which paragraphs in psychotic patient EHRs. We perform this study in support of our long-term goal of creating a readmission risk classifier that can aid clinicians in targeting individual treatment interventions and assessing patient risk of harm (e.g. suicide risk, homicidal risk). Unlike other contemporary approaches in machine learning, we intend to create a model that is clinically explainable and flexible across training data while maintaining consistent performance..To incorporate clinical expertise in the identification of risk factor domains, we undertake an annotation project, detailed in section 3.1. We identify a test set of over 1,600 EHR paragraphs which a team of three domain-expert clinicians annotate paragraph-by-paragraph for relevant risk factor domains. Section 3.2 describes the results of this annotation task. We then use the gold standard from the annotation project to assess the performance of multiple neural classification models trained exclusively on Term Frequency – Inverse Document Frequency (TF-IDF) vectorized EHR data, described in section 4. To further improve the performance of our model, we incorporate domain-relevant MWEs identified using all in-house data. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "12\n",
      "Question 1: What were some limitations of using web data to train a similarity model for EHR texts in McCoy et al.'s study?\n",
      "\n",
      "Answer 1: One limitation was that informative multi-word expressions (MWEs) critical to understanding EHR paragraph topics were not captured in the web data. Additionally, RDoC constructs do not include important indicators of patient health such as appearance or occupation, which are commonly found in EHRs. The study also used data from Bing API, which differs in structure and content from EHRs.\n",
      "Question : for the text McCoy et al. mccoy2015clinical constructed a corpus of web data based on the Research Domain Criteria (RDoC) BIBREF15 , and used this corpus to create a vector space document similarity model for topic extraction. They found that the `negative valence' and `social' RDoC domains were associated with readmission. Using web data (in this case data retrieved from the Bing API) to train a similarity model for EHR texts is problematic since it differs from the target data in both structure and content. Based on reconstruction of the procedure, we conclude that many of the informative MWEs critical to understanding the topics of paragraphs in EHRs are not captured in the web data. Additionally, RDoC is by design a generalized research construct to describe the entire spectrum of mental disorders and does not include domains that are based on observation or causes of symptoms. Important indicators within EHRs of patient health, like appearance or occupation, are not included in the RDoC constructs..Rumshisky et al. rumshisky2016predicting used a corpus of EHRs from patients with a primary diagnosis of major depressive disorder to create a 75-topic LDA topic model that they then used in a readmission prediction classifier pipeline. Like with McCoy et al. mccoy2015clinical, the data used to train the LDA model was not ideal as the generalizability of the data was narrow, focusing on only one disorder. Their model achieved readmission prediction performance with an area under the curve of .784 compared to a baseline of .618. To perform clinical validation of the topics derived from the LDA model, they manually evaluated and annotated the topics, identifying the most informative vocabulary for the top ten topics. With their training data, they found the strongest coherence occurred in topics involving substance use, suicidality, and anxiety disorders. But given the unsupervised nature of the LDA clustering algorithm, the topic coherence they observed is not guaranteed across data sets. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "13\n",
      "What role do MWEs play in correctly identifying risk factor domains?\n",
      "MWEs play a large role in correctly identifying risk factor domains, as factoring them into the models increased classification performance by 15%, a marked improvement over the baseline model. This aligns with the expectation that MWEs made up of quotidian vocabulary hold much more clinical significance than treating the words independently.\n",
      "Question : for the text Table TABREF9 shows the performance of our models on classifying the paragraphs in our gold standard. To assess relative performance of feature representations, we also include performance metrics of our models without MWEs. Because this is a multilabel classification task we use macro-averaging to compute precision, recall, and F1 scores for each paragraph in the testing set. In identifying domains individually, our models achieved the highest per-domain scores on Substance (F1 INLINEFORM0 0.8) and the lowest scores on Interpersonal and Mood (F1 INLINEFORM1 0.5). We observe a consistency in per-domain performance rankings between our MLP and RBF models..The wide variance in per-domain performance is due to a number of factors. Most notably, the training examples we extracted from RPDR – while very comparable to our target OnTrackTM data – may not have an adequate variety of content and range of vocabulary. Although using keyword and MWE matching to create our training corpus has the advantage of being significantly less labor intensive than manually labeling every paragraph in the corpus, it is likely that the homogeneity of language used in the training paragraphs is higher than it would be otherwise. Additionally, all of the paragraphs in the training data are assigned exactly one risk factor domain even if they actually involve multiple risk factor domains, making the clustering behavior of the paragraphs more difficult to define. Figure FIGREF10 illustrates the distribution of paragraphs in vector space using 2-component Linear Discriminant Analysis (LDA) BIBREF26 ..Despite prior research indicating that similar classification tasks to ours are more effectively performed by RBF networks BIBREF27 , BIBREF28 , BIBREF29 , we find that a MLP network performs marginally better with significantly less preprocessing (i.e. k-means and width calculations) involved. We can see in Figure FIGREF10 that Thought Process, Appearance, Substance, and – to a certain extent – Occupation clearly occupy specific regions, whereas Interpersonal, Mood, and Thought Content occupy the same noisy region where multiple domains overlap. Given that similarity is computed using Euclidean distance in an RBF network, it is difficult to accurately classify paragraphs that fall in regions occupied by multiple risk factor domain clusters since prototype centroids from the risk factor domains will overlap and be less differentiable. This is confirmed by the results in Table TABREF9 , where the differences in performance between the RBF and MLP models are more pronounced in the three overlapping domains (0.496 vs 0.448 for Interpersonal, 0.530 vs 0.496 for Mood, and 0.721 vs 0.678 for Thought Content) compared to the non-overlapping domains (0.564 vs 0.566 for Appearance, 0.592 vs 0.598 for Occupation, 0.797 vs 0.792 for Substance, and 0.635 vs 0.624 for Thought Process). We also observe a similarity in the words and phrases with the highest TF-IDF scores across the overlapping domains: many of the Thought Content words and phrases with the highest TF-IDF scores involve interpersonal relations (e.g. `fear surrounding daughter', `father', `family history', `familial conflict') and there is a high degree of similarity between high-scoring words for Mood (e.g. `meets anxiety criteria', `cope with mania', `ocd'[8]) and Thought Content (e.g. `mania', `feels anxious', `feels exhausted')..[8]Obsessive-compulsive disorder.MWEs play a large role in correctly identifying risk factor domains. Factoring them into our models increased classification performance by 15%, a marked improvement over our baseline model. This aligns with our expectations that MWEs comprised of a quotidian vocabulary hold much more clinical significance than when the words in the expressions are treated independently..Threshold similarity scores also play a large role in determining the precision and recall of our models: higher thresholds lead to a smaller number of false positives and a greater number of false negatives for each risk factor domain. Conversely, more paragraphs are incorrectly classified as Other when thresholds are set higher. Since our classifier will be used in future work as an early step in a data analysis pipeline for determining readmission risk, misclassifying a paragraph with an incorrect risk factor domain at this stage can lead to greater inaccuracies at later stages. Paragraphs misclassified as Other, however, will be discarded from the data pipeline. Therefore, we intentionally set a conservative threshold where only the most confidently labeled paragraphs are assigned membership in a particular domain. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "14\n",
      "What tool is used to generate the TF-IDF vector space models?\n",
      "\n",
      "The TfidfVectorizer tool is used to generate the TF-IDF vector space models.\n",
      "Question : for the text Figure FIGREF8 illustrates the data pipeline for generating our training and testing corpora, and applying them to our classification models..We use the TfidfVectorizer tool included in the scikit-learn machine learning toolkit BIBREF19 to generate our TF-IDF vector space models, stemming tokens with the Porter Stemmer tool provided by the NLTK library BIBREF20 , and calculating TF-IDF scores for unigrams, bigrams, and trigrams. Applying Singular Value Decomposition (SVD) to the TF-IDF matrix, we reduce the vector space to 100 dimensions, which Zhang et al. zhang2011comparative found to improve classifier performance..Starting with the approach taken by McCoy et al. mccoy2015clinical, who used aggregate cosine similarity scores to compute domain similarity directly from their TF-IDF vector space model, we extend this method by training a suite of three-layer multilayer perceptron (MLP) and radial basis function (RBF) neural networks using a variety of parameters to compare performance. We employ the Keras deep learning library BIBREF21 using a TensorFlow backend BIBREF22 for this task. The architectures of our highest performing MLP and RBF models are summarized in Table TABREF7 . Prototype vectors for the nodes in the hidden layer of our RBF model are selected via k-means clustering BIBREF23 on each domain paragraph megadocument individually. The RBF transfer function for each hidden layer node is assigned the same width, which is based off the maximum Euclidean distance between the centroids that were computed using k-means..To prevent overfitting to the training data, we utilize a dropout rate BIBREF24 of 0.2 on the input layer of all models and 0.5 on the MLP hidden layer..Since our classification problem is multiclass, multilabel, and open-world, we employ seven nodes with sigmoid activations in the output layer, one for each risk factor domain. This allows us to identify paragraphs that fall into more than one of the seven domains, as well as determine paragraphs that should be classified as Other. Unlike the traditionally used softmax activation function, which is ideal for single-label, closed-world classification tasks, sigmoid nodes output class likelihoods for each node independently without the normalization across all classes that occurs in softmax..We find that the risk factor domains vary in the degree of homogeneity of language used, and as such certain domains produce higher similarity scores, on average, than others. To account for this, we calculate threshold similarity scores for each domain using the formula min=avg(sim)+ INLINEFORM0 * INLINEFORM1 (sim), where INLINEFORM2 is standard deviation and INLINEFORM3 is a constant, which we set to 0.78 for our MLP model and 1.2 for our RBF model through trial-and-error. Employing a generalized formula as opposed to manually identifying threshold similarity scores for each domain has the advantage of flexibility in regards to the target data, which may vary in average similarity scores depending on its similarity to the training data. If a paragraph does not meet threshold on any domain, it is classified as Other. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "15\n",
      "Question 1: What organizations are supporting this work? \n",
      "\n",
      "Answer 1: The National Natural Science Foundation of China, the Open Project of Key Laboratory of Xinjiang Uygur Autonomous Region, the Youth Innovation Promotion Association of the Chinese Academy of Sciences, and the High-level Talents Introduction Project of Xinjiang Uyghur Autonomous Region are supporting this work.\n",
      "Question : for the text This work is supported by the National Natural Science Foundation of China, the Open Project of Key Laboratory of Xinjiang Uygur Autonomous Region, the Youth Innovation Promotion Association of the Chinese Academy of Sciences, and the High-level Talents Introduction Project of Xinjiang Uyghur Autonomous Region. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "16\n",
      "Question 1: What are the two popular word segmentation methods discussed in the text?\n",
      "\n",
      "Answer 1: The two popular word segmentation methods discussed in the text are morpheme segmentation and Byte Pair Encoding (BPE).\n",
      "Question : for the text We will elaborate two popular word segmentation methods and our newly proposed segmentation strategies in this section. The two popular segmentation methods are morpheme segmentation BIBREF4 and Byte Pair Encoding (BPE) BIBREF5. After word segmentation, we additionally add an specific symbol behind each separated subword unit, which aims to assist the NMT model to identify the morpheme boundaries and capture the semantic information effectively. The sentence examples with different segmentation strategies for Turkish-English machine translation task are shown in Table 1. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "17\n",
      "Question 1: What is the benefit of using BPE algorithm in NMT model?\n",
      "\n",
      "Answer 1: The benefit of using BPE algorithm in NMT model is that it can make the model capable of open-vocabulary translation, which means it can translate and produce new words based on the subword units created by the algorithm. This allows the model to generalize better and improve its translation accuracy.\n",
      "Question : for the text BPE BIBREF7 is originally a data compression technique and it is adapted by BIBREF5 for word segmentation and vocabulary reduction by encoding the rare and unknown words as a sequence of subword units, in which the most frequent character sequences are merged iteratively. Frequent character n-grams are eventually merged into a single symbol. This is based on the intuition that various word classes are translatable via smaller units than words. This method making the NMT model capable of open-vocabulary translation, which can generalize to translate and produce new words on the basis of these subword units. The BPE algorithm can be run on the dictionary extracted from a training text, with each word being weighted by its frequency. In this segmentation strategy, we add “@@” behind each no-final subword unit of the segmented word. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "18\n",
      "What are the smallest functional units in agglutinative languages?\n",
      "\n",
      "The smallest functional units in agglutinative languages are morphemes, which are composed of a stem and an unlimited number of suffixes.\n",
      "Question : for the text The words of Turkish and Uyghur are formed by a stem followed with unlimited number of suffixes. Both of the stem and suffix are called morphemes, and they are the smallest functional unit in agglutinative languages. Study indicated that modeling language based on the morpheme units can provide better performance BIBREF6. Morpheme segmentation can segment the complex word into morpheme units of stem and suffix. This representation maintains a full description of the morphological properties of subwords while minimizing the data sparseness caused by inflection and allomorphy phenomenon in highly-inflected languages. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "19\n",
      "What is the segmentation method used in this strategy? \n",
      "\n",
      "The segmentation method used in this strategy is called SCS, which segments each word into a stem unit and a combined suffix unit by adding \"##\" behind the stem unit and \"$$\" behind the combined suffix unit.\n",
      "Question : for the text In this segmentation strategy, each word is segmented into a stem unit and a combined suffix unit. We add “##” behind the stem unit and add “$$” behind the combined suffix unit. We denote this method as SCS. The segmented word can be denoted as two parts of “stem##” and “suffix1suffix2...suffixN$$”. If the original word has no suffix unit, the word is treated as its stem unit. All the following segmentation strategies will follow this rule. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "20\n",
      "Question 1: What is the segmentation strategy used in this method?\n",
      "\n",
      "Answer 1: The segmentation strategy used in this method is to segment each word into a stem unit and a sequence of suffix units. It is denoted as SSS and involves adding “##” behind the stem unit and “$$” behind each singular suffix unit, resulting in a sequence of “stem##”, “suffix1$$”, “suffix2$$” until “suffixN$$”.\n",
      "Question : for the text In this segmentation strategy, each word is segmented into a stem unit and a sequence of suffix units. We add “##” behind the stem unit and add “$$” behind each singular suffix unit. We denote this method as SSS. The segmented word can be denoted as a sequence of “stem##”, “suffix1$$”, “suffix2$$” until “suffixN$$”. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "21\n",
      "What is the proposed word segmentation strategy that combines morpheme segmentation and BPE? \n",
      "\n",
      "The proposed strategy is the morphologically motivated segmentation strategy that learns a BPE model on the stem units in the training corpus and then applies it on the stem unit of each word after morpheme segmentation. This improves translation performance by considering both structure information and morphological information.\n",
      "Question : for the text The problem with morpheme segmentation is that the vocabulary of stem units is still very large, which leads to many rare and unknown words at the training time. The problem with BPE is that it do not consider the morpheme boundaries inside words, which might cause a loss of morphological properties and semantic information. Hence, on the analyses of the above popular word segmentation methods, we propose the morphologically motivated segmentation strategy that combines the morpheme segmentation and BPE for further improving the translation performance of NMT..Compared with the sentence of word surface forms, the corresponding sentence of stem units only contains the structure information without considering morphological information, which can make better generalization over inflectional variants of the same word and reduce data sparseness BIBREF8. Therefore, we learn a BPE model on the stem units in the training corpus rather than the words, and then apply it on the stem unit of each word after morpheme segmentation. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "22\n",
      "Question 1: What is the segmentation strategy used in BPE-SCS?\n",
      "\n",
      "Answer 1: In BPE-SCS, the segmentation strategy used is first segmenting each word into a stem unit and a combined suffix unit as SCS, then applying BPE on the stem unit, and finally adding \"$$\" behind the combined suffix unit. If the stem unit is not segmented, \"##\" is added behind itself, otherwise \"@@\" is added behind each no-final subword of the segmented stem unit.\n",
      "Question : for the text In this segmentation strategy, firstly we segment each word into a stem unit and a combined suffix unit as SCS. Secondly, we apply BPE on the stem unit. Thirdly, we add “$$” behind the combined suffix unit. If the stem unit is not segmented, we add “##” behind itself. Otherwise, we add “@@” behind each no-final subword of the segmented stem unit. We denote this method as BPE-SCS. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "23\n",
      "Question 1:What is the name of the segmentation strategy described in the text?\n",
      "\n",
      "Answer 1: The name of the segmentation strategy described in the text is BPE-SSS.\n",
      "Question : for the text In this segmentation strategy, firstly we segment each word into a stem unit and a sequence of suffix units as SSS. Secondly, we apply BPE on the stem unit. Thirdly, we add “$$” behind each singular suffix unit. If the stem unit is not segmented, we add “##” behind itself. Otherwise, we add “@@” behind each no-final subword of the segmented stem unit. We denote this method as BPE-SSS. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "24\n",
      "What languages were investigated in the paper for morphological segmentation strategies?\n",
      "The paper investigated the low-resource and morphologically-rich languages of Turkish and Uyghur.\n",
      "Question : for the text In this paper, we investigate morphological segmentation strategies on the low-resource and morphologically-rich languages of Turkish and Uyghur. Experimental results show that our proposed morphologically motivated word segmentation method is better suitable for NMT. And the BPE-SSS strategy achieves the best machine translation performance, as it can better preserve the syntactic and semantic information of the words with complex morphology as well as reduce the vocabulary size for model training. Moreover, we also estimate how the number of merge operations on the stem units for BPE-SSS strategy effects the translation quality, and we find that an appropriate vocabulary size is more useful for the NMT model..In future work, we are planning to incorporate more linguistic and morphology knowledge into the training process of NMT to enhance its capacity of capturing syntactic structure and semantic information on the low-resource and morphologically-rich languages. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "25\n",
      "Question 1: Which strategy performs better in machine translation tasks, BPE-SCS or BPE-SSS?\n",
      "\n",
      "Answer 1: Both BPE-SCS and BPE-SSS strategies outperform morpheme segmentation and pure BPE methods. However, BPE-SSS performs significantly better, achieving up to 1.2 BLEU points on Turkish-English machine translation task and 2.5 BLEU points on Uyghur-Chinese machine translation task.\n",
      "Question : for the text According to Table 6 and Table 7, we can find that both the BPE-SCS and BPE-SSS strategies outperform morpheme segmentation and the strong baseline of pure BPE method. Especially, the BPE-SSS strategy is better and it achieves significant improvement of up to 1.2 BLEU points on Turkish-English machine translation task and 2.5 BLEU points on Uyghur-Chinese machine translation task. Furthermore, we also find that the translation performance of our proposed segmentation strategy on Turkish-English machine translation task is not obvious than Uyghur-Chinese machine translation task, the probable reasons are: the training corpus of Turkish-English consists of talk and news data while most of the talk data are short informal sentences compared with the news data, which cannot provide more language information for the NMT model. Moreover, the test corpus consists of news data, so due to the data domain is different, the improvement of machine translation quality is limited..In addition, we estimate how the number of merge operations on the stem units for BPE-SSS strategy effects the machine translation quality. Experimental results are shown in Table 8 and Table 9. We find that the number of 25K for Turkish, 30K and 35K for Uyghur maximizes the translation performance. The probable reason is that these numbers of merge operations are able to generate a more appropriate vocabulary that containing effective morpheme units and moderate subword units, which makes better generalization over the morphologically-rich words. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "26\n",
      "Question 1: What toolkits were utilized for Chinese word segmentation in the mentioned translation tasks?\n",
      "Answer 1: The python toolkits of jieba were utilized for Chinese word segmentation in the mentioned translation tasks.\n",
      "Question : for the text We utilize the Zemberek with a morphological disambiguation tool to segment the Turkish words into morpheme units, and utilize the morphology analysis tool BIBREF12 to segment the Uyghur words into morpheme units. We employ the python toolkits of jieba for Chinese word segmentation. We apply BPE on the target-side words and we set the number of merge operations to 35K for Chinese and 30K for English and we set the maximum sentence length to 150 tokens. The training corpus statistics of Turkish-English and Uyghur-Chinese machine translation tasks are shown in Table 2 and Table 3 respectively. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "27\n",
      "What is the purpose of the BPE-SCS and BPE-SSS segmentation strategies proposed in the text? \n",
      "\n",
      "The purpose of the BPE-SCS and BPE-SSS segmentation strategies proposed in the text is to consider the morphological properties of the languages and eliminate the rare and unknown words while keeping the vocabulary size on the same scale.\n",
      "Question : for the text We set the number of merge operations on the stem units in the consideration of keeping the vocabulary size of BPE, BPE-SCS and BPE-SSS segmentation strategies on the same scale. We will elaborate the number settings for our proposed word segmentation strategies in this section..In the Turkish-English machine translation task, for the pure BPE strategy, we set the number of merge operations on the words to 35K, set the number of merge operations on the stem units for BPE-SCS strategy to 15K, and set the number of merge operations on the stem units for BPE-SSS strategy to 25K. In the Uyghur-Chinese machine translation task, for the pure BPE strategy, we set the number of merge operations on the words to 38K, set the number of merge operations on the stem units for BPE-SCS strategy to 10K, and set the number of merge operations on the stem units for BPE-SSS strategy to 35K. The detailed training corpus statistics with different segmentation strategies of Turkish and Uyghur are shown in Table 4 and Table 5 respectively..According to Table 4 and Table 5, we can find that both the Turkish and Uyghur have a very large vocabulary even in the low-resource training corpus. So we propose the morphological word segmentation strategies of BPE-SCS and BPE-SSS that additionally applying BPE on the stem units after morpheme segmentation, which not only consider the morphological properties but also eliminate the rare and unknown words. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "28\n",
      "Question 1: Which corpora were used for model training in this study?\n",
      "Answer 1: The WIT corpus and SETimes corpus were used for model training in this study.\n",
      "Question : for the text Following BIBREF9, we use the WIT corpus BIBREF10 and SETimes corpus BIBREF11 for model training, and use the newsdev2016 from Workshop on Machine Translation in 2016 (WMT2016) for validation. The test data are newstest2016 and newstest2017. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "29\n",
      "Question 1: What data was used for model training, validation and test in the China Workshop on Machine Translation in 2017?\n",
      "Answer 1: The news data from CWMT2017 was used for model training, validation and test.\n",
      "Question : for the text We use the news data from China Workshop on Machine Translation in 2017 (CWMT2017) for model training, validation and test. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "30\n",
      "Question 1: What architecture is implemented in the Sockeye toolkit?\n",
      "\n",
      "Answer 1: The architecture implemented in the Sockeye toolkit is the Transformer model with self-attention mechanism.\n",
      "Question : for the text We employ the Transformer model BIBREF13 with self-attention mechanism architecture implemented in Sockeye toolkit BIBREF14. Both the encoder and decoder have 6 layers. We set the number of hidden units to 512, the number of heads for self-attention to 8, the source and target word embedding size to 512, and the number of hidden units in feed-forward layers to 2048. We train the NMT model by using the Adam optimizer BIBREF15 with a batch size of 128 sentences, and we shuffle all the training data at each epoch. The label smoothing is set to 0.1. We report the result of averaging the parameters of the 4 best checkpoints on the validation perplexity. Decoding is performed by beam search with beam size of 5. To effectively evaluate the machine translation quality, we report case-sensitive BLEU score with standard tokenization and character n-gram ChrF3 score . generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "31\n",
      "What is the OOV problem in NMT and how does it affect translation performance?\n",
      "\n",
      "The OOV problem in NMT refers to the inability of the model to effectively translate words that are not present in its limited-size vocabulary. This leads to inaccurate and poor translation results, particularly for sentences with too many unknown words. This problem is more serious in low-resource and morphologically-rich languages, such as Turkish and Uyghur, where the complex morpheme structure and derivative morphology result in many unseen words in the training corpus.\n",
      "Question : for the text Neural machine translation (NMT) has achieved impressive performance on machine translation task in recent years for many language pairs BIBREF0, BIBREF1, BIBREF2. However, in consideration of time cost and space capacity, the NMT model generally employs a limited-size vocabulary that only contains the top-N highest frequency words (commonly in the range of 30K to 80K) BIBREF3, which leads to the Out-of-Vocabulary (OOV) problem following with inaccurate and terrible translation results. Research indicated that sentences with too many unknown words tend to be translated much more poorly than sentences with mainly frequent words. For the low-resource and source-side morphologically-rich machine translation tasks, such as Turkish-English and Uyghur-Chinese, all the above issues are more serious due to the fact that the NMT model cannot effectively identify the complex morpheme structure or capture the linguistic and semantic information with too many rare and unknown words in the training corpus..Both the Turkish and Uyghur are agglutinative and highly-inflected languages in which the word is formed by suffixes attaching to a stem BIBREF4. The word consists of smaller morpheme units without any splitter between them and its structure can be denoted as “stem + suffix1 + suffix2 + ... + suffixN”. A stem is attached in the rear by zero to many suffixes that have many inflected and morphological variants depending on case, number, gender, and so on. The complex morpheme structure and relatively free constituent order can produce very large vocabulary because of the derivational morphology, so when translating from the agglutinative languages, many words are unseen at training time. Moreover, due to the semantic context, the same word generally has different segmentation forms in the training corpus..For the purpose of incorporating morphology knowledge of agglutinative languages into word segmentation for NMT, we propose a morphological word segmentation method on the source-side of Turkish-English and Uyghur-Chinese machine translation tasks, which segments the complex words into simple and effective morpheme units while reducing the vocabulary size for model training. In this paper, we investigate and compare the following segmentation strategies:.Stem with combined suffix.Stem with singular suffix.Byte Pair Encoding (BPE).BPE on stem with combined suffix.BPE on stem with singular suffix.The latter two segmentation strategies are our newly proposed methods. Experimental results show that our morphologically motivated word segmentation method can achieve significant improvement of up to 1.2 and 2.5 BLEU points on Turkish-English and Uyghur-Chinese machine translation tasks over the strong baseline of pure BPE method respectively, indicating that it can provide better translation performance for the NMT model. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "32\n",
      "What is the difference between Ataman et al.'s approach and the approach presented in this work?\n",
      "\n",
      "Ataman et al.'s approach uses a category-based hidden markov model to optimize segmentation complexity with constraints on vocabulary, while the approach presented in this work is more simple and does not require any external resources for word segmentation.\n",
      "Question : for the text The NMT system is typically trained with a limited vocabulary, which creates bottleneck on translation accuracy and generalization capability. Many word segmentation methods have been proposed to cope with the above problems, which consider the morphological properties of different languages..Bradbury and Socher BIBREF16 employed the modified Morfessor to provide morphology knowledge into word segmentation, but they neglected the morphological varieties between subword units, which might result in ambiguous translation results. Sanchez-Cartagena and Toral BIBREF17 proposed a rule-based morphological word segmentation for Finnish, which applies BPE on all the morpheme units uniformly without distinguishing their inner morphological roles. Huck BIBREF18 explored target-side segmentation method for German, which shows that the cascading of suffix splitting and compound splitting with BPE can achieve better translation results. Ataman et al. BIBREF19 presented a linguistically motivated vocabulary reduction approach for Turkish, which optimizes the segmentation complexity with constraint on the vocabulary based on a category-based hidden markov model (HMM). Our work is closely related to their idea while ours are more simple and realizable. Tawfik et al. BIBREF20 confirmed that there is some advantage from using a high accuracy dialectal segmenter jointly with a language independent word segmentation method like BPE. The main difference is that their approach needs sufficient monolingual data additionally to train a segmentation model while ours do not need any external resources, which is very convenient for word segmentation on the low-resource and morphologically-rich agglutinative languages. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "33\n",
      "What languages were used in the experimental results for morpheme segmentation, BPE and the proposed morphological segmentation strategies? \n",
      "\n",
      "Turkish-English and Uyghur-Chinese were used in the experimental results for morpheme segmentation, BPE and the proposed morphological segmentation strategies. The results are shown in Table 6 and Table 7 respectively.\n",
      "Question : for the text In this paper, we investigate and compare morpheme segmentation, BPE and our proposed morphological segmentation strategies on the low resource and morphologically-rich agglutinative languages. Experimental results of Turkish-English and Uyghur-Chinese machine translation tasks are shown in Table 6 and Table 7 respectively. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "34\n",
      "Question 1: What is the main purpose of the proposed method for re-processing input features in deep transformer networks? \n",
      "Answer 1: The main purpose of the proposed method is to improve performance by using a self-attention mechanism on both features and hidden states representation in light of the information available at an intermediate network layer. Additionally, the objective function is calculated at intermediate layers to encourage meaningful partial results. The method was observed to generate consistent relative improvements of 10-20% for Librispeech and 3.2-13% for videos.\n",
      "Question : for the text In this paper, we have proposed a method for re-processing the input features in light of the information available at an intermediate network layer. We do this in the context of deep transformer networks, via a self-attention mechanism on both features and hidden states representation. To encourage meaningful partial results, we calculate the objective function at intermediate layers of the network as well as the output layer. This improves performance in and of itself, and when combined with feature re-presentation we observe consistent relative improvements of 10 - 20% for Librispeech and 3.2 - 13% for videos. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "35\n",
      "Question 1: What is the size of the Librispeech training data?\n",
      "\n",
      "Answer 1: The Librispeech training data comprises of three splits that contain 100 and 360 hours sets of clean speech and 500 hours of other speech, resulting in a total of 960 hours of training data.\n",
      "Question : for the text We evaluate our proposed module on both the Librispeech BIBREF12 dataset and a large-scale English video dataset. In the Librispeech training set, there are three splits, containing 100 and 360 hours sets of clean speech and 500 hours of other speech. We combined everything, resulting in 960 hours of training data. For the development set, there are also two splits: dev-clean and dev-other. For the test set, there is an analogous split..The video dataset is a collection of public and anonymized English videos. It consists of a 1000 hour training set, a 9 hour dev set, and a $46.1$ hour test set. The test set comprises an $8.5$ hour curated set of carefully selected very clean videos, a 19 hour clean set and a $18.6$ hour noisy set BIBREF13. For the hybrid ASR experiments on video dataset, alignments were generated with a production system trained with 14k hours..All speech features are extracted by using log Mel-filterbanks with 80 dimensions, a 25 ms window size and a 10 ms time step between two windows. Then we apply mean and variance normalization. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "36\n",
      "What speech features are used in the neural network implementation and how are they processed? \n",
      "\n",
      "Answer 1: The speech features are produced by processing the log Mel-spectrogram with two VGG layers that have configurations including 2-D convolutions with 32 or 64 output filters, kernel=3, stride=1, ReLU activation, and max-pooling kernel=2 for CTC or max-pooling kernel=1 for hybrid. The total number of frames are subsampled by 4x for CTC or 2x for hybrid after the VGG processing.\n",
      "Question : for the text All neural networks are implemented with the in-house extension of the fairseq BIBREF18 toolkit. Our speech features are produced by processing the log Mel-spectrogram with two VGG BIBREF19 layers that have the following configurations: (1) two 2-D convolutions with 32 output filters, kernel=3, stride=1, ReLU activation, and max-pooling kernel=2, (2) two 2-D convolutions with 64 output filters, kernel=3, stride=1 and max-pooling kernel=2 for CTC or max-pooling kernel=1 for hybrid. After the VGG layers, the total number of frames are subsampled by (i) 4x for CTC, or (ii) 2x for hybrid, thus enabling us to reduce the run-time and memory usage significantly. After VGG processing, we use 24 Transformer layers with $d_k=512$ head dimensions (8 heads, each head has 64 dimensions), 2048 feedforward hidden dimensions (total parameters $\\pm $ 80 millions), and dropout $0.15$. For the proposed models, we utilized an auxiliary MLP with two linear layers with 256 hidden units, LeakyReLU activation and softmax (see Sec. SECREF3). We set our position encoding dimensions $d_e=256$ and pre-concatenation projection $d_c=768$ for the feature re-presentation layer. The loss function is either CTC loss or hybrid CE loss. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "37\n",
      "Question 1: What is the baseline model used in the experiments presented in Table TABREF19?\n",
      "\n",
      "Answer 1: The baseline model used in the experiments presented in Table TABREF19 is a 24 layer Transformer network trained with CTC.\n",
      "Question : for the text Table TABREF19 presents CTC based results for the Librispeech dataset, without data augmentation. Our baseline is a 24 layer Transformer network trained with CTC. For the proposed method, we varied the number and placement of iterated loss and the feature re-presentation. The next three results show the effect of using CTC multiple times. We see 12 and 8% relative improvements for test-clean and test-other. Adding feature re-presentation gives a further boost, with net 20 and 18% relative improvements over the baseline..Table TABREF20 shows results for Librispeech with SpecAugment. We test both CTC and CE/hybrid systems. There are consistent gains first from iterated loss, and then from multiple feature presentation. We also run additional CTC experiments with 36 layers Transformer (total parameters $\\pm $120 millions). The baseline with 36 layers has the same performance with 24 layers, but by adding the proposed methods, the 36 layer performance improved to give the best results. This shows that our proposed methods can improve even very deep models..As shown in Table TABREF21, the proposed methods also provide large performance improvements on the curated video set, up to 13% with CTC, and up to 9% with the hybrid model. We also observe moderate gains of between 3.2 and 8% relative on the clean and noisy video sets. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "38\n",
      "Question 1: What is the process for decoding in the CTC training system? \n",
      "\n",
      "Answer 1: The decoding process involves modeling each sub-word using a HMM with two states, then using the best sub-word segmentation of each word to form a lexicon. These HMMs and lexicon are combined with the standard $n$-gram via FST to form a static decoding graph, and the Kaldi decoder is used to produce the best hypothesis.\n",
      "Question : for the text For CTC training, we use word-pieces as our target. During training, the reference is tokenized to 5000 sub-word units using sentencepiece with a uni-gram language model BIBREF14. Neural networks are thus used to produce a posterior distribution for 5001 symbols (5000 sub-word units plus blank symbol) every frame. For decoding, each sub-word is modeled by a HMM with two states where the last states share the same blank symbol probability; the best sub-word segmentation of each word is used to form a lexicon; these HMMs, lexicon are then combined with the standard $n$-gram via FST BIBREF15 to form a static decoding graph. Kaldi decoderBIBREF16 is used to produce the best hypothesis..We further present results with hybrid ASR systems. In this, we use the same HMM topology, GMM bootstrapping and decision tree building procedure as BIBREF13. Specifically, we use context-dependent (CD) graphemes as modeling units. On top of alignments from a GMM model, we build a decision tree to cluster CD graphemes. This results in 7248 context dependent units for Librispeech, and 6560 units for the video dataset. Training then proceeds with the CE loss function. We also apply SpecAugment BIBREF17 online during training, using the LD policy without time warping. For decoding, a standard Kaldi's WFST decoder BIBREF16 is used. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "39\n",
      "Question 1: What is the motivation behind proposing the processing of features in intermediate layers of a deep network?\n",
      "\n",
      "Answer 1: The motivation behind proposing the processing of features in intermediate layers of a deep network is to enable a neural network acoustic model to adaptively process the features depending on partial hypotheses and noise conditions. This can be achieved by re-presenting the feature stream at an intermediate layer of the network that is constructed to be correlated with the ultimate graphemic or phonetic output of the system.\n",
      "Question : for the text In this paper, we propose the processing of features not only in the input layer of a deep network, but in the intermediate layers as well. We are motivated by a desire to enable a neural network acoustic model to adaptively process the features depending on partial hypotheses and noise conditions. Many previous methods for adaptation have operated by linearly transforming either input features or intermediate layers in a two pass process where the transform is learned to maximize the likelihood of some adaptation data BIBREF0, BIBREF1, BIBREF2. Other methods have involved characterizing the input via factor analysis or i-vectors BIBREF3, BIBREF4. Here, we suggest an alternative approach in which adaptation can be achieved by re-presenting the feature stream at an intermediate layer of the network that is constructed to be correlated with the ultimate graphemic or phonetic output of the system..We present this work in the context of Transformer networks BIBREF5. Transformers have become a popular deep learning architecture for modeling sequential datasets, showing improvements in many tasks such as machine translation BIBREF5, language modeling BIBREF6 and autoregressive image generation BIBREF7. In the speech recognition field, Transformers have been proposed to replace recurrent neural network (RNN) architectures such as LSTMs and GRUs BIBREF8. A recent survey of Transformers in many speech related applications may be found in BIBREF9. Compared to RNNs, Transformers have several advantages, specifically an ability to aggregate information across all the time-steps by using a self-attention mechanism. Unlike RNNs, the hidden representations do not need to be computed sequentially across time, thus enabling significant efficiency improvements via parallelization..In the context of Transformer module, secondary feature analysis is enabled through an additional mid-network transformer module that has access both to previous-layer activations and the raw features. To implement this model, we apply the objective function several times at the intermediate layers, to encourage the development of phonetically relevant hypotheses. Interestingly, we find that the iterated use of an auxiliary loss in the intermediate layers significantly improves performance by itself, as well as enabling the secondary feature analysis..This paper makes two main contributions:.We present improvements in the basic training process of deep transformer networks, specifically the iterated use of CTC or CE in intermediate layers, and.We show that an intermediate-layer attention model with access to both previous-layer activations and raw feature inputs can significantly improve performance..We evaluate our proposed model on Librispeech and a large-scale video dataset. From our experimental results, we observe 10-20% relative improvement on Librispeech and 3.2-11% on the video dataset. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "40\n",
      "What mechanism is used to allow the network to (re)-consider input features in the light of intermediate processing?\n",
      "The mechanism used is a self-attention mechanism that combines the information present in the original features with the information available in the activations of an intermediate layer.\n",
      "Question : for the text In this section, we present our proposal for allowing the network to (re)-consider the input features in the light of intermediate processing. We do this by again deploying a self-attention mechanism to combine the information present in the original features with the information available in the activations of an intermediate layer. As described earlier, we calculate the output posteriors and auxiliary loss at the intermediate layer as well. The overall architecture is illustrated in Figure FIGREF6. Here, we have used a 24 layer network, with feature re-presentation after the 12th layer..In the following subsections, we provide detail on the feature re-presentation mechanism, and iterated loss calculation. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "41\n",
      "Question 1: What is the purpose of concatenating the projection of original features and previous hidden layer activations in the intermediate layer?\n",
      "\n",
      "Answer 1: The purpose is to merge the two sources of information and then extract relevant features using an extra Transformer layer, followed by linear projection and ReLU. This process is done by projecting both information sources to the same dimensionality, applying layer normalization, concatenating with position encoding, and using time-axis concatenation. The output preserves the sequence length, and maintaining the necessary sequence length requires selecting either the first half or the second half of the merged information. The use of high-level features as queries is preferable.\n",
      "Question : for the text We process the features in the intermediate later by concatenating a projection of the original features with a projection of previous hidden layer activations, and then applying self-attention..First, we project both the the input and intermediate layer features $(Z_0 \\in \\mathbb {R}^{S \\times d_0}, Z_{k} \\in \\mathbb {R}^{S \\times d_{k}} )$, apply layer normalization and concatenate with position encoding:.where $d_0$ is the input feature dimension, $d_k$ is the Transformer output dimension, $W_1 \\in \\mathbb {R}^{d_0 \\times d_c}, W_2 \\in \\mathbb {R}^{d_{k} \\times d_c}$ and $E \\in \\mathbb {R}^{S \\times d_{e}}$ is a sinusoidal position encoding BIBREF5..After we project both information sources to the same dimensionality, we merge the information by using time-axis concatenation:.Then, we extract relevant features with extra Transformer layer and followed by linear projection and ReLU:.where $W_3 \\in \\mathbb {R}^{d_{k+1}^{^{\\prime }} \\times d_{k+1}}$ is a linear projection. All biases in the formula above are omitted for simplicity..Note that in doing time-axis concatenation, our Key and Value sequences are twice as long as the original input. In the standard self-attention where the Query is the same as the Key and Value, the output preserves the sequence length. Therefore, in order to maintain the necessary sequence length $S$, we select either the first half (split A) or the second half (split B) to represent the combined information. The difference between these two is that the use of split A uses the projected input features as the Query set, while split B uses the projected higher level activations as the Query. In initial experiments, we found that the use of high-level features (split B) as queries is preferable. We illustrates this operation on Figure FIGREF11..Another way of combining information from the features with an intermediate layer is to concatenate the two along with the feature rather than the time axis. However, in initial experiments, we found that time axis concatenation produces better results, and focus on that in the experimental results. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "42\n",
      "What is the purpose of applying the loss function at several intermediate layers of the network?\n",
      "\n",
      "The purpose is to improve performance by applying the loss function at multiple layers instead of just the final output layer. This allows for additional supervision and can lead to better feature representations.\n",
      "Question : for the text We have found it beneficial to apply the loss function at several intermediate layers of the network. Suppose there are $M$ total layers, and define a subset of these layers at which to apply the loss function: $K = \\lbrace k_1, k_2, ..., k_L\\rbrace \\subseteq \\lbrace 1,..,M-1\\rbrace $. The total objective function is then defined as.where $Z_{k_l}$ is the $k_l$-th Transformer layer activations, $Y$ is the ground-truth transcription for CTC and context dependent states for hybrid ASR, and $Loss(P, Y)$ can be defined as CTC objective BIBREF11 or cross entropy for hybrid ASR. The coefficient $\\lambda $ scales the auxiliary loss and we set $\\lambda = 0.3$ based on our preliminary experiments. We illustrate the auxiliary prediction and loss in Figure FIGREF6. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "43\n",
      "What is the key feature of Transformer networks in speech processing?\n",
      "\n",
      "The key feature of Transformer networks in speech processing is self-attention, which has been shown to produce comparable or better performance than LSTMs for encoder-decoder based ASR and CTC-based training.\n",
      "Question : for the text In recent years, Transformer models have become an active research topic in speech processing. The key features of Transformer networks is self-attention, which produces comparable or better performance to LSTMs when used for encoder-decoder based ASR BIBREF23, as well as when trained with CTC BIBREF9. Speech-Transformers BIBREF24 also produce comparable performance to the LSTM-based attention model, but with higher training speed in a single GPU. Abdelrahman et al.BIBREF8 integrates a convolution layer to capture audio context and reduces WER in Librispeech..The use of an objective function in intermediate layers has been found useful in several previous works such as image classification BIBREF25 and language modeling BIBREF26. In BIBREF27, the authors did pre-training with an RNN-T based model by using a hierarchical CTC criterion with different target units. In this paper, we don't need additional types of target unit, instead we just use same tokenization and targets for both intermediate and final losses..The application of the objective function to intermediate layers is also similar in spirit to the use of KL-divergence in BIBREF28, which estimates output posteriors at an intermediate layer and regularizes them towards the distributions at the final layer. In contrast to this approach, the direct application of the objective function does not require the network to have a good output distribution before the new gradient contribution is meaningful. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "44\n",
      "Question 1: What is the self-attention mechanism used in transformer networks?\n",
      "\n",
      "Answer 1: The self-attention mechanism used in transformer networks is a Query, Key, Value triplet that extracts relevant information for each time-step from all time-steps in the preceding layer. In self-attention, the queries, keys, and values are the columns of the input itself, and are linearly projected multiple times with different, learned linear projections before concatenating and projecting to produce the final values.\n",
      "Question : for the text A transformer network BIBREF5 is a powerful approach to learning and modeling sequential data. A transformer network is itself constructed with a series of transformer modules that each perform some processing. Each module has a self-attention mechanism and several feed-forward layers, enabling easy parallelization over time-steps compared to recurrent models such as RNNs or LSTMs BIBREF10. We use the architecture defined in BIBREF5, and provide only a brief summary below..Assume we have an input sequence that is of length $S$: $X = [x_1,...,x_S]$. Each $x_i$ is itself a vector of activations. A transformer layer encodes $X$ into a corresponding output representation $Z = [z_1,...,z_S]$ as described below..Transformers are built around the notion of a self-attention mechanism that is used to extract the relevant information for each time-step $s$ from all time-steps $[1..S]$ in the preceding layer. Self attention is defined in terms of a Query, Key, Value triplet $\\lbrace {Q}, {K}, {V}\\rbrace \\in \\mathbb {R}^{S \\times d_k}$. In self-attention, the queries, keys and values are the columns of the input itself, $[x_1,...,x_S]$. The output activations are computed as:.Transformer modules deploy a multi-headed version of self-attention. As described in BIBREF5, this is done by linearly projecting the queries, keys and values $P$ times with different, learned linear projections. Self-attention is then applied to each of these projected versions of Queries, Keys and Values. These are concatenated and once again projected, resulting in the final values. We refer to the input projection matrices as $W_p^{Q}, W_p^{K}, W_p^{V}$, and to the output projection as $W_O$. Multihead attention is implemented as.Here, $ W_p^Q, W_p^K, W_p^V \\in \\mathbb {R}^{d_{k} \\times d_m}$, $d_m = d_{k} / P$, and $W_O \\in \\mathbb {R}^{Pd_m \\times d_k}$..After self-attention, a transformer module applies a series of linear layer, RELU, layer-norm and dropout operations, as well as the application of residual connections. The full sequence of processing is illustrated in Figure FIGREF3. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "45\n",
      "Question 1: Who provided support for the research project described in this text?\n",
      "\n",
      "Answer 1: The research project in this text was supported by Samsung Research under the project Improving Deep Learning using Latent Structure and NVIDIA Corporation donated a Titan V GPU.\n",
      "Question : for the text I would like to thank Samuel R. Bowman and Kyle Gorman for helpful discussions and suggestions. This work has benefited from the support of Samsung Research under the project Improving Deep Learning using Latent Structure and from the donation of a Titan V GPU by NVIDIA Corporation. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "46\n",
      "What was the main objective of the study on artificial neural network models for morphological inflection?\n",
      "\n",
      "The main objective of the study was to investigate how pretraining on different languages influences a model's learning of inflection in a target language, motivated by the fact that learning a second language in humans is influenced by their native language.\n",
      "Question : for the text Motivated by the fact that, in humans, learning of a second language is influenced by a learner's native language, we investigated a similar question in artificial neural network models for morphological inflection: How does pretraining on different languages influence a model's learning of inflection in a target language?.We performed experiments on eight different source languages and three different target languages. An extensive error analysis of all final models showed that (i) for closely related source and target languages, acquisition of target language inflection gets easier; (ii) knowledge of a prefixing language makes learning of inflection in a suffixing language more challenging, as well as the other way around; and (iii) languages which exhibit an agglutinative morphology facilitate learning of inflection in a second language..Future work might leverage those findings to improve neural network models for morphological inflection in low-resource languages, by choosing suitable source languages for pretraining..Another interesting next step would be to investigate how the errors made by our models compare to those by human L2 learners with different native languages. If the exhibited patterns resemble each other, computational models could be used to predict errors a person will make, which, in turn, could be leveraged for further research or the development of educational material. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "47\n",
      "Question 1: What optimization algorithm was used in training the models?\n",
      "\n",
      "Answer 1: The models were trained using ADAM optimization algorithm.\n",
      "Question : for the text We mostly use the default hyperparameters by sharma-katrapati-sharma:2018:K18-30. In particular, all RNNs have one hidden layer of size 100, and all input and output embeddings are 300-dimensional..For optimization, we use ADAM BIBREF21. Pretraining on the source language is done for exactly 50 epochs. To obtain our final models, we then fine-tune different copies of each pretrained model for 300 additional epochs for each target language. We employ dropout BIBREF22 with a coefficient of 0.3 for pretraining and, since that dataset is smaller, with a coefficient of 0.5 for fine-tuning..We make use of the datasets from the CoNLL–SIGMORPHON 2018 shared task BIBREF9. The organizers provided a low, medium, and high setting for each language, with 100, 1000, and 10000 examples, respectively. For all L1 languages, we train our models on the high-resource datasets with 10000 examples. For fine-tuning, we use the low-resource datasets. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "48\n",
      "Which language is an isolate and has inflectional morphology that makes use of both prefixes and suffixes?\n",
      "\n",
      "Basque is the language that is an isolate and has inflectional morphology that makes similarly frequent use of prefixes and suffixes, with suffixes mostly being attached to nouns, while prefixes and suffixes can both be employed for verbal inflection.\n",
      "Question : for the text For pretraining, we choose languages with different degrees of relatedness and varying morphological similarity to English, Spanish, and Zulu. We limit our experiments to languages which are written in Latin script..As an estimate for morphological similarity we look at the features from the Morphology category mentioned in The World Atlas of Language Structures (WALS). An overview of the available features as well as the respective values for our set of languages is shown in Table TABREF13..We decide on Basque (EUS), French (FRA), German (DEU), Hungarian (HUN), Italian (ITA), Navajo (NAV), Turkish (TUR), and Quechua (QVH) as source languages..Basque is a language isolate. Its inflectional morphology makes similarly frequent use of prefixes and suffixes, with suffixes mostly being attached to nouns, while prefixes and suffixes can both be employed for verbal inflection..French and Italian are Romance languages, and thus belong to the same family as the target language Spanish. Both are suffixing and fusional languages..German, like English, belongs to the Germanic language family. It is a fusional, predominantly suffixing language and, similarly to Spanish, makes use of stem changes..Hungarian, a Finno-Ugric language, and Turkish, a Turkic language, both exhibit an agglutinative morphology, and are predominantly suffixing. They further have vowel harmony systems..Navajo is an Athabaskan language and the only source language which is strongly prefixing. It further exhibits consonant harmony among its sibilants BIBREF19, BIBREF20..Finally, Quechua, a Quechuan language spoken in South America, is again predominantly suffixing and unrelated to all of our target languages. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "49\n",
      "Question 1: How does the morphological richness of Spanish compare to English?\n",
      "Answer 1: Spanish is morphologically rich, and has much larger verbal paradigms than English. English is morphologically impoverished, with only up to 5 different forms in its verbal paradigm and up to 2 in its nominal paradigm.\n",
      "Question : for the text We choose three target languages..English (ENG) is a morphologically impoverished language, as far as inflectional morphology is concerned. Its verbal paradigm only consists of up to 5 different forms and its nominal paradigm of only up to 2. However, it is one of the most frequently spoken and taught languages in the world, making its acquisition a crucial research topic..Spanish (SPA), in contrast, is morphologically rich, and disposes of much larger verbal paradigms than English. Like English, it is a suffixing language, and it additionally makes use of internal stem changes (e.g., o $\\rightarrow $ ue)..Since English and Spanish are both Indo-European languages, and, thus, relatively similar, we further add a third, unrelated target language. We choose Zulu (ZUL), a Bantoid language. In contrast to the first two, it is strongly prefixing. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "50\n",
      "What is the main goal of this study on neural network models and language acquisition? \n",
      "\n",
      "The main goal of this study is to investigate how the learning of a second language's morphological inflection by neural network models is influenced by the learner's native language, or L1. The study analyzes how pretraining the models on different source languages affects their ability to learn the inflectional morphology of three target languages.\n",
      "Question : for the text A widely agreed-on fact in language acquisition research is that learning of a second language (L2) is influenced by a learner's native language (L1) BIBREF0, BIBREF1. A language's morphosyntax seems to be no exception to this rule BIBREF2, but the exact nature of this influence remains unknown. For instance, it is unclear whether it is constraints imposed by the phonological or by the morphosyntactic attributes of the L1 that are more important during the process of learning an L2's morphosyntax..Within the area of natural language processing (NLP) research, experimenting on neural network models just as if they were human subjects has recently been gaining popularity BIBREF3, BIBREF4, BIBREF5. Often, so-called probing tasks are used, which require a specific subset of linguistic knowledge and can, thus, be leveraged for qualitative evaluation. The goal is to answer the question: What do neural networks learn that helps them to succeed in a given task?.Neural network models, and specifically sequence-to-sequence models, have pushed the state of the art for morphological inflection – the task of learning a mapping from lemmata to their inflected forms – in the last years BIBREF6. Thus, in this work, we experiment on such models, asking not what they learn, but, motivated by the respective research on human subjects, the related question of how what they learn depends on their prior knowledge. We manually investigate the errors made by artificial neural networks for morphological inflection in a target language after pretraining on different source languages. We aim at finding answers to two main questions: (i) Do errors systematically differ between source languages? (ii) Do these differences seem explainable, given the properties of the source and target languages? In other words, we are interested in exploring if and how L2 acquisition of morphological inflection depends on the L1, i.e., the \"native language\", in neural network models..To this goal, we select a diverse set of eight source languages from different language families – Basque, French, German, Hungarian, Italian, Navajo, Turkish, and Quechua – and three target languages – English, Spanish and Zulu. We pretrain a neural sequence-to-sequence architecture on each of the source languages and then fine-tune the resulting models on small datasets in each of the target languages. Analyzing the errors made by the systems, we find that (i) source and target language being closely related simplifies the successful learning of inflection in the target language, (ii) the task is harder to learn in a prefixing language if the source language is suffixing – as well as the other way around, and (iii) a source language which exhibits an agglutinative morphology simplifies learning of a second language's inflectional morphology. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "1\n",
      "Question 1: What kind of network architecture are the models based on?\n",
      "\n",
      "Answer 1: The models are based on a pointer-generator network architecture - a recurrent neural network-based sequence-to-sequence network with attention and a copy mechanism.\n",
      "Question : for the text The models we experiment with are based on a pointer–generator network architecture BIBREF10, BIBREF11, i.e., a recurrent neural network (RNN)-based sequence-to-sequence network with attention and a copy mechanism. A standard sequence-to-sequence model BIBREF12 has been shown to perform well for morphological inflection BIBREF13 and has, thus, been subject to cognitively motivated experiments BIBREF14 before. Here, however, we choose the pointer–generator variant of sharma-katrapati-sharma:2018:K18-30, since it performs better in low-resource settings, which we will assume for our target languages. We will explain the model shortly in the following and refer the reader to the original paper for more details. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "2\n",
      "Question 1: What is the final context vector $c_t$ and where is it used?\n",
      "\n",
      "Answer 1: The final context vector $c_t$ is the concatenation of two individual context vectors, each output by one encoder LSTM. It is used as input to the decoder at the corresponding time step $t$.\n",
      "Question : for the text Two separate attention mechanisms are used: one per encoder LSTM. Taking all respective encoder hidden states as well as the current decoder hidden state as input, each of them outputs a so-called context vector, which is a weighted sum of all encoder hidden states. The concatenation of the two individual context vectors results in the final context vector $c_t$, which is the input to the decoder at time step $t$. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "3\n",
      "Question 1: What is the benefit of the copy mechanism in the pointer-generator network architecture?\n",
      "\n",
      "Answer 1: The copy mechanism in the pointer-generator network architecture is beneficial for morphological generation in the low-resource setting, as shown empirically. It allows the model to give certain probability to copying elements from the input over to the output, thus expanding the vocabulary and improving the output quality.\n",
      "Question : for the text Our decoder consists of a uni-directional LSTM. Unlike a standard sequence-to-sequence model, a pointer–generator network is not limited to generating characters from the vocabulary to produce the output. Instead, the model gives certain probability to copying elements from the input over to the output. The probability of a character $y_t$ at time step $t$ is computed as a sum of the probability of $y_t$ given by the decoder and the probability of copying $y_t$, weighted by the probabilities of generating and copying:.$p_{\\textrm {dec}}(y_t)$ is calculated as an LSTM update and a projection of the decoder state to the vocabulary, followed by a softmax function. $p_{\\textrm {copy}}(y_t)$ corresponds to the attention weights for each input character. The model computes the probability $\\alpha $ with which it generates a new output character as.for context vector $c_t$, decoder state $s_t$, embedding of the last output $y_{t-1}$, weights $w_c$, $w_s$, $w_y$, and bias vector $b$. It has been shown empirically that the copy mechanism of the pointer–generator network architecture is beneficial for morphological generation in the low-resource setting BIBREF16. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "4\n",
      "Question 1: What type of networks are used for the encoders in the architecture?\n",
      "\n",
      "Answer 1: Bi-directional long short-term memory (LSTM) networks are used for both encoders in the architecture.\n",
      "Question : for the text Our architecture employs two separate encoders, which are both bi-directional long short-term memory (LSTM) networks BIBREF15: The first processes the morphological tags which describe the desired target form one by one. The second encodes the sequence of characters of the input word. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "5\n",
      "What is pretraining in NLP and how is it used in low-resource settings?\n",
      "\n",
      "Answer 1: Pretraining in NLP refers to the process of estimating some or all model parameters on examples that do not necessarily belong to the final target task. It is commonly used in low-resource settings to learn certain properties of language from raw text, related tasks, or related languages, and then fine-tuned on a specific task with limited data.\n",
      "Question : for the text Pretraining and successive fine-tuning of neural network models is a common approach for handling of low-resource settings in NLP. The idea is that certain properties of language can be learned either from raw text, related tasks, or related languages. Technically, pretraining consists of estimating some or all model parameters on examples which do not necessarily belong to the final target task. Fine-tuning refers to continuing training of such a model on a target task, whose data is often limited. While the sizes of the pretrained model parameters usually remain the same between the two phases, the learning rate or other details of the training regime, e.g., dropout, might differ. Pretraining can be seen as finding a suitable initialization of model parameters, before training on limited amounts of task- or language-specific examples..In the context of morphological generation, pretraining in combination with fine-tuning has been used by kann-schutze-2018-neural, which proposes to pretrain a model on general inflection data and fine-tune on examples from a specific paradigm whose remaining forms should be automatically generated. Famous examples for pretraining in the wider area of NLP include BERT BIBREF17 or GPT-2 BIBREF18: there, general properties of language are learned using large unlabeled corpora..Here, we are interested in pretraining as a simulation of familiarity with a native language. By investigating a fine-tuned model we ask the question: How does extensive knowledge of one language influence the acquisition of another? generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "6\n",
      "Question 1: What dataset was used for the qualitative analysis?\n",
      "Answer 1: The validation set was used for the qualitative analysis.\n",
      "Question : for the text For our qualitative analysis, we make use of the validation set. Therefore, we show validation set accuracies in Table TABREF19 for comparison. As we can see, the results are similar to the test set results for all language combinations. We manually annotate the outputs for the first 75 development examples for each source–target language combination. All found errors are categorized as belonging to one of the following categories. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "7\n",
      "Question 1: What is the difference between AFF and CUT errors in language?\n",
      "\n",
      "Answer 1: AFF errors refer to attaching the wrong prefix or suffix, while CUT errors involve cutting too much of the prefix or suffix from the original word before applying the inflection.\n",
      "Question : for the text AFF: This error refers to a wrong affix. This can be either a prefix or a suffix, depending on the correct target form..Example: ezoJulayi instead of esikaJulayi.CUT: This consists of cutting too much of the lemma's prefix or suffix before attaching the inflected form's prefix or suffix, respectively..Example: irradiseis instead of irradiaseis generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "8\n",
      "What is the first conclusion proposed based on the analysis of Table TABREF35 and Table TABREF19?\n",
      "\n",
      "The first conclusion proposed is that familiarity with languages that exhibit an agglutinative morphology simplifies learning a new language's morphology, particularly when considering HUN and TUR.\n",
      "Question : for the text Table TABREF35 displays the errors found in the 75 first ENG development examples, for each source language. From Table TABREF19, we know that HUN $>$ ITA $>$ TUR $>$ DEU $>$ FRA $>$ QVH $>$ NAV $>$ EUS, and we get a similar picture when analyzing the first examples. Thus, especially keeping HUN and TUR in mind, we cautiously propose a first conclusion: familiarity with languages which exhibit an agglutinative morphology simplifies learning of a new language's morphology..Looking at the types of errors, we find that EUS and NAV make the most stem errors. For QVH we find less, but still over 10 more than for the remaining languages. This makes it seem that models pretrained on prefixing or partly prefixing languages indeed have a harder time to learn ENG inflectional morphology, and, in particular, to copy the stem correctly. Thus, our second hypotheses is that familiarity with a prefixing language might lead to suspicion of needed changes to the part of the stem which should remain unaltered in a suffixing language. DEL(X) and ADD(X) errors are particularly frequent for EUS and NAV, which further suggests this conclusion..Next, the relatively large amount of stem errors for QVH leads to our second hypothesis: language relatedness does play a role when trying to produce a correct stem of an inflected form. This is also implied by the number of MULT errors for EUS, NAV and QVH, as compared to the other languages..Considering errors related to the affixes which have to be generated, we find that DEU, HUN and ITA make the fewest. This further suggests the conclusion that, especially since DEU is the language which is closest related to ENG, language relatedness plays a role for producing suffixes of inflected forms as well..Our last observation is that many errors are not found at all in our data sample, e.g., CHG2E(X) or NO_CHG(C). This can be explained by ENG having a relatively poor inflectional morphology, which does not leave much room for mistakes. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "9\n",
      "What evidence supports the hypothesis that familiarity with prefixing languages impedes acquisition of a suffixing language?\n",
      "\n",
      "Most stem errors happen for the source languages EUS and NAV, and especially MULT errors are much more frequent for those languages than for all others, which supports the hypothesis that familiarity with prefixing languages impedes acquisition of a suffixing one.\n",
      "Question : for the text The errors committed for SPA are shown in Table TABREF37, again listed by source language. Together with Table TABREF19 it gets clear that SPA inflectional morphology is more complex than that of ENG: systems for all source languages perform worse..Similarly to ENG, however, we find that most stem errors happen for the source languages EUS and NAV, which is further evidence for our previous hypothesis that familiarity with prefixing languages impedes acquisition of a suffixing one. Especially MULT errors are much more frequent for EUS and NAV than for all other languages. ADD(X) happens a lot for EUS, while ADD(C) is also frequent for NAV. Models pretrained on either language have difficulties with vowel changes, which reflects in NO_CHG(V). Thus, we conclude that this phenomenon is generally hard to learn..Analyzing next the errors concerning affixes, we find that models pretrained on HUN, ITA, DEU, and FRA (in that order) commit the fewest errors. This supports two of our previous hypotheses: First, given that ITA and FRA are both from the same language family as SPA, relatedness seems to be benficial for learning of the second language. Second, the system pretrained on HUN performing well suggests again that a source language with an agglutinative, as opposed to a fusional, morphology seems to be beneficial as well. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "10\n",
      "What is the relative performance of different source languages in Table TABREF19, and how does it relate to the hypothesis about agglutinative morphology?\n",
      "\n",
      "Answer 1: Table TABREF19 shows that TUR and HUN have high accuracy, which supports the hypothesis that a source language with an agglutinative morphology facilitates learning of inflection in another language. The relative performance of the source languages is: TUR $>$ HUN $>$ DEU $>$ ITA $>$ FRA $>$ NAV $>$ EUS $>$ QVH.\n",
      "Question : for the text In Table TABREF39, the errors for Zulu are shown, and Table TABREF19 reveals the relative performance for different source languages: TUR $>$ HUN $>$ DEU $>$ ITA $>$ FRA $>$ NAV $>$ EUS $>$ QVH. Again, TUR and HUN obtain high accuracy, which is an additional indicator for our hypothesis that a source language with an agglutinative morphology facilitates learning of inflection in another language..Besides that, results differ from those for ENG and SPA. First of all, more mistakes are made for all source languages. However, there are also several finer differences. For ZUL, the model pretrained on QVH makes the most stem errors, in particular 4 more than the EUS model, which comes second. Given that ZUL is a prefixing language and QVH is suffixing, this relative order seems important. QVH also committs the highest number of MULT errors..The next big difference between the results for ZUL and those for ENG and SPA is that DEL(X) and ADD(X) errors, which previously have mostly been found for the prefixing or partially prefixing languages EUS and NAV, are now most present in the outputs of suffixing languages. Namely, DEL(C) occurs most for FRA and ITA, DEL(V) for FRA and QVH, and ADD(C) and ADD(V) for HUN. While some deletion and insertion errors are subsumed in MULT, this does not fully explain this difference. For instance, QVH has both the second most DEL(V) and the most MULT errors..The overall number of errors related to the affix seems comparable between models with different source languages. This weakly supports the hypothesis that relatedness reduces affix-related errors, since none of the pretraining languages in our experiments is particularly close to ZUL. However, we do find more CUT errors for HUN and TUR: again, these are suffixing, while CUT for the target language SPA mostly happened for the prefixing languages EUS and NAV. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "11\n",
      "What is a limitation of the study?\n",
      "\n",
      "A limitation of our work is that we only include languages that are written in Latin script.\n",
      "Question : for the text A limitation of our work is that we only include languages that are written in Latin script. An interesting question for future work might, thus, regard the effect of disjoint L1 and L2 alphabets..Furthermore, none of the languages included in our study exhibits a templatic morphology. We make this choice because data for templatic languages is currently mostly available in non-Latin alphabets. Future work could investigate languages with templatic morphology as source or target languages, if needed by mapping the language's alphabet to Latin characters..Finally, while we intend to choose a diverse set of languages for this study, our overall number of languages is still rather small. This affects the generalizability of the results, and future work might want to look at larger samples of languages. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "12\n",
      "Question 1: What is an example of a REFL error in generated form?\n",
      "Answer 1: An example of a REFL error is doliéramos instead of nos doliéramos, where a reflective pronoun is missing.\n",
      "Question : for the text REFL: This happens when a reflective pronoun is missing in the generated form..Example: doliéramos instead of nos doliéramos.REFL_LOC: This error occurs if the reflective pronouns appears at an unexpected position within the generated form..Example: taparsebais instead of os tapabais.OVERREG: Overregularization errors occur when the model predicts a form which would be correct if the lemma's inflections were regular but they are not..Example: underteach instead of undertaught generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "13\n",
      "What does SUB(V) and SUB(C) refer to in the context of error analysis?\n",
      " \n",
      "SUB(V) and SUB(C) refer to the wrong substitution of one vowel or consonant for another in error analysis.\n",
      "Question : for the text SUB(X): This error consists of a wrong substitution of one character with another. SUB(V) and SUB(C) denote this happening with a vowel or a consonant, respectively. Letters that differ from each other by an accent count as different vowels..Example: decultared instead of decultured.DEL(X): This happens when the system ommits a letter from the output. DEL(V) and DEL(C) refer to a missing vowel or consonant, respectively..Example: firte instead of firtle.NO_CHG(X): This error occurs when inflecting the lemma to the gold form requires a change of either a vowel (NO_CHG(V)) or a consonant (NO_CHG(C)), but this is missing in the predicted form..Example: verto instead of vierto.MULT: This describes cases where two or more errors occur in the stem. Errors concerning the affix are counted for separately..Example: aconcoonaste instead of acondicionaste.ADD(X): This error occurs when a letter is mistakenly added to the inflected form. ADD(V) refers to an unnecessary vowel, ADD(C) refers to an unnecessary consonant..Example: compillan instead of compilan.CHG2E(X): This error occurs when inflecting the lemma to the gold form requires a change of either a vowel (CHG2E(V)) or a consonant (CHG2E(C)), and this is done, but the resulting vowel or consonant is incorrect..Example: propace instead of propague generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "14\n",
      "Which language yields the best final models for ENG and why is it surprising?\n",
      "\n",
      "The language that yields the best final models for ENG is HUN, which is surprising because DEU is the language in the experiments that is closest related to ENG.\n",
      "Question : for the text In Table TABREF18, we show the final test accuracy for all models and languages. Pretraining on EUS and NAV results in the weakest target language inflection models for ENG, which might be explained by those two languages being unrelated to ENG and making at least partial use of prefixing, while ENG is a suffixing language (cf. Table TABREF13). In contrast, HUN and ITA yield the best final models for ENG. This is surprising, since DEU is the language in our experiments which is closest related to ENG..For SPA, again HUN performs best, followed closely by ITA. While the good performance of HUN as a source language is still unexpected, ITA is closely related to SPA, which could explain the high accuracy of the final model. As for ENG, pretraining on EUS and NAV yields the worst final models – importantly, accuracy is over $15\\%$ lower than for QVH, which is also an unrelated language. This again suggests that the prefixing morphology of EUS and NAV might play a role..Lastly, for ZUL, all models perform rather poorly, with a minimum accuracy of 10.7 and 10.8 for the source languages QVH and EUS, respectively, and a maximum accuracy of 24.9 for a model pretrained on Turkish. The latter result hints at the fact that a regular and agglutinative morphology might be beneficial in a source language – something which could also account for the performance of models pretrained on HUN. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "15\n",
      "What is the focus of the research mentioned in this text?\n",
      "The research mentioned in this text focuses on human L1 and L2 acquisition of inflectional morphology, including the impact of the native language on L2 acquisition.\n",
      "Question : for the text Finally, a lot of research has focused on human L1 and L2 acquisition of inflectional morphology BIBREF35, BIBREF36, BIBREF37, BIBREF38, BIBREF39, BIBREF40..To name some specific examples, marques2011study investigated the effect of a stay abroad on Spanish L2 acquisition, including learning of its verbal morphology in English speakers. jia2003acquisition studied how Mandarin Chinese-speaking children learned the English plural morpheme. nicoladis2012young studied the English past tense acquisition in Chinese–English and French–English bilingual children. They found that, while both groups showed similar production accuracy, they differed slightly in the type of errors they made. Also considering the effect of the native language explicitly, yang2004impact investigated the acquisition of the tense-aspect system in an L2 for speakers of a native language which does not mark tense explicitly..Finally, our work has been weakly motivated by bliss2006l2. There, the author asked a question for human subjects which is similar to the one we ask for neural models: How does the native language influence L2 acquisition of inflectional morphology? generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "16\n",
      "Question 1: What are some examples of NLP tasks that have used cross-lingual transfer learning?\n",
      "\n",
      "Answer 1: Some examples include automatic speech recognition, entity recognition, language modeling, parsing, and machine translation.\n",
      "Question : for the text Cross-lingual transfer learning has been used for a large variety NLP of tasks, e.g., automatic speech recognition BIBREF25, entity recognition BIBREF26, language modeling BIBREF27, or parsing BIBREF28, BIBREF29, BIBREF30. Machine translation has been no exception BIBREF31, BIBREF32, BIBREF33. Recent research asked how to automatically select a suitable source language for a given target language BIBREF34. This is similar to our work in that our findings could potentially be leveraged to find good source languages. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "17\n",
      "What is the difference between the approach of Kyle and the approach of the current researchers in investigating morphological inflection systems?\n",
      "\n",
      "The current researchers focus on transfer learning and pretrain and fine-tune pointer-generator networks, while Kyle's approach focused on monolingual models and did not investigate transfer learning.\n",
      "Question : for the text Most research on inflectional morphology in NLP within the last years has been related to the SIGMORPHON and CoNLL–SIGMORPHON shared tasks on morphological inflection, which have been organized yearly since 2016 BIBREF6. Traditionally being focused on individual languages, the 2019 edition BIBREF23 contained a task which asked for transfer learning from a high-resource to a low-resource language. However, source–target pairs were predefined, and the question of how the source language influences learning besides the final accuracy score was not considered. Similarly to us, kyle performed a manual error analysis of morphological inflection systems for multiple languages. However, they did not investigate transfer learning, but focused on monolingual models..Outside the scope of the shared tasks, kann-etal-2017-one investigated cross-lingual transfer for morphological inflection, but was limited to a quantitative analysis. Furthermore, that work experimented with a standard sequence-to-sequence model BIBREF12 in a multi-task training fashion BIBREF24, while we pretrain and fine-tune pointer–generator networks. jin-kann-2017-exploring also investigated cross-lingual transfer in neural sequence-to-sequence models for morphological inflection. However, their experimental setup mimicked kann-etal-2017-one, and the main research questions were different: While jin-kann-2017-exploring asked how cross-lingual knowledge transfer works during multi-task training of neural sequence-to-sequence models on two languages, we investigate if neural inflection models demonstrate interesting differences in production errors depending on the pretraining language. Besides that, we differ in the artificial neural network architecture and language pairs we investigate. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "18\n",
      "Q: Why is the presence of rich inflectional morphology problematic in NLP systems? \n",
      "A: The presence of rich inflectional morphology is problematic for NLP systems as it increases word form sparsity. This means that there are more inflected forms for each lexical entry, making it harder for NLP systems to accurately process and recognize them. Additionally, learning and memorizing the rules and exceptions of inflectional morphology can also be a challenge for L2 language learners.\n",
      "Question : for the text Many of the world's languages exhibit rich inflectional morphology: the surface form of an individual lexical entry changes in order to express properties such as person, grammatical gender, or case. The citation form of a lexical entry is referred to as the lemma. The set of all possible surface forms or inflections of a lemma is called its paradigm. Each inflection within a paradigm can be associated with a tag, i.e., 3rdSgPres is the morphological tag associated with the inflection dances of the English lemma dance. We display the paradigms of dance and eat in Table TABREF1..The presence of rich inflectional morphology is problematic for NLP systems as it increases word form sparsity. For instance, while English verbs can have up to 5 inflected forms, Archi verbs have thousands BIBREF7, even by a conservative count. Thus, an important task in the area of morphology is morphological inflection BIBREF8, BIBREF9, which consists of mapping a lemma to an indicated inflected form. An (irregular) English example would be.with PAST being the target tag, denoting the past tense form. Additionally, a rich inflectional morphology is also challenging for L2 language learners, since both rules and their exceptions need to be memorized..In NLP, morphological inflection has recently frequently been cast as a sequence-to-sequence problem, where the sequence of target (sub-)tags together with the sequence of input characters constitute the input sequence, and the characters of the inflected word form the output. Neural models define the state of the art for the task and obtain high accuracy if an abundance of training data is available. Here, we focus on learning of inflection from limited data if information about another language's morphology is already known. We, thus, loosely simulate an L2 learning setting. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "19\n",
      "Question 1: What is the definition of the paradigm of a lemma in a language?\n",
      "Answer 1: The paradigm of a lemma in a language is defined as a set of inflected forms of the lemma, denoted as $f_k[w]$, that correspond to different tags $t_k$ and are strings consisting of letters from an alphabet $\\Sigma$. The task of morphological inflection involves predicting a missing form $f_i[w]$ from the paradigm, given the lemma $w$ and the tag $t_i$.\n",
      "Question : for the text Let ${\\cal M}$ be the paradigm slots which are being expressed in a language, and $w$ a lemma in that language. We then define the paradigm $\\pi $ of $w$ as:.$f_k[w]$ denotes an inflected form corresponding to tag $t_{k}$, and $w$ and $f_k[w]$ are strings consisting of letters from an alphabet $\\Sigma $..The task of morphological inflection consists of predicting a missing form $f_i[w]$ from a paradigm, given the lemma $w$ together with the tag $t_i$. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "20\n",
      "Question 1: What is the purpose of extending the bilingual Mboshi-French parallel corpus?\n",
      "\n",
      "Answer 1: The purpose of extending the bilingual Mboshi-French parallel corpus is to create a multilingual corpus for the endangered language Mboshi and make it available to the community.\n",
      "Question : for the text In this work we train bilingual UWS models using the endangered language Mboshi as target and different well-resourced languages as aligned information. Results show that similar languages rank better in terms of segmentation performance, and that by combining the information learned by different models, segmentation is further improved. This might be due to the different language-dependent structures that are captured by using more than one language. Lastly, we extend the bilingual Mboshi-French parallel corpus, creating a multilingual corpus for the endangered language Mboshi that we make available to the community. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "21\n",
      "What is the reason for English performing notably well in the experiments?\n",
      "\n",
      "The reason for English performing notably well in the experiments is believed to be due to the statistics features of the resulting text. The English portion of the dataset contains the smallest vocabulary among all languages, and since the systems are trained in very low-resource settings, vocabulary-related features can greatly impact the system's language-model capacity and final alignment quality.\n",
      "Question : for the text The experiment settings from this paper and evaluation protocol for the Mboshi corpus (Boundary F-scores using the ZRC speech reference) are the same from BIBREF8. Table presents the results for bilingual UWS and multilingual leveraging. For the former, we reach our best result by using as aligned information the French, the original aligned language for this dataset. Languages closely related to French (Spanish and Portuguese) ranked better, while our worst result used German. English also performs notably well in our experiments. We believe this is due to the statistics features of the resulting text. We observe in Table that the English portion of the dataset contains the smallest vocabulary among all languages. Since we train our systems in very low-resource settings, vocabulary-related features can impact greatly the system's capacity to language-model, and consequently the final quality of the produced alignments. Even in high-resource settings, it was already attested that some languages are more difficult to model than others BIBREF9..For the multilingual selection experiments, we experimented combining the languages from top to bottom as they appear Table (ranked by performance; e.g. 1-3 means the combination of FR(1), EN(2) and PT(3)). We observe that the performance improvement is smaller than the one observed in previous work BIBREF10, which we attribute to the fact that our dataset was artificially augmented. This could result in the available multilingual form of supervision not being as rich as in a manually generated dataset. Finally, the best boundary segmentation result is obtained by performing multilingual voting with all the languages and an agreement of 50%, which indicates that the information learned by different languages will provide additional complementary evidence..Lastly, following the methodology from BIBREF8, we extract the most confident alignments (in terms of ANE) discovered by the bilingual models. Table presents the top 10 most confident (discovered type, translation) pairs. Looking at the pairs the bilingual models are most confident about, we observe there are some types discovered by all the bilingual models (e.g. Mboshi word itua, and the concatenation oboá+ngá). However, the models still differ for most of their alignments in the table. This hints that while a portion of the lexicon might be captured independently of the language used, other structures might be more dependent of the chosen language. On this note, BIBREF11 suggests the notion of word cannot always be meaningfully defined cross-linguistically. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "22\n",
      "What is the Computational Language Documentation (CLD) research field?\n",
      "\n",
      "Answer 1: The Computational Language Documentation (CLD) research field aims to replace part of the manual steps performed by linguists during language documentation initiatives by automatic approaches.\n",
      "Question : for the text The Cambridge Handbook of Endangered Languages BIBREF3 estimates that at least half of the 7,000 languages currently spoken worldwide will no longer exist by the end of this century. For these endangered languages, data collection campaigns have to accommodate the challenge that many of them are from oral tradition, and producing transcriptions is costly. This transcription bottleneck problem can be handled by translating into a widely spoken language to ensure subsequent interpretability of the collected recordings, and such parallel corpora have been recently created by aligning the collected audio with translations in a well-resourced language BIBREF1, BIBREF2, BIBREF4. Moreover, some linguists suggested that more than one translation should be collected to capture deeper layers of meaning BIBREF5..This work is a contribution to the Computational Language Documentation (CLD) research field, that aims to replace part of the manual steps performed by linguists during language documentation initiatives by automatic approaches. Here we investigate the unsupervised word discovery and segmentation task, using the bilingual-rooted approach from BIBREF6. There, words in the well-resourced language are aligned to unsegmented phonemes in the endangered language in order to identify group of phonemes, and to cluster them into word-like units. We experiment with the Mboshi-French parallel corpus, translating the French text into four other well-resourced languages in order to investigate language impact in this CLD approach. Our results hint that this language impact exists, and that models based on different languages will output different word-like units. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "23\n",
      "Question 1: What approach is used to discover words in Mboshi?\n",
      "\n",
      "Answer 1: The bilingual neural-based Unsupervised Word Segmentation (UWS) approach is used to discover words in Mboshi. This approach uses Neural Machine Translation (NMT) models trained between language pairs, with the translation serving as the source language and the unsegmented phonemic sequence as the target. Soft-alignment probability matrices between the source and target sequences are used for clustering neighbor phonemes aligned to the same translation word, which creates segmentation in the target side. The product of this approach is a set of (discovered-units, translation words) pairs.\n",
      "Question : for the text We use the bilingual neural-based Unsupervised Word Segmentation (UWS) approach from BIBREF6 to discover words in Mboshi. In this approach, Neural Machine Translation (NMT) models are trained between language pairs, using as source language the translation (word-level) and as target, the language to document (unsegmented phonemic sequence). Due to the attention mechanism present in these networks BIBREF7, posterior to training, it is possible to retrieve soft-alignment probability matrices between source and target sequences. These matrices give us sentence-level source-to-target alignment information, and by using it for clustering neighbor phonemes aligned to the same translation word, we are able to create segmentation in the target side. The product of this approach is a set of (discovered-units, translation words) pairs. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "24\n",
      "Question 1: What is ANE Selection?\n",
      "\n",
      "Answer 1: ANE Selection is a method used to include multilingual information into bilingual models. For every language pair and aligned sentence in the dataset, a soft-alignment probability matrix is generated. We use Average Normalized Entropy (ANE) computed over these matrices for selecting the most confident one for segmenting each phoneme sequence.\n",
      "Question : for the text In this work we apply two simple methods for including multilingual information into the bilingual models from BIBREF6. The first one, Multilingual Voting, consists of merging the information learned by models trained with different language pairs by performing a voting over the final discovered boundaries. The voting is performed by applying an agreement threshold $T$ over the output boundaries. This threshold balances between accepting all boundaries from all the bilingual models (zero agreement) and accepting only input boundaries discovered by all these models (total agreement). The second method is ANE Selection. For every language pair and aligned sentence in the dataset, a soft-alignment probability matrix is generated. We use Average Normalized Entropy (ANE) BIBREF8 computed over these matrices for selecting the most confident one for segmenting each phoneme sequence. This exploits the idea that models trained on different language pairs will have language-related behavior, thus differing on the resulting alignment and segmentation over the same phoneme sequence. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "25\n",
      "Question 1: Which languages were added to the BIBREF2 dataset in the extended bilingual corpus?\n",
      "\n",
      "Answer 1: The BIBREF2 bilingual corpus was extended with translations in four other well-resourced languages, namely English, German, Portuguese, and Spanish.\n",
      "Question : for the text In this work we extend the bilingual Mboshi-French parallel corpus BIBREF2, fruit of the documentation process of Mboshi (Bantu C25), an endangered language spoken in Congo-Brazzaville. The corpus contains 5,130 utterances, for which it provides audio, transcriptions and translations in French. We translate the French into four other well-resourced languages through the use of the $DeepL$ translator. The languages added to the dataset are: English, German, Portuguese and Spanish. Table shows some statistics for the produced Multilingual Mboshi parallel corpus. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "26\n",
      "Question 1: How does DenseNMT differ from residual-based encoder/decoder models in terms of information flow?\n",
      "\n",
      "Answer 1: DenseNMT can utilize several low-dimensional vectors from its previous layers and a high-dimensional vector from the first layer as its information, while residual-based models can only absorb a single high-dimensional vector from its previous layer. DenseNMT also allows feature reuse and encourages upper layers to focus on creating new features, and the attention block allows for direct guidance of the decoder's generation. During back-propagation, gradient information can be passed directly to all encoder layers simultaneously.\n",
      "Question : for the text Figure FIGREF9 and Figure FIGREF13 show the difference of information flow compared with a residual-based encoder/decoder. For residual-based models, each layer can absorb a single high-dimensional vector from its previous layer as the only information, while for DenseNMT, each layer can utilize several low-dimensional vectors from its previous layers and a high-dimensional vector from the first layer (embedding layer) as its information. In DenseNMT, each layer directly provides information to its later layers. Therefore, the structure allows feature reuse, and encourages upper layers to focus on creating new features. Furthermore, the attention block allows the embedding vectors (as well as other hidden layers) to guide the decoder's generation more directly; therefore, during back-propagation, the gradient information can be passed directly to all encoder layers simultaneously. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "27\n",
      "Question 1: What is the purpose of DenseNMT in translation tasks?\n",
      "\n",
      "Answer 1: The purpose of DenseNMT is to use the information from embeddings more efficiently and pass abundant information from the encoder side to the decoder side in order to improve translation accuracy and speed up the information flow.\n",
      "Question : for the text In this work, we have proposed DenseNMT as a dense-connection framework for translation tasks, which uses the information from embeddings more efficiently, and passes abundant information from the encoder side to the decoder side. Our experiments have shown that DenseNMT is able to speed up the information flow and improve translation accuracy. For the future work, we will combine dense connections with other deep architectures, such as RNNs BIBREF7 and self-attention networks BIBREF4 . generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "28\n",
      "Question 1: What preprocessing method was used for the IWSLT14 German-English dataset?\n",
      "\n",
      "Answer 1: The IWSLT14 German-English dataset was preprocessed using the byte-pair-encoding (BPE) method, and 25k BPE codes were learned using the joint corpus of source and target languages. 7k random sentence pairs were selected as the development set, and the test set was a concatenation of dev2010, tst2010, tst2011, and tst2012.\n",
      "Question : for the text We use three datasets for our experiments: IWSLT14 German-English, Turkish-English, and WMT14 English-German..We preprocess the IWSLT14 German-English dataset following byte-pair-encoding (BPE) method BIBREF13 . We learn 25k BPE codes using the joint corpus of source and target languages. We randomly select 7k from IWSLT14 German-English as the development set , and the test set is a concatenation of dev2010, tst2010, tst2011 and tst2012, which is widely used in prior works BIBREF14 , BIBREF15 , BIBREF16 ..For the Turkish-English translation task, we use the data provided by IWSLT14 BIBREF17 and the SETimes corpus BIBREF17 following BIBREF18 . After removing sentence pairs with length ratio over 9, we obtain 360k sentence pairs. Since there is little commonality between the two languages, we learn 30k size BPE codes separately for Turkish and English. In addition to this, we give another preprocessing for Turkish sentences and use word-level English corpus. For Turkish sentences, following BIBREF19 , BIBREF18 , we use the morphology tool Zemberek with disambiguation by the morphological analysis BIBREF20 and removal of non-surface tokens. Following BIBREF18 , we concatenate tst2011, tst2012, tst2013, tst2014 as our test set. We concatenate dev2010 and tst2010 as the development set..We preprocess the WMT14 English-German dataset using a BPE code size of 40k. We use the concatenation of newstest2013 and newstest2012 as the development set. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "29\n",
      "What is the proposed attention block in this paper?\n",
      "\n",
      "The proposed attention block in this paper is called DenseAtt, which serves for the dense connection between the encoder and decoder side. There are two options proposed, DenseAtt-1 and DenseAtt-2, with the latter being more expressive and flexible. The attention block is designed to pass more abundant information from the encoder side to the decoder side.\n",
      "Question : for the text Prior works show a trend of designing more expressive attention mechanisms (as discussed in Section 2). However, most of them only use the last encoder layer. In order to pass more abundant information from the encoder side to the decoder side, the attention block needs to be more expressive. Following the recent development of designing attention architectures, we propose DenseAtt as the dense attention block, which serves for the dense connection between the encoder and the decoder side. More specifically, two options are proposed accordingly. For each decoding step in the corresponding decoder layer, the two options both calculate attention using multiple encoder layers. The first option is more compressed, while the second option is more expressive and flexible. We name them as DenseAtt-1 and DenseAtt-2 respectively. Figure FIGREF15 shows the architecture of (a) multi-step attention BIBREF2 , (b) DenseAtt-1, and (c) DenseAtt-2 in order. In general, a popular multiplicative attention module can be written as: DISPLAYFORM0 .where INLINEFORM0 represent query, key, value respectively. We will use this function INLINEFORM1 in the following descriptions..In the decoding phase, we use a layer-wise attention mechanism, such that each decoder layer absorbs different attention information to adjust its output. Instead of treating the last hidden layer as the encoder's output, we treat the concatenation of all hidden layers from encoder side as the output. The decoder layer multiplies with the encoder output to obtain the attention weights, which is then multiplied by a linear combination of the encoder output and the sentence embedding. The attention output of each layer INLINEFORM0 can be formally written as: DISPLAYFORM0 .where INLINEFORM0 is the multiplicative attention function, INLINEFORM1 is a concatenation operation that combines all features, and INLINEFORM2 is a linear transformation function that maps each variable to a fixed dimension in order to calculate the attention value. Notice that we explicitly write the INLINEFORM3 term in ( EQREF19 ) to keep consistent with the multi-step attention mechanism, as pictorially shown in Figure FIGREF15 (a)..Notice that the transformation INLINEFORM0 in DenseAtt-1 forces the encoder layers to be mixed before doing attention. Since we use multiple hidden layers from the encoder side to get an attention value, we can alternatively calculate multiple attention values before concatenating them. In another word, the decoder layer can get different attention values from different encoder layers. This can be formally expressed as: DISPLAYFORM0 .where the only difference from Eq. ( EQREF19 ) is that the concatenation operation is substituted by a summation operation, and is put after the attention function INLINEFORM0 . This method further increases the representation power in the attention block, while maintaining the same number of parameters in the model. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "30\n",
      "Question 1: What is the advantage of using concatenation in the dense encoder architecture?\n",
      "\n",
      "Answer 1: The advantage of using concatenation in the dense encoder architecture is that it encourages feature reuse and can be more compact and expressive, even with a smaller number of features per layer. Additionally, later layers in the dense encoder are able to use features from all previous layers through concatenation. This is different from residual connections, which only allow for information flow from previous layers.\n",
      "Question : for the text Different from residual connections, later layers in the dense encoder are able to use features from all previous layers by concatenating them: DISPLAYFORM0 .Here, INLINEFORM0 is defined in Eq. ( EQREF10 ), INLINEFORM1 represents concatenation operation. Although this brings extra connections to the network, with smaller number of features per layer, the architecture encourages feature reuse, and can be more compact and expressive. As shown in Figure FIGREF9 , when designing the model, the hidden size in each layer is much smaller than the hidden size of the corresponding layer in the residual-connected model..While each encoder layer perceives information from its previous layers, each decoder layer INLINEFORM0 has two information sources: previous layers INLINEFORM1 , and attention values INLINEFORM2 . Therefore, in order to allow dense information flow, we redefine the generation of INLINEFORM3 -th layer as a nonlinear function over all its previous decoder layers and previous attentions. This can be written as: DISPLAYFORM0 .where INLINEFORM0 is the attention value using INLINEFORM1 -th decoder layer and information from encoder side, which will be specified later. Figure FIGREF13 shows the comparison of a dense decoder with a regular residual decoder. The dimensions of both attention values and hidden layers are chosen with smaller values, yet the perceived information for each layer consists of a higher dimension vector with more representation power. The output of the decoder is a linear transformation of the concatenation of all layers by default. To compromise to the increment of dimensions, we use summary layers, which will be introduced in Section 3.3. With summary layers, the output of the decoder is only a linear transformation of the concatenation of the upper few layers. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "31\n",
      "What is DenseNMT architecture?\n",
      "\n",
      "DenseNMT architecture allows each layer to provide its information to all subsequent layers directly, compared to residual connected NMT models. It uses a multi-layer structure for both encoder and decoder and convolution based transformation for efficiency and high performance. It is agnostic to the transformation function and can work well with other transformations like LSTM, self-attention, and depthwise separable convolution.\n",
      "Question : for the text In this section, we introduce our DenseNMT architecture. In general, compared with residual connected NMT models, DenseNMT allows each layer to provide its information to all subsequent layers directly. Figure FIGREF9 - FIGREF15 show the design of our model structure by parts..We start with the formulation of a regular NMT model. Given a set of sentence pairs INLINEFORM0 , an NMT model learns parameter INLINEFORM1 by maximizing the log-likelihood function: DISPLAYFORM0 .For every sentence pair INLINEFORM0 , INLINEFORM1 is calculated based on the decomposition: DISPLAYFORM0 .where INLINEFORM0 is the length of sentence INLINEFORM1 . Typically, NMT models use the encoder-attention-decoder framework BIBREF1 , and potentially use multi-layer structure for both encoder and decoder. Given a source sentence INLINEFORM2 with length INLINEFORM3 , the encoder calculates hidden representations by layer. We denote the representation in the INLINEFORM4 -th layer as INLINEFORM5 , with dimension INLINEFORM6 , where INLINEFORM7 is the dimension of features in layer INLINEFORM8 . The hidden representation at each position INLINEFORM9 is either calculated by: DISPLAYFORM0 .for recurrent transformation INLINEFORM0 such as LSTM and GRU, or by: DISPLAYFORM0 .for parallel transformation INLINEFORM0 . On the other hand, the decoder layers INLINEFORM1 follow similar structure, while getting extra representations from the encoder side. These extra representations are also called attention, and are especially useful for capturing alignment information..In our experiments, we use convolution based transformation for INLINEFORM0 due to both its efficiency and high performance, more formally, DISPLAYFORM0 . INLINEFORM0 is the gated linear unit proposed in BIBREF11 and the kernel size is INLINEFORM1 . DenseNMT is agnostic to the transformation function, and we expect it to also work well combining with other transformations, such as LSTM, self-attention and depthwise separable convolution. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "32\n",
      "What is the BLEU score achieved by DenseNMT on the Turkish-English task?\n",
      "\n",
      "DenseNMT achieved a new benchmark result with an average BLEU score of 24.36 on the Turkish-English task.\n",
      "Question : for the text For the IWSLT14 German-English dataset, we compare with the best results reported from literatures. To be consistent with prior works, we also provide results using our model directly on the dataset without BPE preprocessing. As shown in Table TABREF39 , DenseNMT outperforms the phrase-structure based network NPMT BIBREF16 (with beam size 10) by 1.2 BLEU, using a smaller beam size, and outperforms the actor-critic method based algorithm BIBREF15 by 2.8 BLEU. For reference, our model trained on the BPE preprocessed dataset achieves 32.26 BLEU, which is 1.93 BLEU higher than our word-based model. For Turkish-English task, we compare with BIBREF19 which uses the same morphology preprocessing as our Tr-En-morph. As shown in Table TABREF37 , our baseline is higher than the previous result, and we further achieve new benchmark result with 24.36 BLEU average score. For WMT14 English-German, from Table TABREF41 , we can see that DenseNMT outperforms ConvS2S model by 0.36 BLEU score using 35% fewer training iterations and 20% fewer parameters. We also compare with another convolution based NMT model: SliceNet BIBREF3 , which explores depthwise separable convolution architectures. SliceNet-Full matches our result, and SliceNet-Super outperforms by 0.58 BLEU score. However, both models have 2.2x more parameters than our model. We expect DenseNMT structure could help improve their performance as well. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "33\n",
      "Question 1: How does the performance of DenseNMT models compare to the baselines in different datasets and genres?\n",
      "\n",
      "Answer 1: In almost all genres, DenseNMT models outperform the baselines. With an embedding size of 256, where all models achieve their best scores, DenseNMT outperforms baselines by 0.7-1.0 BLEU on De-En, 0.5-1.3 BLEU on Tr-En, and 0.8-1.5 BLEU on Tr-En-morph. Significant gains are also observed using other embedding sizes.\n",
      "Question : for the text Table TABREF32 shows the results for De-En, Tr-En, Tr-En-morph datasets, where the best accuracy for models with the same depth and of similar sizes are marked in boldface. In almost all genres, DenseNMT models are significantly better than the baselines. With embedding size 256, where all models achieve their best scores, DenseNMT outperforms baselines by 0.7-1.0 BLEU on De-En, 0.5-1.3 BLEU on Tr-En, 0.8-1.5 BLEU on Tr-En-morph. We observe significant gain using other embedding sizes as well..Furthermore, in Table TABREF36 , we investigate DenseNMT models through ablation study. In order to make the comparison fair, six models listed have roughly the same number of parameters. On De-En, Tr-En and Tr-En-morph, we see improvement by making the encoder dense, making the decoder dense, and making the attention dense. Fully dense-connected model DenseNMT-4L-1 further improves the translation accuracy. By allowing more flexibility in dense attention, DenseNMT-4L-2 provides the highest BLEU scores for all three experiments..From the experiments, we have seen that enlarging the information flow in the attention block benefits the models. The dense attention block provides multi-layer information transmission from the encoder to the decoder, and to the output as well. Meanwhile, as shown by the ablation study, the dense-connected encoder and decoder both give more powerful representations than the residual-connected counterparts. As a result, the integration of the three parts improve the accuracy significantly. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "34\n",
      "What is one advantage of using the DenseNMT model with smaller embedding sizes?\n",
      "\n",
      "One advantage of using the DenseNMT model with smaller embedding sizes is that the embedding information can be well concentrated on fewer dimensions, such as 64, which is helpful when building models on mobile and small devices where the model size is critical. Additionally, smaller embedding sizes can achieve comparable or even better results compared to baseline models with larger embedding sizes.\n",
      "Question : for the text From Table TABREF32 , we also observe that DenseNMT performs better with small embedding sizes compared to residual-connected models with regular embedding size. For example, on Tr-En model, the 8-layer DenseNMT-8L-2 model with embedding size 64 matches the BLEU score of the 8-layer BASE model with embedding size 256, while the number of parameter of the former one is only INLINEFORM0 of the later one. In all genres, DenseNMT model with embedding size 128 is comparable or even better than the baseline model with embedding size 256..While overlarge embedding sizes hurt accuracy because of overfitting issues, smaller sizes are not preferable because of insufficient representation power. However, our dense models show that with better model design, the embedding information can be well concentrated on fewer dimensions, e.g., 64. This is extremely helpful when building models on mobile and small devices where the model size is critical. While there are other works that stress the efficiency issue by using techniques such as separable convolution BIBREF3 , and shared embedding BIBREF4 , our DenseNMT framework is orthogonal to those approaches. We believe that other techniques would produce more efficient models through combining with our DenseNMT framework. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "35\n",
      "What is the proposed framework in this paper and how does it differ from previous methods?\n",
      "\n",
      "The proposed framework in this paper is a densely connected neural machine translation framework (DenseNMT) that efficiently propagates information from the encoder to the decoder through the attention component, inspired by the idea of using dense connections for training computer vision tasks. It differs from previous methods, such as residual networks (ResNet), by utilizing dense connections in all three blocks to improve performance without increasing the number of parameters.\n",
      "Question : for the text Neural machine translation (NMT) is a challenging task that attracts lots of attention in recent years. Starting from the encoder-decoder framework BIBREF0 , NMT starts to show promising results in many language pairs. The evolving structures of NMT models in recent years have made them achieve higher scores and become more favorable. The attention mechanism BIBREF1 added on top of encoder-decoder framework is shown to be very useful to automatically find alignment structure, and single-layer RNN-based structure has evolved into deeper models with more efficient transformation functions BIBREF2 , BIBREF3 , BIBREF4 ..One major challenge of NMT is that its models are hard to train in general due to the complexity of both the deep models and languages. From the optimization perspective, deeper models are hard to efficiently back-propagate the gradients, and this phenomenon as well as its solution is better explored in the computer vision society. Residual networks (ResNet) BIBREF5 achieve great performance in a wide range of tasks, including image classification and image segmentation. Residual connections allow features from previous layers to be accumulated to the next layer easily, and make the optimization of the model efficiently focus on refining upper layer features..NMT is considered as a challenging problem due to its sequence-to-sequence generation framework, and the goal of comprehension and reorganizing from one language to the other. Apart from the encoder block that works as a feature generator, the decoder network combining with the attention mechanism bring new challenges to the optimization of the models. While nowadays best-performing NMT systems use residual connections, we question whether this is the most efficient way to propagate information through deep models. In this paper, inspired by the idea of using dense connections for training computer vision tasks BIBREF6 , we propose a densely connected NMT framework (DenseNMT) that efficiently propagates information from the encoder to the decoder through the attention component. Taking the CNN-based deep architecture as an example, we verify the efficiency of DenseNMT. Our contributions in this work include: (i) by comparing the loss curve, we show that DenseNMT allows the model to pass information more efficiently, and speeds up training; (ii) we show through ablation study that dense connections in all three blocks altogether help improve the performance, while not increasing the number of parameters; (iii) DenseNMT allows the models to achieve similar performance with much smaller embedding size; (iv) DenseNMT on IWSLT14 German-English and Turkish-English translation tasks achieves new benchmark BLEU scores, and the result on WMT14 English-German task is more competitive than the residual connections based baseline model. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "36\n",
      "What is the difference between the BASE-4L and the DenseNMT-4L-1 and DenseNMT-4L-2 models?\n",
      "\n",
      "The DenseNMT-4L-1 and DenseNMT-4L-2 models use dense connections and have the same number of layers as the BASE-4L model, but their hidden sizes are set as 128 in order to keep the model size consistent. Meanwhile, the BASE-4L model has a 256 hidden size by default.\n",
      "Question : for the text As the baseline model (BASE-4L) for IWSLT14 German-English and Turkish-English, we use a 4-layer encoder, 4-layer decoder, residual-connected model, with embedding and hidden size set as 256 by default. As a comparison, we design a densely connected model with same number of layers, but the hidden size is set as 128 in order to keep the model size consistent. The models adopting DenseAtt-1, DenseAtt-2 are named as DenseNMT-4L-1 and DenseNMT-4L-2 respectively. In order to check the effect of dense connections on deeper models, we also construct a series of 8-layer models. We set the hidden number to be 192, such that both 4-layer models and 8-layer models have similar number of parameters. For dense structured models, we set the dimension of hidden states to be 96..Since NMT model usually allocates a large proportion of its parameters to the source/target sentence embedding and softmax matrix, we explore in our experiments to what extent decreasing the dimensions of the three parts would harm the BLEU score. We change the dimensions of the source embedding, the target embedding as well as the softmax matrix simultaneously to smaller values, and then project each word back to the original embedding dimension through a linear transformation. This significantly reduces the number of total parameters, while not influencing the upper layer structure of the model..We also introduce three additional models we use for ablation study, all using 4-layer structure. Based on the residual connected BASE-4L model, (1) DenseENC-4L only makes encoder side dense, (2) DenseDEC-4L only makes decoder side dense, and (3) DenseAtt-4L only makes the attention dense using DenseAtt-2. There is no summary layer in the models, and both DenseENC-4L and DenseDEC-4L use hidden size 128. Again, by reducing the hidden size, we ensure that different 4-layer models have similar model sizes..Our design for the WMT14 English-German model follows the best performance model provided in BIBREF2 . The construction of our model is straightforward: our 15-layer model DenseNMT-En-De-15 uses dense connection with DenseAtt-2, INLINEFORM0 . The hidden number in each layer is INLINEFORM1 that of the original model, while the kernel size maintains the same. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "37\n",
      "Question 1: What is the purpose of the summary layer in deeper models?\n",
      "\n",
      "Answer 1: The summary layer is introduced to avoid the calculation bottleneck for later layers due to large feature dimensions. It summarizes the features for all previous layers and projects back to the embedding size, so that later layers of both the encoder and the decoder side do not need to look back further. The summary layers can be considered as contextualized word vectors in a given sentence.\n",
      "Question : for the text Since the number of features fed into nonlinear operation is accumulated along the path, the parameter size increases accordingly. For example, for the INLINEFORM0 -th encoder layer, the input dimension of features is INLINEFORM1 , where INLINEFORM2 is the feature dimension in previous layers, INLINEFORM3 is the embedding size. In order to avoid the calculation bottleneck for later layers due to large INLINEFORM4 , we introduce the summary layer for deeper models. It summarizes the features for all previous layers and projects back to the embedding size, so that later layers of both the encoder and the decoder side do not need to look back further. The summary layers can be considered as contextualized word vectors in a given sentence BIBREF12 . We add one summary layer after every INLINEFORM5 layers, where INLINEFORM6 is the hyperparameter we introduce. Accordingly, the input dimension of features is at most INLINEFORM7 for the last layer of the encoder. Moreover, combined with the summary layer setting, our DenseAtt mechanism allows each decoder layer to calculate the attention value focusing on the last few encoder layers, which consists of the last contextual embedding layer and several dense connected layers with low dimension. In practice, we set INLINEFORM8 as 5 or 6. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "38\n",
      "Question 1: How do DenseNMT models compare to baseline models in terms of training loss curves?\n",
      "\n",
      "Answer 1: DenseNMT models consistently perform better than baseline models, as their loss curves are always below those of the baseline models. This is particularly evident in the WMT14 English-German dataset. Adding dense connections slightly influences training speed, but it leads to fewer epochs and a better BLEU score. However, it results in an 8.1% reduction in per-iteration training speed.\n",
      "Question : for the text We first show that DenseNMT helps information flow more efficiently by presenting the training loss curve. All hyperparameters are fixed in each plot, only the models are different. In Figure FIGREF30 , the loss curves for both training and dev sets (before entering the finetuning period) are provided for De-En, Tr-En and Tr-En-morph. For clarity, we compare DenseNMT-4L-2 with BASE-4L. We observe that DenseNMT models are consistently better than residual-connected models, since their loss curves are always below those of the baseline models. The effect is more obvious on the WMT14 English-German dataset. We rerun the best model provided by BIBREF2 and compare with our model. In Figure FIGREF33 , where train/test loss curve are provided, DenseNMT-En-De-15 reaches the same level of loss and starts finetuning (validation loss starts to increase) at epoch 13, which is 35% faster than the baseline..Adding dense connections changes the architecture, and would slightly influence training speed. For the WMT14 En-De experiments, the computing time for both DenseNMT and the baseline (with similar number of parameters and same batch size) tested on single M40 GPU card are 1571 and 1710 word/s, respectively. While adding dense connections influences the per-iteration training slightly (8.1% reduction of speed), it uses many fewer epochs, and achieves a better BLEU score. In terms of training time, DenseNMT uses 29.3%(before finetuning)/22.9%(total) less time than the baseline. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "39\n",
      "Question 1: What optimizer is used in the experiments?\n",
      "\n",
      "Answer 1: The experiments use Nesterov Accelerated Gradient (NAG) as the optimizer.\n",
      "Question : for the text We use Nesterov Accelerated Gradient (NAG) BIBREF21 as our optimizer, and the initial learning rate is set to INLINEFORM0 . For German-English and Turkish-English experiments, the learning rate will shrink by 10 every time the validation loss increases. For the English-German dataset, in consistent with BIBREF2 , the learning rate will shrink by 10 every epoch since the first increment of validation loss. The system stops training until the learning rate is less than INLINEFORM1 . All models are trained end-to-end without any warmstart techniques. We set our batch size for the WMT14 English-German dataset to be 48, and additionally tune the length penalty parameter, in consistent with BIBREF2 . For other datasets, we set batch size to be 32. During inference, we use a beam size of 5. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "40\n",
      "Question 1: Who provided aid in accessing data in the research and provided thoughtful insight?\n",
      "\n",
      "Answer 1: Members of the Cornell AI, Policy, and Practice Group, Cristian Danescu-Niculescu-Mizil, Ian Lomeli, Justine Zhang, and Kate Donahue provided aid in accessing data and their thoughtful insight in the research.\n",
      "Question : for the text The authors thank members of the Cornell AI, Policy, and Practice Group, and (alphabetically by first name) Cristian Danescu-Niculescu-Mizil, Ian Lomeli, Justine Zhang, and Kate Donahue for aid in accessing data and their thoughtful insight. This research was supported by NSF Award DMS-1830274, ARO Award W911NF19-1-0057, a Simons Investigator Award, a Vannevar Bush Faculty Fellowship, and ARO MURI. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "41\n",
      "What is the Names Only method used in the data collection process? \n",
      "\n",
      "The Names Only method uses a curated list of full names relevant to the subreddit, focusing on sports and politics. For sports, it collected names of all NBA and NFL player active during 1980–2019 from basketball-reference.com and pro-football-reference.com. For politics, it collected the names of congresspeople from the @unitedstates project. This method searches for any combination of any part of these names such that at least two partial names are separated by `and', `or', `v.s.', `vs', or `/' and the rest are separated by `, '. Finally, it captures full metadata for the lists.\n",
      "Question : for the text We take our data mostly from Reddit, a large social media website divided into subcommunities called `subreddits' or `subs'. Each subreddit has a theme (usually clearly expressed in its name), and we have focused our study on subreddits primarily in sports and politics, in part because of the richness of proper names in these domains: r/nba, r/nfl, r/politics, r/Conservative, r/Libertarian, r/The_Donald, r/food, along with a variety of NBA team subreddits (e.g., r/rockets for the Houston Rockets). Apart from the team-specific and food subreddits, these are among the largest and most heavily used subreddits BIBREF23. We gather text data from comments made by users in discussion threads. In all cases, we have data from when the subreddit started until mid-2018. (Data was contributed by Cristian Danescu-Niculescu-Mizil.) Reddit in general, and the subreddits we examined in particular, are rapidly growing, both in terms of number of users and number of comments..Some of the subreddits we looked at (particularly sports subreddits) exhibited very distinctive `seasons', where commenting spikes (Fig. FIGREF2). These align with, e.g., the season of the given sport. When studying data across time, our convention is to bin the data by year, but we adjust the starting point of a year based on these seasons. Specifically, a year starts in May for r/nfl, August for r/nba, and February for all politics subreddits..We use two methods to identify lists from user comments: `All Words' and `Names Only', with the latter focusing on proper names. In both cases, we collect a number of lists and discard lists for any pair of words that appear fewer than 30 times within the time frame that we examined (see Table TABREF3 for summary statistics)..The All Words method simply searches for two words $A$ and $B$ separated by `and' or `or', where a word is merely a series of characters separated by a space or punctuation. This process only captures lists of length two, or binomials. We then filter out lists containing words from a collection of stop-words that, by their grammatical role or formatting structure, are almost exclusively involved in false positive lists. No metadata is captured for these lists beyond the month and year of posting..The Names Only method uses a curated list of full names relevant to the subreddit, focusing on sports and politics. For sports, we collected names of all NBA and NFL player active during 1980–2019 from basketball-reference.com and pro-football-reference.com. For politics, we collected the names of congresspeople from the @unitedstates project BIBREF24. To form lists, we search for any combination of any part of these names such that at least two partial names are separated by `and', `or', `v.s.', `vs', or `/' and the rest are separated by `,'. While we included a variety of separators, about 83% of lists include only `and', about 17% include `or' and the rest of the separators are negligible. Most lists that we retrieve in this way are of length 2, but we also found lists up to length 40 (Fig. FIGREF5). Finally, we also captured full metadata for these lists, including a timestamp, the user, any flairs attributed to the user (short custom text that appears next to the username), and other information..We additionally used wine reviews and a variety of news paper articles for additional analysis. The wine data gives reviews of wine from WineEnthusiast and is hosted on Kaggle BIBREF25. While not specifically dated, the reviews were scraped between June and November of 2017. There are 20 different reviewers included, but the amount of reviews each has ranges from tens to thousands. The news data consists of news articles pulled from a variety of sources, including (in random order) the New York Times, Breitbart, CNN, the Atlantic, Buzzfeed News, National Review, New York Post, NPR, Reuters, and the Washington Post. The articles are primarily from 2016 and early 2017 with a few from 2015. The articles are scraped from home-page headline and RSS feeds BIBREF26. Metadata was limited for both of these data sets. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "42\n",
      "What are the three properties that the new framework for interpreting binomials is based on?\n",
      "The new framework for interpreting binomials is based on three properties: asymmetry, movement, and agreement.\n",
      "Question : for the text In this paper we introduce a new framework to interpret binomials, based on three properties: asymmetry (how frozen a binomial is), movement (how binomial orderings change over time), and agreement (how consistent binomial orderings are between communities), which we will visualize as a cube with three dimensions. Again, prior work has focused essentially entirely on asymmetry, and we argue that this can only really be understood in the context of the other two dimensions..For this paper we will use the convention {A,B} to refer to an unordered pair of words, and [A,B] to refer to an ordered pair where A comes before B. We say that [A,B] and [B,A] are the two possible orientations of {A,B}. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "43\n",
      "What is the definition of asymmetry in the context of binomials?\n",
      "\n",
      "The asymmetry of an unordered list {x,y} is A_x,y = 2 * |o_x,y - 0.5|, where o_x,y is the ordinality of the pair {x,y}.\n",
      "Question : for the text Previous work has one main measure of binomials — their `frozen-ness'. A binomial is `frozen' if it always appears with a particular order. For example, if the pair {`arrow', `bow'} always occurs as [`bow', `arrow'] and never as [`arrow', `bow'], then it is frozen. This leaves open the question of how describe the large number of binomials that are not frozen. To address this point, we instead consider the ordinality of a list, or how often the list is `in order' according to some arbitrary underlying reference order. Unless otherwise specified, the underlying order is assumed to be alphabetical. If the list [`cat', `dog'] appears 40 times and the list [`dog', `cat'] 10 times, then the list {`cat', `dog'} would have an ordinality of 0.8..Let $n_{x,y}$ be the number of times the ordered list $[x,y]$ appears, and let $f_{x,y} = n_{x,y} / (n_{x,y} + n_{y,x})$ be the fraction of times that the unordered version of the list appears in that order. We formalize ordinality as follows. [Ordinality] Given an ordering $<$ on words (by default, we assume alphabetical ordering), the ordinality $o_{x,y}$ of the pair $\\lbrace x,y\\rbrace $ is equal to $f_{x,y}$ if $x < y$ and $f_{y,x}$ otherwise..Similarly, we introduce the concept of asymmetry in the context of binomials, which is how often the word appears in its dominant order. In our framework, a `frozen' list is one with ordinality 0 or 1 and would be considered a high asymmetry list, with asymmetry of 1. A list that appears as [`A', `B'] half of the time and [`B', `A'] half of the time (or with ordinality 0.5) would be considered a low asymmetry list, with asymmetry of 0..[Asymmetry] The asymmetry of an unordered list $\\lbrace x,y\\rbrace $ is $A_{x,y} = 2 \\cdot \\vert o_{x,y} - 0.5 \\vert $..The Reddit data described above gives us access to new dimensions of binomials not previously addressed. We define movement as how the ordinality of a list changes over time [Movement] Let $o_{x,y,t}$ be the ordinality of an unordered list $\\lbrace x,y\\rbrace $ for data in year $t \\in T$. The movement of $\\lbrace x,y\\rbrace $ is $M_{x,y} = \\max _{t \\in T} o_{x,y,t} - \\min _{t \\in T} o_{x,y,t}$. And agreement describes how the ordinality of a list differs between different communities. [Agreement] Let $o_{x,y,c}$ be the ordinality of an unordered list ${x,y}$ for data in community (subreddit) $c \\in C$. The agreement of $\\lbrace x,y\\rbrace $ is $A_{x,y} = 1 - (\\max _{c \\in C} o_{x,y,c} - \\min _{c \\in C} o_{x,y,c})$. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "44\n",
      "Q: What is the significance of examining points near the corners of the unit cube in the 3-dimensional space defined by the vectors for asymmetry, movement, and agreement?\n",
      "\n",
      "A: By examining points near the corners of the unit cube, we can get a better understanding of the range of binomials and identify unusual examples that may provide insights into linguistic or cultural shifts. The corners correspond to points with coordinates made up entirely of 0s or 1s and represent natural high asymmetry, low movement, high agreement binomials, but there are no good examples of high asymmetry, low movement, low agreement binomials.\n",
      "Question : for the text Let the point $(A,M,G)_{x,y}$ be a vector of the asymmetry, movement, and agreement for some unordered list $\\lbrace x,y\\rbrace $. These vectors then define a 3-dimensional space in which each list occupies a point. Since our measures for asymmetry, agreement, and movement are all defined from 0 to 1, their domains form a unit cube (Fig. FIGREF8). The corners of this cube correspond to points with coordinates are entirely made up of 0s or 1s. By examining points near the corners of this cube, we can get a better understanding of the range of binomials. Some corners are natural — it is easy to imagine a high asymmetry, low movement, high agreement binomial — such as {`arrow', `bow'} from earlier. On the other hand, we have found no good examples of a high asymmetry, low movement, low agreement binomial. There are a few unusual examples, such as {10, 20}, which has 0.4 asymmetry, 0.2 movement, and 0.1 agreement and is clearly visible as an isolated point in Fig. FIGREF8..Asymmetry. While a majority of binomials have low asymmetry, almost all previous work has focused exclusively on high-asymmetry binomials. In fact, asymmetry is roughly normally distributed across binomials with an additional increase of highly asymmetric binomials (Fig. FIGREF9). This implies that previous work has overlooked the vast majority of binomials, and an investigation into whether rules proposed for highly asymmetric binomials also functions for other binomials is a core piece of our analysis..Movement. The vast majority of binomials have low movement. However, the exceptions to this can be very informative. Within r/nba a few of these pairs show clear change in linguistics and/or culture. The binomial [`rpm', `vorp'] (a pair of basketball statistics) started at 0.74 ordinality and within three years dropped to 0.32 ordinality, showing a potential change in users' representation of how these statistics relate to each other. In r/politics, [`daughter', `son'] moved from 0.07 ordinality to 0.36 ordinality over ten years. This may represent a cultural shift in how users refer to children, or a shift in topics discussed relating to children. And in r/politics, ['dems', 'obama'] went from 0.75 ordinality to 0.43 ordinality from 2009–2018, potentially reflecting changes in Obama's role as a defining feature of the Democratic Party. Meanwhile the ratio of unigram frequency of `dems' to `obama' actually increased from 10% to 20% from 2010 to 2017. Similarly, [`fdr', `lincoln'] moved from 0.49 ordinality to 0.17 ordinality from 2015–2018. This is particularly interesting, since in 2016 `fdr' had a unigram frequency 20% higher than `lincoln', but in 2017 they are almost the same. This suggests that movement could be unrelated to unigram frequency changes. Note also that the covariance for movement across subreddits is quite low TABREF10, and movement in one subreddit is not necessarily reflected by movement in another..Agreement. Most binomials have high agreement (Table TABREF11) but again the counterexamples are informative. For instance, [`score', `kick'] has ordinality of 0.921 in r/nba and 0.204 in r/nfl. This likely points to the fact that American football includes field goals. A less obvious example is the list [`ceiling', `floor']. In r/nba and r/nfl, it has ordinality 0.44, and in r/politics, it has ordinality 0.27..There are also differences among proper nouns. One example is [`france', `israel'], which has ordinality 0.6 in r/politics, 0.16 in r/Libertarian, and 0.51 in r/The_Donald (and the list does not appear in r/Conservative). And the list [`romney', `trump'] has ordinality 0.48 in r/poltics, 0.55 in r/The_Donald, and 0.73 in r/Conservative. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "45\n",
      "Question 1: What is the Proximity Principle?\n",
      "\n",
      "Answer 1: The Proximity Principle is the concept that the proper noun with which a speaker identifies more will tend to come first in binomial orderings. This is evidenced when commenters refer to their sports team first, or politicians refer to their party first.\n",
      "Question : for the text Analyzing binomial orderings on a large scale has led to surprising results. Although most binomials are not frozen in the traditional sense, there is little movement in their ordinality across time or communities. A list that appears in the order [A, B] 60% of the time in one subreddit in one year is likely to show up as [A, B] very close to 60% of the time in all subreddits in all years. This suggests that binomial order should be predictable, but there is evidence that this is difficult: the most common theories on frozen binomial ordering were largely ineffective at predicting binomial ordering in general..Given the challenge in predicting orderings, we searched for methods or principles that could yield better performance, and identified two promising approaches. First, models built on standard word embeddings produce predictions of binomial orders that are much more effective than simpler existing theories. Second, we established the Proximity Principle: the proper noun with which a speaker identifies more will tend to come first. This is evidenced when commenters refer to their sports team first, or politicians refer to their party first. Further analysis of the global structure of binomials reveals interesting patterns and a surprising acyclic nature in names. Analysis of longer lists in the form of multinomials suggests that the rules governing their orders may be different..We have also found promising results in some special cases. We expect that more domain-specific studies will offer rich structure..It is a challenge to adapt the long history of work on the question of frozen binomials to the large, messy environment of online text and social media. However, such data sources offer a unique opportunity to re-explore and redefine these questions. It seems that binomial orderings offer new insights into language, culture, and human cognition. Understanding what changes in these highly stable conventions mean — and whether or not they can be predicted — is an interesting avenue for future research. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "46\n",
      "Question 1: What types of text did McGuire and McGuire analyze in relation to Reddit's informal text corpus? \n",
      "Answer 1: McGuire and McGuire made a distinct separation between informal and formal text, and they analyzed highly stylized wine reviews and news articles from a diverse set of publications.\n",
      "Question : for the text While Reddit provides a very large corpus of informal text, McGuire and McGuire make a distinct separation between informal and formal text BIBREF28. As such, we briefly analyze highly stylized wine reviews and news articles from a diverse set of publications. Both data sets follow the same basic principles outlined above. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "47\n",
      "Question 1: Are there significant ideological differences between the examined publications based on binomial usage?\n",
      "\n",
      "Answer 1: The analysis suggests that while there are certainly examples of binomials that differ significantly for one publication or group, there does not appear to be a sharp divide based on ideology. Most highly controversial binomials reflect political structures rather than ideological differences. However, subtle political or ideological differences could still be present between the publications.\n",
      "Question : for the text We focused our analysis on NYT, Buzzfeed, Reuters, CNN, the Washington Post, NPR, Breitbart, and the Atlantic. Much like in political subreddits, one might expect to see a split between various publications based upon ideology. However, this is not obviously the case. While there are certainly examples of binomials that seem to differ significantly for one publication or for a group of publications (Buzzfeed, in particular, frequently goes against the grain), there does not seem to be a sharp divide. Individual examples are difficult to draw conclusions from, but can suggest trends. (`China', `Russia') is a particularly controversial binomial. While the publications vary quite a bit, only Breitbart has an ordinality of above 0.5. In fact, country pairs are among the most controversial binomials within the publications (e.g. (`iraq', `syria'), (`afghanisatan', `iraq')), while most other highly controversial binomials reflect other political structures, such as (`house', `senate'), (`migrants', 'refugees'), and (`left', `right'). That so many controversial binomials reflect politics could point to subtle political or ideological differences between the publications. Additionally, the close similarity between Breitbart and more mainstream publications could be due to a similar effect we saw with r/The_Donald - mainly large amounts of quoted text. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "48\n",
      "Question 1: Are wine reviews expected to have more frozen binomials compared to other text forms?\n",
      "Answer 1: No, contrary to expectations, wine reviews do not exhibit more frozen binomials compared to other text forms.\n",
      "Question : for the text Wine reviews are a highly stylized form of text. In this case reviews are often just a few sentences, and they use a specialized vocabulary meant for wine tasting. While one might hypothesize that such stylized text exhibits more frozen binomials, this is not the case (Tab TABREF28). There is some evidence of an additional freezing effect in binomials such as ('aromas', 'flavors') and ('scents', 'flavors') which both are frozen in the wine reviews, but are not frozen on Reddit. However, this does not seem to have a more general effect. Additionally, there are a number of binomials which appear frozen on Reddit, but have low asymmetry in the wine reviews, such as ['lemon', 'lime']. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "49\n",
      "What is the significance of cycles in the directed graphs constructed for binomial orderings?\n",
      "\n",
      "Cycles in the directed graphs constructed for binomial orderings reveal the non-global ordering of binomials and suggest a global partial hierarchy of individuals. They are particularly important when considering only nodes that are names, as they may signal a hierarchy within that particular community. However, some of these cycles at high asymmetry might also be due to English words that are also names.\n",
      "Question : for the text We can discover new structure in binomial orderings by taking a more global view. We do this by building directed graphs based on ordinality. In these graphs, nodes are words and an arrow from A to B indicates that there are at least 30 lists containing A and B and that those lists have order [A,B] at least 50% of the time. For our visualizations, the size of the node indicates how many distinct lists the word appears in,and color indicates how many list instances contain the word in total..If we examine the global structure for r/nba, we can pinpoint a number of patterns (Fig. FIGREF31). First, most nodes within the purple circle correspond to names, while most nodes outside of it are not names. The cluster of circles in the lower left are a combination of numbers and years, where dark green corresponds to numbers, purple corresponds to years, and pink corresponds years represented as two-digit numbers (e.g., `96'). On the right, the brown circle contains adjectives, while above the blue circle contains heights (e.g., 6'5\"), and in the two circles in the lower middle, the left contains cities while the right contains team names. The darkest red node in the center of the graph corresponds to `lebron'..Constructing a similar graph for our wines dataset, we can see clusters of words. In Fig FIGREF32, the colors represent clusters as formed through modularity. These clusters are quite distinct. Green nodes mostly refer to the structure or body of a wine, red are adjectives describing taste, teal and purple are fruits, dark green is wine varietals, gold is senses, and light blue is time (e.g. `year', `decade', etc.).We can also consider the graph as we change the threshold of asymmetry for which an edge is included. If the asymmetry is large enough, the graph is acyclic, and we can consider how small the ordinality threshold must be in order to introduce a cycle. These cycles reveal the non-global ordering of binomials. The graph for r/nba begins to show cycles with a threshold asymmetry of 0.97. Three cycles exist at this threshold: [`ball', `catch', `shooter'], [`court', `pass', `set', `athleticism'], and [`court', `plays', `set', `athleticism']..Restricting the nodes to be names is also revealing. Acyclic graphs in this context suggest a global partial hierarchy of individuals. For r/nba, the graph is no longer acyclic at an asymmetry threshold of 0.76, with the cycle [`blake', `jordan', `bryant', `kobe']. Similarly, the graph for r/nfl (only including names) is acyclic until the threshold reaches 0.73 with cycles [`tannehill', `miller', `jj watt', `aaron rodgers', `brady'], and [`hoyer', `savage', `watson', `hopkins', `miller', `jj watt', `aaron rodgers', `brady']..Figure FIGREF33 shows these graphs for the three political subreddits, where the nodes are the 30 most common politician names. The graph visualizations immediately show that these communities view politicians differently. We can also consider cycles in these graphs and find that the graph is completely acyclic when the asymmetry threshold is at least 0.9. Again, this suggests that, at least among frozen binomials, there is in fact a global partial order of names that might signal hierarchy. (Including non-names, though, causes the r/politics graph to never be acyclic for any asymmetry threshold, since the cycle [`furious', `benghazi', `fast'] consists of completely frozen binomials.) We find similar results for r/Conservative and r/Libertarian, which are acyclic with thresholds of 0.58 and 0.66, respectively. Some of these cycles at high asymmetry might be due to English words that are also names (e.g. `law'), but one particularly notable cycle from r/Conservative is [`rubio', `bush', `obama', `trump', `cruz']. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "50\n",
      "What is a binomial? \n",
      "\n",
      "Answer 1: A binomial is a list of two words usually separated by a conjunction such as 'and' or 'or'. They have been extensively studied in the field of linguistics, with a focus on understanding the ordering of the two words in the list.\n",
      "Question : for the text Lists are extremely common in text and speech, and the ordering of items in a list can often reveal information. For instance, orderings can denote relative importance, such as on a to-do list, or signal status, as is the case for author lists of scholarly publications. In other cases, orderings might come from cultural or historical conventions. For example, `red, white, and blue' is a specific ordering of colors that is recognizable to those familiar with American culture..The orderings of lists in text and speech is a subject that has been repeatedly touched upon for more than a century. By far the most frequently studied aspect of list ordering is the binomial, a list of two words usually separated by a conjunction such as `and' or `or', which is the focus of our paper. The academic treatment of binomial orderings dates back more than a century to Jespersen BIBREF0, who proposed in 1905 that the ordering of many common English binomials could be predicted by the rhythm of the words. In the case of a binomial consisting of a monosyllable and a disyllable, the prediction was that the monosyllable would appear first followed by the conjunction `and'. The idea was that this would give a much more standard and familiar syllable stress to the overall phrase, e.g., the binomial `bread and butter' would have the preferable rhythm compared to `butter and bread.'.This type of analysis is meaningful when the two words in the binomial nearly always appear in the same ordering. Binomials like this that appear in strictly one order (perhaps within the confines of some text corpus), are commonly termed frozen binomials BIBREF1, BIBREF2. Examples of frozen binomials include `salt and pepper' and `pros and cons', and explanations for their ordering in English and other languages have become increasingly complex. Early work focused almost exclusively on common frozen binomials, often drawn from everyday speech. More recent work has expanded this view to include nearly frozen binomials, binomials from large data sets such as books, and binomials of particular types such as food, names, and descriptors BIBREF3, BIBREF4, BIBREF5, BIBREF6, BIBREF7, BIBREF8. Additionally, explanations have increasingly focused on meaning rather than just sound, implying value systems inherent to the speaker or the culture of the language's speakers (one such example is that men are usually listed before women in English BIBREF9). The fact that purely phonetic explanations have been insufficient suggests that list orderings rely at least partially on semantics, and it has previously been suggested that these semantics could be revealing about the culture in which the speech takes place BIBREF3. Thus, it is possible that understanding these orderings could reveal biases or values held by the speaker..Overall, this prior research has largely been confined to pristine examples, often relying on small samples of lists to form conclusions. Many early studies simply drew a small sample of what the author(s) considered some of the more representative or prominent binomials in whatever language they were studying BIBREF10, BIBREF1, BIBREF11, BIBREF0, BIBREF12, BIBREF13, BIBREF14, BIBREF15, BIBREF3. Other researchers have used books or news articles BIBREF2, BIBREF4, or small samples from the Web (web search results and Google books) BIBREF5. Many of these have lacked a large-scale text corpus and have relied on a focused set of statistics about word orderings..Thus, despite the long history of this line of inquiry, there is an opportunity to extend it significantly by examining a broad range of questions about binomials coming from a large corpus of online text data produced organically by many people. Such an analysis could produce at least two types of benefits. First, such a study could help us learn about cultural phenomena embedded in word orderings and how they vary across communities and over time. Second, such an analysis could become a case study for the extension of theories developed at small scales in this domain to a much larger context..The present work: Binomials in large-scale online text. In this work, we use data from large-scale Internet text corpora to study binomials at a massive scale, drawing on text created by millions of users. Our approach is more wholesale than prior work - we focus on all binomials of sufficient frequency, without first restricting to small samples of binomials that might be frozen. We draw our data from news publications, wine reviews, and Reddit, which in addition to large volume, also let us characterize binomials in new ways, and analyze differences in binomial orderings across communities and over time. Furthermore, the subject matter on Reddit leads to many lists about people and organizations that lets us study orderings of proper names — a key setting for word ordering which has been difficult to study by other means..We begin our analysis by introducing several new key measures for the study of binomials, including a quantity we call asymmetry that measures how frequently a given binomial appears in some ordering. By looking at the distribution of asymmetries across a wide range of binomials, we find that most binomials are not frozen, barring a few strong exceptions. At the same time, there may still be an ordering preference. For example, `10 and 20' is not a frozen binomial; instead, the binomial ordering `10 and 20' appears 60% of the time and `20 and 10' appears 40% of time..We also address temporal and community structure in collections of binomials. While it has been recognized that the orderings of binomials may change over time or between communities BIBREF5, BIBREF10, BIBREF1, BIBREF13, BIBREF14, BIBREF15, there has been little analysis of this change. We develop new metrics for the agreement of binomial orderings across communities and the movement of binomial orderings over time. Using subreddits as communities, these metrics reveal variations in orderings, some of which suggest cultural change influencing language. For example, in one community, we find that over a period of 10 years, the binomial `son and daughter' went from nearly frozen to appearing in that order only 64% of the time..While these changes do happen, they are generally quite rare. Most binomials — frozen or not — are ordered in one way about the same percentage of the time, regardless of community or the year. We develop a null model to determine how much variation in binomial orderings we might expect across communities and across time, if binomial orderings were randomly ordered according to global asymmetry values. We find that there is less variation across time and communities in the data compared to this model, implying that binomial orderings are indeed remarkably stable..Given this stability, one might expect that the dominant ordinality of a given binomial is still predictable, even if the binomial is not frozen. For example, one might expect that the global frequency of a single word or the number of syllables in a word would predict ordering in many cases. However, we find that these simple predictors are quite poor at determining binomial ordering..On the other hand, we find that a notion of `proximity' is robust at predicting ordering in some cases. Here, the idea is that the person producing the text will list the word that is conceptually “closer” to them first — a phenomenon related to a “Me First” principle of binomial orderings suggested by Cooper and Ross BIBREF3. One way in which we study this notion of proximity is through sports team subreddits. For example, we find that when two NBA team names form a binomial on a specific team's subreddit, the team that is the subject of the subreddit tends to appear first..The other source of improved predictions comes from using word embeddings BIBREF16: we find that a model based on the positions of words in a standard pre-trained word embedding can be a remarkably reliable predictor of binomial orderings. While not applicable to all words, such as names, this type of model is strongly predictive in most cases..Since binomial orderings are in general difficult to predict individually, we explore a new way of representing the global binomial ordering structure, we form a directed graph where an edge from $i$ to $j$ means that $i$ tends to come before $j$ in binomials. These graphs show tendencies across the English language and also reveal peculiarities in the language of particular communities. For instance, in a graph formed from the binomials in a sports community, the names of sports teams and cities are closely clustered, showing that they are often used together in binomials. Similarly, we identify clusters of names, numbers, and years. The presence of cycles in these graphs are also informative. For example, cycles are rare in graphs formed from proper names in politics, suggesting a possible hierarchy of names, and at the same time very common for other binomials. This suggests that no such hierarchy exists for most of the English language, further complicating attempts to predict binomial order..Finally, we expand our work to include multinomials, which are lists of more than two words. There already appears to be more structure in trinomials (lists of three) compared to binomials. Trinomials are likely to appear in exactly one order, and when they appear in more than one order the last word is almost always the same across all instances. For instance, in one section of our Reddit data, `Fraud, Waste, and Abuse' appears 34 times, and `Waste, Fraud, and Abuse' appears 20 times. This could point to, for example, recency principles being more important in lists of three than in lists of two. While multinomials were in principle part of the scope of past research in this area, they were difficult to study in smaller corpora, suggesting another benefit of working at our current scale. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "1\n",
      "What are the three basic categories that prior research on binomial orderings can be separated into?\n",
      "The three basic categories are phonological rules, semantic rules, and metadata rules.\n",
      "Question : for the text Interest in list orderings spans the last century BIBREF10, BIBREF1, with a focus almost exclusively on binomials. This research has primarily investigated frozen binomials, also called irreversible binomials, fixed coordinates, and fixed conjuncts BIBREF11, although some work has also looked at non-coordinate freezes where the individual words are nonsensical by themselves (e.g., `dribs and drabs') BIBREF11. One study has directly addressed mostly frozen binomials BIBREF5, and we expand the scope of this paper by exploring the general question of how frequently binomials appear in a particular order. Early research investigated languages other than English BIBREF1, BIBREF10, but most recent research has worked almost exclusively with English. Overall, this prior research can be separated into three basic categories — phonological rules, semantic rules, and metadata rules..Phonology. The earliest research on binomial orderings proposed mostly phonological explanations, particularly rhythm BIBREF0, BIBREF12. Another highly supported proposal is Panini's Law, which claims that words with fewer syllables come first BIBREF17; we find only very mild preference for this type of ordering. Cooper and Ross's work expands these to a large list of rules, many overlapping, and suggests that they can compound BIBREF3; a number of subsequent papers have expanded on their work BIBREF11, BIBREF15, BIBREF9, BIBREF17..Semantics. There have also been a number of semantic explanations, mostly in the form of categorical tendencies (such as `desirable before undesirable') that may have cultural differences BIBREF10, BIBREF1. The most influential of these may be the `Me First' principle codified by Cooper and Ross. This suggests that the first word of a binomial tends to follow a hierarchy that favors `here', `now', present generation, adult, male, and positive. Additional hierarchies also include a hierarchy of food, plants vs. animals, etc. BIBREF3..Frequency. More recently, it has been proposed that the more cognitively accessible word might come first, which often means the word the author sees or uses most frequently BIBREF18. There has also been debate on whether frequency may encompass most phonological and semantic rules that have been previously proposed BIBREF13, BIBREF4. We find that frequency is in general a poor predictor of word ordering..Combinations. Given the number of theories, there have also been attempts to give a hierarchy of rules and study their interactions BIBREF4, BIBREF5. This research has complemented the proposals of Cooper and Ross BIBREF3. These types of hierarchies are also presented as explanations for the likelihood of a binomial becoming frozen BIBREF5..Names. Work on the orderings of names has been dominated by a single phenomenon: men's names usually come before women's names. Explanations range from a power differential, to men being more `agentic' within `Me First', to men's names being more common or even exhibiting more of the phonological features of words that usually come first BIBREF8, BIBREF5, BIBREF18, BIBREF3, BIBREF13, BIBREF9, BIBREF19, BIBREF6. However, it has also been demonstrated that this preference may be affected by the author's own gender and relationship with the people named BIBREF6, BIBREF19, as well as context more generally BIBREF20..Orderings on the Web. List orderings have also been explored in other Web data, specifically on the ordering of tags applied to images BIBREF21. There is evidence that these tags are ordered intentionally by users, and that a bias to order tag A before tag B may be influenced by historical precedent in that environment but also by the relative importance of A and B BIBREF21. Further work also demonstrates that exploiting the order of tags on images can improve models that rank those images BIBREF22. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "2\n",
      "What does the data suggest about the stability of binomial orderings across communities and time?\n",
      "\n",
      "The data suggests that binomial orderings are extremely stable across communities and time, showing smaller variation than the null model predicts.\n",
      "Question : for the text In this section, we establish a null model under which different communities or time slices have the same probability of ordering a binomial in a particular way. With this, we would expect to see variation in binomial asymmetry. We find that our data shows smaller variation than this null model predicts, suggesting that binomial orderings are extremely stable across communities and time. From this, we might also expect that orderings are predictable; but we find that standard predictors in fact have limited success. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "3\n",
      "What is the most successful prediction method for determining the ordering of binomials?\n",
      "\n",
      "Answer 1: Using word embeddings, specifically the Google News word embeddings, was the most successful prediction method for determining the ordering of binomials. This method resulted in accuracy ranging from 70% to 85% across various subreddits and 80-100% accuracy on frozen binomials.\n",
      "Question : for the text Given the stability of binomials within our data, we now try to predict their ordering. We consider deterministic or rule-based methods that predict the order for a given binomial. We use two classes of evaluation measures for success on this task: (i) by token — judging each instance of a binomial separately; and (ii) by type — judging all instances of a particular binomial together. We further characterize these into weighted and unweighted..To formalize these notions, first consider any unordered list $\\lbrace x,y\\rbrace $ that appears $n_{x,y}$ times in the orientation $[x,y]$ and $n_{y,x}$ times in the orientation $[y,x]$. Since we can only guess one order, we will have either $n_{x,y}$ or $n_{y,x}$ successful guesses for $\\lbrace x,y\\rbrace $ when guessing by token. The unweighted token score (UO) and weighted token score (WO) are the macro and micro averages of this accuracy..If predicting by type, let $S$ be the lists such that the by-token prediction is successful at least half of the time. Then the unweighted type score (UT) and weighted type score (WT) are the macro and micro averages of $S$..Basic Features. We first use predictors based on rules that have previously been proposed in the literature: word length, number of phonemes, number of syllables, alphabetical order, and frequency. We collect all binomials but make predictions only on binomials appearing at least 30 times total, stratified by subreddit. However, none of these features appear to be particularly predictive across the board (Table TABREF15). A simple linear regression model predicts close to random, which bolsters the evidence that these classical rules for frozen binomials are not predictive for general binomials..Perhaps the oldest suggestion to explain binomial orderings is that if there are two words A and B, and A is monosyllabic and B is disyllabic, then A comes before B BIBREF0. Within r/politics, we gathered an estimate of number of syllables for each word as given by a variation on the CMU Pronouncing Dictionary BIBREF27 (Tables TABREF16 and TABREF17). In a weak sense, Jespersen was correct that monosyllabic words come before disyllabic words more often than not; and more generally, shorter words come before longer words more often than not. However, as predictors, these principles are close to random guessing..Paired Predictions. Another measure of predictive power is predicting which of two binomials has higher asymmetry. In this case, we take two binomials with very different asymmetry and try to predict which has higher asymmetry by our measures (we use the top-1000 and bottom-1000 binomials in terms of asymmetry for these tasks). For instance, we may predict that [`red', `turquoise'] is more asymmetric than [`red', `blue'] because the differences in lengths is more extreme. Overall, the basic predictors from the literature are not very successful (Table TABREF18)..Word Embeddings. If we turn to more modern approaches to text analysis, one of the most common is word embeddings BIBREF16. Word embeddings assign a vector $x_i$ to each word $i$ in the corpus, such that the relative position of these vectors in space encode information lingustically relevant relationships among the words. Using the Google News word embeddings, via a simple logistic model, we produce a vector $v^*$ and predict the ordering of a binomial on words $i$ and $j$ from $v^* \\cdot (x_i - x_j)$. In this sense, $v^*$ can be thought of as a “sweep-line” direction through the space containing the word vectors, such that the ordering along this sweep-line is the predicted ordering of all binomials in the corpus. This yields surprisingly accurate results, with accuracy ranging from 70% to 85% across various subreddits (Table TABREF20), and 80-100% accuracy on frozen binomials. This is by far the best prediction method we tested. It is important to note that not all words in our binomials could be associated with an embedding, so it was necessary to remove binomials containing words such as names or slang. However, retesting our basic features on this data set did not show any improvement, implying that the drastic change in predictive power is not due to the changed data set. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "4\n",
      "What is the distribution of $p^*_{l}$ over $l \\in L$?\n",
      "The distribution of $p^*_{l}$ over $l \\in L$ has a median of 0, mean of 0.0145, and standard deviation of 0.0344.\n",
      "Question : for the text Recall that the asymmetry of binomials with respect to alphabetic order (excluding frozen binomials) is roughly normal centered around $0.5$ (Fig. FIGREF9). One way of seeing this type of distribution would be if binomials are ordered randomly, with $p=0.5$ for each order. In this case, if each instance $l$ of a binomial $\\lbrace x,y\\rbrace $ takes value 0 (non-alphabetical ordering) or 1 (alphabetical ordering), then $l \\sim \\text{Bernoulli}(0.5)$. If $\\lbrace x,y\\rbrace $ appears $n$ times, then the number of instances of value 1 is distributed by $W \\sim \\text{Bin}(n, 0.5)$, and $W / n$ is approximately normally distributed with mean 0.5..One way to test this behavior is to first estimate $p$ for each list within each community. If the differences in these estimates are not normal, then the above model is incorrect. We first omit frozen binomials before any analysis. Let $L$ be a set of unordered lists and $C$ be a set of communities. We estimate $p$ for list $l \\in L$ in community $c \\in C$ by $\\hat{p}_{l,c} = o_{l,c}$, the ordinality of $l$ in $C$. Next, for all $l \\in L$ let $p^*_{l} = \\max _{c \\in C}(\\hat{p}_{l, c}) - \\min _{ c \\in C}(\\hat{p}_{l, c})$. The distribution of $p^*_{l}$ over $l \\in L$ has median 0, mean 0.0145, and standard deviation 0.0344. We can perform a similar analysis over time. Define $Y$ as our set of years, and $\\hat{p}_{l, y} = o_{l,y}$ for $y \\in Y$ our estimates. The distribution of $p^{\\prime }_{l} = \\max _{y \\in Y}(\\hat{p}_{l, y}) - \\min _{y \\in Y}(\\hat{p}_{l, y})$ over $l \\in L$ has median 0.0216, mean 0.0685, and standard deviation 0.0856. The fact that $p$ varies very little across both time and communities suggests that there is some $p_l$ for each $l \\in L$ that is consistent across time and communities, which is not the case in the null model, where these values would be normally distributed..We also used a bootstrapping technique to understand the mean variance in ordinality for lists over communities and years. Specifically, let $o_{l, c, y}$ be the ordinality of list $l$ in community $c$ and year $y$, $O_l$ be the set of $o_{l,c,y}$ for a given list $l$, and $s_l$ be the standard deviation of $O_l$. Finally, let $\\bar{s}$ be the average of the $s_l$. We re-sample data by randomizing the order of each binomial instance, sampling its orderings by a binomial random variable with success probability equal to its ordinality across all seasons and communities ($p_l$). We repeated this process to get samples estimates $\\lbrace \\bar{s}_1, \\ldots , \\bar{s}_{k}\\rbrace $, where $k$ is the size of the set of seasons and communities. These averages range from 0.0277 to 0.0278 and are approximately normally distributed (each is a mean over an approximately normal scaled Binomial random variable). However, $\\bar{s} = 0.0253$ for our non-randomized data. This is significantly smaller than the randomized data and implies that the true variation in $p_l$ across time and communities is even smaller than a binomial distribution would predict. One possible explanation for this is that each instance of $l$ is not actually independent, but is in fact anti-correlated, violating one of the conditions of the binomial distribution. An explanation for that could be that users attempt to draw attention by intentionally going against the typical ordering BIBREF1, but it is an open question what the true model is and why the variation is so low. Regardless, it is clear that the orientation of binomials varies very little across years and communities (Fig. FIGREF13). generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "5\n",
      "What is the main difference between studying binomials and trinomials?\n",
      "\n",
      "The main difference is that trinomials have six possible orderings compared to only two possible orderings for binomials.\n",
      "Question : for the text Binomials are the most studied type of list, but trinomials — lists of three — are also common enough in our dataset to analyze. Studying trinomials adds new aspects to the set of questions: for example, while binomials have only two possible orderings, trinomials have six possible orderings. However, very few trinomials show up in all six orderings. In fact, many trinomials show up in exactly one ordering: about 36% of trinomials being completely frozen amongst trinomials appearing at least 30 times in the data. To get a baseline comparison, we found an equal number of the most common binomials, and then subsampled instances of those binomials to equate the number of instances with the trinomials. In this case, only 21% of binomials are frozen. For trinomials that show up in at least two orderings, it is most common for the last word to keep the same position (e.g., [a, b, c] and [b, a, c]). For example, in our data, [`fraud', `waste', `abuse'] appears 34 times, and [`waste', `fraud', `abuse'] appears 20 times. This may partially be explained by many lists that contain words such as `other', `whatever', or `more'; for instance, [`smarter', `better', `more'] and [`better', `smarter', `more'] are the only two orderings we observe for this set of three words..Additionally, each trinomial [a, b, c] contains three binomials within it: [a, b], [b, c], and [a, c]. It is natural to compare orderings of {a, b} in general with orderings of occurrences of {a, b} that lie inside trinomials. We use this comparison to define the compatibility of {a, b}, as follows..Compatibility Let {a, b} be a binomial with dominant ordering [a, b]; that is, [a, b] is at least as frequent as [b, a]. We define the compatibility of {a, b} to be the fraction of instances of {a, b} occurring inside trinomials that have the order [a,b]..There are only a few cases where binomials have compatibility less than 0.5, and for most binomials, the asymmetry is remarkably consistent between binomials and trinomials (Fig. FIGREF37). In general, asymmetry is larger than compatibility — this occurs for 4569 binomials, compared to 3575 where compatibility was greater and 690 where the two values are the same. An extreme example is the binomial {`fairness', `accuracy'}, which has asymmetry 0.77 and compatibility 0.22. It would be natural to consider these questions for tetranomials and longer lists, but these are rarer in our data and correspondingly harder to draw conclusions from. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "6\n",
      "Question 1: What groups of proper nouns were investigated in the study using Reddit data?\n",
      "\n",
      "Answer 1: The study investigated proper nouns of NBA and NFL players (1970-2019), congresspeople (pre-1800 and 2000-2019), teams and political groups, and subreddits related to r/nba, r/nfl, and r/politics.\n",
      "Question : for the text Proper nouns, and names in particular, have been a focus within the literature on frozen binomials BIBREF8, BIBREF5, BIBREF18, BIBREF3, BIBREF13, BIBREF9, BIBREF6, BIBREF19, BIBREF20, BIBREF28, but these studies have largely concentrated on the effect of gender in ordering BIBREF8, BIBREF5, BIBREF18, BIBREF3, BIBREF13, BIBREF9, BIBREF6, BIBREF19, BIBREF20. With Reddit data, however, we have many conversations about large numbers of celebrities, with significant background information on each. As such, we can investigate proper nouns in three subreddits: r/nba, r/nfl, and r/politics. The names we used are from NBA and NFL players (1970–2019) and congresspeople (pre-1800 and 2000–2019) respectively. We also investigated names of entities for which users might feel a strong sense of identification, such as a team or political group they support, or a subreddit to which they subscribe. We hypothesized that the group with which the user identifies the most would come first in binomial orderings. Inspired by the `Me First Principle', we call this the Proximity Principle. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "7\n",
      "Question 1: What was the best predictor of a player's popularity in r/nba?\n",
      "Answer 1: The best predictor of a player's popularity in r/nba was how often their team was mentioned, according to the study. Interestingly, the number of times the player's name was mentioned overall (unigram frequency) was not a good predictor.\n",
      "Question : for the text First, we examined names in r/nba. One advantage of using NBA players is that we have detailed statistics for ever player in every year. We tested a number of these statistics, and while all of them predicted statistically significant numbers ($p <$ 1e-6) of binomials, they were still not very predictive in a practical sense (Table TABREF23). The best predictor was actually how often the player's team was mentioned. Interestingly, the unigram frequency (number of times the player's name was mentioned overall) was not a good predictor. It is relevant to these observations that some team subreddits (and thus, presumably, fanbases) are significantly larger than others. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "8\n",
      "What is the Proximity Principle and how does it apply to political subreddits?\n",
      "Answer 1: The Proximity Principle refers to the tendency to list items in order of proximity or relevance. In the context of political subreddits, it means that left-leaning subreddits like r/politics are more likely to say \"democrats and republicans\" while right-leaning subreddits are more likely to say \"republicans and democrats.\"\n",
      "Question : for the text In our case, political names are drawn from every congressperson (and their nicknames) in both houses of the US Congress through the 2018 election. It is worth noting that one of these people is Philadelph Van Trump. It is presumed that most references to `trump' refer to Donald Trump. There may be additional instances of mistaken identities. We restrict the names to only congresspeople that served before 1801 or after 1999, also including `trump'..One might guess that political subreddits refer to politicians of their preferred party first. However, this was not the case, as Republicans are mentioned first only about 43%–46% of the time in all subreddits (Table TABREF27). On the other hand, the Proximity Principle does seem to come into play when discussing ideology. For instance, r/politics — a left-leaning subreddit — is more likely to say `democrats and republicans' while the other political subreddits in our study — which are right-leaning — are more likely to say `republicans and democrats'..Another relevant measure for lists of proper nouns is the ratio of the number of list instances containing a name to the unigram frequency of that name. We restrict our investigation to names that are not also English words, and only names that have a unigram frequency of at least 30. The average ratio is 0.0535, but there is significant variation across names. It is conceivable that this list ratio is revealing about how often people are talked about alone instead of in company. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "9\n",
      "What is the Proximity Principle mentioned in the text?\n",
      "\n",
      "The Proximity Principle is a principle mentioned in the text which states that proper nouns which appear closer in a sentence or list are more likely to be related or associated with each other. The text also provides an example of this principle in action with regards to lists of NBA team names and subreddits.\n",
      "Question : for the text Additionally, we also investigated lists of names of sports teams and subreddits as proper nouns. In this case we exploit an interesting structure of the r/nba subreddit which is not evident at scale in other subreddits we examined. In addition to r/nba, there exists a number of subreddits that are affiliated with a particular NBA team, with the purpose of allowing discussion between fans of that team. This implies that most users in a team subreddit are fans of that team. We are then able to look for lists of NBA teams by name, city, and abbreviation. We found 2520 instances of the subreddit team coming first, and 1894 instances of the subreddit team coming second. While this is not a particularly strong predictor, correctly predicting 57% of lists, it is one of the strongest we found, and a clear illustration of the Proximity Principle..We can do a similar calculation with subreddit names, by looking between subreddits. While the team subreddits are not large enough for this calculation, many of the other subreddits are. We find that lists of subreddits in r/nba that include `r/nba' often start with `r/nba', and a similar result holds for r/nfl (Table TABREF25)..While NBA team subreddits show a fairly strong preference to name themselves first, this preference is slightly less strong among sport subreddits, and even less strong among politics subreddits. One potential factor here is that r/politics is a more general subreddit, while the rest are more specific — perhaps akin to r/nba and the team subreddits. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "10\n",
      "What is the approach used to measure computers' ability to recover Calvino's thematic groupings?\n",
      "\n",
      "The approach involves computing a vector representation for every city and performing unsupervised clustering of these representations.\n",
      "Question : for the text We focus on measuring to what extent computers can recover Calvino's thematic groupings when given just raw text of the city descriptions. At a high level, our approach (Figure FIGREF4 ) involves (1) computing a vector representation for every city and (2) performing unsupervised clustering of these representations. The rest of this section describes both of these steps in more detail. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "11\n",
      "Question 1: Who helped improve the paper's clarity, coverage of related work, and analysis experiments?\n",
      "Answer 1: Nader Akoury, Garrett Bernstein, Chenghao Lv, Ari Kobren, Kalpesh Krishna, Saumya Lal, Tu Vu, Zhichao Yang, Mengxue Zhang and the UMass NLP group helped improve the paper's clarity, coverage of related work, and analysis experiments.\n",
      "Question : for the text We thank the anonymous reviewers for their insightful comments. Additionally, we thank Nader Akoury, Garrett Bernstein, Chenghao Lv, Ari Kobren, Kalpesh Krishna, Saumya Lal, Tu Vu, Zhichao Yang, Mengxue Zhang and the UMass NLP group for suggestions that improved the paper's clarity, coverage of related work, and analysis experiments. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "12\n",
      "Question 1: How did the researchers group the 55 city representations into eleven clusters of five cities each?\n",
      "\n",
      "Answer 1: The researchers used a simple clustering algorithm to approximate the grouping process. They defined \"cluster strength\" as the relative difference between \"intra-group\" Euclidean distance and \"inter-group\" Euclidean distance and proposed random exchanges of memberships until convergence, only accepting proposals when the cluster strength increased. The quality of the computationally-derived clusters was evaluated against those of Calvino using cluster purity.\n",
      "Question : for the text Given 55 city representations, how do we group them into eleven clusters of five cities each? Initially, we experimented with a graph-based community detection algorithm that maximizes cluster modularity BIBREF20 , but we found no simple way to constrain this method to produce a specific number of equally-sized clusters. The brute force approach of enumerating all possible cluster assignments is intractable given the large search space ( INLINEFORM0 possible assignments). We devise a simple clustering algorithm to approximate this process. First, we initialize with random cluster assignments and define “cluster strength” to be the relative difference between “intra-group” Euclidean distance and “inter-group” Euclidean distance. Then, we iteratively propose random exchanges of memberships, only accepting these proposals when the cluster strength increases, until convergence. To evaluate the quality of the computationally-derived clusters against those of Calvino, we measure cluster purity BIBREF21 : given a set of predicted clusters INLINEFORM1 and ground-truth clusters INLINEFORM2 that both partition a set of INLINEFORM3 data points, INLINEFORM4  generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "13\n",
      "Question 1: What are the limitations of using NLP techniques in literary analysis? \n",
      "Answer 1: The limitations of using NLP techniques in literary analysis include difficulties in understanding abstract themes, despite the potential for obtaining new insights.\n",
      "Question : for the text Our work takes a first step towards computationally engaging with literary criticism on a single book using state-of-the-art text representation methods. While we demonstrate that NLP techniques can be used to support literary analyses and obtain new insights, they also have clear limitations (e.g., in understanding abstract themes). As text representation methods become more powerful, we hope that (1) computational tools will become useful for analyzing novels with more conventional structures, and (2) literary criticism will be used as a testbed for evaluating representations. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "14\n",
      "Question 1: What method is used to produce a single city embedding in the experiments with pretrained representations?\n",
      "\n",
      "Answer 1: To produce a single city embedding, the researchers compute the TF-IDF weighted element-wise mean of the token-level representations.\n",
      "Question : for the text While each of the city descriptions is relatively short, Calvino's writing is filled with rare words, complex syntactic structures, and figurative language. Capturing the essential components of each city in a single vector is thus not as simple as it is with more standard forms of text. Nevertheless, we hope that representations from language models trained over billions of words of text can extract some meaningful semantics from these descriptions. We experiment with three different pretrained representations: ELMo BIBREF5 , BERT BIBREF6 , and GloVe BIBREF18 . To produce a single city embedding, we compute the TF-IDF weighted element-wise mean of the token-level representations. For all pretrained methods, we additionally reduce the dimensionality of the city embeddings to 40 using PCA for increased compatibility with our clustering algorithm. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "15\n",
      "Question 1: What is the purpose of collecting human judgments in addition to comparing computational methods?\n",
      "\n",
      "Answer 1: The purpose of collecting human judgments is to further ground the results obtained from the computational methods. This provides an additional comparison point and helps to better understand the accuracy and effectiveness of the methods.\n",
      "Question : for the text While the results from the above section allow us to compare our three computational methods against each other, we additionally collect human judgments to further ground our results. In this section, we first describe our human experiment before quantitatively analyzing our results. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "16\n",
      "Question 1: What do the quantitative results suggest about vector-based city representations?\n",
      "\n",
      "Answer 1: The quantitative results suggest that while vector-based city representations capture some thematic similarities, there is much room for improvement.\n",
      "Question : for the text Our quantitative results suggest that while vector-based city representations capture some thematic similarities, there is much room for improvement. In this section, we first investigate whether the learned clusters provide evidence for any arguments put forth by literary critics on the novel. Then, we explore possible reasons that the learned clusters deviate from Calvino's. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "17\n",
      "What is the unique structure of the novel Invisible Cities?\n",
      "\n",
      "Answer 1: The novel Invisible Cities consists of 55 prose poems, each of which describes an imaginary city. Each city's description is short and self-contained, which allows for computational analysis of the novel's themes.\n",
      "Question : for the text Literary critics form interpretations of meaning in works of literature. Building computational models that can help form and test these interpretations is a fundamental goal of digital humanities research BIBREF0 . Within natural language processing, most previous work that engages with literature relies on “distant reading” BIBREF1 , which involves discovering high-level patterns from large collections of stories BIBREF2 , BIBREF3 . We depart from this trend by showing that computational techniques can also engage with literary criticism at a closer distance: concretely, we use recent advances in text representation learning to test a single literary theory about the novel Invisible Cities by Italo Calvino..Framed as a dialogue between the traveler Marco Polo and the emperor Kublai Khan, Invisible Cities consists of 55 prose poems, each of which describes an imaginary city. Calvino categorizes these cities into eleven thematic groups that deal with human emotions (e.g., desires, memories), general objects (eyes, sky, signs), and unusual properties (continuous, hidden, thin). Many critics argue that Calvino's labels are not meaningful, while others believe that there is a distinct thematic separation between the groups, including the author himself BIBREF4 . The unique structure of this novel — each city's description is short and self-contained (Figure FIGREF1 ) — allows us to computationally examine this debate..As the book is too small to train any models, we leverage recent advances in large-scale language model-based representations BIBREF5 , BIBREF6 to compute a representation of each city. We feed these representations into a clustering algorithm that produces exactly eleven clusters of five cities each and evaluate them against both Calvino's original labels and crowdsourced human judgments. While the overall correlation with Calvino's labels is low, both computers and humans can reliably identify some thematic groups associated with concrete objects..While prior work has computationally analyzed a single book BIBREF7 , our work goes beyond simple word frequency or n-gram counts by leveraging the power of pretrained language models to engage with literary criticism. Admittedly, our approach and evaluations are specific to Invisible Cities, but we believe that similar analyses of more conventionally-structured novels could become possible as text representation methods improve. We also highlight two challenges of applying computational methods to literary criticisms: (1) text representation methods are imperfect, especially when given writing as complex as Calvino's; and (2) evaluation is difficult because there is no consensus among literary critics on a single “correct” interpretation. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "18\n",
      "Question 1: What is the purpose of reviewing critical opinions on Calvino's thematic groups? \n",
      "Answer 1: The purpose of reviewing critical opinions is to contextualize and analyze the validity of Calvino's thematic groups in characterizing his city descriptions.\n",
      "Question : for the text Before describing our method and results, we first review critical opinions on both sides of whether Calvino's thematic groups meaningfully characterize his city descriptions. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "19\n",
      "Question 1: Which city representation method yields the highest purity among the three methods?\n",
      "\n",
      "Answer 1: The ELMo city representation method yields the highest purity among the three methods.\n",
      "Question : for the text We compare clusters computed on different representations using community purity; additionally, we compare these computational methods to humans by their accuracy on the odd-one-out task..City representations computed using language model-based representation (ELMo and BERT) achieve significantly higher purity than a clustering induced from random representations, indicating that there is at least some meaningful coherence to Calvino's thematic groups (first row of Table TABREF11 ). ELMo representations yield the highest purity among the three methods, which is surprising as BERT is a bigger model trained on data from books (among other domains). Both ELMo and BERT outperform GloVe, which intuitively makes sense because the latter do not model the order or structure of the words in each description..While the purity of our methods is higher than that of a random clustering, it is still far below 1. To provide additional context to these results, we now switch to our “odd-one-out” task and compare directly to human performance. For each triplet of cities, we identify the intruder as the city with the maximum Euclidean distance from the other two. Interestingly, crowd workers achieve only slightly higher accuracy than ELMo city representations; their interannotator agreement is also low, which indicates that close reading to analyze literary coherence between multiple texts is a difficult task, even for human annotators. Overall, results from both computational and human approaches suggests that the author-assigned labels are not entirely arbitrary, as we can reliably recover some of the thematic groups. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "20\n",
      "Question 1: What are some limitations of previous computational literary analysis techniques?\n",
      "\n",
      "Answer 1: Previous computational literary analysis techniques have largely relied on word counting, topic modeling, and naive Bayes classifiers, and are not able to capture the meaning of sentences or paragraphs. They have discovered general patterns from multiple literary works, but are not able to engage with specific literary criticism about a single narrative.\n",
      "Question : for the text Most previous work within the NLP community applies distant reading BIBREF1 to large collections of books, focusing on modeling different aspects of narratives such as plots and event sequences BIBREF22 , BIBREF23 , BIBREF24 , BIBREF25 , characters BIBREF2 , BIBREF26 , BIBREF27 , BIBREF28 , and narrative similarity BIBREF3 . In the same vein, researchers in computational literary analysis have combined statistical techniques and linguistics theories to perform quantitative analysis on large narrative texts BIBREF29 , BIBREF30 , BIBREF31 , BIBREF32 , BIBREF33 , but these attempts largely rely on techniques such as word counting, topic modeling, and naive Bayes classifiers and are therefore not able to capture the meaning of sentences or paragraphs BIBREF34 . While these works discover general patterns from multiple literary works, we are the first to use cutting-edge NLP techniques to engage with specific literary criticism about a single narrative..There has been other computational work that focuses on just a single book or a small number of books, much of it focused on network analysis: BIBREF35 extract character social networks from Alice in Wonderland, while BIBREF36 recover social networks from 19th century British novels. BIBREF37 disentangles multiple narrative threads within the novel Infinite Jest, while BIBREF7 provides several automated statistical methods for close reading and test them on the award-winning novel Cloud Atlas (2004). Compared to this work, we push further on modeling the content of the narrative by leveraging pretrained language models. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "21\n",
      "What was the accuracy drop after removing residual connections and hierarchical stack in the CMRD model?\n",
      "The accuracy drop was 1.95%.\n",
      "Question : for the text To prove the effectiveness of our structure of the Conditional Memory Relation Decoder (CMRD), we conduct ablation experiments on the WoZ2.0 dataset. We observe an accuracy drop of 1.95% after removing residual connections and the hierarchical stack of our attention modules. This proves the effectiveness of our hierarchical attention design. After the MLP is replaced with a linear layer of hidden size 512 and the ReLU activation function, the accuracy further drops by 3.45%. This drop is partly due to the reduction of the number of the model parameters, but it also proves that stacking more layers in an MLP can improve the relational reasoning performance given a concatenation of multiple representations from different sources..We also conduct the ablation study on the MultiWoZ dataset for a more precise analysis on the hierarchical generation process. For joint domain accuracy, we calculate the probability that all domains generated in each turn are exactly matched with the labels provided. The joint domain-slot accuracy further calculate the probability that all domains and slots generated are correct, while the joint goal accuracy requires all the domains, slots and values generated are exactly matched with the labels. From abm, We can further calculate that given the correct slot prediction COMER has 83.52% chance to make the correct value prediction. While COMER has done great job on domain prediction (95.53%) and value prediction (83.52%), the accuracy of the slot prediction given the correct domain is only 57.30%. We suspect that this is because we only use the previous belief state to represent the dialogue history, and the inter-turn reasoning ability on the slot prediction suffers from the limited context and the accuracy is harmed due to the multi-turn mapping problem BIBREF4 . We can also see that the JDS Acc. has an absolute boost of 5.48% when we switch from the combined slot representation to the nested tuple representation. This is because the subordinate relationship between the domains and the slots can be captured by the hierarchical sequence generation, while this relationship is missed when generating the domain and slot together via the combined slot representation. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "22\n",
      "Question 1: What is the main advantage of the Conditional Memory Relation Network (COMER) compared to other dialogue state tracking models?\n",
      "Answer 1: The main advantage of COMER is that it has a constant inference time complexity with respect to the number of domains, slots, and values pre-defined in an ontology, making it highly scalable. Additionally, its joint goal accuracy is comparable to the state-of-the-arts on both the MultiWoZ dataset and the WoZ dataset.\n",
      "Question : for the text In this work, we proposed the Conditional Memory Relation Network (COMER), the first dialogue state tracking model that has a constant inference time complexity with respect to the number of domains, slots and values pre-defined in an ontology. Besides its scalability, the joint goal accuracy of our model also achieve the similar performance compared with the state-of-the-arts on both the MultiWoZ dataset and the WoZ dataset. Due to the flexibility of our hierarchical encoder-decoder framework and the CMR decoder, abundant future research direction remains as applying the transformer structure, incorporating open vocabulary and copy mechanism for explicit unseen words generation, and inventing better dialogue history access mechanism to accommodate efficient inter-turn reasoning..Acknowledgements. This work is partly supported by NSF #1750063. We thank all the reviewers for their constructive suggestions. We also want to thank Zhuowen Tu and Shengnan Zhang for the early discussions of the project. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "23\n",
      "Question 1: What is CMRD and how does it work?\n",
      "\n",
      "Answer 1: CMRD stands for Conditional Memory Relation Decoder, which is a type of neural network model used for generating sequences of tokens based on different encoded information sources. It uses hierarchical memory access and relation reasoning under a given condition to generate the next token and hidden representation. The model has a shared parameter structure and can support additional memory sources by adding residual connections and attention blocks. The CMR decoder embeds the token with a fixed token embedding and encodes it with an LSTM, which is then combined with the condition representation and passed through an attention module and a Multi-Layer Perceptron. The hidden representation of the next token is fed into a dropout layer and a linear layer to generate the next token. The model can make predictions on a dynamic vocabulary and implicitly supports the generation of unseen words.\n",
      "Question : for the text Inspired by Residual Dense Networks BIBREF16 , End-to-End Memory Networks BIBREF17 and Relation Networks BIBREF18 , we here propose the Conditional Memory Relation Decoder (CMRD). Given a token embedding, $\\mathbf {e}_x$ , CMRD outputs the next token, $s$ , and the hidden representation, $h_s$ , with the hierarchical memory access of different encoded information sources, $H_B$ , $H_a$ , $H_u$ , and the relation reasoning under a certain given condition $\\mathbf {c}$ , $\n",
      "\\mathbf {s}, \\mathbf {h}_s= \\textrm {CMRD}(\\mathbf {e}_x, \\mathbf {c}, H_B, H_a, H_u),\n",
      "$ .the final output matrices $S,H_s \\in R^{l_s\\times d_m}$ are concatenations of all generated $\\mathbf {s}$ and $\\mathbf {h}_s$ (respectively) along the sequence length dimension, where $d_m$ is the model size, and $l_s$ is the generated sequence length. The general structure of the CMR decoder is shown in Figure 4 . Note that the CMR decoder can support additional memory sources by adding the residual connection and the attention block, but here we only show the structure with three sources: belief state representation ( $H_B$ ), system transcript representation ( $H_a$ ), and user utterance representation ( $H_u$ ), corresponding to a dialogue state tracking scenario. Since we share the parameters between all of the decoders, thus CMRD is actually a 2-dimensional auto-regressive model with respect to both the condition generation and the sequence generation task..At each time step $t$ , the CMR decoder first embeds the token $x_t$ with a fixed token embedding $E\\in R^{d_e\\times d_v}$ , where $d_e$ is the embedding size and $d_v$ is the vocabulary size. The initial token $x_0$ is “[CLS]\". The embedded vector $\\textbf {e}_{x_t}$ is then encoded with an LSTM, which emits a hidden representation $\\textbf {h}_0 \\in R^{d_m}$ , $\n",
      "\\textbf {h}_0= \\textrm {LSTM}(\\textbf {e}_{x_t},\\textbf {q}_{t-1}).\n",
      "$ .where $\\textbf {q}_t$ is the hidden state of the LSTM. $\\textbf {q}_0$ is initialized with an average of the hidden states of the belief encoder, the system encoder and the user encoder which produces $H_B$ , $H_a$ , $H_u$ respectively.. $\\mathbf {h}_0$ is then summed (element-wise) with the condition representation $\\mathbf {c}\\in R^{d_m}$ to produce $\\mathbf {h}_1$ , which is (1) fed into the attention module; (2) used for residual connection; and (3) concatenated with other $\\mathbf {h}_i$ , ( $i>1$ ) to produce the concatenated working memory, $\\mathbf {r_0}$ , for relation reasoning, $\n",
      "\\mathbf {h}_1 & =\\mathbf {h}_0+\\mathbf {c},\\\\\n",
      "\\mathbf {h}_2 & =\\mathbf {h}_1+\\text{Attn}_{\\text{belief}}(\\mathbf {h}_1,H_e),\\\\\n",
      "\\mathbf {h}_3 & = \\mathbf {h}_2+\\text{Attn}_{\\text{sys}}(\\mathbf {h}_2,H_a),\\\\\n",
      "\\mathbf {h}_4 & = \\mathbf {h}_3+\\text{Attn}_{\\text{usr}}(\\mathbf {h}_3,H_u),\\\\\n",
      "\\mathbf {r} & = \\mathbf {h}_1\\oplus \\mathbf {h}_2\\oplus \\mathbf {h}_3\\oplus \\mathbf {h}_4 \\in R^{4d_m},\n",
      "$ . where $\\text{Attn}_k$ ( $k\\in \\lbrace  \\text{belief}, \\text{sys},\\text{usr}\\rbrace $ ) are the attention modules applied respectively to $H_B$ , $H_a$ , $H_u$ , and $\\oplus $ means the concatenation operator. The gradients are blocked for $ \\mathbf {h}_1,\\mathbf {h}_2,\\mathbf {h}_3$ during the back-propagation stage, since we only need them to work as the supplementary memories for the relation reasoning followed..The attention module takes a vector, $\\mathbf {h}\\in R^{d_m}$ , and a matrix, $H\\in R^{d_m\\times l}$ as input, where $l$ is the sequence length of the representation, and outputs $\\mathbf {h}_a$ , a weighted sum of the column vectors in $H$ . $\n",
      "\\mathbf {a} & =W_1^T\\mathbf {h}+\\mathbf {b}_1& &\\in R^{d_m},\\\\\n",
      "\\mathbf {c} &=\\text{softmax}(H^Ta)& &\\in R^l,\\\\\n",
      "\\mathbf {h} &=H\\mathbf {c}& &\\in R^{d_m},\\\\\n",
      "\\mathbf {h}_a &=W_2^T\\mathbf {h}+\\mathbf {b}_2& &\\in R^{d_m},\n",
      "$ . where the weights $W_1\\in R^{d_m \\times d_m}$ , $W_2\\in R^{d_m \\times d_m}$ and the bias $b_1\\in R^{d_m}$ , $b_2\\in R^{d_m}$ are the learnable parameters..The order of the attention modules, i.e., first attend to the system and the user and then the belief, is decided empirically. We can interpret this hierarchical structure as the internal order for the memory processing, since from the daily life experience, people tend to attend to the most contemporary memories (system/user utterance) first and then attend to the older history (belief states). All of the parameters are shared between the attention modules..The concatenated working memory, $\\mathbf {r}_0$ , is then fed into a Multi-Layer Perceptron (MLP) with four layers, $\n",
      "\\mathbf {r}_1 & =\\sigma (W_1^T\\mathbf {r}_0+\\mathbf {b}_1),\\\\\n",
      "\\mathbf {r}_2 & =\\sigma (W_2^T\\mathbf {r}_1+\\mathbf {b}_2),\\\\\n",
      "\\mathbf {r}_3 & = \\sigma (W_3^T\\mathbf {r}_2+\\mathbf {b}_3),\\\\\n",
      "\\mathbf {h}_s & = \\sigma (W_4^T\\mathbf {r}_3+\\mathbf {b}_4),\n",
      "$ . where $\\sigma $ is a non-linear activation, and the weights $W_1 \\in R^{4d_m \\times d_m}$ , $W_i \\in R^{d_m \\times d_m}$ and the bias $b_1 \\in R^{d_m}$ , $b_i \\in R^{d_m}$ are learnable parameters, and $2\\le i\\le 4$ . The number of layers for the MLP is decided by the grid search..The hidden representation of the next token, $\\mathbf {h}_s$ , is then (1) emitted out of the decoder as a representation; and (2) fed into a dropout layer with drop rate $p$ , and a linear layer to generate the next token, $\n",
      "\\mathbf {h}_k & =\\text{dropout}(\\mathbf {h}_s)& &\\in R^{d_m},\\\\\n",
      "\\mathbf {h}_o & =W_k^T\\mathbf {h}_k+\\mathbf {b}_k& &\\in R^{d_e},\\\\\n",
      "\\mathbf {p}_s & =\\text{softmax}(E^T\\mathbf {h}_o)& &\\in R^{d_v},\\\\\n",
      "s & =\\text{argmax}(\\mathbf {p}_s)& &\\in R,\n",
      "$ . where the weight $W_k\\in R^{d_m \\times d_e}$ and the bias $b_k\\in R^{d_e}$ are learnable parameters. Since $d_e$ is the embedding size and the model parameters are independent of the vocabulary size, the CMR decoder can make predictions on a dynamic vocabulary and implicitly supports the generation of unseen words. When training the model, we minimize the cross-entropy loss between the output probabilities, $\\mathbf {p}_s$ , and the given labels. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "24\n",
      "Question 1: What is the purpose of the encoder in the system?\n",
      "Answer 1: The purpose of the encoder is to obtain contextual embeddings for the user utterance, system action, and static embeddings for the previous belief state. It feeds each of them into a Bidirectional LSTM to obtain the final representation.\n",
      "Question : for the text Let $X$ represent a user utterance or system transcript consisting of a sequence of words $\\lbrace w_1,\\ldots ,w_T\\rbrace $ . The encoder first passes the sequence $\\lbrace \\mathit {[CLS]},w_1,\\ldots ,w_T,\\mathit {[SEP]}\\rbrace $ into a pre-trained BERT model and obtains its contextual embeddings $E_{X}$ . Specifically, we leverage the output of all layers of BERT and take the average to obtain the contextual embeddings..For each domain/slot appeared in the training set, if it has more than one word, such as `price range', `leave at', etc., we feed it into BERT and take the average of the word vectors to form the extra slot embedding $E_{s}$ . In this way, we map each domain/slot to a fixed embedding, which allows us to generate a domain/slot as a whole instead of a token at each time step of domain/slot sequence decoding. We also construct a static vocabulary embedding $E_{v}$ by feeding each token in the BERT vocabulary into BERT. The final static word embedding $E$ is the concatenation of the $E_{v}$ and $E_{s}$ ..After we obtain the contextual embeddings for the user utterance, system action, and the static embeddings for the previous belief state, we feed each of them into a Bidirectional LSTM BIBREF15 . .$$\\begin{aligned}\n",
      "\\mathbf {h}_{a_t} & = \\textrm {BiLSTM}(\\mathbf {e}_{X_{a_t}}, \\mathbf {h}_{a_{t-1}}) \\\\\n",
      "\\mathbf {h}_{u_t} & = \\textrm {BiLSTM}(\\mathbf {e}_{X_{u_t}}, \\mathbf {h}_{u_{t-1}}) \\\\\n",
      "\\mathbf {h}_{b_t} & = \\textrm {BiLSTM}(\\mathbf {e}_{X_{b_t}}, \\mathbf {h}_{b_{t-1}}) \\\\\n",
      "\\mathbf {h}_{a_0} & = \\mathbf {h}_{u_0} = \\mathbf {h}_{b_0} = c_{0}, \\\\\n",
      "\\end{aligned}$$   (Eq. 7) .where $c_{0}$ is the zero-initialized hidden state for the BiLSTM. The hidden size of the BiLSTM is $d_m/2$ . We concatenate the forward and the backward hidden representations of each token from the BiLSTM to obtain the token representation $\\mathbf {h}_{k_t}\\in R^{d_m}$ , $k\\in \\lbrace a,u,b\\rbrace $ at each time step $t$ . The hidden states of all time steps are concatenated to obtain the final representation of $H_{k}\\in R^{T \\times d_m}, k \\in \\lbrace a,u,B\\rbrace $ . The parameters are shared between all of the BiLSTMs. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "25\n",
      "Question 1: What is the theoretical Inference Time Multiplier (ITM) used as a metric of scalability and how is it calculated?\n",
      "\n",
      "Answer 1: The theoretical Inference Time Multiplier (ITM) measures how much slower a model will perform when transferred from the WoZ2.0 dataset to the MultiWoZ dataset. It is calculated using the equation K= h(t)h(s)h(n)h(m), where h(x) is a function that returns 1 if the Inference Time Complexity (ITC) of variable x is O(1), or returns the ratio of x's size in the MultiWoZ dataset to its size in the WoZ2.0 dataset if the ITC of x is not O(1). The ITM is used as a metric of scalability to compare DST models for different datasets.\n",
      "Question : for the text We first test our model on the single domain dataset, WoZ2.0 BIBREF19 . It consists of 1,200 dialogues from the restaurant reservation domain with three pre-defined slots: food, price range, and area. Since the name slot rarely occurs in the dataset, it is not included in our experiments, following previous literature BIBREF3 , BIBREF20 . Our model is also tested on the multi-domain dataset, MultiWoZ BIBREF9 . It has a more complex ontology with 7 domains and 25 predefined slots. Since the combined slot-value pairs representation of the belief states has to be applied for the model with $O(n)$ ITC, the total number of slots is 35. The statistics of these two datsets are shown in Table 2 ..Based on the statistics from these two datasets, we can calculate the theoretical Inference Time Multiplier (ITM), $K$ , as a metric of scalability. Given the inference time complexity, ITM measures how many times a model will be slower when being transferred from the WoZ2.0 dataset, $d_1$ , to the MultiWoZ dataset, $d_2$ , $\n",
      "K= h(t)h(s)h(n)h(m)\\\\\n",
      "$ $\n",
      "h(x)=\\left\\lbrace \n",
      "\\begin{array}{lcl}\n",
      "1 & &O(x)=O(1),\\\\\n",
      "\\frac{x_{d_2}}{x_{d_1}}& & \\text{otherwise},\\\\\n",
      "\\end{array}\\right.\n",
      "\n",
      "$ .where $O(x)$ means the Inference Time Complexity (ITC) of the variable $x$ . For a model having an ITC of $O(1)$ with respect to the number of slots $n$ , and values $m$ , the ITM will be a multiplier of 2.15x, while for an ITC of $O(n)$ , it will be a multiplier of 25.1, and 1,143 for $O(mn)$ ..As a convention, the metric of joint goal accuracy is used to compare our model to previous work. The joint goal accuracy only regards the model making a successful belief state prediction if all of the slots and values predicted are exactly matched with the labels provided. This metric gives a strict measurement that tells how often the DST module will not propagate errors to the downstream modules in a dialogue system. In this work, the model with the highest joint accuracy on the validation set is evaluated on the test set for the test joint accuracy measurement. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "26\n",
      "Question 1: What is the unique approach taken by COMER for multi-label state prediction in dialogues?\n",
      "\n",
      "Answer 1: COMER adapts the task into a sequence generation problem via a Seq2Seq framework and uses a novel Conditional Memory Relation Decoder (CMRD) for sequence decoding instead of formulating it as a collection of binary prediction problems.\n",
      "Question : for the text Given a dialogue $D$ which consists of $T$ turns of user utterances and system actions, our target is to predict the state at each turn. Different from previous methods which formulate multi-label state prediction as a collection of binary prediction problems, COMER adapts the task into a sequence generation problem via a Seq2Seq framework..As shown in f3, COMER consists of three encoders and three hierarchically stacked decoders. We propose a novel Conditional Memory Relation Decoder (CMRD) for sequence decoding. Each encoder includes an embedding layer and a BiLSTM. The encoders take in the user utterance, the previous system actions, and the previous belief states at the current turn, and encodes them into the embedding space. The user encoder and the system encoder use the fixed BERT model as the embedding layer..Since the slot value pairs are un-ordered set elements of a domain in the belief states, we first order the sequence of domain according to their frequencies as they appear in the training set BIBREF14 , and then order the slot value pairs in the domain according to the slot's frequencies of as they appear in a domain. After the sorting of the state elements, We represent the belief states following the paradigm: (Domain1- Slot1, Value1; Slot2, Value2; ... Domain2- Slot1, Value1; ...) for a more concise representation compared with the nested tuple representation..All the CMRDs take the same representations from the system encoder, user encoder and the belief encoder as part of the input. In the procedure of hierarchical sequence generation, the first CMRD takes a zero vector for its condition input $\\mathbf {c}$ , and generates a sequence of the domains, $D$ , as well as the hidden representation of domains $H_D$ . For each $d$ in $D$ , the second CMRD then takes the corresponding $h_d$ as the condition input and generates the slot sequence $S_d$ , and representations, $H_{S,d}$ . Then for each $s$ in $S$ , the third CMRD generates the value sequence $D$0 based on the corresponding $D$1 . We update the belief state with the new $D$2 pairs and perform the procedure iteratively until a dialogue is completed. All the CMR decoders share all of their parameters..Since our model generates domains and slots instead of taking pre-defined slots as inputs, and the number of domains and slots generated each turn is only related to the complexity of the contents covered in a specific dialogue, the inference time complexity of COMER is $O(1)$ with respect to the number of pre-defined slots and values. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "27\n",
      "What is the model size used in the experiments?\n",
      "\n",
      "The model size used in the experiments is $d_m=512$.\n",
      "Question : for the text We use the $\\text{BERT}_\\text{large}$ model for both contextual and static embedding generation. All LSTMs in the model are stacked with 2 layers, and only the output of the last layer is taken as a hidden representation. ReLU non-linearity is used for the activation function, $\\sigma $ ..The hyper-parameters of our model are identical for both the WoZ2.0 and the MultiwoZ datasets: dropout rate $p=0.5$ , model size $d_m=512$ , embedding size $d_e=1024$ . For training on WoZ2.0, the model is trained with a batch size of 32 and the ADAM optimizer BIBREF21 for 150 epochs, while for MultiWoZ, the AMSGrad optimizer BIBREF22 and a batch size of 16 is adopted for 15 epochs of training. For both optimizers, we use a learning rate of 0.0005 with a gradient clip of 2.0. We initialize all weights in our model with Kaiming initialization BIBREF23 and adopt zero initialization for the bias. All experiments are conducted on a single NVIDIA GTX 1080Ti GPU. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "28\n",
      "What is the proposed approach for dialogue state tracking in this work?\n",
      "The proposed approach for dialogue state tracking in this work is to formulate it as a sequence generation problem and use the COnditional MEmory Relation Network (COMER), which is a hierarchically stacked encoder-decoder network that generates slot sequences and corresponding value sequences. The model applies BERT contextualized word embeddings and BPE for sequence encoding to ensure the uniqueness of the representations of unseen words. The model is scalable and has a constant inference time complexity.\n",
      "Question : for the text A Dialogue State Tracker (DST) is a core component of a modular task-oriented dialogue system BIBREF7 . For each dialogue turn, a DST module takes a user utterance and the dialogue history as input, and outputs a belief estimate of the dialogue state. Then a machine action is decided based on the dialogue state according to a dialogue policy module, after which a machine response is generated..Traditionally, a dialogue state consists of a set of requests and joint goals, both of which are represented by a set of slot-value pairs (e.g. (request, phone), (area, north), (food, Japanese)) BIBREF8 . In a recently proposed multi-domain dialogue state tracking dataset, MultiWoZ BIBREF9 , a representation of dialogue state consists of a hierarchical structure of domain, slot, and value is proposed. This is a more practical scenario since dialogues often include multiple domains simultaneously..Many recently proposed DSTs BIBREF2 , BIBREF10 are based on pre-defined ontology lists that specify all possible slot values in advance. To generate a distribution over the candidate set, previous works often take each of the slot-value pairs as input for scoring. However, in real-world scenarios, it is often not practical to enumerate all possible slot value pairs and perform scoring from a large dynamically changing knowledge base BIBREF11 . To tackle this problem, a popular direction is to build a fixed-length candidate set that is dynamically updated throughout the dialogue development. cpt briefly summaries the inference time complexity of multiple state-of-the-art DST models following this direction. Since the inference complexity of all of previous model is at least proportional to the number of the slots, these models will struggle to scale to multi-domain datasets with much larger numbers of pre-defined slots..In this work, we formulate the dialogue state tracking task as a sequence generation problem, instead of formulating the task as a pair-wise prediction problem as in existing work. We propose the COnditional MEmory Relation Network (COMER), a scalable and accurate dialogue state tracker that has a constant inference time complexity. .Specifically, our model consists of an encoder-decoder network with a hierarchically stacked decoder to first generate the slot sequences in the belief state and then for each slot generate the corresponding value sequences. The parameters are shared among all of our decoders for the scalability of the depth of the hierarchical structure of the belief states. COMER applies BERT contextualized word embeddings BIBREF12 and BPE BIBREF13 for sequence encoding to ensure the uniqueness of the representations of the unseen words. The word embeddings for sequence generation are initialized and fixed with the static word embeddings generated from BERT to have the potential of generating unseen words. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "29\n",
      "Question 1: What is the issue with the single value assumption in dialogue state tracking datasets? \n",
      "\n",
      "Answer 1: The single value assumption in dialogue state tracking datasets means that the belief state assumes a slot can only be mapped to a single value in a dialogue turn, which can lead to fallacies. For example, in the WoZ2.0 dataset, the belief state label of (food, seafood) would make it impossible for the downstream module to generate responses about Chinese restaurants, when a correct representation of the belief state would be (food, seafood $>$ chinese) to prioritize the retrieval of information about seafood and then Chinese restaurants.\n",
      "Question : for the text f1 shows a multi-domain dialogue in which the user wants the system to first help book a train and then reserve a hotel. For each turn, the DST will need to track the slot-value pairs (e.g. (arrive by, 20:45)) representing the user goals as well as the domain that the slot-value pairs belongs to (e.g. train, hotel). Instead of representing the belief state via a hierarchical structure, one can also combine the domain and slot together to form a combined slot-value pair (e.g. (train; arrive by, 20:45) where the combined slot is “train; arrive by\"), which ignores the subordination relationship between the domain and the slots..A typical fallacy in dialogue state tracking datasets is that they make an assumption that the slot in a belief state can only be mapped to a single value in a dialogue turn. We call this the single value assumption. Figure 2 shows an example of this fallacy from the WoZ2.0 dataset: Based on the belief state label (food, seafood), it will be impossible for the downstream module in the dialogue system to generate sample responses that return information about Chinese restaurants. A correct representation of the belief state could be (food, seafood $>$ chinese). This would tell the system to first search the database for information about seafood and then Chinese restaurants. The logical operator “ $>$ \" indicates which retrieved information should have a higher priority to be returned to the user. Thus we are interested in building DST modules capable of generating structured sequences, since this kind of sequence representation of the value is critical for accurately capturing the belief states of a dialogue. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "30\n",
      "What information can be derived from the system attention in the MultiWoZ test set dialogue?\n",
      "The system attention in the MultiWoZ test set dialogue can only find information indicating the slot \"departure\" from the system utterance under the domain condition, and attend to the evidence \"leaving\" correctly during the generation step of \"departure\" as it is the first attention module and has no previous context information.\n",
      "Question : for the text f5 shows an example of the belief state prediction result in one turn of a dialogue on the MultiWoZ test set. The visualization includes the CMRD attention scores over the belief states, system transcript and user utterance during the decoding stage of the slot sequence..From the system attention (top right), since it is the first attention module and no previous context information is given, it can only find the information indicating the slot “departure” from the system utterance under the domain condition, and attend to the evidence “leaving” correctly during the generation step of “departure”. From the user attention, we can see that it captures the most helpful keywords that are necessary for correct prediction, such as “after\" for “day\" and “leave at”, “to\" for “destination\". Moreover, during the generation step of “departure”, the user attention successfully discerns that, based on the context, the word “leave” is not the evidence that need to be accumulated and choose to attend nothing in this step. For the belief attention, we can see that the belief attention module correctly attends to a previous slot for each generation step of a slot that has been presented in the previous state. For the generation step of the new slot “destination\", since the previous state does not have the “destination\" slot, the belief attention module only attends to the `-' mark after the `train' domain to indicate that the generated word should belong to this domain. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "31\n",
      "What is the advantage of the StateNet approach in dialogue state tracking? \n",
      "\n",
      "Answer 1: The advantage of the StateNet approach in dialogue state tracking is that its parameters are independent of the number of slot values in the candidate set, and it also supports online training or inference with dynamically changing slots and values. Additionally, given a slot that needs tracking, it only needs to perform inference once to make the prediction for a turn. However, its inference time complexity is proportional to the number of slots.\n",
      "Question : for the text Semi-scalable Belief Tracker BIBREF1 proposed an approach that can generate fixed-length candidate sets for each of the slots from the dialogue history. Although they only need to perform inference for a fixed number of values, they still need to iterate over all slots defined in the ontology to make a prediction for a given dialogue turn. In addition, their method needs an external language understanding module to extract the exact entities from a dialogue to form candidates, which will not work if the label value is an abstraction and does not have the exact match with the words in the dialogue..StateNet BIBREF3 achieves state-of-the-art performance with the property that its parameters are independent of the number of slot values in the candidate set, and it also supports online training or inference with dynamically changing slots and values. Given a slot that needs tracking, it only needs to perform inference once to make the prediction for a turn, but this also means that its inference time complexity is proportional to the number of slots..TRADE BIBREF4 achieves state-of-the-art performance on the MultiWoZ dataset by applying the copy mechanism for the value sequence generation. Since TRADE takes $n$ combinations of the domains and slots as the input, the inference time complexity of TRADE is $O(n)$ . The performance improvement achieved by TRADE is mainly due to the fact that it incorporates the copy mechanism that can boost the accuracy on the ‘name’ slot, which mainly needs the ability in copying names from the dialogue history. However, TRADE does not report its performance on the WoZ2.0 dataset which does not have the ‘name’ slot..DSTRead BIBREF6 formulate the dialogue state tracking task as a reading comprehension problem by asking slot specified questions to the BERT model and find the answer span in the dialogue history for each of the pre-defined combined slot. Thus its inference time complexity is still $O(n)$ . This method suffers from the fact that its generation vocabulary is limited to the words occurred in the dialogue history, and it has to do a manual combination strategy with another joint state tracking model on the development set to achieve better performance..Contextualized Word Embedding (CWE) was first proposed by BIBREF25 . Based on the intuition that the meaning of a word is highly correlated with its context, CWE takes the complete context (sentences, passages, etc.) as the input, and outputs the corresponding word vectors that are unique under the given context. Recently, with the success of language models (e.g. BIBREF12 ) that are trained on large scale data, contextualizeds word embedding have been further improved and can achieve the same performance compared to (less flexible) finely-tuned pipelines..Sequence Generation Models. Recently, sequence generation models have been successfully applied in the realm of multi-label classification (MLC) BIBREF14 . Different from traditional binary relevance methods, they proposed a sequence generation model for MLC tasks which takes into consideration the correlations between labels. Specifically, the model follows the encoder-decoder structure with an attention mechanism BIBREF26 , where the decoder generates a sequence of labels. Similar to language modeling tasks, the decoder output at each time step will be conditioned on the previous predictions during generation. Therefore the correlation between generated labels is captured by the decoder. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "32\n",
      "What is the actual inference time multiplier of the model?\n",
      "\n",
      "The actual inference time multiplier of the model is 2.54 as calculated by evaluating the runtime of the best-performing models on the validation sets of both the WoZ2.0 and MultiWoZ datasets. The runtime on the validation set of WoZ2.0 is 65.6 seconds, while on MultiWoZ, the runtime is 835.2 seconds, which is 5 times larger than that of WoZ2.0.\n",
      "Question : for the text To measure the actual inference time multiplier of our model, we evaluate the runtime of the best-performing models on the validation sets of both the WoZ2.0 and MultiWoZ datasets. During evaluation, we set the batch size to 1 to avoid the influence of data parallelism and sequence padding. On the validation set of WoZ2.0, we obtain a runtime of 65.6 seconds, while on MultiWoZ, the runtime is 835.2 seconds. Results are averaged across 5 runs. Considering that the validation set of MultiWoZ is 5 times larger than that of WoZ2.0, the actual inference time multiplier is 2.54 for our model. Since the actual inference time multiplier roughly of the same magnitude as the theoretical value of 2.15, we can confirm empirically that we have the $O(1)$ inference time complexity and thus obtain full scalability to the number of slots and values pre-defined in an ontology..c compares our model with the previous state-of-the-art on both the WoZ2.0 test set and the MultiWoZ test set. For the WoZ2.0 dataset, we maintain performance at the level of the state-of-the-art, with a marginal drop of 0.3% compared with previous work. Considering the fact that WoZ2.0 is a relatively small dataset, this small difference does not represent a significant big performance drop. On the muli-domain dataset, MultiWoZ, our model achieves a joint goal accuracy of 45.72%, which is significant better than most of the previous models other than TRADE which applies the copy mechanism and gains better generalization ability on named entity coping. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "33\n",
      "What is the significance of the experiment's results on the debate about connectionist networks' ability to learn compositional solutions?\n",
      "\n",
      "The results of the zero-shot generalization experiments with novel lengths and words provide evidence against a \"memorize-and-interpolate\" account of deep neural networks, suggesting that the GRU and LSTM architectures exhibit at least basic forms of compositional generalization. This is significant in the decades-long debate on connectionist networks' ability to learn compositional solutions.\n",
      "Question : for the text We established that our Siamese recurrent networks (with SRN, GRU or LSTM cells) are able to recognize logical entailment relations without any a priori cues about syntax or semantics of the input expressions. Indeed, some of the recurrent set-ups even outperform tree-shaped networks, whose topology is specifically designed to deal with such tasks. This indicates that recurrent networks can develop representations that can adequately process a formal language with a nontrivial hierarchical structure. The formal language we defined did not exploit the full expressive power of first-order predicate logic; nevertheless by using standard first-order predicate logic, a standard theorem prover, and a set-up where the training set only covers a tiny fraction of the space of possible logical expressions, our experiments avoid the problems observed in earlier attempts to demonstrate logical reasoning in recurrent networks..The experiments performed in the last few sections moreover show that the GRU and LSTM architectures exhibit at least basic forms of compositional generalization. In particular, the results of the zero-shot generalization experiments with novel lengths and novel words cannot be explained with a `memorize-and-interpolate' account, i.e. an account of the working of deep neural networks that assumes all they do is store enormous training sets and generalize only locally. These results are relevant pieces of evidence in the decades-long debate on whether or not connectionist networks are fundamentally able to learn compositional solutions. Although we do not have the illusion that our work will put this debate to an end, we hope that it will help bring deep learning enthusiasts and skeptics a small step closer. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "34\n",
      "What is the main question surrounding state-of-the-art models for natural language processing tasks based on deep neural networks?\n",
      "The main question is to what extent these models have learned compositional generalizations that characterize language or if they rely on storing massive amounts of exemplars and only make 'local' generalizations.\n",
      "Question : for the text State-of-the-art models for almost all popular natural language processing tasks are based on deep neural networks, trained on massive amounts of data. A key question that has been raised in many different forms is to what extent these models have learned the compositional generalizations that characterize language, and to what extent they rely on storing massive amounts of exemplars and only make `local' generalizations BIBREF0 , BIBREF1 , BIBREF2 , BIBREF3 , BIBREF4 , BIBREF5 , BIBREF6 , BIBREF7 . This question has led to (sometimes heated) debates between deep learning enthusiasts that are convinced neural networks can do almost anything, and skeptics that are convinced some types of generalization are fundamentally beyond reach for deep learning systems, pointing out that crucial tests distinguishing between generalization and memorization have not been applied..In this paper, we take a pragmatic perspective on these issues. As the target for learning we use entailment relations in an artificial language, defined using first order logic (FOL), that is unambiguously compositional. We ask whether popular deep learning methods are capable in principle of acquiring the compositional rules that characterize it, and focus in particular on recurrent neural networks that are unambiguously `connectionist': trained recurrent nets do not rely on symbolic data and control structures such as trees and global variable binding, and can straightforwardly be implemented in biological networks BIBREF8 or neuromorphic hardware BIBREF9 . We report positive results on this challenge, and in the process develop a series of tests for compositional generalization that address the concerns of deep learning skeptics..The paper makes three main contributions. First, we develop a protocol for automatically generating data that can be used in entailment recognition tasks. Second, we demonstrate that several deep learning architectures succeed at one such task. Third, we present and apply a number of experiments to test whether models are capable of compositional generalization. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "35\n",
      "Question 1: What is the main model used in the study?\n",
      "\n",
      "Answer 1: The main model used in the study is a recurrent network, which is a \"Siamese\" network that uses the same parameters to process the left and right sentence. It consists of a comparison layer and a classification layer, and a softmax function is applied to determine the most probable target class. The comparison layer takes the concatenation of two sentence vectors as input, and the number of cells equals the number of words, so it differs per sentence. The set-up resembles the Siamese architecture for learning sentence similarity and the LSTM classifier described in previous studies.\n",
      "Question : for the text Our main model is a recurrent network, sketched in Figure 4 . It is a so-called `Siamese' network because it uses the same parameters to process the left and the right sentence. The upper part of the model is identical to BIBREF13 's recursive networks. It consists of a comparison layer and a classification layer, after which a softmax function is applied to determine the most probable target class. The comparison layer takes the concatenation of two sentence vectors as input. The number of cells equals the number of words, so it differs per sentence..Our set-up resembles the Siamese architecture for learning sentence similarity of BIBREF25 and the LSTM classifier described in BIBREF18 . In the diagram, the dashed box indicates the location of an arbitrary recurrent unit. We consider SRN BIBREF26 , GRU BIBREF27 and LSTM BIBREF28 . generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "36\n",
      "Question 1: Did the simplest recurrent network, SRN, outperform the tree-shaped matrix model in both training and testing accuracy scores?\n",
      "\n",
      "Answer 1: Yes, according to Table UID18, the SRN achieved higher training and testing accuracy scores than the tree-shaped matrix model, and this result was consistent across the five different model runs.\n",
      "Question : for the text Training and testing accuracies after 50 training epochs, averaged over five different model runs, are shown in Table UID18 . All recurrent models outperform the summing baseline. Even the simplest recurrent network, the SRN, achieves higher training and testing accuracy scores than the tree-shaped matrix model. The GRU and LSTM even beat the tensor model. The LSTM obtains slightly lower scores than the GRU, which is unexpected given its more complex design, but perhaps the current challenge does not require separate forget and input gates. For more insight into the types of errors made by the best-performing (GRU-based) model, we refer to the confusion matrices in Appendix \"Error statistics\" ..The consistently higher testing accuracy provides evidence that the recurrent networks are not only capable of recognizing FOL entailment relations between unseen sentences. They can also outperform the tree-shaped models on this task, although they do not use any of the symbolic structure that seemed to explain the success of their recursive predecessors. The recurrent classifiers have learned to apply their own strategies, which we will investigate in the remainder of this paper. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "37\n",
      "Question 1: What is the difference between the artificial language used in this data generation process and the one used in BIBREF13?\n",
      "\n",
      "Answer 1: The language used in this data generation process is significantly more complex, and instead of natural logic, it uses FOL (First-order logic), whereas the language used in BIBREF13 only had a fixed background logic.\n",
      "Question : for the text The data generation process is inspired by BIBREF13 : an artificial language is defined, sentences are generated according to its grammar and the entailment relation between pairs of such sentences is established according to a fixed background logic. However, our language is significantly more complex, and instead of natural logic we use FOL. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "38\n",
      "Question 1: What is the difference in generalization capabilities between SRN, GRU, and LSTM models?\n",
      "\n",
      "Answer 1: The GRU and LSTM models are capable of generalizing to unseen sentence lengths 6 and 9 very well, while the SRN faces serious difficulties. The gates in the GRU and LSTM models appear to play a crucial role in this difference in generalization capabilities.\n",
      "Question : for the text We test if our recurrent models are capable of generalization to unseen lengths. Neural models are often considered incapable of such generalization, allegedly because they are limited to the training space BIBREF33 , BIBREF34 , BIBREF35 , BIBREF36 . We want to test if this is the case for the recurrent models studied in this paper. The language $\\mathcal {L}$ licenses a heavily constrained set of grammatical configurations, but it does allow the sentence length to vary according to the number of included negations. A perfectly compositional model should be able to interpret statements containing any number of negations, on condition that it has seen an instantiation at least once at each position where this is allowed..In a new experiment, we train the models on pairs of sentences with length 5, 7 or 8, and test on pairs of sentences with lengths 6 or 9. As before, the training and test sets contain some 30,000 and 5,000 sentence pairs, respectively. Results are shown in Table UID19 ..All recurrent models obtain (near-)perfect training accuracy scores. What happens on the test set is interesting. It turns out that the GRU and LSTM can generalize from lengths 5, 7 and 8 to 6 and 9 very well, while the SRN faces serious difficulties. It seems that training on lengths 5, 7 and 8, and thereby skipping length 6, enables the GRU and LSTM to generalize to unseen sentence lengths 6 and 9. Training on lengths 5-7 and testing on lengths 8-9 yields low test scores for all models. The GRU and LSTM gates appear to play a crucial role, because the results show that the SRN does not have this capacity at all. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "39\n",
      "What is the purpose of the final zero-shot learning experiment mentioned in the text?\n",
      "\n",
      "The purpose of the final zero-shot learning experiment is to assess the flexibility of the relational semantics that the networks have learned by replacing sets of nouns instead of single words, in order to establish compositional capacities.\n",
      "Question : for the text In the next experiment, we assess whether our GRU-based model, which performed best in the preceding experiments, is capable of zero-shot generalization to sentences with novel words. The current set-up cannot deal with unknown words, so instead of randomly initializing an embedding matrix that is updated during training, we use pretrained, 50-dimensional GloVe embeddings BIBREF37 that are kept constant. Using GloVe embeddings, the GRU model obtains a mean training accuracy of 100.0% and a testing accuracy of 95.9% (averaged over five runs). The best-performing model (with 100.0% training and 97.1% testing accuracy) is used in the following zero-shot experiments..One of the most basic relations on the level of lexical semantics is synonymy, which holds between words with equivalent meanings. In the language $\\mathcal {L}$ , a word can be substituted with one of its synonyms without altering the entailment relation assigned to the sentence pairs that contain it. If the GRU manages to perform well on such a modified data set after receiving the pretrained GloVe embedding of the unseen word, this is a first piece of evidence for its zero-shot generalization skills. We test this for several pairs of synonymous words. The best-performing GRU is first evaluated with respect to the fragment of the test data containing the original word $w$ , and consequently with respect to that same fragment after replacing the original word with its synonym $s(w)$ . The pairs of words, the cosine distance $cos\\_dist(w,s(w))$ between their GloVe embeddings and the obtained results are listed in Table 6 ..For the first three examples in Table 6 , substitution only decreases testing accuracy by a few percentage points. Apparently, the word embeddings of the synonyms encode the lexical properties that the GRU needs to recognize that the same entailment relations apply to the sentence pairs. This does not prove that the model has distilled essential information about hyponymy from the GloVe embeddings. It could also be that the word embeddings of the replacement words are geometrically very similar to the originals, so that it is an algebraic necessity that the same results arise. However, this suspicion is inconsistent with the result of changing `hate' into `detest'. The cosine distance between these words is 0.56, so according to this measure their vectors are more similar than those representing `love' and `adore' (which have a cosine distance of 0.57). Nonetheless, replacing `hate' with `detest' confuses the model, whereas substitution of `love' into `adore' only decreases testing accuracy by 4.5 percentage points. This illustrates that robustness of the GRU in this respect is not a matter of simple vector similarity. In those cases where substitution into synonyms does not confuse the model it must have recognized a non-trivial property of the new word embedding that licenses particular inferences..In our next experiment, we replace a word not by its synonym, but by a word that has the same semantics in the context of artificial language $\\mathcal {L}$ . We thus consider pairs of words that can be substituted with each other without affecting the entailment relation between any pair of sentences in which they feature. We call such terms `ontological twins'. Technically, if $\\odot $ is an arbitrary lexical entailment relation and $\\mathcal {O}$ is an ontology, then $w$ and $v$ are ontological twins if and only if $w, v \\in \\mathcal {O}$ and for all $u \\in \\mathcal {O}$ , if $u \\notin \\lbrace  w,v \\rbrace  $ then $w \\odot u \\Leftrightarrow v \\odot u$ . This trivially applies to self-identical terms or synonyms, but in the strictly defined hierarchy of $\\mathcal {L}$ it is also the case for pairs of terms $\\odot $0 that maintain the same lexical entailment relations to all other terms in the taxonomy..Examples of ontological twins in the taxonomy of nouns $\\mathcal {N}^{\\mathcal {L}}$ are `Romans' and `Venetians' . This can easily be verified in the Venn diagram of Figure 1 by replacing `Romans' with `Venetians' and observing that the same hierarchy applies. The same holds for e.g. `Germans' and `Polish' or for `children' and `students'. For several such word-twin pairs the GRU is evaluated with respect to the fragment of the test data containing the original word $w$ , and with respect to that same fragment after replacing the original word with ontological twin $t(w)$ . Results are shown in Table 7 ..The examples in Table 7 suggest that the best-performing GRU is largely robust with respect to substitution into ontological twins. Replacing `Romans' with other urban Italian demonyms hardly affects model accuracy on the modified fragment of the test data. As before, there appears to be no correlation with vector similarity because the cosine distance between the different twin pairs has a much higher variation than the corresponding accuracy scores. `Germans' can be changed into `Polish' without significant deterioration, but substitution with `Dutch' greatly decreases testing accuracy. The situation is even worse for `Spanish'. Again, cosine similarity provides no explanation - `Spanish' is still closer to `Germans' than `Neapolitans' to `Romans'. Rather, the accuracy appears to be negatively correlated with the geographical distance between the national demonyms. After replacing `children' with `students', `women' or `linguists', testing scores are still decent..So far, we replaced individual words in order to assess whether the GRU can generalize from the vocabulary to new notions that have comparable semantics in the context of this entailment recognition task. The examples have illustrated that the model tends to do this quite well. In the last zero-shot learning experiment, we replace sets of nouns instead of single words, in order to assess the flexibility of the relational semantics that our networks have learned. Formally, the replacement can be regarded as a function $r$ , mapping words $w$ to substitutes $r(w)$ . Not all items have to be replaced. For an ontology $\\mathcal {O}$ , the function $r$ must be such that for any $w, v \\in \\mathcal {O}$ and lexical entailment relation $\\odot $ , $w \\odot v \\Leftrightarrow r(w) \\odot r(v)$ . The result of applying $r$ can be called an `alternative hierarchy'..An example of an alternative hierarchy is the result of the replacement function $r_1$ that maps `Romans' to `Parisians' and `Italians' to `French'. Performing this substitution in the Venn diagram of Figure 1 shows that the taxonomy remains structurally intact. The best-performing GRU is evaluated on the fragment of the test data containing `Romans' or `Italians', and consequently on the same fragment after implementing replacement $r_1$ and providing the model with the GloVe embeddings of the unseen words. Replacement $r_1$ is incrementally modified up until replacement $r_4$ , which substitutes all nouns in $\\mathcal {N}^{\\mathcal {L}}$ . The results of applying $r_1$ to $r_4$ are shown in Table 8 ..The results are positive: the GRU obtains 86.7% accuracy even after applying $r_4$ , which substitutes the entire ontology $\\mathcal {N}^{\\mathcal {L}}$ so that no previously encountered nouns are present in the test set anymore, although the sentences remain thematically somewhat similar to the original sentences. Testing scores are above 87% for the intermediate substitutions $r_1$ to $r_3$ . This outcome clearly shows that the classifier does not depend on a strongly customized word vector distribution in order to recognize higher-level entailment relations. Even if all nouns are replaced by alternatives with embeddings that have not been witnessed or optimized beforehand, the model obtains a high testing accuracy. This establishes obvious compositional capacities, because familiarity with structure and information about lexical semantics in the form of word embeddings are enough for the model to accommodate configurations of unseen words..What happens when we consider ontologies that have the same structure, but are thematically very different from the original ontology? Three such alternative hierarchies are considered: $r_{animals}$ , $r_{religion}$ and $r_{America}$ . Each of these functions relocalizes the noun ontology in a totally different domain of discourse, as indicated by their names. Table 9 specifies the functions and their effect..Testing accuracy decreases drastically, which indicates that the model is sensitive to the changing topic. Variation between the scores obtained after the three transformations is limited. Although they are much lower than before, they are still far above chance level for a seven-class problem. This suggests that the model is not at a complete loss as to the alternative noun hierarchies. Possibly, including a few relevant instances during training could already improve the results. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "40\n",
      "What is Compositionality?\n",
      "\n",
      "Compositionality is the ability to interpret and generate a potentially infinite number of constructions from known constituents, and is considered a fundamental aspect of human learning and reasoning.\n",
      "Question : for the text Compositionality is the ability to interpret and generate a possibly infinite number of constructions from known constituents, and is commonly understood as one of the fundamental aspects of human learning and reasoning ( BIBREF30 , BIBREF31 ). It has often been claimed that neural networks operate on a merely associative basis, lacking the compositional capacities to develop systematicity without an abundance of training data. See e.g. BIBREF1 , BIBREF2 , BIBREF32 . Especially recurrent models have recently been regarded quite sceptically in this respect, following the negative results established by BIBREF3 and BIBREF4 . Their research suggests that recurrent networks only perform well provided that there are no systematic discrepancies between train and test data, whereas human learning is robust with respect to such differences thanks to compositionality..In this section, we report more positive results on compositional reasoning of our Siamese networks. We focus on zero-shot generalization: correct classification of examples of a type that has not been observed before. Provided that atomic constituents and production rules are understood, compositionality does not require that abundantly many instances embodying a semantic category are observed. We will consider in turn what set-up is required to demonstrate zero-shot generalization to unseen lengths, and to generalization to sentences composed of novel words. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "41\n",
      "Question 1: What benchmark did the proposed method outperform the previous state-of-the-art in?\n",
      "\n",
      "Answer 1: The proposed method outperformed the previous state-of-the-art on the Winograd Schema Challenge.\n",
      "Question : for the text We introduce a simple unsupervised method for Commonsense Reasoning tasks. Key to our proposal are large language models, trained on a number of massive and diverse text corpora. The resulting systems outperform previous best systems on both Pronoun Disambiguation Problems and Winograd Schema Challenge. Remarkably on the later benchmark, we are able to achieve 63.7% accuracy, comparing to 52.8% accuracy of the previous state-of-the-art, who utilizes supervised learning and expensively annotated knowledge bases. We analyze our system's answers and observe that it discovers key features of the question that decides the correct answer, indicating good understanding of the context and commonsense knowledge. We also demonstrated that ensembles of models benefit the most when trained on a diverse set of text corpora..We anticipate that this simple technique will be a strong building block for future systems that utilize reasoning ability on commonsense knowledge. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "42\n",
      "Question 1: What is the name of the dataset created based on questions in commonsense reasoning tasks and how many documents did it result in?\n",
      "\n",
      "Answer 1: The dataset is named STORIES and resulted in nearly 1,000,000 documents.\n",
      "Question : for the text As previous systems collect relevant data from knowledge bases after observing questions during evaluation BIBREF24 , BIBREF25 , we also explore using this option. Namely, we build a customized text corpus based on questions in commonsense reasoning tasks. It is important to note that this does not include the answers and therefore does not provide supervision to our resolvers. In particular, we aggregate documents from the CommonCrawl dataset that has the most overlapping n-grams with the questions. The score for each document is a weighted sum of $F_1(n)$ scores when counting overlapping n-grams: $Similarity\\_Score_{document} = \\frac{\\sum _{n=1}^4nF_1(n)}{\\sum _{n=1}^4n}$ .The top 0.1% of highest ranked documents is chosen as our new training corpus. Details of the ranking is shown in Figure 2 . This procedure resulted in nearly 1,000,000 documents, with the highest ranking document having a score of $8\\times 10^{-2}$ , still relatively small to a perfect score of $1.0$ . We name this dataset STORIES since most of the constituent documents take the form of a story with long chain of coherent events..We train four different LMs on STORIES and add them to the previous ensemble of 10 LMs, resulting in a gain of 2% accuracy in the final system as shown in Table 5 . Remarkably, single models trained on this corpus are already extremely strong, with a word-level LM achieving 62.6% accuracy, even better than the ensemble of 10 models previously trained on 4 other text corpora (61.5%). generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "43\n",
      "Question 1: What was the issue with using full CommonCrawl for training LMs for commonsense reasoning tasks?\n",
      "\n",
      "Answer 1: The issue with using full CommonCrawl for training LMs for commonsense reasoning tasks is that there is a large amount of low quality training text on the lower end of the ranking, which are mostly unintelligible or unrecognized by our vocabulary.\n",
      "Question : for the text Using the similarity scoring technique in section \"Customized training data for Winograd Schema Challenge\" , we observe a large amount of low quality training text on the lower end of the ranking. Namely, these are documents whose content are mostly unintelligible or unrecognized by our vocabulary. Training LMs for commonsense reasoning tasks on full CommonCrawl, therefore, might not be ideal. On the other hand, we detected and removed a portion of PDP-122 questions presented as an extremely high ranked document. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "44\n",
      "Question 1: How does the proposed method detect keywords for decision making between two candidates?\n",
      "\n",
      "Answer 1: The proposed method detects keywords by analyzing the probability ratio of the LM assigning words to either the correct or incorrect candidate. The ratio is denoted as $q_t$, and the maximum value of this indicates the keyword that may have been given a low probability after the wrong substitution. This method helps in predicting the keyword in each Winograd Schema question.\n",
      "Question : for the text We introduce a method to potentially detect keywords at which our proposed resolvers make decision between the two candidates $c_{correct}$ and $c_{incorrect}$ . Namely, we look at the following ratio: $q_t = \\frac{P_\\theta (w_t | w_1, w_2, ..., w_{t-1}; w_k \\leftarrow c_{correct})}{P_\\theta (w_t | w_1, w_2, ..., w_{t-1}; w_k \\leftarrow c_{incorrect})}$ .Where $1 \\le t \\le n$ for full scoring, and $k +1 \\le t \\le n$ for partial scoring. It follows that the choice between $c_{correct}$ or $c_{incorrect}$ is made by the value of $Q = \\prod _tq_t$ being bigger than $1.0$ or not. By looking at the value of each individual $q_t$ , it is possible to retrieve words with the largest values of $q_t$ and hence most responsible for the final value of $Q$ ..We visualize the probability ratios $q_t$ to have more insights into the decisions of our resolvers. Figure 3 displays a sample of incorrect decisions made by full scoring and is corrected by partial scoring. Interestingly, we found $q_t$ with large values coincides with the special keyword of each Winograd Schema in several cases. Intuitively, this means the LMs assigned very low probability for the keyword after observing the wrong substitution. It follows that we can predict the keyword in each the Winograd Schema question by selecting top word positions with the highest value of $q_t$ ..For questions with keyword appearing before the reference, we detect them by backward-scoring models. Namely, we ensemble 6 LMs, each trained on one text corpora with word order reversed. This ensemble also outperforms the previous best system on WSC-273 with a remarkable accuracy of 58.2%. Overall, we are able to discover a significant amount of special keywords (115 out of 178 correctly answered questions) as shown in Table 6 . This strongly indicates a correct understanding of the context and a good grasp of commonsense knowledge in the resolver's decision process. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "45\n",
      "Question 1: What do the authors describe in this section?\n",
      "\n",
      "Answer 1: In this section, the authors describe tests for commonsense reasoning and the LMs (Language Models) that are used to solve these tasks. They also provide details about the training text corpora used in their experiments.\n",
      "Question : for the text In this section we describe tests for commonsense reasoning and the LMs used to solve these tasks. We also detail training text corpora used in our experiments. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "46\n",
      "Question 1: Which text corpus consistently yielded the highest accuracy for both word-level and character-level input processing in the experiments?\n",
      "\n",
      "Answer 1: STORIES consistently yielded the highest accuracy for both word-level and character-level input processing in the experiments.\n",
      "Question : for the text In this set of experiments, we examine the effect of training data on commonsense reasoning test performance. Namely, we train both word-level and character-level LMs on each of the five corpora: LM-1-Billion, CommonCrawl, SQuAD, Gutenberg Books, and STORIES. A held-out dataset from each text corpus is used for early stopping on the corresponding training data..To speed up training on these large corpora, we first train the models on the LM-1-Billion text corpus. Each trained model is then divided into three groups of parameters: Embedding, Recurrent Cell, and Softmax. Each of the three is optionally transferred to train the same architectures on CommonCrawl, SQuAD and Gutenberg Books. The best transferring combination is chosen by cross-validation..Figure 5 -left and middle show that STORIES always yield the highest accuracy for both types of input processing. We next rank the text corpora based on ensemble performance for more reliable results. Namely, we compare the previous ensemble of 10 models against the same set of models trained on each single text corpus. This time, the original ensemble trained on a diverse set of text corpora outperforms all other single-corpus ensembles including STORIES. This highlights the important role of diversity in training data for commonsense reasoning accuracy of the final system. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "47\n",
      "Question 1: In the sentence \"John picked up the bat and hit the ball with it,\" what is the antecedent of the pronoun \"it\"?\n",
      "\n",
      "Answer 1: The antecedent of the pronoun \"it\" in the sentence is \"the ball.\"\n",
      "Question : for the text Although deep neural networks have achieved remarkable successes (e.g., BIBREF1 , BIBREF2 , BIBREF3 , BIBREF4 , BIBREF5 , BIBREF6 , BIBREF7 , BIBREF8 , BIBREF9 , BIBREF10 , BIBREF11 , BIBREF12 , BIBREF13 , BIBREF14 ), their dependence on supervised learning has been challenged as a significant weakness. This dependence prevents deep neural networks from being applied to problems where labeled data is scarce. An example of such problems is common sense reasoning, such as the Winograd Schema Challenge BIBREF0 , where the labeled set is typically very small, on the order of hundreds of examples. Below is an example question from this dataset:.Although it is straightforward for us to choose the answer to be \"the trophy\" according to our common sense, answering this type of question is a great challenge for machines because there is no training data, or very little of it..In this paper, we present a surprisingly simple method for common sense reasoning with Winograd schema multiple choice questions. Key to our method is th e use of language models (LMs), trained on a large amount of unlabeled data, to score multiple choice questions posed by the challenge and similar datasets. More concretely, in the above example, we will first substitute the pronoun (\"it\") with the candidates (\"the trophy\" and \"the suitcase\"), and then use LMs to compute the probability of the two resulting sentences (\"The trophy doesn’t fit in the suitcase because the trophy is too big.\" and \"The trophy doesn’t fit in the suitcase because the suitcase is too big.\"). The substitution that results in a more probable sentence will be the correct answer..A unique feature of Winograd Schema questions is the presence of a special word that decides the correct reference choice. In the above example, \"big\" is this special word. When \"big\" is replaced by \"small\", the correct answer switches to \"the suitcase\". Although detecting this feature is not part of the challenge, further analysis shows that our system successfully discovers this special word to make its decisions in many cases, indicating a good grasp of commonsense knowledge. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "48\n",
      "Question 1: What is the first step in the experiments described in the text?\n",
      "Answer 1: The first step in the experiments is to test LMs trained on all text corpora with PDP-60 and WSC-273.\n",
      "Question : for the text Our experiments start with testing LMs trained on all text corpora with PDP-60 and WSC-273. Next, we show that it is possible to customize training data to obtain even better results. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "49\n",
      "Question 1: What is the main solution proposed for coreference resolution?\n",
      "\n",
      "Answer 1: The main solution proposed for coreference resolution is language modeling, which involves substituting the pronoun in the original sentence with each of the candidate choices and identifying which substitution results in a more probable sentence based on what the language model has learned from training data.\n",
      "Question : for the text We first substitute the pronoun in the original sentence with each of the candidate choices. The problem of coreference resolution then reduces to identifying which substitution results in a more probable sentence. By reframing the problem this way, language modeling becomes a natural solution by its definition. Namely, LMs are trained on text corpora, which encodes human knowledge in the form of natural language. During inference, LMs are able to assign probability to any given text based on what they have learned from training data. An overview of our method is shown in Figure 1 ..Suppose the sentence $S$ of $n$ consecutive words has its pronoun to be resolved specified at the $k^{th}$ position: $S = \\lbrace w_1, .., w_{k-1}, w_{k} \\equiv p, w_{k+1}, .., w_{n}\\rbrace $ . We make use of a trained language model $P_\\theta (w_t | w_{1}, w_2, .., w_{t-1})$ , which defines the probability of word $w_t$ conditioned on the previous words $w_1, ..., w_{t-1}$ . The substitution of a candidate reference $c$ in to the pronoun position $k$ results in a new sentence $S_{w_k\\leftarrow c}$ (we use notation $n$0 to mean that word $n$1 is substituted by candidate $n$2 ). We consider two different ways of scoring the substitution:.which scores how probable the resulting full sentence is, and.which scores how probable the part of the resulting sentence following $c$ is, given its antecedent. In other words, it only scores a part $S_{w_k\\leftarrow c}$ conditioned on the rest of the substituted sentence. An example of these two scores is shown in Table 1 . In our experiments, we find that partial scoring strategy is generally better than the naive full scoring strategy. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "50\n",
      "What is the reason for a large percentage of incorrect decisions according to the experiments?\n",
      "The $q_t$ at the pronoun position is most responsible for a very large percentage of incorrect decisions because rare words in the training corpus are assigned a very low probability, overpowering subsequent $q_t$ values.\n",
      "Question : for the text In this set of experiments, we look at wrong predictions from a word-level LM. With full scoring strategy, we observe that $q_t$ at the pronoun position is most responsible for a very large percentage of incorrect decisions as shown in Figfure 3 and Table 7 . For example, with the test \"The trophy cannot fit in the suitcase because it is too big.\", the system might return $c_{incorrect} = $ \"suitcase\" simply because $c_{correct} = $ \"trophy\" is a very rare word in its training corpus and therefore, is assigned a very low probability, overpowering subsequent $q_t$ values..Following this reasoning, we apply a simple fix to full scoring by normalizing its score with the unigram count of $c$ : $Score_{full~normalized} = Score_{full} / Count(c)$ . Partial scoring, on the other hand, disregards $c$ altogether. As shown in Figure 4 , this normalization fixes full scoring in 9 out of 10 tested LMs on PDP-122. On WSC-273, the result is very decisive as partial scoring strongly outperforms the other two scoring in all cases. Since PDP-122 is a larger superset of PDP-60, we attribute the different behaviour observed on PDP-60 as an atypical case due to its very small size. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "1\n",
      "Question 1: What is the dimensionality of the output gate of each LSTM?\n",
      "\n",
      "Answer 1: The output gate of each LSTM uses peepholes and a projection layer to reduce its output dimensionality to 1024.\n",
      "Question : for the text The base model consists of two layers of Long-Short Term Memory (LSTM) BIBREF31 with 8192 hidden units. The output gate of each LSTM uses peepholes and a projection layer to reduce its output dimensionality to 1024. We perform drop-out on LSTM's outputs with probability 0.25..For word inputs, we use an embedding lookup of 800000 words, each with dimension 1024. For character inputs, we use an embedding lookup of 256 characters, each with dimension 16. We concatenate all characters in each word into a tensor of shape (word length, 16) and add to its two ends the <begin of word> and <end of word> tokens. The resulting concatenation is zero-padded to produce a fixed size tensor of shape (50, 16). This tensor is then processed by eight different 1-D convolution (Conv) kernels of different sizes and number of output channels, listed in Table 8 , each followed by a ReLU acitvation. The output of all CNNs are then concatenated and processed by two other fully-connected layers with highway connection that persist the input dimensionality. The resulting tensor is projected down to a 1024-feature vector. For both word input and character input, we perform dropout on the tensors that go into LSTM layers with probability 0.25..We use a single fully-connected layer followed by a $Softmax$ operator to process the LSTM's output and produce a distribution over word vocabulary of size 800K. During training, LM loss is evaluated using importance sampling with negative sample size of 8192. This loss is minimized using the AdaGrad BIBREF37 algorithm with a learning rate of 0.2. All gradients on LSTM parameters and Character Embedding parameters are clipped by their global norm at 1.0. To avoid storing large matrices in memory, we shard them into 32 equal-sized smaller pieces. In our experiments, we used 8 different variants of this base model as listed in Table 9 ..In Table 10 , we listed all LMs and their training text corpora used in each of the experiments in Section \"Main results\" . generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "2\n",
      "Question 1: What is the difference between using word vectors and language models to answer analogy questions?\n",
      "\n",
      "Answer 1: While both approaches use language modeling to capture common sense knowledge, using word vectors is only able to answer simple analogy questions, while Winograd Schema questions require more contextual information, hence the use of language models instead.\n",
      "Question : for the text Unsupervised learning has been used to discover simple commonsense relationships. For example, Mikolov et al. BIBREF15 , BIBREF16 show that by learning to predict adjacent words in a sentence, word vectors can be used to answer analogy questions such as: Man:King::Woman:?. Our work uses a similar intuition that language modeling can naturally capture common sense knowledge. The difference is that Winograd Schema questions require more contextual information, hence our use of LMs instead of just word vectors..Neural LMs have also been applied successfully to improve downstream applications BIBREF17 , BIBREF18 , BIBREF19 , BIBREF20 . In BIBREF17 , BIBREF18 , BIBREF19 , BIBREF20 , researchers have shown that pre-trained LMs can be used as feature representations for a sentence, or a paragraph to improve NLP applications such as document classification, machine translation, question answering, etc. The combined evidence suggests that LMs trained on a massive amount of unlabeled data can capture many aspects of natural language and the world's knowledge, especially commonsense information..Previous attempts on solving the Winograd Schema Challenge usually involve heavy utilization of annotated knowledge bases, rule-based reasoning, or hand-crafted features BIBREF21 , BIBREF22 , BIBREF23 . In particular, Rahman and Ng BIBREF24 employ human annotators to build more supervised training data. Their model utilizes nearly 70K hand-crafted features, including querying data from Google Search API. Sharma et al. BIBREF25 rely on a semantic parser to understand the question, query texts through Google Search, and reason on the graph produced by the parser. Similarly, Schüller BIBREF23 formalizes the knowledge-graph data structure and a reasoning process based on cognitive linguistics theories. Bailey et al. BIBREF22 introduces a framework for reasoning, using expensive annotated knowledge bases as axioms..The current best approach makes use of the skip-gram model to learn word representations BIBREF26 . The model incorporates several knowledge bases to regularize its training process, resulting in Knowledge Enhanced Embeddings (KEE). A semantic similarity scorer and a deep neural network classifier are then combined on top of KEE to predict the answers. The final system, therefore, includes both supervised and unsupervised models, besides three different knowledge bases. In contrast, our unsupervised method is simpler while having significantly higher accuracy. Unsupervised training is done on text corpora which can be cheaply curated..Using language models in reading comprehension tests also produced many great successes. Namely Chu et al. BIBREF27 used bi-directional RNNs to predict the last word of a passage in the LAMBADA challenge. Similarly, LMs are also used to produce features for a classifier in the Store Close Test 2017, giving best accuracy against other methods BIBREF28 . In a broader context, LMs are used to produce good word embeddings, significantly improved a wide variety of downstream tasks, including the general problem of question answering BIBREF19 , BIBREF29 . generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "3\n",
      "Question 1: What is the accuracy achieved by the ensemble of five unsupervised models on PDP-60?\n",
      "\n",
      "Answer 1: The ensemble of five unsupervised models achieved 70.0% accuracy on PDP-60, which outperforms the best system in the 2016 competition and the more recent reported results from Quan Liu et al.\n",
      "Question : for the text We first examine unsupervised single-model resolvers on PDP-60 by training one character-level and one word-level LM on the Gutenberg corpus. In Table 2 , these two resolvers outperform previous results by a large margin. For this task, we found full scoring gives better results than partial scoring. In Section \"Partial scoring is better than full scoring.\" , we provide evidences that this is an atypical case due to the very small size of PDP-60..Next, we allow systems to take in necessary components to maximize their test performance. This includes making use of supervised training data that maps commonsense reasoning questions to their correct answer. Here we simply train another three variants of LMs on LM-1-Billion, CommonCrawl, and SQuAD and ensemble all of them. As reported in Table 3 , this ensemble of five unsupervised models outperform the best system in the 2016 competition (58.3%) by a large margin. Specifically, we achieve 70.0% accuracy, better than the more recent reported results from Quan Liu et al (66.7%) BIBREF26 , who makes use of three knowledge bases and a supervised deep neural network. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "4\n",
      "Question 1: What is the accuracy achieved by the word-level resolver on the harder task WSC-273?\n",
      "\n",
      "Answer 1: The word-level resolver achieves an accuracy of 56.4% on the harder task WSC-273.\n",
      "Question : for the text On the harder task WSC-273, our single-model resolvers also outperform the current state-of-the-art by a large margin, as shown in Table 4 . Namely, our word-level resolver achieves an accuracy of 56.4%. By training another 4 LMs, each on one of the 4 text corpora LM-1-Billion, CommonCrawl, SQuAD, Gutenberg Books, and add to the previous ensemble, we are able to reach 61.5%, nearly 10% of accuracy above the previous best result. This is a drastic improvement considering this previous best system outperforms random guess by only 3% in accuracy..This task is more difficult than PDP-60. First, the overall performance of all competing systems are much lower than that of PDP-60. Second, incorporating supervised learning and expensive annotated knowledge bases to USSM provides insignificant gain this time (+3%), comparing to the large gain on PDP-60 (+19%). generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "5\n",
      "What is the Markov random field presented in this section?\n",
      "\n",
      "The Markov random field presented in this section is for morpho-syntactic agreement. It defines a joint distribution over sequences of morpho-syntactic tags, conditioned on a labeled dependency tree with associated part-of-speech tags.\n",
      "Question : for the text In this section, we present a Markov random field BIBREF17 for morpho-syntactic agreement. This model defines a joint distribution over sequences of morpho-syntactic tags, conditioned on a labeled dependency tree with associated part-of-speech tags. Given an intervention on a gendered word, we can use this model to infer the manner in which the remaining tags must be updated to preserve morpho-syntactic agreement..A dependency tree for a sentence (see fig:tree for an example) is a set of ordered triples INLINEFORM0 , where INLINEFORM1 and INLINEFORM2 are positions in the sentence (or a distinguished root symbol) and INLINEFORM3 is the label of the edge INLINEFORM4 in the tree; each position occurs exactly once as the first element in a triple. Each dependency tree INLINEFORM5 is associated with a sequence of morpho-syntactic tags INLINEFORM6 and a sequence of part-of-speech (POS) tags INLINEFORM7 . For example, the tags INLINEFORM8 and INLINEFORM9 for ingeniero are INLINEFORM10 and INLINEFORM11 , respectively, because ingeniero is a masculine, singular noun. For notational simplicity, we define INLINEFORM12 to be the set of all length- INLINEFORM13 sequences of morpho-syntactic tags..We define the probability of INLINEFORM0 given INLINEFORM1 and INLINEFORM2 as DISPLAYFORM0 . where the binary factor INLINEFORM0 scores how well the morpho-syntactic tags INLINEFORM1 and INLINEFORM2 agree given the POS tags INLINEFORM3 and INLINEFORM4 and the label INLINEFORM5 . For example, consider the INLINEFORM6 (adjectival modifier) edge from experto to ingeniero in fig:tree. The factor INLINEFORM7 returns a high score if the corresponding morpho-syntactic tags agree in gender and number (e.g., INLINEFORM8 and INLINEFORM9 ) and a low score if they do not (e.g., INLINEFORM10 and INLINEFORM11 ). The unary factor INLINEFORM12 scores a morpho-syntactic tag INLINEFORM13 outside the context of the dependency tree. As we explain in sec:constraint, we use these unary factors to force or disallow particular tags when performing an intervention; we do not learn them. eq:dist is normalized by the following partition function: INLINEFORM14 . INLINEFORM0 can be calculated using belief propagation; we provide the update equations that we use in sec:bp. Our model is depicted in fig:fg. It is noteworthy that this model is delexicalized—i.e., it considers only the labeled dependency tree and the POS tags, not the actual words themselves. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "6\n",
      "Question 1: Who is acknowledged in the article?\n",
      "\n",
      "Answer 1: The last author is acknowledged and they received a Facebook Fellowship.\n",
      "Question : for the text The last author acknowledges a Facebook Fellowship. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "7\n",
      "Question 1: What is the feminine translation of the adjective \"bello\"?\n",
      "\n",
      "Answer 1: The feminine translation of \"bello\" is \"bella\" (tab:fem).\n",
      "Question : for the text tab:fem and tab:masc contain the feminine and masculine translations of the four adjectives that we used. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "8\n",
      "Question 1: What are the belief propagation update equations?\n",
      "\n",
      "Answer 1: The belief propagation update equations are given by DISPLAYFORM0 DISPLAYFORM1, where INLINEFORM0 returns the set of neighbouring nodes of the node INLINEFORM1. The belief at any node is given by DISPLAYFORM0.\n",
      "Question : for the text Our belief propagation update equations are DISPLAYFORM0 DISPLAYFORM1 . where INLINEFORM0 returns the set of neighbouring nodes of node INLINEFORM1 . The belief at any node is given by DISPLAYFORM0  generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "9\n",
      "Question 1: What is the main limitation the authors faced when evaluating their approach for converting between masculine-inflected and feminine-inflected noun phrases?\n",
      "\n",
      "Answer 1: The main limitation the authors faced when evaluating their approach is the lack of an existing annotated corpus of paired sentences that can be used as \"ground truth.\"\n",
      "Question : for the text We presented a new approach for converting between masculine-inflected and feminine-inflected noun phrases in morphologically rich languages. To do this, we introduced a Markov random field with an optional neural parameterization that infers the manner in which a sentence must change to preserve morpho-syntactic agreement when altering the grammatical gender of particular nouns. To the best of our knowledge, this task has not been studied previously. As a result, there is no existing annotated corpus of paired sentences that can be used as “ground truth.” Despite this limitation, we evaluated our approach both intrinsically and extrinsically, achieving promising results. For example, we demonstrated that our approach reduces gender stereotyping in neural language models. Finally, we also identified avenues for future work, such as the inclusion of co-reference information. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "10\n",
      "What optimizer was used to train both parameterizations of the model?\n",
      "\n",
      "The Adam optimizer was used to train both parameterizations of the model.\n",
      "Question : for the text We used the Adam optimizer BIBREF23 to train both parameterizations of our model until the change in dev-loss was less than INLINEFORM0 bits. We set INLINEFORM1 without tuning, and chose a learning rate of INLINEFORM2 and weight decay factor of INLINEFORM3 after tuning. We tuned INLINEFORM4 in the set INLINEFORM5 and chose INLINEFORM6 . For the neural parameterization, we set INLINEFORM7 and INLINEFORM8 without any tuning. Finally, we trained the inflection model using only gendered words..We evaluate our approach both intrinsically and extrinsically. For the intrinsic evaluation, we focus on whether our approach yields the correct morpho-syntactic tags and the correct reinflections. For the extrinsic evaluation, we assess the extent to which using the resulting transformed sentences reduces gender stereotyping in neural language models. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "11\n",
      "What measure of gender stereotyping did the researchers use in their evaluation?\n",
      "\n",
      "They compared the log ratio of the prefix probabilities under a language model for gendered, animate nouns with four adjectives to assess gender stereotyping reduction.\n",
      "Question : for the text We extrinsically evaluate our approach by assessing the extent to which it reduces gender stereotyping. Following DBLP:journals/corr/abs-1807-11714, focus on neural language models. We choose language models over word embeddings because standard measures of gender stereotyping for word embeddings cannot be applied to morphologically rich languages..As our measure of gender stereotyping, we compare the log ratio of the prefix probabilities under a language model INLINEFORM0 for gendered, animate nouns, such as ingeniero, combined with four adjectives: good, bad, smart, and beautiful. The translations we use for these adjectives are given in sec:translation. We chose the first two adjectives because they should be used equally to describe men and women, and the latter two because we expect that they will reveal gender stereotypes. For example, consider DISPLAYFORM0 .If this log ratio is close to 0, then the language model is as likely to generate sentences that start with el ingeniero bueno (the good male engineer) as it is to generate sentences that start with la ingeniera bueno (the good female engineer). If the log ratio is negative, then the language model is more likely to generate the feminine form than the masculine form, while the opposite is true if the log ratio is positive. In practice, given the current gender disparity in engineering, we would expect the log ratio to be positive. If, however, the language model were trained on a corpus to which our CDA approach had been applied, we would then expect the log ratio to be much closer to zero..Because our approach is specifically intended to yield sentences that are grammatical, we additionally consider the following log ratio (i.e., the grammatical phrase over the ungrammatical phrase): DISPLAYFORM0 .We trained the linear parameterization using UD treebanks for Spanish, Hebrew, French, and Italian (see tab:data). For each of the four languages, we parsed one million sentences from Wikipedia (May 2018 dump) using BIBREF24 's parser and extracted taggings and lemmata using the method of BIBREF25 . We automatically extracted an animacy gazetteer from WordNet BIBREF26 and then manually filtered the output for correctness. We provide the size of the languages' animacy gazetteers and the percentage of automatically parsed sentences that contain an animate noun in tab:anim. For each sentence containing a noun in our animacy gazetteer, we created a copy of the sentence, intervened on the noun, and then used our approach to transform the sentence. For sentences containing more than one animate noun, we generated a separate sentence for each possible combination of genders. Choosing which sentences to duplicate is a difficult task. For example, alemán in Spanish can refer to either a German man or the German language; however, we have no way of distinguishing between these two meanings without additional annotations. Multilingual animacy detection BIBREF27 might help with this challenge; co-reference information might additionally help..For each language, we trained the BPE-RNNLM baseline open-vocabulary language model of BIBREF28 using the original corpus, the corpus following CDA using naïve swapping of gendered words, and the corpus following CDA using our approach. We then computed gender stereotyping and grammaticality as described above. We provide example phrases in tab:lm; we provide a more extensive list of phrases in app:queries..fig:bias demonstrates depicts gender stereotyping and grammaticality for each language using the original corpus, the corpus following CDA using naïve swapping of gendered words, and the corpus following CDA using our approach. It is immediately apparent that our approch reduces gender stereotyping. On average, our approach reduces gender stereotyping by a factor of 2.5 (the lowest and highest factors are 1.2 (Ita) and 5.0 (Esp), respectively). We expected that naïve swapping of gendered words would also reduce gender stereotyping. Indeed, we see that this simple heuristic reduces gender stereotyping for some but not all of the languages. For Spanish, we also examine specific words that are stereotyped toward men or women. We define a word to be stereotyped toward one gender if 75% of its occurrences are of that gender. fig:espbias suggests a clear reduction in gender stereotyping for specific words that are stereotyped toward men or women..The grammaticality of the corpora following CDA differs between languages. That said, with the exception of Hebrew, our approach either sacrifices less grammaticality than naïve swapping of gendered words and sometimes increases grammaticality over the original corpus. Given that we know the model did not perform as accurately for Hebrew (see tab:intrinsic), this finding is not surprising. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "12\n",
      "Question 1: How many phrases were generated for each noun in the animacy gazetteer?\n",
      "Answer 1: Sixteen phrases were generated for each noun in the animacy gazetteer.\n",
      "Question : for the text For each noun in our animacy gazetteer, we generated sixteen phrases. Consider the noun engineer as an example. We created four phrases—one for each translation of The good engineer, The bad engineer, The smart engineer, and The beautiful engineer. These phrases, as well as their prefix log-likelihoods are provided below in tab:query. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "13\n",
      "Question 1: Why is the sentence \"he is an engineer\" more likely to appear in a corpus than \"she is an engineer\"?\n",
      "\n",
      "Answer 1: The current gender disparity in engineering results in a higher representation of men compared to women in the engineering field, causing a gender imbalance in texts where both male and female engineers are referred to.\n",
      "Question : for the text Men and women are mentioned at different rates in text BIBREF11 . This problem is exacerbated in certain contexts. For example, the sentence he is an engineer is more likely to appear in a corpus than she is an engineer due to the current gender disparity in engineering. This imbalance in representation can have a dramatic downstream effect on NLP systems trained on such a corpus, such as giving preference to male engineers over female engineers in an automated resumé filtering system. Gender stereotypes of this sort have been observed in word embeddings BIBREF5 , BIBREF3 , contextual word embeddings BIBREF12 , and co-reference resolution systems BIBREF13 , BIBREF9 inter alia. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "14\n",
      "Question 1: What algorithm is used for exact inference in a tree-shaped MRF?\n",
      "\n",
      "Answer 1: Belief propagation algorithm is used for exact inference in a tree-shaped MRF.\n",
      "Question : for the text Because our MRF is acyclic and tree-shaped, we can use belief propagation BIBREF18 to perform exact inference. The algorithm is a generalization of the forward-backward algorithm for hidden Markov models BIBREF19 . Specifically, we pass messages from the leaves to the root and vice versa. The marginal distribution of a node is the point-wise product of all its incoming messages; the partition function INLINEFORM0 is the sum of any node's marginal distribution. Computing INLINEFORM1 takes polynomial time BIBREF18 —specifically, INLINEFORM2 where INLINEFORM3 is the number of morpho-syntactic tags. Finally, inferring the highest-probability morpho-syntactic tag sequence INLINEFORM4 given INLINEFORM5 and INLINEFORM6 can be performed using the max-product modification to belief propagation. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "15\n",
      "What is the purpose of intervening on a gendered word in the text?\n",
      "\n",
      "The purpose of intervening on a gendered word is to transform sentences to match a desired gender. By changing the morpho-syntactic tag of a gendered word and using a model to infer the necessary updates to preserve morpho-syntactic agreement, the goal is to change as little as possible while achieving the desired gender change.\n",
      "Question : for the text As explained in sec:gender, our goal is to transform sentences like sent:msc to sent:fem by intervening on a gendered word and then using our model to infer the manner in which the remaining tags must be updated to preserve morpho-syntactic agreement. For example, if we change the morpho-syntactic tag for ingeniero from [msc;sg] to [fem;sg], then we must also update the tags for el and experto, but do not need to update the tag for es, which should remain unchanged as [in; pr; sg]. If we intervene on the INLINEFORM0 word in a sentence, changing its tag from INLINEFORM1 to INLINEFORM2 , then using our model to infer the manner in which the remaining tags must be updated means using INLINEFORM3 to identify high-probability tags for the remaining words..Crucially, we wish to change as little as possible when intervening on a gendered word. The unary factors INLINEFORM0 enable us to do exactly this. As described in the previous section, the strength parameter INLINEFORM1 determines the extent to which INLINEFORM2 should remain unchanged following an intervention—the larger the value, the less likely it is that INLINEFORM3 will be changed..Once the new tags have been inferred, the final step is to reinflect the lemmata to their new forms. This task has received considerable attention from the NLP community BIBREF21 , BIBREF22 . We use the inflection model of D18-1473. This model conditions on the lemma INLINEFORM0 and morpho-syntactic tag INLINEFORM1 to form a distribution over possible inflections. For example, given experto and INLINEFORM2 , the trained inflection model will assign a high probability to expertas. We provide accuracies for the trained inflection model in tab:reinflect. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "16\n",
      "What was the baseline used for both Spanish and Hebrew in the study?\n",
      "\n",
      "The baseline used for Spanish only activated values that relate adjectives and determiners to nouns, while the baseline for Hebrew only activated values that relate adjectives and verbs to nouns.\n",
      "Question : for the text To the best of our knowledge, this task has not been studied previously. As a result, there is no existing annotated corpus of paired sentences that can be used as “ground truth.” We therefore annotated Spanish and Hebrew sentences ourselves, with annotations made by native speakers of each language. Specifically, for each language, we extracted sentences containing animate nouns from that language's UD treebank. The average length of these extracted sentences was 37 words. We then manually inspected each sentence, intervening on the gender of the animate noun and reinflecting the sentence accordingly. We chose Spanish and Hebrew because gender agreement operates differently in each language. We provide corpus statistics for both languages in the top two rows of tab:data..We created a hard-coded INLINEFORM0 to serve as a baseline for each language. For Spanish, we only activated, i.e. set to a number greater than zero, values that relate adjectives and determiners to nouns; for Hebrew, we only activated values that relate adjectives and verbs to nouns. We created two separate baselines because gender agreement operates differently in each language..To evaluate our approach, we held all morpho-syntactic subtags fixed except for gender. For each annotated sentence, we intervened on the gender of the animate noun. We then used our model to infer which of the remaining tags should be updated (updating a tag means swapping the gender subtag because all morpho-syntactic subtags were held fixed except for gender) and reinflected the lemmata. Finally, we used the annotations to compute the tag-level INLINEFORM0 score and the form-level accuracy, excluding the animate nouns on which we intervened..We present the results in tab:intrinsic. Recall is consistently significantly lower than precision. As expected, the baselines have the highest precision (though not by much). This is because they reflect well-known rules for each language. That said, they have lower recall than our approach because they fail to capture more subtle relationships..For both languages, our approach struggles with conjunctions. For example, consider the phrase él es un ingeniero y escritor (he is an engineer and a writer). Replacing ingeniero with ingeniera does not necessarily result in escritor being changed to escritora. This is because two nouns do not normally need to have the same gender when they are conjoined. Moreover, our MRF does not include co-reference information, so it cannot tell that, in this case, both nouns refer to the same person. Note that including co-reference information in our MRF would create cycles and inference would no longer be exact. Additionally, the lack of co-reference information means that, for Spanish, our approach fails to convert nouns that are noun-modifiers or indirect objects of verbs..Somewhat surprisingly, the neural parameterization does not outperform the linear parameterization. We proposed the neural parameterization to allow parameter sharing among edges with different parts of speech and labels; however, this parameter sharing does not seem to make a difference in practice, so the linear parameterization is sufficient. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "17\n",
      "Question 1: What is one type of societal bias that NLP systems can inadvertently replicate or amplify?\n",
      "\n",
      "Answer 1: One type of societal bias that NLP systems can inadvertently replicate or amplify is gender stereotyping.\n",
      "Question : for the text One of the biggest challenges faced by modern natural language processing (NLP) systems is the inadvertent replication or amplification of societal biases. This is because NLP systems depend on language corpora, which are inherently “not objective; they are creations of human design” BIBREF0 . One type of societal bias that has received considerable attention from the NLP community is gender stereotyping BIBREF1 , BIBREF2 , BIBREF3 . Gender stereotypes can manifest in language in overt ways. For example, the sentence he is an engineer is more likely to appear in a corpus than she is an engineer due to the current gender disparity in engineering. Consequently, any NLP system that is trained such a corpus will likely learn to associate engineer with men, but not with women BIBREF4 ..To date, the NLP community has focused primarily on approaches for detecting and mitigating gender stereotypes in English BIBREF5 , BIBREF6 , BIBREF7 . Yet, gender stereotypes also exist in other languages because they are a function of society, not of grammar. Moreover, because English does not mark grammatical gender, approaches developed for English are not transferable to morphologically rich languages that exhibit gender agreement BIBREF8 . In these languages, the words in a sentence are marked with morphological endings that reflect the grammatical gender of the surrounding nouns. This means that if the gender of one word changes, the others have to be updated to match. As a result, simple heuristics, such as augmenting a corpus with additional sentences in which he and she have been swapped BIBREF9 , will yield ungrammatical sentences. Consider the Spanish phrase el ingeniero experto (the skilled engineer). Replacing ingeniero with ingeniera is insufficient—el must also be replaced with la and experto with experta..In this paper, we present a new approach to counterfactual data augmentation BIBREF10 for mitigating gender stereotypes associated with animate nouns (i.e., nouns that represent people) for morphologically rich languages. We introduce a Markov random field with an optional neural parameterization that infers the manner in which a sentence must change when altering the grammatical gender of particular nouns. We use this model as part of a four-step process, depicted in fig:pipeline, to reinflect entire sentences following an intervention on the grammatical gender of one word. We intrinsically evaluate our approach using Spanish and Hebrew, achieving tag-level INLINEFORM0 scores of 83% and 72% and form-level accuracies of 90% and 87%, respectively. We also conduct an extrinsic evaluation using four languages. Following DBLP:journals/corr/abs-1807-11714, we show that, on average, our approach reduces gender stereotyping in neural language models by a factor of 2.5 without sacrificing grammaticality. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "18\n",
      "Question 1: What is the technique used for optimizing the negative log-likelihood in this specific case? \n",
      "\n",
      "Answer 1: The technique used is gradient descent, where the gradient of the negative log-likelihood is computed using automatic differentiation for the given tree parameters.\n",
      "Question : for the text We use gradient-based optimization. We treat the negative log-likelihood INLINEFORM0 as the loss function for tree INLINEFORM1 and compute its gradient using automatic differentiation BIBREF20 . We learn the parameters of sec:param by optimizing the negative log-likelihood using gradient descent. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "19\n",
      "Question 1: What is the purpose of the unary factors in this study?\n",
      "\n",
      "Answer 1: The unary factors are used to force or disallow particular tags when performing an intervention. They are defined by a strength parameter and determine to what extent a tag should remain unchanged following an intervention.\n",
      "Question : for the text We consider a linear parameterization and a neural parameterization of the binary factor INLINEFORM0 ..We define a matrix INLINEFORM0 for each triple INLINEFORM1 , where INLINEFORM2 is the number of morpho-syntactic subtags. For example, INLINEFORM3 has two subtags INLINEFORM4 and INLINEFORM5 . We then define INLINEFORM6 as follows: INLINEFORM7 . where INLINEFORM0 is a multi-hot encoding of INLINEFORM1 ..As an alternative, we also define a neural parameterization of INLINEFORM0 to allow parameter sharing among edges with different parts of speech and labels: INLINEFORM1 . where INLINEFORM0 , INLINEFORM1 , and INLINEFORM2 and INLINEFORM3 define the structure of the neural parameterization and each INLINEFORM4 is an embedding function..We use the unary factors only to force or disallow particular tags when performing an intervention. Specifically, we define DISPLAYFORM0 .where INLINEFORM0 is a strength parameter that determines the extent to which INLINEFORM1 should remain unchanged following an intervention. In the limit as INLINEFORM2 , all tags will remain unchanged except for the tag directly involved in the intervention. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "20\n",
      "Question 1: What is the focus of the NLP community's approach to gender stereotypes in languages with rich morphology?\n",
      "\n",
      "Answer 1: The NLP community's approach to mitigating gender stereotypes in languages with rich morphology is focused on detecting and reducing gender biases in co-reference resolution and language models. In contrast to previous work in English, the most closely related work is that of BIBREF9, which uses CDA to reduce gender stereotypes but yields ungrammatical sentences in morphologically rich languages. The current approach is specifically designed to yield grammatical sentences when applied to such languages.\n",
      "Question : for the text In contrast to previous work, we focus on mitigating gender stereotypes in languages with rich morphology—specifically languages that exhibit gender agreement. To date, the NLP community has focused on approaches for detecting and mitigating gender stereotypes in English. For example, BIBREF5 proposed a way of mitigating gender stereotypes in word embeddings while preserving meanings; BIBREF10 studied gender stereotypes in language models; and BIBREF13 introduced a novel Winograd schema for evaluating gender stereotypes in co-reference resolution. The most closely related work is that of BIBREF9 , who used CDA to reduce gender stereotypes in co-reference resolution; however, their approach yields ungrammatical sentences in morphologically rich languages. Our approach is specifically intended to yield grammatical sentences when applied to such languages. BIBREF29 also focused on morphologically rich languages, specifically Arabic, but in the context of gender identification in machine translation. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "21\n",
      "Question 1: Who provided support for the project mentioned in the text?\n",
      "Answer 1: The project mentioned in the text was supported by a grant of Labex EFL ANR-10-LABX-0083 (and Idex ANR-18-IDEX-0001) for AA and MIT–IBM AI Laboratory and the MIT–SenseTimeAlliance on Artificial Intelligence for RPL.\n",
      "Question : for the text This project is supported by a grant of Labex EFL ANR-10-LABX-0083 (and Idex ANR-18-IDEX-0001) for AA and MIT–IBM AI Laboratory and the MIT–SenseTimeAlliance on Artificial Intelligence for RPL. We would like to thank the anonymous reviewers for their comments and Anne Abeillé for her advice and feedback. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "22\n",
      "Question 1: What is the main difference between the coordination behavior of humans and neural models according to the experiments presented in the text?\n",
      "\n",
      "Answer 1: The neural models appear to be using a linear combination of NP constituent number to drive CoordNP/verb number agreement, with the second noun weighted more heavily than the first, while human behavior is best modeled as a percolation process.\n",
      "Question : for the text The experiments presented here extend and refine a line of research investigating what linguistic knowledge is acquired by neural language models. Previous studies have demonstrated that sequential models trained on a simple regime of optimizing the next word can learn long-distance syntactic dependencies in impressive detail. Our results provide complimentary insights, demonstrating that a range of model architectures trained on a variety of datasets can learn fine-grained information about the interaction of CoordNPs and local syntactic context, but their behavior remains unhumanlike in many key ways. Furthermore, to our best knowledge, this work presents the first psycholinguistic analysis of neural language models trained on French, a high-resource language that has so far been under-investigated in this line of research..In the simple coordination experiment, we demonstrated that models were able to capture some of the agreement behaviors of humans, although their performance deviated in crucial aspects. Whereas human behavior is best modeled as a `percolation' process, the neural models appear to be using a linear combination of NP constituent number to drive CoordNP/verb number agreement, with the second noun weighted more heavily than the first. In these experiments, supervision afforded by the RNNG and ActionLSTM models did not translate into more robust or humanlike learning outcomes. The complex coordination experiments provided evidence that the neural models tested were not using a simple `bag of features' strategy, but were sensitive to syntactic context. All models tested were able to interpret material that had similar surface form in ways that corresponded to two different tree-structural descriptions, based on local context. The inverted coordination experiment provided a contrasting example, in which models were unable to modulate expectations based on subtleties in the syntactic environment..Across all our experiments, the French models performed consistently better on subject/verb number agreement than on subject/predicate gender agreement. Although there are likely more examples of subject/verb number agreement in the French training data, gender agreement is syntactically mandated and widespread in French. It remains an open question why all but one of the models tested were unable to leverage the numerous examples of gender agreement seen in various contexts during training to drive correct subject/predicate expectations. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "23\n",
      "What is the purpose of this experiment?\n",
      "\n",
      "The purpose of this experiment is to assess whether the models tested have learned basic representations of number and gender features for non-coordinated Noun Phrases, in order to provide a baseline for following experiments.\n",
      "Question : for the text In order to provide a baseline for following experiments, here we assess whether the models tested have learned basic representations of number and gender features for non-coordinated Noun Phrases. We test number agreement in English and French as well as gender agreement in French. Both English and French have two grammatical number feature: singular (sg) and plural (pl). French has two grammatical gender features: masculine (m) and feminine (f)..The experimental materials include sentences where the subject NPs contain a single noun which can either match with the matrix verb (in the case of number agreement) or a following predicative adjective (in the case of gender agreement). Conditions are given in Table TABREF9 and Table TABREF10. We measure model behavior by computing the plural expectation, or the surprisal of the singular continuation minus the surprisal of the plural continuation for each condition and took the average for each condition. We expect a positive plural expectation in the Npl conditions and a negative plural expectation in the Nsg conditions. For gender expectation we compute a gender expectation, which is S(feminine continuation) $-$ S(masculine continuation). We measure surprisal at the verbs and predicative adjectives themselves..The results for this experiment are in Figure FIGREF11, with the plural expectation and gender expectation on the y-axis and conditions on the x-axis. For this and subsequent experiments error bars represent 95% confidence intervals for across-item means. For number agreement, all the models in English and French show positive plural expectation when the head noun is plural and negative plural expectation when it is singular. For gender agreement, however, only the LSTM (frWaC) shows modulation of gender expectation based on the gender of the head noun. This is most likely due to the lower frequency of predicative adjectives compared to matrix verbs in the corpus. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "24\n",
      "Question 1: What types of agreement are tested in this section on neural language models?\n",
      "Answer 1: Number agreement is tested in both English and French, while gender agreement is tested in French.\n",
      "Question : for the text In this section, we test whether neural language models can use grammatical features hosted on multiple components of a coordination phrase—the coordinated nouns as well as the coordinating conjunction—to drive downstream expectations. We test number agreement in both English and French and gender agreement in French. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "25\n",
      "What is the difference in agreement between nouns coordinated with et versus ou in French?\n",
      "\n",
      "When two nouns are coordinated with et in French, agreement must be masculine if there is one masculine element in the coordinate structure. However, when coordinated with ou, both masculine and feminine agreement is acceptable.\n",
      "Question : for the text In French, if two nouns are coordinated with et (and-coordination), agreement must be masculine if there is one masculine element in the coordinate structure. If the nouns are coordinated with ou (or-coordination), both masculine and feminine agreement is acceptable BIBREF23, BIBREF24. Although linear proximity effects have been tested for a number of languages that employ grammatical gender, as in e.g. Slavic languages BIBREF25, there is no systematic study for French..To assess whether the French neural models learned humanlike gender agreement, we created 24 test items, following the examples in Table TABREF16, and measured the masculine expectation. In our test items, the coordinated subject NP is followed by a predicative adjective, which either takes on masculine or feminine gender morphology..Results from the experiment can be seen in Figure FIGREF17. No models shows qualitative difference based on the coordinator, and only the LSTM (frWaC) shows significant behavior difference between conditions. Here, we find positive masculine expectation in the m_and_m and f_and_m conditions, and negative masculine expectation in the f_and_f condition, as expected. However, in the m_and_f condition, the masculine expectation is not significantly different from zero, where we would expect it to be positive. In the or-coordination conditions, following our expectation, masculine expectation is positive when both conjuncts are masculine and negative when both are feminine. For the LSTM (FTB) and ActionLSTM models, the masculine expectation is positive (although not significantly so) in all conditions, consistent with results in Section SECREF3. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "26\n",
      "What do the results of the experiment suggest about the neural models' ability to learn coordinate NP agreement?\n",
      "\n",
      "The results of the experiment suggest that while the neural models show some success at learning coordinate NP agreement, their success may be the result of an overly simple heuristic. The models appear to rely on a linear combination of nominal number/gender features, with the earlier noun playing a less important role. Ideally, a model that captures human grammatical preferences would show little difference across conditions in the surprisal differential for and conditions and would have positive results in all cases, but the models tested show gradient performance based on the number of plural conjuncts.\n",
      "Question : for the text In simple subject/verb number agreement, the number features of the CoordNP are determined by the coordinating conjunction and the number features of the two coordinated NPs. CoordNPs formed by and are plural and thus require plural verbs; CoordNPs formed by or allow either plural or singular verbs, often with the number features of the noun linearly closest to the verb playing a more important role, although this varies cross-linguistically BIBREF20. Forced-choice preference experiments in BIBREF21 reveal that English native speakers prefer singular agreement when the closest conjunct in an or-CoordNP is singular and plural agreement when the closest conjunct is plural. In French, both singular and plural verbs are possible when two singular NPs are joined via disjunction BIBREF22..In order to assess whether the neural models learn the basic CoordNP licensing for English, we adapted 37 items from BIBREF21, along the 16 conditions outlined in Table TABREF14. Test items consist of the sentence preamble, followed by either the singular or plural BE verb, half the time in present tense (is/are) and half the time in past tense (was/were). We measured the plural expectation, following the procedure in Section SECREF3. We created 24 items using the same conditions as the English experiment to test the models trained in French, using the 3rd person singular and plural form of verb aller, `to go' (va, vont). Within each item, nouns match in gender; across all conditions half the nouns are masculine, half feminine..The results for this experiment can be seen in Figure FIGREF12, with the results for English on the left and French on the right. The results for and are on the top row, or on the bottom row. For all figures the y-axis shows the plural expectation, or the difference in surprisal between the singular condition and the plural condition. Turning first to English-and (Figure FIGREF12), all models show plural expectation (the bars are significantly greater than zero) in the pl_and_pl and sg_and_pl conditions, as expected. For the pl_and_sg condition, only the LSTM (enWiki) and ActionLSTM are greater than zero, indicating humanlike behavior. For the sg_and_sg condition, only the LSTM (enWiki) model shows the correct plural expectation. For the French-and (Figure FIGREF12), all models show positive plural expectation in all conditions, as expected, except for the LSTM (FTB) in the sg_and_sg condition..Examining the results for English-or, we find that all models demonstrate humanlike expectation in the pl_or_pl and sg_or_pl conditions. The LSTM (1B), LSTM (PTB), and RNNG models show zero or negative singular expectation for the pl_or_sg conditions, as expected. However the LSTM (enWiki) and ActionLSTM models show positive plural expectation in this condition, indicating that they have not learned the humanlike generalizations. All models show significantly negative plural expectation in the sg_or_sg condition, as expected. In the French-or cases, models show almost identical behavior to the and conditions, except the LSTM (frWaC) shows smaller plural expectation when singular nouns are linearly proximal to the verb..These results indicate moderate success at learning coordinate NP agreement, however this success may be the result of an overly simple heuristic. It appears that expectation for both plural and masculine continuations are driven by a linear combination of the two nominal number/gender features transferred into log-probability space, with the earlier noun mattering less than the later noun. A model that optimally captures human grammatical preferences should show no or only slight difference across conditions in the surprisal differential for the and conditions, and be greater than zero in all cases. Yet, all the models tested show gradient performance based on the number of plural conjuncts. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "27\n",
      "Question 1: What were the two experiments conducted to control for the potential confound in the models' approach to plural and masculine licensing?\n",
      "\n",
      "Answer 1: The two experiments conducted were the Complex Coordination Control experiments and the Complex Coordination Critical experiments.\n",
      "Question : for the text One possible explanation for the results presented in the previous section is that the models are using a `bag of features' approach to plural and masculine licensing that is opaque to syntactic context: Following a coordinating conjunction surrounded by nouns, models simply expect the following verb to be plural, proportionally to the number of plural nouns..In this section, we control for this potential confound by conducting two experiments: In the Complex Coordination Control experiments we assess models' ability to extend basic CoordNP licensing into sententially-embedded environments, where the CoordNP can serve as an embedded subject. In the Complex Coordination Critical experiments, we leverage the sentential embedding environment to demonstrate that when the CoordNPs cannot plausibly serve as the subject of the embedded phrase, models are able to suppress the previously-demonstrated expectations set up by these phrases. These results demonstrate that models are not following a simple strategy for predicting downstream number and gender features, but are building up CoordNP representations on the fly, conditioned on the local syntactic context. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "28\n",
      "What do the results of the control experiments show for English and French number agreement and French gender agreement?\n",
      "\n",
      "The results show that the models behave similarly to simple coordination contexts in English number agreement and demonstrate significant plural prediction in all cases except for LSTM (FTB) in French number agreement. Only LSTM (frWaC) shows sensitivity to different conditions in French gender agreement, with positive masculine expectation in the m_and_m condition and negative expectation in the f_and_f condition. These results indicate that the behavior shown in Section SECREF13 extends to more complex syntactic environments.\n",
      "Question : for the text Following certain sentential-embedding verbs, CoordNPs serve unambiguously as the subject of the verb's sentence complement and should trigger number agreement behavior in the main verb of the embedded clause, similar to the behavior presented in SECREF13. To assess this, we use the 37 test items in English and 24 items in French in section SECREF13, following the conditions in Table TABREF19 (for number agreement), testing only and coordination. For gender agreement, we use the same test items and conditions for and coordination in Section SECREF15, but with the Coordinated NPs embedded in a context similar to SECREF18. As before, we derived the plural expectation by measuring the difference in surprisal between the singular and plural continuations and the gender expectation by computing the difference in surprisal between the masculine and feminine predicates... Je croyais que les prix et les dépenses étaient importants/importantes..I thought that the.pl price.mpl and the.pl expense.fpl were important.mpl/fpl.I thought that the prices and the expenses were important..The results for the control experiments can be seen in Figure FIGREF20, with English number agreement on the top row, French number agreement in the middle row and French gender agreement on the bottom. The y-axis shows either plural or masculine expectation, with the various conditions along the x-axis. For English number agreement, we find that the models behave similarly as they do for simple coordination contexts. All models show significant plural expectation when the closest noun is plural, with only two models demonstrating plural expectation in the sg_and_sg case. The French number agreement tests show similar results, with all models except LSTM (FTB) demonstrating significant plural prediction in all cases. Turning to French gender agreement, only the LSTM (frWaC) shows sensitivity to the various conditions, with positive masculine expectation in the m_and_m condition and negative expectation in the f_and_f condition, as expected. These results indicate that the behavior shown in Section SECREF13 extends to more complex syntactic environments—in this case to sentential embeddings. Interestingly, for some models, such as the LSTM (1B), behavior is more humanlike when the CoordNP serves as the subject of an embedded sentence. This may be because the model, which has a large number of hidden states and may be extra sensitive to fine-grained syntactic information carried on lexical items BIBREF2, is using the complementizer, that, to drive more robust expectations. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "29\n",
      "What was the purpose of the critical experiment in assessing the models' strategy for CoordNP/verb number agreement?\n",
      "\n",
      "The purpose of the critical experiment was to assess whether the models' strategy for CoordNP/verb number agreement is sensitive to syntactic context.\n",
      "Question : for the text In order to assess whether the models' strategy for CoordNP/verb number agreement is sensitive to syntactic context, we contrast the results presented above to those from a second, critical experiment. Here, two coordinated nouns follow a verb that cannot take a sentential complement, as in the examples given in Table TABREF23. Of the two possible continuations—are or is—the plural is only grammatically licensed when the second of the two conjuncts is plural. In these cases, the plural continuation may lead to a final sentence where the first noun serves as the verb's object and the second introduces a second main clause coordinated with the first, as in I fixed the doors and the windows are still broken. For the same reason, the singular-verb continuation is only licensed when the noun immediately following and is singular..We created 37 test items in both English and French, and calculated the plural expectation. If the models were following a simple strategy to drive CoordNP/verb number agreement, then we should see either no difference in plural expectation across the four conditions or behavior no different from the control experiment. If, however, the models are sensitive to the licensing context, we should see a contrast based solely on the number features of the second conjunct, where plural expectation is positive when the second conjunct is plural, and negative otherwise..Experimental items for a critical gender test were created similarly, as in Example SECREF22. As with plural agreement, gender expectation should be driven solely by the second conjunct: For the f_and_m and m_and_m conditions, the only grammatical continuation is one where the adjectival predicate bears masculine gender morphology. Conversely, for the m_and_f or f_and_f conditions, the only grammatical continuation is one where the adjectival predicate bears feminine morphology. As in SECREF13, we created 24 test items and measured the gender expectation by calculating the difference in surprisal between the masculine and feminine continuations... Nous avons accepté les prix et les dépenses étaient importants/importantes..we have accepted the.pl price.mpl and the expense.fpl were important.mpl/fpl.We have accepted the prices and the expenses were important..The results from the critical experiments are in Figure FIGREF21, with the English number agreement on the top row, French number agreement in the middle and gender expectation on the bottom row. Here the y-axis shows either plural expectation or masculine expectation, with the various conditions are on the x-axis. The results here are strikingly different from those in the control experiments. For number agreement, all models in both languages show strong plural expectation in conditions where the second noun is plural (blue and green bars), as they do in the control experiments. Crucially, when the second noun is singular, the plural expectation is significantly negative for all models (save for the French LSTM (FTB) pl_and_sg condition). Turning to gender agreement, only the LSTM (frWaC) model shows differentiation between the four conditions tested. However, whereas the f_and_m and m_and_f gender expectations are not significantly different from zero in the control condition, in the critical condition they pattern with the purely masculine and purely feminine conditions, indicating that, in this syntactic context, the model has successfully learned to base gender expectation solely off of the second noun..These results are inconsistent with a simple `bag of features' strategy that is insensitive to local syntactic context. They indicate that both models can interpret the same string as either a coordinated noun phrase, or as an NP object and the start of a coordinated VP with the second NP as its subject. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "30\n",
      "What was the purpose of the experiment?\n",
      "The purpose of the experiment was to assess whether neural models use number features hosted on a verb to modulate their expectations for upcoming CoordNPs.\n",
      "Question : for the text In addition to using phrase-level features to drive expectation about downstream lexical items, human processors can do the inverse—use lexical features to drive expectations about upcoming syntactic chunks. In this experiment, we assess whether neural models use number features hosted on a verb to modulate their expectations for upcoming CoordNPs..To assess whether neural language models learn inverted coordination rules, we adapted items from Section SECREF13 in both English (37 items) and French (24 items), following the paradigm in Table TABREF24. The first part of the phrase contains either a plural or singular verb and a plural or singular noun. In this case, we sample the surprisal for the continuations and (or is grammatical in all conditions, so it is omitted from this study). Our expectation is that `and' is less surprising in the Vpl_Nsg condition than in the Vsg_Nsg condition, where a CoordNP is not licensed by the grammar in either French or English (as in *What is the pig and the cat eating?). We also expect lower surprisal for and in the Vpl_Nsg condition, where it is obligatory for a grammatical continuation, than in the Vpl_Npl condition, where it is optional..For French experimental items, the question is embedded into a sentential-complement taking verb, following Example SECREF6, due to the fact that unembedded subject-verb inverted questions sound very formal and might be relatively rare in the training data... Je me demande où vont le maire et.I myself ask where go.3PL the.MSG mayor.MSG and.The results for both languages are shown in Figure FIGREF25, with the surprisal at the coordinator on the y-axis and the various conditions on the x-axis. No model in either language shows a signficant difference in surprisal between the Vpl_Nsg and Vpl_Npl conditions or between the Vpl_Nsg and Vsg_Nsg conditions. The LSTM (1B) shows significant difference between the Vpl_Nsg and Vpl_Npl conditions, but in the opposite direction than expected, with the coordinator less surprising in the latter condition. These results indicate that the models are unable to use the fine-grained context sensitivity to drive expectations for CoordNPs, at least in the inversion setting. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "31\n",
      "What is the focus of the study conducted in this paper? \n",
      "The study assesses whether state-of-the-art neural models can compute and employ phrase-level gender and number features of coordinated subject Noun Phrases (CoordNPs) with two nouns and whether models can deploy these more abstract properties to drive downstream expectations.\n",
      "Question : for the text Humans deploy structure-sensitive expectations to guide processing during natural language comprehension BIBREF0. While it has been shown that neural language models show similar structure-sensitivity in their predictions about upcoming material BIBREF1, BIBREF2, previous work has focused on dependencies that are conditioned by features attached to a single word, such as subject number BIBREF3, BIBREF4 or wh-question words BIBREF5. There has been no systematic investigation into models' ability to compute phrase-level features—features that are attached to a set of words—and whether models can deploy these more abstract properties to drive downstream expectations..In this work, we assess whether state-of-the-art neural models can compute and employ phrase-level gender and number features of coordinated subject Noun Phrases (CoordNPs) with two nouns. Typical syntactic phrases are endocentric: they are headed by a single child, whose features determine the agreement requirements for the entire phrase. In Figure FIGREF1, for example, the word star heads the subject NP The star; since star is singular, the verb must be singular. CoordNPs lack endocentricity: neither conjunct NP solely determines the features of the NP as a whole. Instead, these feature values are determined by compositional rules sensitive to the features of the conjuncts and the identity of the coordinator. In Figure FIGREF1, because the coordinator is and, the subject NP number is plural even though both conjuncts (the star and the moon) are singular. As this case demonstrates, the agreement behavior for CoordNPs must be driven by more abstract, constituent-level representations, and cannot be reduced to features hosted on a single lexical item..We use four suites of experiments to assess whether neural models are able to build up phrase-level representations of CoordNPs on the fly and deploy them to drive humanlike behavior. First, we present a simple control experiment to show that models can represent number and gender features of non-coordinate NPs (Non-coordination Agreement). Second, we show that models modulate their expectations for downstream verb number based on the CoordNP's coordinating conjunction combined with the features of the coordinated nouns (Simple Coordination). We rule out the possibility that models are using simple heuristics by designing a set of stimuli where a simple heuristic would fail due to structural ambiguity (Complex Coordination). The striking success for all models in this experiment indicates that even neural models with no explicit hierarchical bias, trained on a relatively small amount of text are able to learn fine-grained and robust generalizations about the interaction between CoordNPs and local syntactic context. Finally, we use subject–auxiliary inversion to test whether an upstream lexical item modulates model expectation for the phrasal-level features of a downstream CoordNP (Inverted Coordination). Here, we find that all models are insensitive to the fine-grained features of this particular syntactic context. Overall, our results indicate that neural models can learn fine-grained information about the interaction of Coordinated NPs and local syntactic context, but their behavior remains unhumanlike in many key respects. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "32\n",
      "Question 1: What are the three possible actions in the text model for linearized bracketed tree structure?\n",
      "\n",
      "Answer 1: The three possible actions in the text model for linearized bracketed tree structure are opening a new non-terminal node and opening bracket, generating a terminal node, and closing a bracket.\n",
      "Question : for the text models the linearized bracketed tree structure of a sentence by learning to predict the next action required to construct a phrase-structure parse BIBREF16. The action space consists of three possibilities: open a new non-terminal node and opening bracket; generate a terminal node; and close a bracket. To compute surprisal values for a given token, we approximate $P(w_i|w_{1\\cdots i-1)}$ by marginalizing over the most-likely partial parses found by word-synchronous beam search BIBREF17. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "33\n",
      "Question 1: What method is used to estimate surprisal in the model?\n",
      "\n",
      "Answer 1: The model uses word-synchronous beam search to estimate surprisal, following the approach in BIBREF19, and employs the same hyper-parameter settings as BIBREF18.\n",
      "Question : for the text jointly model the word sequence as well as the underlying syntactic structure BIBREF18. Following BIBREF19, we estimate surprisal using word-synchronous beam search BIBREF17. We use the same hyper-parameter settings as BIBREF18..The annotation schemes used to train the syntactically-supervised models differ slightly between French and English. In the PTB (English) CoordNPs are flat structures bearing an `NP' label. In FTB (French), CoordNPs are binary-branching, labeled as NPs, except for the phrasal node dominating the coordinating conjunction, which is labeled `COORD'. We examine the effects of annotation schemes on model performance in Appendix SECREF8. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "34\n",
      "Question 1: What corpora were used to train the LSTM language models?\n",
      "\n",
      "Answer 1: Two LSTM language models were trained on the sentences from Penn Treebank (LSTM (PTB)) and French Treebank (LSTM (FTB)). Additionally, two pretrained English language models were incorporated, one trained on the Billion Word benchmark (LSTM (1B)) and the other trained on English Wikipedia (LSTM (enWiki)). Lastly, a large LSTM language model (LSTM (frWaC)) was trained on a random subset of the frWaC dataset.\n",
      "Question : for the text are trained to output the probability distribution of the upcoming word given a context, without explicitly representing the structure of the context BIBREF9, BIBREF10. We trained two two-layer recurrent neural language models with long short-term memory architecture BIBREF11 on a relatively small corpus. The first model, referred as `LSTM (PTB)' in the following sections, was trained on the sentences from Penn Treebank BIBREF12. The second model, referred as `LSTM (FTB)', was trained on the sentences from French Treebank BIBREF13. We set the size of input word embedding and LSTM hidden layer of both models as 256..We also compare LSTM language models trained on large corpora. We incorporate two pretrained English language models: one trained on the Billion Word benchmark (referred as `LSTM (1B)') from BIBREF14, and the other trained on English Wikipedia (referred as `LSTM (enWiki)') from BIBREF3. For French, we trained a large LSTM language model (referred as `LSTM (frWaC)') on a random subset (about 4 million sentences, 138 million word tokens) of the frWaC dataset BIBREF15. We set the size of the input embeddings and hidden layers to 400 for the LSTM (frWaC) model since it is trained on a large dataset. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "35\n",
      "What is the psycholinguistics paradigm for model assessment?\n",
      "\n",
      "The psycholinguistics paradigm for model assessment involves testing models using hand-crafted sentences designed to test underlying network knowledge. The assumption is that if a model implicitly learns humanlike linguistic knowledge during training, its expectations for upcoming words should qualitatively match human expectations in novel contexts. Surprisal values are used to quantify model expectations.\n",
      "Question : for the text To determine whether state-of-the-art neural architectures are capable of learning humanlike CoordNP/verb agreement properties, we adopt the psycholinguistics paradigm for model assessment. In this paradigm the models are tested using hand-crafted sentences designed to test underlying network knowledge. The assumption here is that if a model implicitly learns humanlike linguistic knowledge during training, its expectations for upcoming words should qualitatively match human expectations in novel contexts. For example, BIBREF1 and BIBREF6 assessed how well neural models had learned the subject/verb number agreement by feeding them with the prefix The keys to the cabinet .... If the models predicted the grammatical continuation are over the ungrammatical continuation is, they can be said to have learned the number agreement insofar as the number of the head noun and not the number of the distractor noun, cabinet, drives expectations about the number of the matrix verb..If models are able to robustly modulate their expectations based on the internal components of the CoordNP, this will provide evidence that the networks are building up a context-sensitive phrase-level representation. We quantify model expectations as surprisal values. Surprisal is the negative log-conditional probability $S(x_i) = -\\log _2 p(x_i|x_1 \\dots x_{i-1})$ of a sentence's $i^{th}$ word $x_i$ given the previous words. Surprisal tells us how strongly $x_i$ is expected in context and is known to correlate with human processing difficulty BIBREF7, BIBREF0, BIBREF8. In the CoordNP/Verb agreement studies presented here, cases where the proceeding context sets high expectation for a number-inflected verb form $w_i$, (e.g. singular `is') we would expect $S(w_i)$ to be lower than its number-mismatched counterpart (e.g. plural `are'). generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "36\n",
      "Question 1: What do Table TABREF28 and TABREF29 present? \n",
      "\n",
      "Answer 1: Table TABREF28 and TABREF29 present statistics of subject/predicate agreement patterns in the Penn Treebank (PTB) and French Treebank (FTB).\n",
      "Question : for the text We present statistics of subject/predicate agreement patterns in the Penn Treebank (PTB) and French Treebank (FTB) in Table TABREF28 and TABREF29. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "37\n",
      "What was the purpose of training two additional RNNG models on sentences from the Penn Treebank dataset? \n",
      "\n",
      "The purpose of training two additional RNNG models on sentences from the Penn Treebank dataset was to test whether an explicit COORD phrasal tag improves model performance and to investigate the effects of CoordNP annotation schemes on the behaviors of structurally-supervised models.\n",
      "Question : for the text This section further investigates the effects of CoordNP annotation schemes on the behaviors of structurally-supervised models. We test whether an explicit COORD phrasal tag improves model performance. We trained two additional RNNG models on 38,546 sentences from the Penn Treebank annotated with two different schemes: The first, RNNG (PTB-control) was trained with the original Penn Treebank annotation. The second, RNNG (PTB-coord), was trained on the same sentences, but with an extended coordination annotation scheme, meant to employ the scheme employed in the FTB, adapted from BIBREF26. We stripped empty categories from their scheme and only kept the NP-COORD label for constituents inside a coordination structure. Figure FIGREF26 illustrates the detailed annotation differences between two datasets. We tested both models on all the experiments presented in Sections SECREF3-SECREF6 above..Turning to the results of these six experiments: We see little difference between the two models in the Non-coordination agreement experiment. For the Complex coordination control and Complex coordination critical experiments, both models are largely the same as well. However, in the Simple and-coordination and Simple or-coordination experiments the values for all conditions are shifted upwards for the RNNG PTB-coord model, indicating higher over-all preference for the plural continuation. Furthermore, the range of values is reduced in the RNNG PTB-coord model, compared to the RNNG PTB-control model. These results indicate that adding an explicit COORD phrasal label does not drastically change model performance: Both models still appear to be using a linear combination of number features to drive plural vs. singular expectation. However, the explicit representation has made the interior of the coordination phrase more opaque to the model (each feature matters less) and has slightly shifted model preference towards plural continuations. In this sense, the PTB-coord model may have learned a generalization about CoordNPs, but this generalization remains unlike the ones learned by humans. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "38\n",
      "Question 1: Who provided financial support for this research?\n",
      "\n",
      "Answer 1: The research was financially supported by the Ministry of Science and Technology (MOST) in Taiwan.\n",
      "Question : for the text We would like to thank reviewers for their insightful comments on the paper. This work was financially supported by Ministry of Science and Technology (MOST) in Taiwan. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "39\n",
      "Question 1: What type of attention mechanisms are proposed for the hierarchical decoder and how are the weights determined?\n",
      "\n",
      "Answer 1: The proposed attention mechanisms for the hierarchical decoder are content-based, and the weights are determined based on hidden states of neural models, including the current step's hidden state, hidden states from previous decoder layers, and a learned weight matrix. Attention values are calculated at each decoding step and used to compute a weighted sum as a context vector, which is then concatenated to decoder inputs as additional information.\n",
      "Question : for the text In order to model the relationship between layers in a generating hierarchy, we further design attention mechanisms for the hierarchical decoder. The proposed attention mechanisms are content-based, which means the weights are determined based on hidden states of neural models: DISPLAYFORM0 .where INLINEFORM0 is the hidden state at the current step, INLINEFORM1 are the hidden states from the previous decoder layer, and INLINEFORM2 is a learned weight matrix. At each decoding step, attention values INLINEFORM3 are calculated by these methods and then used to compute the weighted sum as a context vector, which is then concatenated to decoder inputs as additional information. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "40\n",
      "What is the proposed approach to address the difficulty of generating complex sequences with a simple encoder-decoder structure in NLG?\n",
      "\n",
      "The proposed approach is to use a hierarchical decoder, where several decoding layers are responsible for learning different types of linguistic patterns instead of learning all relevant knowledge together. Each layer is only responsible for decoding a portion of the required knowledge, and the linguistic knowledge is incorporated into the decoding process and divided into several subsets. Part-of-speech tags are used as the additional linguistic features to construct the decoding hierarchy, and each layer is responsible for decoding the words associated with a specific set of POS patterns.\n",
      "Question : for the text In spite of the intuitive and elegant design of the seq2seq model, it is still difficult to generate complex and decent sequences by a simple encoder-decoder structure, because a single decoder is not capable of learning all diction, grammar, and other related linguistic knowledge at the same time. Some prior work applied additional techniques such as reranker and beam-search to select a better result among multiple generated sequences BIBREF13 , BIBREF16 . However, it is still an unsolved issue to the NLG community..Therefore, we propose a hierarchical decoder to address the above issue, where the core idea is to allow the decoding layers to focus on learning different types of patterns instead of learning all relevant knowledge together. The hierarchical decoder is composed of several decoding layers, each of which is only responsible for learning a portion of the required knowledge. Namely, the linguistic knowledge can be incorporated into the decoding process and divided into several subsets..We use part-of-speech (POS) tags as the additional linguistic features to construct the decoding hierarchy in this paper, where POS tags of the words in the target sentence are separated into several subsets, and each layer is responsible for decoding the words associated with a specific set of POS patterns. An example is shown in the right part of Figure FIGREF2 , where the first layer at the bottom is in charge of decoding nouns, pronouns, and proper nouns, and the second layer is for verbs, and so on. The prior work manually designed the decoding hierarchy by considering the subjective intuition about how children learn to speak BIBREF8 : infants first learn to say keywords, which are often nouns. For example, when an infant says “Daddy, toilet.”, it actually means “Daddy, I want to go to the toilet.”. Along with the growth of the age, children learn more grammars and vocabulary and then start adding verbs to the sentences, further adding adverbs, and so on. However, the hand-crafted linguistic order may not be optimal, so we experiment and analyze the model on various generating linguistic hierarchies to deeply investigate the effect of linguistic pattern ordering..In the hierarchical decoder, the initial state of each GRU-based decoding layer INLINEFORM0 is the extracted feature INLINEFORM1 from the encoder, and the input at every step is the last predicted token INLINEFORM2 concatenated with the output from the previous layer INLINEFORM3 , DISPLAYFORM0 .where INLINEFORM0 is the INLINEFORM1 -th hidden state of the INLINEFORM2 -th GRU decoding layer and INLINEFORM3 is the INLINEFORM4 -th outputted word in the INLINEFORM5 -th layer. We use the cross entropy loss as our training objective for optimization, where the difference between the predicted distribution and target distribution is minimized. To facilitate training and improve the performance, several strategies including scheduled sampling, a repeat input mechanism, curriculum learning, and an attention mechanism are utilized. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "41\n",
      "Question 1: What does the paper investigate?\n",
      "Answer 1: The paper investigates a seq2seq-based model with a hierarchical decoder that utilizes different linguistic patterns.\n",
      "Question : for the text This paper investigates the seq2seq-based model with a hierarchical decoder that leverages various linguistic patterns. The experiments on different generating linguistic orders demonstrates the generalization about the proposed hierarchical decoder, which is not limited to a specific generating hierarchy. However, there is no universal decoding hierarchy, while the main factor for designing a suitable generating order is the nature of the dataset. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "42\n",
      "Question 1: How does the proposed hierarchical decoder help with training the network?\n",
      "\n",
      "Answer 1: The proposed hierarchical decoder, which consists of several decoding layers, is suitable for applying the curriculum learning principle. This principle suggests that progressively harder tasks could significantly accelerate a network's training. The training procedure is to train each decoding layer for some epochs from the bottommost layer to the topmost one.\n",
      "Question : for the text The proposed hierarchical decoder consists of several decoding layers, the expected output sequences of upper layers are longer than the ones in the lower layers. The framework is suitable for applying the curriculum learning BIBREF20 , of which core concept is that a curriculum of progressively harder tasks could significantly accelerate a network’s training. The training procedure is to train each decoding layer for some epochs from the bottommost layer to the topmost one. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "43\n",
      "Question 1: What is the architecture of the proposed hierarchical NLG model?\n",
      "Answer 1: The model architecture is based on an encoder-decoder (seq2seq) structure with attentional hierarchical decoders. The model includes encoding and decoding phases where a given semantic representation sequence is fed into a RNN-based encoder to capture the temporal dependency and then into an RNN-based decoder to decode word sequences. The recurrent unit of the encoder is bidirectional gated recurrent unit (GRU).\n",
      "Question : for the text The framework of the proposed hierarchical NLG model is illustrated in Figure FIGREF2 , where the model architecture is based on an encoder-decoder (seq2seq) structure with attentional hierarchical decoders BIBREF14 , BIBREF15 . In the encoder-decoder architecture, a typical generation process includes encoding and decoding phases: First, a given semantic representation sequence INLINEFORM0 is fed into a RNN-based encoder to capture the temporal dependency and project the input to a latent feature space; the semantic representation sequence is also encoded into an one-hot representation as the initial state of the encoder in order to maintain the temporal-independent condition as shown in the left part of Figure FIGREF2 . The recurrent unit of the encoder is bidirectional gated recurrent unit (GRU) BIBREF14 , DISPLAYFORM0 .Then the encoded semantic vector, INLINEFORM0 , is fed into an RNN-based decoder as the initial state to decode word sequences, as shown in the right part of Figure FIGREF2 . generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "44\n",
      "What is the goal of NLG in a dialogue system?\n",
      "The goal of NLG in a dialogue system is to generate natural language sentences given the semantics provided by the dialogue manager to feedback to users. It is a key component to a dialogue system and crucial for better user experience.\n",
      "Question : for the text Spoken dialogue systems that can help users to solve complex tasks have become an emerging research topic in artificial intelligence and natural language processing areas BIBREF0 , BIBREF1 , BIBREF2 , BIBREF3 . With a well-designed dialogue system as an intelligent personal assistant, people can accomplish certain tasks more easily via natural language interactions. Today, there are several virtual intelligent assistants, such as Apple's Siri, Google's Home, Microsoft's Cortana, and Amazon's Alexa, in the market. A typical dialogue system pipeline can be divided into several parts: a recognized result of a user's speech input is fed into a natural language understanding module (NLU) to classify the domain along with domain-specific intents and fill in a set of slots to form a semantic frame BIBREF4 , BIBREF5 , BIBREF6 . A dialogue state tracking (DST) module predicts the current state of the dialogue by means of the semantic frames extracted from multi-turn conversations. Then the dialogue policy determines the system action for the next step given the current dialogue state. Finally the semantic frame of the system action is then fed into a natural language generation (NLG) module to construct a response utterance to the user BIBREF7 , BIBREF8 ..As a key component to a dialogue system, the goal of NLG is to generate natural language sentences given the semantics provided by the dialogue manager to feedback to users. As the endpoint of interacting with users, the quality of generated sentences is crucial for better user experience. The common and mostly adopted method is the rule-based (or template-based) method BIBREF9 , which can ensure the natural language quality and fluency. In spite of robustness and adequacy of the rule-based methods, frequent repetition of identical, tedious output makes talking to a template-based machine unsatisfactory. Furthermore, scalability is an issue, because designing sophisticated rules for a specific domain is time-consuming BIBREF10 ..Recurrent neural network-based language model (RNNLM) have demonstrated the capability of modeling long-term dependency in sequence prediction by leveraging recurrent structures BIBREF11 , BIBREF12 . Previous work proposed an RNNLM-based NLG that can be trained on any corpus of dialogue act-utterance pairs without hand-crafted features and any semantic alignment BIBREF13 . The following work based on sequence-to-sequence (seq2seq) further obtained better performance by employing encoder-decoder structure with linguistic knowledge such as syntax trees BIBREF14 , BIBREF15 , BIBREF16 , BIBREF17 . However, due to grammar complexity and lack of diction knowledge, it is still challenging to generate long and complex sentences by a simple encoder-decoder structure..To address the issue, previous work attempted separating decoding jobs in a decoding hierarchy, which is constructed in terms of part-of-speech (POS) tags BIBREF8 . The original single decoding process is separated into a multi-level decoding hierarchy, where each decoding layer generates words associated with a specific POS set. This paper extends the idea to a more flexible design by incorporating attention mechanisms into the decoding hierarchy. Because prior work designs the decoding hierarchy in a hand-crafted manner based on a subjective intuition BIBREF8 , in this work, we experiment on various generating hierarchies to investigate the importance of linguistic pattern ordering in hierarchical language generation. The experiments show that our proposed method outperforms the classic seq2seq model with a smaller model size; in addition, the concept of the hierarchical decoder is proven general enough for various generating hierarchies. Furthermore, this paper also provides the design guidelines and insights of designing the decoding hierarchy. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "45\n",
      "What is the purpose of the repeat-input mechanism in hierarchical decoding?\n",
      "\n",
      "The repeat-input mechanism is designed to repeat outputs from the last layer as inputs until the current decoding layer outputs the same token. This approach has two main benefits: telling the decoder that repeated tokens are important to encourage generation, and mitigating the impact of length differences between expected output sequences of different layers.\n",
      "Question : for the text The concept of the hierarchical decoding is to hierarchically generate the sequence, gradually adding words associated with different linguistic patterns. Therefore, the generated sequences from the decoders become longer as the generating process proceeds to the higher decoding layers, and the sequence generated by a upper layer should contain the words predicted by the lower layers. To facilitate the behavior, previous work designs a strategy that repeats the outputs from the last layer as inputs until the current decoding layer outputs the same token, so-called the repeat-input mechanism BIBREF8 . This approach offers at least two merits: (1) Repeating inputs tells the decoder that the repeated tokens are important to encourage the decoder to generate them. (2) If the expected output sequence of a layer is much shorter than the one of the next layer, the large difference in length becomes a critical issue of the hierarchical decoder, because the output sequence of a layer will be fed into the next layer. With the repeat-input mechanism, the impact of length difference can be mitigated. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "46\n",
      "What is the benefit of using a hierarchical decoder in NLG?\n",
      "The benefit of using a hierarchical decoder in NLG is that it separates the generation process into several phases, which has been proven to be a promising method for improving model performance. Additionally, it allows for the use of curriculum learning strategies, which can lead to even greater improvements in model performance. The determining factor for the best generating hierarchy is based on the dataset and factors such as the distribution over length of subsequences and the linguistic nature of the dataset.\n",
      "Question : for the text In the experiments, we borrow the idea of hierarchical decoding proposed by the previous work BIBREF8 and investigate various extensions of generating hierarchies. To examine the effectiveness of hierarchical decoders, we control our model size to be smaller than the baseline's. Specifically, the decoder in the baseline seq2seq model has hidden layers of size 400, while our models with hierarchical decoders have four decoding layers of size 100 for fair comparison..Table TABREF13 compares the performance between a baseline and proposed models with different generating linguistic orders. For all generating hierarchies with different orders, simply replacing the decoder by a hierarchical decoder achieves significant improvement in every evaluation metrics; for example, the topmost generating hierarchy in Table TABREF13 has 49.25% improvement in BLEU, 30.03% in ROUGE-1, 96.48% in ROUGE-2, and 25.99% in ROUGE-L respectively. In other words, separating the generation process into several phases is proven to be a promising method. Performing curriculum learning strategy offers a considerable improvement, take the topmost generating hierarchy in Table TABREF13 for example, this method yields a 102.07% improvement in BLEU, 48.26% in ROUGE-1, 144.8% in ROUGE-2, and 39.18% in ROUGE-L. Despite that applying repeat-input mechanism alone does not offer benefit, combining these two strategies together further achieves the best performance. Note that these methods do not require any additional parameters..Unfortunately, even some of the attentional hierarchical decoders achieve the best results in the generating hierarchies (Table TABREF18 ). Mostly, the additional attention mechanisms are not capable of bringing benefit for model performance. The reason may be that the decoding process is designed for gradually importing words in the specific set of linguistic patterns to the output sequence, each decoder layer is responsible of copying the output tokens from the previous layer and insert new words into the sequence precisely. Because of this nature, a decoder needs explicit information of the structure of a sentence rather than implicit high-level latent information. For instance, when a decoder is trying to insert some Verb words into the output sequence, knowing the position of subject and object would be very helpful..The above results show that among these six different generating hierarchy, the generating order: (1) verbs INLINEFORM0 (2) nouns, proper nouns, and pronouns INLINEFORM1 (3) adjectives and adverbs INLINEFORM2 (4) the other POS tags yields the worst performance. Table TABREF23 shows that the gap of average length of target sequences between the first and the second decoder layer is the largest among all the hierarchies; in average, the second decoder needs to insert up to 8 words into the sequence based on 3.62 words from the first decoder layer in this generation process, which is absolutely difficult. The essence of the hierarchical design is to separate the job of the decoder into several phases; if the job of each phase is balanced, it is intuitive that it is more suitable for applying curriculum learning and improve the model performance..The model performance is also related to linguistic structures of sentences: the fifth and the sixth generating hierarchies in Table TABREF13 have very similar trends, where the length of target sentences of each decoder layer is almost identical as shown in Table TABREF23 . However, the model performance differs a lot. An adverb word could be used to modify anything but nouns and pronouns, which means that the number of adverbs used for modifying verbs would be a factor to determine the generating order as well. In our cases, almost all adverbs in the dataset are used to describe adjectives, indicating that generating verbs before inserting adverbs to sequences may not provide enough useful information; instead, it would possibly obstruct the model learning. We can also find that in all experiments, inserting adverbs before verbs would be better..In summary, the concept of the hierarchical decoder is simple and useful, separating a difficult job to many phases is demonstrated to be a promising direction and not limited to a specific generating hierarchy. Furthermore, the generating linguistic orders should be determined based on the dataset, and the important factors include the distribution over length of subsequences and the linguistic nature of the dataset for designing a proper generating hierarchy in NLG. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "47\n",
      "Question 1: What is teacher forcing?\n",
      "Answer 1: Teacher forcing is a strategy for training RNN that uses the model output from a prior time step as an input, and it works by using the expected output at the current time step as the input at the next time step, rather than the output generated by the network.\n",
      "\n",
      "Question 2: What is the scheduled sampling approach?\n",
      "Answer 2: The scheduled sampling approach is a method of triggering teacher forcing techniques with a certain probability. In the proposed framework, two types of scheduled sampling approaches are designed - inner-layer and inter-layer.\n",
      "\n",
      "Question 3: How does inner-layer schedule sampling work?\n",
      "Answer 3: Inner-layer schedule sampling is the classic teacher forcing strategy where the expected output at the current time step is used as the input at the next time step.\n",
      "\n",
      "Question 4: How does inter-layer schedule sampling work?\n",
      "Answer 4: Inter-layer schedule sampling uses the labels instead of the actual output tokens of the last layer.\n",
      "Question : for the text Teacher forcing BIBREF18 is a strategy for training RNN that uses model output from a prior time step as an input, and it works by using the expected output at the current time step INLINEFORM0 as the input at the next time step, rather than the output generated by the network. The teacher forcing techniques can also be triggered only with a certain probability, which is known as the scheduled sampling approach BIBREF19 . We adopt scheduled sampling methods in our experiments. In the proposed framework, an input of a decoder contains not only the output from the last step but one from the last decoding layer. Therefore, we design two types of scheduled sampling approaches – inner-layer and inter-layer..Inner-layer schedule sampling is the classic teacher forcing strategy: DISPLAYFORM0 .Inter-layer schedule sampling uses the labels instead of the actual output tokens of the last layer: DISPLAYFORM0  generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "48\n",
      "What is the E2E NLG challenge dataset utilized in the experiments?\n",
      "The E2E NLG challenge dataset BIBREF21 is utilized in the experiments, which is a crowd-sourced dataset of 50k instances in the restaurant domain.\n",
      "Question : for the text The E2E NLG challenge dataset BIBREF21 is utilized in our experiments, which is a crowd-sourced dataset of 50k instances in the restaurant domain. Our models are trained on the official training set and verified on the official testing set. As shown in Figure FIGREF2 , the inputs are semantic frames containing specific slots and corresponding values, and the outputs are the associated natural language utterances with the given semantics. For example, a semantic frame with the slot-value pairs “name[Bibimbap House], food[English], priceRange[moderate], area [riverside], near [Clare Hall]” corresponds to the target sentence “Bibimbap House is a moderately priced restaurant who's main cuisine is English food. You will find this local gem near Clare Hall in the Riverside area.”..The data preprocessing includes trimming punctuation marks, lemmatization, and turning all words into lowercase. To prepare the labels of each layer within the hierarchical structure of the proposed method, we utilize spaCy toolkit to perform POS tagging for the target word sequences. Some properties such as names of restaurants are delexicalized (for example, replaced with a symbol “RESTAURANT_NAME”) to avoid data sparsity. In our experiments, we perform six different generating linguistic orders, in which each hierarchy is constructed based on different permutations of the POS tag sets: (1) nouns, proper nouns, and pronouns (2) verbs (3) adjectives and adverbs (4) others..The probability of activating inter-layer and inner-layer teacher forcing is set to 0.5, the probability of teacher forcing is attenuated every epoch, and the decaying ratio is 0.9. The models are trained for 20 training epochs without early stop; when curriculum learning is applied, only the first layer is trained during first five epochs, the second decoder layer starts to be trained at the sixth epoch, and so on. To evaluate the quality of the generated sequences regarding both precision and recall, the evaluation metrics include BLEU and ROUGE (1, 2, L) scores with multiple references BIBREF22 . generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "49\n",
      "Question 1: What is the objective of the proposed model in hierarchical NLG? \n",
      "\n",
      "Answer 1: The objective of the proposed model in hierarchical NLG is to optimize the conditional probability, so that the difference between the predicted distribution and the target distribution can be minimized. This is done through training each decoder with curriculum learning based on the objective. The labels in this case are word labels and the number of samples is given by INLINEFORM0.\n",
      "Question : for the text The objective of the proposed model is to optimize the conditional probability INLINEFORM0 , so that the difference between the predicted distribution and the target distribution, INLINEFORM1 , can be minimized: DISPLAYFORM0 .where INLINEFORM0 is the number of samples and the labels INLINEFORM1 are the word labels. Each decoder in the hierarchical NLG is trained based on curriculum learning with the objective. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "50\n",
      "What pre-trained word embeddings were used in the model?\n",
      "Answer 1: The model uses 300-dim word2vec pre-trained from Google News as the pre-trained word embeddings.\n",
      "Question : for the text Following the settings of qin-EtAl:2017:Long, we use two splitting methods of PDTB dataset for comprehensive comparison. The first is PDTB-Lin BIBREF3 , which uses section 2-21, 22 and 23 as training, dev and test sets respectively. The second is PDTB-Ji BIBREF8 , which uses section 2-20, 0-1, and 21-22 as training, dev and test sets respectively. According to TACL536, five relation types have few training instances and no dev and test instance. Removing the five types, there remain 11 second level types. During training, instances with more than one annotated relation types are considered as multiple instances, each of which has one of the annotations. At test time, a prediction that matches one of the gold types is considered as correct. All sentences in the dataset are padded or truncated to keep the same 100-word length..For the results of both splitting methods, we share some hyperparameters. Table 1 is some of the shared hyperparameter settings. The pre-trained word embeddings are 300-dim word2vec BIBREF32 pre-trained from Google News. So $d_w = 300, d_s = 100, d_c = 300$ , then for the final embedding ( $\\mathbf {e}_i$ ), $d_e = 700$ . For the encoder block in sentence-level module, kernel size is same for every layer. We use AdaGrad optimization BIBREF33 ..The encoder block layer number is different for the two splitting methods. The layer number for PDTB-Ji splitting method is 4, and the layer number for PDTB-Lin splitting method is 5..Compared to other recent state-of-the-art systems in Table 2 , our model achieves new state-of-the-art performance in two splitting methods with great improvements. As to our best knowledge, our model is the first one that exceeds the 48% accuracy in 11-way classification..Ablation Study.To illustrate the effectiveness of our model and the contribution of each module, we use the PTDB-Ji splitting method to do a group of experiments. For the baseline model, we use 4 layer stacked convolutional encoder blocks without the residual connection in the block with only pre-trained word embeddings. We only use the output of the last layer and the output is processed by 2-max pooling without attention and sent to the relation classifier and connective classifier. Without the two residual connections, using 4 layers may be not the best for baseline model but is more convenient to comparison..Firstly, we add modules from high level to low level accumulatively to observe the performance improvement. Table 3 is the results, which demonstrate that every module has considerable effect on the performance..Then we test the effects of the two residual connections on the performance. The results are in Table 3 . The baseline $^+$ means baseline + bi-attention, i.e., the second row of Table 3 . We find that Res 1 (residual connection in the block) is much more useful than Res 2 (residual connection for pair representation), and they work together can bring even better performance..Without ELMo (the same setting as 4-th row in Table 3 ), our data settings is the same as qin-EtAl:2017:Long whose performance was state-of-the-art and will be compared directly. We see that even without the pre-trained ELMo encoder, our performance is better, which is mostly attributed to our better sentence pair representations..Subword-Level Embedding For the usefulness of subword-level embedding, we compare its performance to a model with character-level embedding, which was ever used in qin-zhang-zhao:2016:COLING. We use the same model setting as the 4-th row of Table 3 , and then replace subword with character sequence. The subword embedding augmented result is 47.03%, while the character embedding result is 46.37%, which verifies that the former is a better input representation for the task..Parameters for Sentence-Level Module As previously discussed, argument specific parameter settings may result in better sentence-level encoders. We use the model which is the same as the third row in Table 3 . If shared parameters are used, the result is 45.97%, which is lower than argument specific parameter settings (46.29%). The comparison shows argument specific parameter settings indeed capture the difference of argument representations and facilitate the sentence pair representation..Encoder Block Type and Layer Number In section 3.3, we consider two encoder types, here we compare their effects on the model performance. Like the previous part, The model setting is also the same as the third row in Table 3 except for the block type and layer number. The results are shown in Figure 4 ..The results in the figure show that both types may reach similar level of top accuracies, as the order of word is not important to the task. We also try to add position information to the convolutional type encoder, and receive a dropped accuracy. This further verifies the order information does not matter too much for the task. For most of the other numbers of layers, the recurrent type shows better, as the number of layers has an impact on the window size of convolutional encoders. When convolutional type is used, the training procedure is much faster, but choosing the suitable kernel size needs extra efforts..Bi-Attention.We visualize the attention weight of one instance in Figure 5 . For lower layers, the attended part is more concentrated. For higher layers, the weights are more average and the attended part moves to the sentence border. This is because the window size is bigger for higher layers, and the convolutional kernel may have higher weights on words at the window edge. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "1\n",
      "Question 1: What is the dataset splitting method used for the first level classification?\n",
      "\n",
      "Answer 1: The dataset splitting method used for the first level classification is the same as PDTB-Ji without removing instances, following the settings of previous works.\n",
      "Question : for the text Settings For the first level classification, we perform both 4-way classification and one-vs-others binary classification. Following the settings of previous works, the dataset splitting method is the same as PDTB-Ji without removing instances. The model uses 5 block layers with kernel size 3, other details are the same as that for 11-way classification on PDTB-Ji..Results Table 4 is the result comparison on first level classification. For binary classification, the result is computed by $F_1$ score (%), and for 4-way classification, the result is computed by macro average $F_1$ score (%). Our model gives the state-of-the-art performance for 4-way classification by providing an $F_1$ score greater than 50% for the first time according to our best knowledge. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "2\n",
      "What are the two classifiers used in the model?\n",
      "\n",
      "The two classifiers used in the model are one for relation classification and another one for connective classification.\n",
      "Question : for the text We use two classifiers in our model. One is for relation classification, and another one is for connective classification. The classifier is only a multiple layer perceptron (MLP) with softmax layer. qin-EtAl:2017:Long used adversarial method to utilize the connectives, but this method is not suitable for our adopted attention module since the attended part of a sentence will be distinctly different when the argument is with and without connectives. They also proposed a multi-task method that augments the model with an additional classifier for connective prediction, and the input of it is also the pair representation. It is straightforward and simple enough, and can help the model learn better representations, so we include this module in our model. The implicit connectives are provided by PDTB 2.0 dataset, and the connective classifier is only used during training. The loss function for both classifiers is cross entropy loss, and the total loss is the sum of the two losses, i.e., $Loss = Loss_{relation} + Loss_{connective}$ . generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "3\n",
      "Question 1: What is the proposed approach for implicit discourse relation recognition in the paper?\n",
      "\n",
      "Answer 1: The proposed approach in the paper is a deeper neural model augmented by different grained text representations that work together to produce task-related representations of the sentence pair.\n",
      "Question : for the text In this paper, we propose a deeper neural model augmented by different grained text representations for implicit discourse relation recognition. These different module levels work together and produce task-related representations of the sentence pair. Our experiments show that the model is effective and achieve the state-of-the-art performance. As to our best knowledge, this is the first time that an implicit discourse relation classifier gives an accuracy higher than 48% for 11-way and an $F_1$ score higher than 50% for 4-way classification tasks. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "4\n",
      "What are the three levels of senses in PDTB 2.0?\n",
      "The three levels of senses in PDTB 2.0 are Level-1 Class, Level-2 Type, and Level-3 Subtypes.\n",
      "Question : for the text Our model is evaluated on the benchmark PDTB 2.0 for two types of classification tasks..PDTB 2.0 has three levels of senses: Level-1 Class, Level-2 Type, and Level-3 Subtypes. The first level consists of four major relation Classes: COMPARISON, CONTINGENCY, EXPANSION, and TEMPORAL. The second level contains 16 Types..All our experiments are implemented by PyTorch. The pre-trained ELMo encoder is from AllenNLP toolkit BIBREF31 . generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "5\n",
      "What is the purpose of improving the task of implicit discourse relation recognition?\n",
      "\n",
      "The purpose of improving the task of implicit discourse relation recognition is to aid in downstream tasks such as machine translation and question answering.\n",
      "Question : for the text  This work is licenced under a Creative Commons Attribution 4.0 International Licence. Licence details: http://creativecommons.org/licenses/by/4.0/.Discourse parsing is a fundamental task in natural language processing (NLP) which determines the structure of the whole discourse and identifies the relations between discourse spans such as clauses and sentences. Improving this task can be helpful to many downstream tasks such as machine translation BIBREF0 , question answering BIBREF1 , and so on. As one of the important parts of discourse parsing, implicit discourse relation recognition task is to find the relation between two spans without explicit connectives (e.g., but, so, etc.), and needs recovering the relation from semantic understanding of texts..The Penn Discourse Treebank 2.0 (PDTB 2.0) BIBREF2 is a benchmark corpus for discourse relations. In PDTB style, the connectives can be explicit or implicit, and one entry of the data is separated into Arg1 and Arg2, accompanied with a relation sense. Since the release of PDTB 2.0 dataset, many methods have been proposed, ranging from traditional feature-based methods BIBREF3 , BIBREF4 to latest neural-based methods BIBREF5 , BIBREF6 . Especially through many neural network methods used for this task such as convolutional neural network (CNN) BIBREF7 , recursive neural network BIBREF8 , embedding improvement BIBREF9 , attention mechanism BIBREF10 , gate mechanism BIBREF11 , multi-task method BIBREF6 , the performance of this task has improved a lot since it was first introduced. However, this task is still very challenging with the highest reported accuracy still lower than 50% due to the hardness for the machines to understand the text meaning and the relatively small task corpus..In this work, we focus on improving the learned representations of sentence pairs to address the implicit discourse relation recognition. It is well known that text representation is the core part of state-of-the-art deep learning methods for NLP tasks, and improving the representation from all perspective will benefit the concerned task..The representation is improved by two ways in our model through three-level hierarchy. The first way is embedding augmentation. Only with informative embeddings, can the final representations be better. This is implemented in our word-level module. We augment word embeddings with subword-level embeddings and pre-trained ELMo embeddings. Subwords coming from unsupervised segmentation demonstrate a better consequent performance than characters for being a better minimal representation unit. The pre-trained contextualized word embeddings (ELMo) can make the embeddings contain more contextual information which is also involved with character-level inputs. The second way is a deep residual bi-attention encoder. Since this task is about classifying sentence pairs, the encoder is implemented in sentence and sentence-pair levels. A deeper model can support richer representations but is hard to train, especially with a small dataset. So we apply residual connections BIBREF12 to each module for facilitating signal propagation and alleviating gradient degradation. The stacked encoder blocks make the single sentence representation richer and bi-attention module mixes two sentence representations focusingly. With introducing richer and deeper representation enhancement, we report the deepest model so far for the task..Our representation enhanced model will be evaluated on the benchmark PDTB 2.0 and demonstrate state-of-the-art performance to verify its effectiveness..This paper is organized as follows. Section 2 reviews related work. Section 3 introduces our model. Section 4 shows our experiments and analyses the results. Section 5 concludes this work. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "6\n",
      "Question 1: How many modules is the model comprised of?\n",
      "\n",
      "Answer 1: The model is mainly consisted of three parts - word-level module, sentence-level module, and pair-level module.\n",
      "Question : for the text Figure 1 illustrates an overview of our model, which is mainly consisted of three parts: word-level module, sentence-level module, and pair-level module. Token sequences of sentence pairs (Arg1 and Arg2) are encoded by word-level module first and every token becomes a word embedding augmented by subword and ELMo. Then these embeddings are fed to sentence-level module and processed by stacked encoder blocks (CNN or RNN encoder block). Every block layer outputs representation for each token. Furthermore, the output of each layer is processed by bi-attention module in the pair-level module, and concatenated to pair representation, which is finally sent to classifiers which are multiple layer perceptrons (MLP) with softmax. The model details are given in the rest of this section. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "7\n",
      "Question 1: How are the word representations contextualized?\n",
      "\n",
      "Answer 1: The word representations are contextualized through the sentence-level module and the contextualized representations of each layer are sent to pair-level module.\n",
      "Question : for the text Through the sentence-level module, the word representations are contextualized, and these contextualized representations of each layer are sent to pair-level module..Suppose the encoder block layer number is $l$ , and the outputs of $j$ -th block layer for Arg1 and Arg2 are $\\mathbf {v}_1^j, \\mathbf {v}_2^j \\in \\mathbb {R}^{N \\times d_e}$ , each row of which is the embedding for the corresponding word. $N$ is the length of word sequence (sentence). Each sentence is padded or truncated to let all sentences have the same length. They are sent to a bi-attention module, the attention matrix is $\n",
      "\\mathbf {M}_j = (\\mathop {FFN}(\\mathbf {v}_1^j)) {\\mathbf {v}_2^j}^T\n",
      "\\in \\mathbb {R}^{N \\times N}\n",
      "$ . $\\mathop {FFN}$ is a feed froward network (similar to Eq. 10 ) applied to the last dimension corresponding to the word. Then the projected representations are $\\begin{split}\n",
      "\\mathbf {w}_2^j &= \\mathop {softmax}(\\mathbf {M}_j) {\\mathbf {v}_2^j} \\in \\mathbb {R}^{N \\times d_e}\\\\\n",
      "\\mathbf {w}_1^j &= \\mathop {softmax}(\\mathbf {M}_j^T) {\\mathbf {v}_1^j} \\in \\mathbb {R}^{N \\times d_e}\n",
      "\\end{split}$ .where the $\\mathop {softmax}$ is applied to each row of the matrix. We apply 2-max pooling on each projected representation and concatenate them as output of the $j$ -th bi-attention module $\n",
      "\\mathbf {o}_j = [\\mathop {top2}(\\mathbf {w}_1^j);~ \\mathop {top2}(\\mathbf {w}_2^j)]\n",
      "\\in \\mathbb {R}^{4 d_e}\n",
      "$ .The number of max pooling operation (top-2) is selected from experiments and it is a balance of more salient features and less noise. The final pair representation is .$$\\mathbf {o} = [\\mathbf {o}_1, \\mathbf {o}_2, \\cdots , \\mathbf {o}_l] \\in \\mathbb {R}^{4 l d_e}$$   (Eq. 12) .Since the output is concatenated from different layers and the outputs of lower layers are sent directly to the final representation, this also can be seen as residual connections (Res 2). Then the output as Eq. 12 is fed to an MLP classifier with softmax. The parameters for bi-attention modules in different levels are shared. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "8\n",
      "What is the focus of this work in relation to implicit discourse relation classification task?\n",
      "\n",
      "The focus of this work is to conduct an empirical study on different levels of representation enhancement for implicit discourse relation classification task, which is different from all the existing work.\n",
      "Question : for the text After the release of Penn Discourse Treebank 2.0, many works have been made to solve this concerned task. lin-kan-ng:2009:EMNLP is the first work who considered the second-level classification of the task by empirically evaluating the impact of surface features. Feature based methods BIBREF4 , BIBREF13 , BIBREF14 , BIBREF15 mainly focused on using linguistic, or semantic features from the discourse units, or the relations between unit pairs and word pairs. zhang-EtAl:2015:EMNLP4 is the first one who modeled this task using end-to-end neural network and gained great performance improvement. Neural network methods also used by lots of works BIBREF16 , BIBREF17 for better performance. Since then, a lot of methods have been proposed. braud2015comparing found that word embeddings trained by neural networks is very useful to this task. qin-zhang-zhao:2016:COLING augmented their system with character-level and contextualized embeddings. Recurrent networks and convolutional networks have been used as basic blocks in many works BIBREF18 , BIBREF19 , BIBREF7 . TACL536 used recursive neural networks. Attention mechanism was used by liu-li:2016:EMNLP2016, cai2017discourse and others. wu-EtAl:2016:EMNLP2016 and lan-EtAl:2017:EMNLP20172 applied multi-task component. qin-EtAl:2017:Long utilized adversarial nets to migrate the connective-based features to implicit ones..Sentence representation is a key component in many NLP tasks. Usually, better representation means better performance. Plenty of work on language modeling has been done, as language modeling can supply better sentence representations. Since the pioneering work of Bengio2006, neural language models have been well developed BIBREF20 , BIBREF21 , BIBREF22 . Sentence representation is directly handled in a series of work. lin2017structured used self attention mechanism and used matrix to represent sentence, and conneau-EtAl:2017:EMNLP2017 used encoders pre-trained on SNLI BIBREF23 and MultiNLI BIBREF24 ..Different from all the existing work, for the first time to our best knowledge, this work is devoted to an empirical study on different levels of representation enhancement for implicit discourse relation classification task. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "9\n",
      "Question 1: What is the sentence-level module in the model composed of?\n",
      "\n",
      "Answer 1: The sentence-level module is composed of stacked encoder blocks, with each block receiving the output of the previous layer as input and sending its output to the pair-level module. The parameters in different layers are not the same, and two encoder types are considered: convolutional and recurrent. Additionally, argument-aware parameter settings are introduced for different arguments, as they may have different semantic perspectives.\n",
      "Question : for the text The resulting word embeddings $\\mathbf {e}_i$ (Eq. 4 ) are sent to sentence-level module. The sentence-level module is composed of stacked encoder blocks. The block in each layer receives output of the previous layer as input and sends output to next layer. It also sends its output to the pair-level module. Parameters in different layers are not the same..We consider two encoder types, convolutional type and recurrent type. We only use one encoder type in one experiment..For the sentence-level module for different arguments (Arg1 and Arg2), many previous works used same parameters to encode different arguments, that is, one encoder for two type arguments. But as indicated by prasad2008penn, Arg1 and Arg2 may have different semantic perspective, we thus introduce argument-aware parameter settings for different arguments..Figure 3 is the convolutional encoder block. Suppose the input for the encoder block is $\\mathbf {x}_i ~ (i=1, \\cdots , N)$ , then $\\mathbf {x}_i \\in \\mathbb {R}^{d_e}$ . The input is sent to a convolutional layer and mapped to output $\\mathbf {y}_i = [\\mathbf {A}_i \\; \\mathbf {B}_i] \\in \\mathbb {R}^{2d_e}$ . After the convolutional operation, gated linear units (GLU) BIBREF29 is applied, i.e., $\n",
      "\\mathbf {z}_i = \\mathbf {A}_i \\odot \\sigma (\\mathbf {B}_i) \\in \\mathbb {R}^{d_e}\n",
      "$ .There is also a residual connection (Res 1) in the block, which means adding the output of $\\mathop {GLU}$ and the input of the block as final output, so $\\mathbf {z}_i + \\mathbf {x}_i$ is the output of the block corresponding to the input $\\mathbf {x}_i$ . The output $\\mathbf {z}_i + \\mathbf {x}_i$ for all $i = 1, \\cdots , N$ is sent to both the next layer and the pair-level module as input..Similar to the convolutional one, recurrent encoder block is shown in Figure 3 . The input $\\mathbf {x}_i$ is encoded by a biGRU BIBREF30 layer first, $\n",
      "\\mathbf {y}_i = \\mathop {biGRU}(\\mathbf {x}_i) \\in \\mathbb {R}^{2d_e}\n",
      "$ .then this is sent to a feed forword network, .$$\\mathbf {z}_i = \\mathbf {W}_r \\mathbf {y}_i^T + \\mathbf {b}_r \\in \\mathbb {R}^{d_e}$$   (Eq. 10) . $\\mathbf {W}_r \\in \\mathbb {R}^{2d_e \\times d_e}$ and $\\mathbf {b}_r \\in \\mathbb {R}^{d_e}$ are parameters. There is also a similar residual connection (Res 1) in the block, so $\\mathbf {z}_i + \\mathbf {x}_i$ for all $i = 1, \\cdots , N$ is the final output of the recurrent encoder block. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "10\n",
      "Question 1: What is the subword-level embedding and how is it encoded for each word?\n",
      "Answer 1: The subword-level embedding is introduced to represent words with internal structure, and it is encoded using a subword encoder that applies convolutional operations followed by max pooling to a subword embedding sequence. The resulting embeddings are concatenated and fed to a highway network. The subword units are computationally discovered using unsupervised segmentation over words with the byte pair encoding (BPE) algorithm. The subword-level embedding is concatenated with pre-trained word embeddings and contextualized word embeddings encoded by pre-trained ELMo encoders to create a complete embedding for each token.\n",
      "Question : for the text An inputed token sequence of length $N$ is encoded by the word-level module into an embedding sequence $(\\mathbf {e}_1, \\mathbf {e}_2, \\mathbf {e}_3, \\cdots , \\mathbf {e}_N)$ . For each embedded token $\\mathbf {e}_i$ , it is concatenated from three parts, .$$\\mathbf {e}_i = [\\mathbf {e}_i^w;~ \\mathbf {e}_i^s;~ \\mathbf {e}_i^c] \\in \\mathbb {R}^{d_e}$$   (Eq. 4) . $\\mathbf {e}_i^w \\in \\mathbb {R}^{d_w}$ is pre-trained word embedding for this token, and is fixed during the training procedure. Our experiments show that fine-tuning the embeddings slowed down the training without better performance. $\\mathbf {e}_i^s \\in \\mathbb {R}^{d_s}$ is subword-level embedding encoded by subword encoder. $\\mathbf {e}_i^c \\in \\mathbb {R}^{d_c}$ is contextualized word embedding encoded by pre-trained ELMo encoders, whose parameters are also fixed during training. Subword is merged from single-character segmentation and the input of ELMo encoder is also character..Character-level embeddings have been used widely in lots of works and its effectiveness is verified for out-of-vocabulary (OOV) or rare word representation. However, character is not a natural minimal unit for there exists word internal structure, we thus introduce a subword-level embedding instead..Subword units can be computationally discovered by unsupervised segmentation over words that are regarded as character sequences. We adopt byte pair encoding (BPE) algorithm introduced by sennrich-haddow-birch:2016:P16-12 for this segmentation. BPE segmentation actually relies on a series of iterative merging operation over bigrams with the highest frequency. The number of merging operation times is roughly equal to the result subword vocabulary size..For each word, the subword-level embedding is encoded by a subword encoder as in Figure 2 . Firstly, the subword sequence (of length $n$ ) of the word is mapped to subword embedding sequence $(\\mathbf {se}_1, \\mathbf {se}_2, \\mathbf {se}_3, \\cdots , \\mathbf {se}_n)$ (after padding), which is randomly initialized. Then $K$ (we empirically set $K$ =2) convolutional operations $Conv_1, Conv_2, \\cdots , Conv_K$ followed by max pooling operation are applied to the embedding sequence, and the sequence is padded before the convolutional operation. For the $i$ -th convolution kernel $Conv_i$ , suppose the kernel size is $k_i$ , then the output of $Conv_i$ on embeddings $\\mathbf {se}_{j}$ to $(\\mathbf {se}_1, \\mathbf {se}_2, \\mathbf {se}_3, \\cdots , \\mathbf {se}_n)$0 is $(\\mathbf {se}_1, \\mathbf {se}_2, \\mathbf {se}_3, \\cdots , \\mathbf {se}_n)$1 .The final output of $Conv_i$ after max pooling is $\\begin{split}\n",
      "\\mathbf {u}_i &= \\mathop {maxpool}{(\\mathbf {C}_1,~ \\cdots ,~ \\mathbf {C}_j,~ \\cdots ,~ \\mathbf {C}_n)}\n",
      "\\end{split}$ .Finally, the $K$ outputs are concatenated, $\n",
      "\\mathbf {u} = [\\mathbf {u}_1;~ \\mathbf {u}_2;~ \\cdots ;~ \\mathbf {u}_K] \\in \\mathbb {R}^{d_s}\n",
      "$ .to feed a highway network BIBREF25 , .$$\\mathbf {g} &=& \\sigma (\\mathbf {W}_g \\mathbf {u}^T + \\mathbf {b}_g) \\in \\mathbb {R}^{d_s} \\nonumber \\\\\n",
      "\\mathbf {e}_i^s &=& \\mathbf {g} \\odot \\mathop {ReLU}(\\mathbf {W}_h \\mathbf {u}^T + \\mathbf {b}_h)\n",
      "+ (\\mathbf {1} - \\mathbf {g}) \\odot \\mathbf {u} \\nonumber \\\\\n",
      "&\\in & \\mathbb {R}^{d_s}$$   (Eq. 6) .where $\\mathbf {g}$ denotes the gate, and $\\mathbf {W}_g \\in \\mathbb {R}^{d_s \\times d_s}, \\mathbf {b}_g \\in \\mathbb {R}^{d_s},\n",
      "\\mathbf {W}_h \\in \\mathbb {R}^{d_s \\times d_s}, \\mathbf {b}_h \\in \\mathbb {R}^{d_s}$ are parameters. $\\odot $ is element-wise multiplication. The above Eq. 6 gives the subword-level embedding for the $i$ -th word..ELMo (Embeddings from Language Models) BIBREF26 is a pre-trained contextualized word embeddings involving character-level representation. It is shown useful in some works BIBREF27 , BIBREF28 . This embedding is trained by bidirectional language models on large corpus using character sequence for each word token as input. The ELMo encoder employs CNN and highway networks over characters, whose output is given to a multiple-layer biLSTM with residual connections. Then the output is contextualized embeddings for each word. It is also can be seen as a hybrid encoder for character, word, and sentence. This encoder can add lots of contextual information to each word, and can ease the semantics learning of the model..For the pre-trained ELMo encoder, the output is the result of the last two biLSTM layers. Suppose $\\mathbf {c}_i$ is the character sequence of $i$ -th word in a sentence, then the encoder output is $\n",
      "[\\cdots , \\mathbf {h}_i^0, \\cdots ;~ \\cdots , \\mathbf {h}_i^1, \\cdots ]\n",
      "= \\mathop {ELMo}(\\cdots , \\mathbf {c}_i, \\cdots )\n",
      "$ .where $\\mathbf {h}_i^0$ and $\\mathbf {h}_i^1$ denote the outputs of first and second layers of ELMo encoder for $i$ -th word..Following Peters2018ELMo, we use a self-adjusted weighted average of $\\mathbf {h}_i^0, \\mathbf {h}_i^1$ , $\\begin{split}\n",
      "\\mathbf {s} &= \\mathop {softmax}(\\mathbf {w}) \\in \\mathbb {R}^2\\\\\n",
      "\\mathbf {h} &= \\gamma \\sum _{j=0}^1 s_j \\mathbf {h}_i^j \\in \\mathbb {R}^{d_c^{\\prime }}\n",
      "\\end{split}$ .where $\\gamma \\in \\mathbb {R}$ and $\\mathbf {w} \\in \\mathbb {R}^2$ are parameters tuned during training and $d_c^{\\prime }$ is the dimension of the ELMo encoder's outputs. Then the result is fed to a feed forward network to reduce its dimension, .$$\\mathbf {e}_i^c = \\mathbf {W}_c \\mathbf {h}^T + \\mathbf {b}_c \\in \\mathbb {R}^{d_c}$$   (Eq. 7) . $\\mathbf {W}_c \\in \\mathbb {R}^{d_c^{\\prime } \\times d_c}$ and $\\mathbf {b}_c \\in \\mathbb {R}^{d_c}$ are parameters. The above Eq. 7 gives ELMo embedding for the $i$ -th word. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "11\n",
      "Question 1: What datasets were used for the model performance comparisons?\n",
      "Answer 1: Traditional datasets were not used for model performance comparisons. Instead, open-source models and libraries were compared using an English-news validation dataset which is relevant to the specific task. All validation datasets and raw output results can be found at their GitHub link.\n",
      "Question : for the text Through this exercise, we were able to test out the best suitable model architecture and data preparation steps so that similar models could be trained for Indian languages. Building cased or caseless NERs for English was not the final goal and this has already been benchmarked and explored before in previous approaches explained in \"Related Work\" section. We didn't use traditional datasets for model performance comparisons & benchmarks. As mentioned before, all the comparisons are being done with open-source models and libraries from the productionization point of view. We used a english-news validation dataset which is important and relevant to our specific task and all validation datasets and raw output results can be found at our github link ..Wikipedia titles for Indian languages are very very less and resulting tagged data is even less to run deep architectures. We are trying out translations/transliterations of the English-Wiki-Titles to improve Indic-languages entity/topics data..This approach is also useful in building news-summarizing models as it detects almost all important n-grams present in the news. Output of this model can be introduced in a summarization network to add more bias towards important words and bias for their inclusion. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "12\n",
      "What is the final size of the parallel corpus created using pre-trained Bert-Tokenizer from hugging-face? \n",
      "\n",
      "The final size of the parallel corpus created using pre-trained Bert-Tokenizer from hugging-face is around 150 million sentences, with around 3 billion words (all lower cased) and with around 5 billion tokens approximately.\n",
      "Question : for the text We need good amount of data to try deep learning state-of-the-art algorithms. There are lot of open datasets available for names, locations, organisations, but not for topics as defined in Abstract above. Also defining and inferring topics is an individual preference and there are no fix set of rules for its definition. But according to our definition, we can use wikipedia titles as our target topics. English wikipedia dataset has more than 18 million titles if we consider all versions of them till now. We had to clean up the titles to remove junk titles as wikipedia title almost contains all the words we use daily. To remove such titles, we deployed simple rules as follows -.Remove titles with common words : \"are\", \"the\", \"which\".Remove titles with numeric values : 29, 101.Remove titles with technical components, driver names, transistor names : X00, lga-775.Remove 1-gram titles except locations (almost 80% of these also appear in remaining n-gram titles).After doing some more cleaning we were left with 10 million titles. We have a dump of 15 million English news articles published in past 4 years. Further, we reduced number of articles by removing duplicate and near similar articles. We used our pre-trained doc2vec models and cosine similarity to detect almost similar news articles. Then selected minimum articles required to cover all possible 2-grams to 5-grams. This step is done to save some training time without loosing accuracy. Do note that, in future we are planning to use whole dataset and hope to see gains in F1 and Recall further. But as per manual inspection, our dataset contains enough variations of sentences with rich vocabulary which contains names of celebrities, politicians, local authorities, national/local organisations and almost all locations, India and International, mentioned in the news text, in last 4 years..We then created a parallel corpus format as shown in Table 1. Using pre-trained Bert-Tokenizer from hugging-face, converted words in sentences to tokenes. Caseless-BERT pre-trained tokenizer is used. Notice that some of the topic words are broken into tokens and NER tag has been repeated accordingly. For example, in Table 1 second row, word \"harassment\" is broken into \"har ##ass ##ment\". Similarly, one \"NER\" tag is repeated three times to keep the length of sequence-pair same. Finally, for around 3 million news articles, parallel corpus is created, which is of around 150 million sentences, with around 3 billion words (all lower cased) and with around 5 billion tokens approximately. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "13\n",
      "What is the estimated amount of global GDP lost to corruption each year?\n",
      "Around $1-1.5 trillion or around two percent of global GDP is lost to corruption every year, according to a study by the Natural Resource Governance Institute (NRGI). The figure only reflects the direct costs of corruption and does not factor in the lost opportunities for innovation and productivity. Countries that address corruption and improve rule of law can expect long-term increases in per capita income, as well as reductions in infant mortality and improvements in education.\n",
      "Question : for the text Lets check some examples for detailed analysis of the models and their results. Following is the economy related news..Example 1 : around $1–1.5 trillion or around two percent of global gdp, are lost to corruption every year, president of the natural resource governance institute nrgi has said. speaking at a panel on integrity in public governance during the world bank group and international monetary fund annual meeting on sunday, daniel kaufmann, president of nrgi, presented the statistic, result of a study by the nrgi, an independent, non-profit organisation based in new york. however, according to kaufmann, the figure is only the direct costs of corruption as it does not factor in the opportunities lost on innovation and productivity, xinhua news agency reported. a country that addresses corruption and significantly improves rule of law can expect a huge increase in per capita income in the long run, the study showed. it will also see similar gains in reducing infant mortality and improving education, said kaufmann..Detected NERs can be seen per model in Table 4. Our model do not capture numbers as we have removed all numbers from my wiki-titles as topics. Reason behind the same is that we can easily write regex to detect currency, prices, time, date and deep learning is not required for the same. Following are few important n-grams only our models was able to capture -.capita income.infant mortality.international monetary fund annual meeting.natural resource governance institute.public governance.At the same time, we can see that Spacy did much better than Stanford-caseless NER and Flair could not capture any of the NERs. Another example of a news in political domain and detected NERs can be seen per model in Table 5..Example 2 : wearing the aam aadmi party's trademark cap and with copies of the party's five-year report card in hand, sunita kejriwal appears completely at ease. it's a cold winter afternoon in delhi, as the former indian revenue service (irs) officer hits the campaign trail to support her husband and batchmate, chief minister arvind kejriwal. emerging from the background for the first time, she is lending her shoulder to the aap bandwagon in the new delhi assembly constituency from where the cm, then a political novice, had emerged as the giant killer by defeating congress incumbent sheila dikshit in 2013..Correct n-grams captured only by our model are -.aam aadmi party.aap bandwagon.delhi assembly constituency.giant killer.indian revenue service.political novice.In this example, Stanford model did better and captured names properly, for example \"sheila dikshit\" which Spacy could not detect but Spacy captureed almost all numeric values along with numbers expressed in words..It is important to note that, our model captures NERs with some additional words around them. For example, \"president of nrgi\" is detected by the model but not \"ngri\". But model output does convey more information than the later. To capture the same for all models (and to make comparison fair), partial match has been enabled and if correct NER is part of predictied NER then later one is marked as matched. This could be the reason for good score for Spacy. Note that, partial match is disabled for Wikipedia Titles match task as shown in Table 3. Here, our model outperformed all the models. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "14\n",
      "Question 1: Which model was used for training and fine-tuning for Indian languages in addition to English?\n",
      "Answer 1: The BERT-Multilingual model was used for training and fine-tuning for Indian languages in addition to English.\n",
      "Question : for the text We tried multiple variations of LSTM and GRU layes, with/without CRF layer. There is a marginal gain in using GRU layers over LSTM. Also, we saw gain in using just one layers of GRU instead of more. Finally, we settled on the architecture, shown in Figure 1 for the final training, based on validation set scores with sample training set..Text had to be tokenized using pytorch-pretrained-bert as explained above before passing to the network. Architecture is built using tensorflow/keras. Coding inspiration taken from BERT-keras and for CRF layer keras-contrib. If one is more comfortable in pytorch there are many examples available on github, but pytorch-bert-crf-ner is better for an easy start..We used BERT-Multilingual model so that we can train and fine-tune the same model for other Indian languages. You can take BERT-base or BERT-large for better performance with only English dataset. Or you can use DistilBERT for English and DistilmBERT for 104 languages for faster pre-training and inferences. Also, we did not choose AutoML approach for hyper-parameter tuning which could have resulted in much more accurate results but at the same time could have taken very long time as well. So instead, chose and tweaked the parameters based on initial results..We trained two models, one with sequence length 512 to capture document level important n-grams and second with sequence length 64 to capture sentence/paragraph level important n-grams. Through experiments it was evident that, sequence length plays a vital role in deciding context and locally/globally important n-grams. Final output is a concatenation of both the model outputs. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "15\n",
      "Question 1: Which model performs the best in capturing Wikipedia topics according to the tables provided?\n",
      "\n",
      "Answer 1: According to the tables provided, the model created by the authors clearly surpasses other models in all scores when it comes to capturing Wikipedia topics.\n",
      "Question : for the text Comparison with existing open-source NER libraries is not exactly fair as they are NOT trained for detecting topics and important n-grams, also NOT trained for case-less text. But they are useful in testing and benchmarking if our model is detecting traditional NERs or not, which it should capture, as Wikipedia titles contains almost all Names, Places and Organisation names. You can check the sample output here.Comparisons have been made among Flair-NER, Stanford-caseless-NER (used english.conll.4class.caseless as it performed better than 3class and 7class), Spacy-NER and our models. Of which only Stanford-NER provides case-less models. In Table 2, scores are calculated by taking traditional NER list as reference. In Table 4, same is done with Wikipedia Titles reference set..As you can see in Table 2 & 3, recall is great for our model but precision is not good as Model is also trying to detect new potential topics which are not there even in reference Wikipedia-Titles and NER sets. In capturing Wikipedia topics our model clearly surpasses other models in all scores..Spacy results are good despite not being trained for case-less data. In terms of F1 and overall stability Spacy did better than Stanford NER, on our News Validation set. Similarly, Stanford did well in Precision but could not catch up with Spacy and our model in terms of Recall. Flair overall performed poorly, but as said before these open-source models are not trained for our particular use-case. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "16\n",
      "What is the importance of sequence length in training a topic model using BERT?\n",
      "\n",
      "The sequence length in training a topic model using BERT is important as it determines how many BERT-tokens can be passed for inference, training time, and accuracy. Ideally, the longer the sequence length, the faster the inference would be. However, the sequence length should be chosen according to the specific use case. In this particular study, the authors used sequence lengths of 512 and 64 and stopped training both models when they reached 70% precision and 90% recall on training and testing sets.\n",
      "Question : for the text Trained the topic model on single 32gb NVidia-V100 and it took around 50 hours to train the model with sequence length 512. We had to take 256gb ram machine to accommodate all data in memory for faster read/write. Also, trained model with 64 sequence length in around 17 hours..It is very important to note that sequence length decides how many bert-tokens you can pass for inference and also decides training time and accuracy. Ideally more is better because inference would be faster as well. For 64 sequence length, we are moving 64-token window over whole token-text and recognising topics in each window. So, one should choose sequence length according to their use case. Also, we have explained before our motivation of choosing 2 separate sequence lengths models..We stopped the training for both the models when it crossed 70% precision, 90% recall on training and testing sets, as we were just looking to get maximum recall and not bothered about precision in our case. Both the models reach this point at around 16 epochs. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "17\n",
      "What are the three broad types of Named-Entity-Recognition (NER) approaches?\n",
      "\n",
      "Answer 1: The three broad types of NER approaches are detecting NER with predefined dictionaries and rules, statistical approaches, and deep learning approaches.\n",
      "Question : for the text Named-Entity-Recognition(NER) approaches can be categorised broadly in three types. Detecting NER with predefined dictionaries and rulesBIBREF2, with some statistical approachesBIBREF3 and with deep learning approachesBIBREF4..Stanford CoreNLP NER is a widely used baseline for many applications BIBREF5. Authors have used approaches of Gibbs sampling and conditional random field (CRF) for non-local information gathering and then Viterbi algorithm to infer the most likely state in the CRF sequence outputBIBREF6..Deep learning approaches in NLP use document, word or token representations instead of one-hot encoded vectors. With the rise of transfer learning, pretrained Word2VecBIBREF7, GloVeBIBREF8, fasttextBIBREF9 which provides word embeddings were being used with recurrent neural networks (RNN) to detect NERs. Using LSTM layers followed by CRF layes with pretrained word-embeddings as input has been explored hereBIBREF10. Also, CNNs with character embeddings as inputs followed by bi-directional LSTM and CRF layers, were explored hereBIBREF11..With the introduction of attentions and transformersBIBREF12 many deep architectures emerged in last few years. Approach of using these pretrained models like ElmoBIBREF13, FlairBIBREF14 and BERTBIBREF0 for word representations followed by variety of LSMT and CRF combinations were tested by authors in BIBREF15 and these approaches show state-of-the-art performance..There are very few approaches where caseless NER task is explored. In this recent paperBIBREF16 authors have explored effects of \"Cased\" entities and how variety of networks perform and they show that the most effective strategy is a concatenation of cased and lowercased training data, producing a single model with high performance on both cased and uncased text..In another paperBIBREF17, authors have proposed True-Case pre-training before using BiLSTM+CRF approach to detect NERs effectively. Though it shows good results over previous approaches, it is not useful in Indian Languages context as there is no concept of cases..In our approach, we are focusing more on data preparation for our definition of topics using some of the state-of-art architectures based on BERT, LSTM/GRU and CRF layers as they have been explored in previous approaches mentioned above. Detecting caseless topics with higher recall and reasonable precision has been given a priority over f1 score. And comparisons have been made with available and ready-to-use open-source libraries from the productionization perspective. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "18\n",
      "Question 1: Who supported this research?\n",
      "Answer 1: This research was supported by the JHU HLTCOE, DARPA AIDA, and NSF-GRFP (1232825).\n",
      "Question : for the text The authors thank Rebecca Knowles and Chandler May for their valuable feedback on this work. This research was supported by the JHU HLTCOE, DARPA AIDA, and NSF-GRFP (1232825). The U.S. Government is authorized to reproduce and distribute reprints for Governmental purposes. The views and conclusions contained in this publication are those of the authors and should not be interpreted as representing official policies or endorsements of DARPA or the U.S. Government. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "19\n",
      "What are the limitations of Winogender schemas in detecting gender bias in NLP systems?\n",
      "\n",
      "Winogender schemas have high positive predictive value but low negative predictive value, meaning that they may demonstrate the presence of gender bias in a system, but not prove its absence. While they can be used to probe for other manifestations of gender bias, human judgments, which carry their own implicit biases, serve as a lower bound for equitability in automated systems.\n",
      "Question : for the text We have introduced “Winogender schemas,” a pronoun resolution task in the style of Winograd schemas that enables us to uncover gender bias in coreference resolution systems. We evaluate three publicly-available, off-the-shelf systems and find systematic gender bias in each: for many occupations, systems strongly prefer to resolve pronouns of one gender over another. We demonstrate that this preferential behavior correlates both with real-world employment statistics and the text statistics that these systems use. We posit that these systems overgeneralize the attribute of gender, leading them to make errors that humans do not make on this evaluation. We hope that by drawing attention to this issue, future systems will be designed in ways that mitigate gender-based overgeneralization..It is important to underscore the limitations of Winogender schemas. As a diagnostic test of gender bias, we view the schemas as having high positive predictive value and low negative predictive value; that is, they may demonstrate the presence of gender bias in a system, but not prove its absence. Here we have focused on examples of occupational gender bias, but Winogender schemas may be extended broadly to probe for other manifestations of gender bias. Though we have used human-validated schemas to demonstrate that existing NLP systems are comparatively more prone to gender-based overgeneralization, we do not presume that matching human judgment is the ultimate objective of this line of research. Rather, human judgements, which carry their own implicit biases, serve as a lower bound for equitability in automated systems. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "20\n",
      "What are the three types of machine learning paradigms used for the evaluation of the coreference resolution systems in the work?\n",
      "\n",
      "The three types of machine learning paradigms used for the evaluation of the coreference resolution systems in the work are rule-based systems, feature-driven statistical systems, and neural systems.\n",
      "Question : for the text In this work, we evaluate three publicly-available off-the-shelf coreference resolution systems, representing three different machine learning paradigms: rule-based systems, feature-driven statistical systems, and neural systems. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "21\n",
      "What is the focus of the study on gender bias in coreference resolution systems?\n",
      "The focus of the study is on gender bias with respect to occupations and how it manifests in coreference resolution systems through the use of \"Winogender schemas\".\n",
      "Question : for the text There is a classic riddle: A man and his son get into a terrible car crash. The father dies, and the boy is badly injured. In the hospital, the surgeon looks at the patient and exclaims, “I can't operate on this boy, he's my son!” How can this be?.That a majority of people are reportedly unable to solve this riddle is taken as evidence of underlying implicit gender bias BIBREF0 : many first-time listeners have difficulty assigning both the role of “mother” and “surgeon” to the same entity..As the riddle reveals, the task of coreference resolution in English is tightly bound with questions of gender, for humans and automated systems alike (see Figure 1 ). As awareness grows of the ways in which data-driven AI technologies may acquire and amplify human-like biases BIBREF1 , BIBREF2 , BIBREF3 , this work investigates how gender biases manifest in coreference resolution systems..There are many ways one could approach this question; here we focus on gender bias with respect to occupations, for which we have corresponding U.S. employment statistics. Our approach is to construct a challenge dataset in the style of Winograd schemas, wherein a pronoun must be resolved to one of two previously-mentioned entities in a sentence designed to be easy for humans to interpret, but challenging for data-driven systems BIBREF4 . In our setting, one of these mentions is a person referred to by their occupation; by varying only the pronoun's gender, we are able to test the impact of gender on resolution. With these “Winogender schemas,” we demonstrate the presence of systematic gender bias in multiple publicly-available coreference resolution systems, and that occupation-specific bias is correlated with employment statistics. We release these test sentences to the public..In our experiments, we represent gender as a categorical variable with either two or three possible values: female, male, and (in some cases) neutral. These choices reflect limitations of the textual and real-world datasets we use. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "22\n",
      "What is the main difference between the work presented in this paper and the work by zhao-wang:2018:N18-1?\n",
      "\n",
      "Answer: The main difference is that this paper focuses on schema construction and validation, with extensive analysis of observed system bias, revealing its correlation with biases present in real-world and textual statistics, while zhao-wang:2018:N18-1 present methods of debiasing existing systems, showing that simple approaches such as augmenting training data with gender-swapped examples or directly editing noun phrase counts in the B&L resource are effective at reducing system bias, as measured by the schemas.\n",
      "Question : for the text Here we give a brief (and non-exhaustive) overview of prior work on gender bias in NLP systems and datasets. A number of papers explore (gender) bias in English word embeddings: how they capture implicit human biases in modern BIBREF1 and historical BIBREF15 text, and methods for debiasing them BIBREF16 . Further work on debiasing models with adversarial learning is explored by DBLP:journals/corr/BeutelCZC17 and zhang2018mitigating..Prior work also analyzes social and gender stereotyping in existing NLP and vision datasets BIBREF17 , BIBREF18 . tatman:2017:EthNLP investigates the impact of gender and dialect on deployed speech recognition systems, while zhao-EtAl:2017:EMNLP20173 introduce a method to reduce amplification effects on models trained with gender-biased datasets. koolen-vancranenburgh:2017:EthNLP examine the relationship between author gender and text attributes, noting the potential for researcher interpretation bias in such studies. Both larson:2017:EthNLP and koolen-vancranenburgh:2017:EthNLP offer guidelines to NLP researchers and computational social scientists who wish to predict gender as a variable. hovy-spruit:2016:P16-2 introduce a helpful set of terminology for identifying and categorizing types of bias that manifest in AI systems, including overgeneralization, which we observe in our work here..Finally, we note independent but closely related work by zhao-wang:2018:N18-1, published concurrently with this paper. In their work, zhao-wang:2018:N18-1 also propose a Winograd schema-like test for gender bias in coreference resolution systems (called “WinoBias”). Though similar in appearance, these two efforts have notable differences in substance and emphasis. The contribution of this work is focused primarily on schema construction and validation, with extensive analysis of observed system bias, revealing its correlation with biases present in real-world and textual statistics; by contrast, zhao-wang:2018:N18-1 present methods of debiasing existing systems, showing that simple approaches such as augmenting training data with gender-swapped examples or directly editing noun phrase counts in the B&L resource are effective at reducing system bias, as measured by the schemas. Complementary differences exist between the two schema formulations: Winogender schemas (this work) include gender-neutral pronouns, are syntactically diverse, and are human-validated; WinoBias includes (and delineates) sentences resolvable from syntax alone; a Winogender schema has one occupational mention and one “other participant” mention; WinoBias has two occupational mentions. Due to these differences, we encourage future evaluations to make use of both datasets. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "23\n",
      "Question 1: What is the correlation between the gender preferences for occupations in the tested coreference systems and real-world employment statistics?\n",
      "\n",
      "Answer 1: The gender preferences for occupations in the tested coreference systems correlate with real-world employment statistics, as shown in Figure 4 and table 1.\n",
      "Question : for the text We evaluate examples of each of the three coreference system architectures described in \"Coreference Systems\" : the BIBREF5 sieve system from the rule-based paradigm (referred to as RULE), BIBREF6 from the statistical paradigm (STAT), and the BIBREF11 deep reinforcement system from the neural paradigm (NEURAL)..By multiple measures, the Winogender schemas reveal varying degrees of gender bias in all three systems. First we observe that these systems do not behave in a gender-neutral fashion. That is to say, we have designed test sentences where correct pronoun resolution is not a function of gender (as validated by human annotators), but system predictions do exhibit sensitivity to pronoun gender: 68% of male-female minimal pair test sentences are resolved differently by the RULE system; 28% for STAT; and 13% for NEURAL..Overall, male pronouns are also more likely to be resolved as occupation than female or neutral pronouns across all systems: for RULE, 72% male vs 29% female and 1% neutral; for STAT, 71% male vs 63% female and 50% neutral; and for NEURAL, 87% male vs 80% female and 36% neutral. Neutral pronouns are often resolved as neither occupation nor participant, possibly due to the number ambiguity of “they/their/them.”.When these systems' predictions diverge based on pronoun gender, they do so in ways that reinforce and magnify real-world occupational gender disparities. Figure 4 shows that systems' gender preferences for occupations correlate with real-world employment statistics (U.S. Bureau of Labor Statistics) and the gender statistics from text BIBREF14 which these systems access directly; correlation values are in Table 1 . We also identify so-called “gotcha” sentences in which pronoun gender does not match the occupation's majority gender (BLS) if occupation is the correct answer; all systems perform worse on these “gotchas.” (See Table 2 .).Because coreference systems need to make discrete choices about which mentions are coreferent, percentage-wise differences in real-world statistics may translate into absolute differences in system predictions. For example, the occupation “manager” is 38.5% female in the U.S. according to real-world statistics (BLS); mentions of “manager” in text are only 5.18% female (B&L resource); and finally, as viewed through the behavior of the three coreference systems we tested, no managers are predicted to be female. This illustrates two related phenomena: first, that data-driven NLP pipelines are susceptible to sequential amplification of bias throughout a pipeline, and second, that although the gender statistics from B&L correlate with BLS employment statistics, they are systematically male-skewed (Figure 3 ). generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "24\n",
      "Question 1: What is the purpose of the evaluation set created by the authors?\n",
      "\n",
      "Answer 1: The purpose of the evaluation set created by the authors is to reveal cases where coreference systems may be more or less likely to recognize a pronoun as coreferent with a particular occupation based on pronoun gender.\n",
      "Question : for the text Our intent is to reveal cases where coreference systems may be more or less likely to recognize a pronoun as coreferent with a particular occupation based on pronoun gender, as observed in Figure 1 . To this end, we create a specialized evaluation set consisting of 120 hand-written sentence templates, in the style of the Winograd Schemas BIBREF4 . Each sentence contains three referring expressions of interest:.We use a list of 60 one-word occupations obtained from Caliskan183 (see supplement), with corresponding gender percentages available from the U.S. Bureau of Labor Statistics. For each occupation, we wrote two similar sentence templates: one in which pronoun is coreferent with occupation, and one in which it is coreferent with participant (see Figure 2 ). For each sentence template, there are three pronoun instantiations (female, male, or neutral), and two participant instantiations (a specific participant, e.g., “the passenger,” and a generic paricipant, “someone.”) With the templates fully instantiated, the evaluation set contains 720 sentences: 60 occupations $\\times $ 2 sentence templates per occupation $\\times $ 2 participants $\\times $ 3 pronoun genders. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "25\n",
      "Question 1: What are the potential directions for future work identified in the study on semantic parsing in context?\n",
      "\n",
      "Answer 1: The potential directions for future work identified in the study are incorporating common sense for better pronoun inference and modeling contextual clues in a more explicit manner.\n",
      "Question : for the text This work conducts an exploratory study on semantic parsing in context, to realize how far we are from effective context modeling. Through a thorough comparison, we find that existing context modeling methods are not as effective as expected. A simple concatenation method can be much competitive. Furthermore, by performing a fine-grained analysis, we summarize two potential directions as our future work: incorporating common sense for better pronouns inference, and modeling contextual clues in a more explicit manner. By open-sourcing our code and materials, we believe our work can facilitate the community to debug models in a fine-grained level and make more progress. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "26\n",
      "Question 1: What is the purpose of the experiments conducted in Sections SECREF36 and SECREF40?\n",
      "\n",
      "Answer 1: The experiments conducted in Sections SECREF36 and SECREF40 aim to study whether the introduced methods can effectively model context in the task of SPC, and to perform a fine-grained analysis on various contextual phenomena.\n",
      "Question : for the text We conduct experiments to study whether the introduced methods are able to effectively model context in the task of SPC (Section SECREF36), and further perform a fine-grained analysis on various contextual phenomena (Section SECREF40). generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "27\n",
      "Question 1: What is SyntaxSQL-con and how does it work?\n",
      "\n",
      "Answer 1: SyntaxSQL-con is one of the strong baselines considered in the SParC dataset paper. It involves using a BiLSTM model to encode dialogue history on top of the SyntaxSQLNet model. This approach is similar to the Turn model used in our study.\n",
      "Question : for the text We consider three models as our baselines. SyntaxSQL-con and CD-Seq2Seq are two strong baselines introduced in the SParC dataset paper BIBREF2. SyntaxSQL-con employs a BiLSTM model to encode dialogue history upon the SyntaxSQLNet model (analogous to our Turn) BIBREF23, while CD-Seq2Seq is adapted from BIBREF4 for cross-domain settings (analogous to our Turn+Tree Copy). EditSQL BIBREF5 is a STOA baseline which mainly makes use of SQL attention and token-level copy (analogous to our Turn+SQL Attn+Action Copy). generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "28\n",
      "Question 1: How many dialogues are there in the CoSQL dataset for development?\n",
      "\n",
      "Answer 1: There are 292 dialogues in the CoSQL dataset for development.\n",
      "Question : for the text Two large complex cross-domain datasets are used: SParC BIBREF2 consists of 3034 / 422 dialogues for train / development, and CoSQL BIBREF6 consists of 2164 / 292 ones. The average turn numbers of SParC and CoSQL are $3.0$ and $5.2$, respectively. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "29\n",
      "Question 1: What are the three metrics used for evaluating the predicted SQL query accuracy?\n",
      "\n",
      "Answer 1: The three metrics used for evaluating the predicted SQL query accuracy are Question Match (Ques.Match), Interaction Match (Int.Match), and Turn $i$ Match.\n",
      "Question : for the text We evaluate each predicted SQL query using exact set match accuracy BIBREF2. Based on it, we consider three metrics: Question Match (Ques.Match), the match accuracy over all questions, Interaction Match (Int.Match), the match accuracy over all dialogues, and Turn $i$ Match, the match accuracy over questions at turn $i$. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "30\n",
      "Question 1: What optimizer is used in the implementation and what is the learning rate for BERT?\n",
      "Answer 1: The implementation uses the Adam optimizer with a learning rate of 1e-3 on all modules except for BERT, for which a learning rate of 1e-5 is used.\n",
      "Question : for the text Our implementation is based on PyTorch BIBREF18, AllenNLP BIBREF19 and the library transformers BIBREF20. We adopt the Adam optimizer and set the learning rate as 1e-3 on all modules except for BERT, for which a learning rate of 1e-5 is used BIBREF21. The dimensions of word embedding, action embedding and distance embedding are 100, while the hidden state dimensions of question encoder, grammar-based decoder, turn-level encoder and SQL encoder are 200. We initialize word embedding using Glove BIBREF22 for non-BERT models. For methods which use recent $h$ questions, $h$ is set as 5 on both datasets. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "31\n",
      "Question 1: What are the three kinds of contextual phenomena in questions?\n",
      "\n",
      "Answer 1: The three kinds of contextual phenomena in questions are semantically complete, coreference, and ellipsis.\n",
      "Question : for the text By a careful investigation on contextual phenomena, we summarize them in multiple hierarchies. Roughly, there are three kinds of contextual phenomena in questions: semantically complete, coreference and ellipsis. Semantically complete means a question can reflect all the meaning of its corresponding SQL. Coreference means a question contains pronouns, while ellipsis means the question cannot reflect all of its SQL, even if resolving its pronouns. In the fine-grained level, coreference can be divided into 5 types according to its pronoun BIBREF1. Ellipsis can be characterized by its intention: continuation and substitution. Continuation is to augment extra semantics (e.g. ${\\rm Filter}$), and substitution refers to the situation where current question is intended to substitute particular semantics in the precedent question. Substitution can be further branched into 4 types: explicit vs. implicit and schema vs. operator. Explicit means the current question provides contextual clues (i.e. partial context overlaps with the precedent question) to help locate the substitution target, while implicit does not. On most cases, the target is schema or operator. In order to study the effect of context modeling methods on various phenomena, as shown in Table TABREF39, we take the development set of SParC as an example to perform our analysis. The analysis begins by presenting Ques.Match of three representative models on above fine-grained types in Figure FIGREF42. As shown, though different methods have different strengths, they all perform poorly on certain types, which will be elaborated below. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "32\n",
      "Question 1: What is a key factor in determining the scope of an antecedent?\n",
      "\n",
      "Answer 1: The scope of an antecedent is a key factor, which can either be whole or partial. A whole scope refers to the antecedent being the precedent answer, while a partial scope refers to the antecedent being part of the precedent question.\n",
      "Question : for the text Diving deep into the coreference (left of Figure FIGREF42), we observe that all methods struggle with two fine-grained types: definite noun phrases and one anaphora. Through our study, we find the scope of antecedent is a key factor. An antecedent is one or more entities referred by a pronoun. Its scope is either whole, where the antecedent is the precedent answer, or partial, where the antecedent is part of the precedent question. The above-mentioned fine-grained types are more challenging as their partial proportion are nearly $40\\%$, while for demonstrative pronoun it is only $22\\%$. It is reasonable as partial requires complex inference on context. Considering the 4th example in Table TABREF39, “one” refers to “pets” instead of “age” because the accompanying verb is “weigh”. From this observation, we draw the conclusion that current context modeling methods do not succeed on pronouns which require complex inference on context. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "33\n",
      "Question 1: Why do all models have a better performance on continuation than substitution?\n",
      "\n",
      "Answer 1: All models have a better performance on continuation than substitution because there are redundant semantics in substitution, while not in continuation.\n",
      "Question : for the text As for ellipsis (right of Figure FIGREF42), we obtain three interesting findings by comparisons in three aspects. The first finding is that all models have a better performance on continuation than substitution. This is expected since there are redundant semantics in substitution, while not in continuation. Considering the 8th example in Table TABREF39, “horsepower” is a redundant semantic which may raise noise in SQL prediction. The second finding comes from the unexpected drop from implicit(substitution) to explicit(substitution). Intuitively, explicit should surpass implicit on substitution as it provides more contextual clues. The finding demonstrates that contextual clues are obviously not well utilized by the context modeling methods. Third, compared with schema(substitution), operator(substitution) achieves a comparable or better performance consistently. We believe it is caused by the cross-domain setting, which makes schema related substitution more difficult. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "34\n",
      "What is the conclusion of the study on context modeling methods in the task of SPC?\n",
      "Answer 1: The existing context modeling methods in the task of SPC are not as effective as expected and do not show a significant advantage over the simple concatenation method, according to the study.\n",
      "Question : for the text Taking Concat as a representative, we compare the performance of our model with other models, as shown in Table TABREF34. As illustrated, our model outperforms baselines by a large margin with or without BERT, achieving new SOTA performances on both datasets. Compared with the previous SOTA without BERT on SParC, our model improves Ques.Match and Int.Match by $10.6$ and $5.4$ points, respectively..To conduct a thorough comparison, we evaluate 13 different context modeling methods upon the same parser, including 6 methods introduced in Section SECREF2 and 7 selective combinations of them (e.g., Concat+Action Copy). The experimental results are presented in Figure FIGREF37. Taken as a whole, it is very surprising to observe that none of these methods can be consistently superior to the others. The experimental results on BERT-based models show the same trend. Diving deep into the methods only using recent questions as context, we observe that Concat and Turn perform competitively, outperforming Gate by a large margin. With respect to the methods only using precedent SQL as context, Action Copy significantly surpasses Tree Copy and SQL Attn in all metrics. In addition, we observe that there is little difference in the performance of Action Copy and Concat, which implies that using precedent SQL as context gives almost the same effect with using recent questions. In terms of the combinations of different context modeling methods, they do not significantly improve the performance as we expected..As mentioned in Section SECREF1, intuitively, methods which only use the precedent SQL enjoys better generalizability. To validate it, we further conduct an out-of-distribution experiment to assess the generalizability of different context modeling methods. Concretely, we select three representative methods and train them on questions at turn 1 and 2, whereas test them at turn 3, 4 and beyond. As shown in Figure FIGREF38, Action Copy has a consistently comparable or better performance, validating the intuition. Meanwhile, Concat appears to be strikingly competitive, demonstrating it also has a good generalizability. Compared with them, Turn is more vulnerable to out-of-distribution questions..In conclusion, existing context modeling methods in the task of SPC are not as effective as expected, since they do not show a significant advantage over the simple concatenation method. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "35\n",
      "What is the task of Semantic Parsing in Context (SPC)?\n",
      "SPC is the task of translating incomplete, context-dependent natural language questions into their corresponding executable logic form, such as SQL, in a dialogue system.\n",
      "Question : for the text Semantic parsing, which translates a natural language sentence into its corresponding executable logic form (e.g. Structured Query Language, SQL), relieves users from the burden of learning techniques behind the logic form. The majority of previous studies on semantic parsing assume that queries are context-independent and analyze them in isolation. However, in reality, users prefer to interact with systems in a dialogue, where users are allowed to ask context-dependent incomplete questions BIBREF0. That arises the task of Semantic Parsing in Context (SPC), which is quite challenging as there are complex contextual phenomena. In general, there are two sorts of contextual phenomena in dialogues: Coreference and Ellipsis BIBREF1. Figure FIGREF1 shows a dialogue from the dataset SParC BIBREF2. After the question “What is id of the car with the max horsepower?”, the user poses an elliptical question “How about with the max mpg?”, and a question containing pronouns “Show its Make!”. Only when completely understanding the context, could a parser successfully parse the incomplete questions into their corresponding SQL queries..A number of context modeling methods have been suggested in the literature to address SPC BIBREF3, BIBREF4, BIBREF2, BIBREF5, BIBREF6. These methods proposed to leverage two categories of context: recent questions and precedent logic form. It is natural to leverage recent questions as context. Taking the example from Figure FIGREF1, when parsing $Q_3$, we also need to take $Q_1$ and $Q_2$ as input. We can either simply concatenate the input questions, or use a model to encode them hierarchically BIBREF4. As for the second category, instead of taking a bag of recent questions as input, it only considers the precedent logic form. For instance, when parsing $Q_3$, we only need to take $S_2$ as context. With such a context, the decoder can attend over it, or reuse it via a copy mechanism BIBREF4, BIBREF5. Intuitively, methods that fall into this category enjoy better generalizability, as they only rely on the last logic form as context, no matter at which turn. Notably, these two categories of context can be used simultaneously..However, it remains unclear how far we are from effective context modeling. First, there is a lack of thorough comparisons of typical context modeling methods on complex SPC (e.g. cross-domain). Second, none of previous works verified their proposed context modeling methods with the grammar-based decoding technique, which has been developed for years and proven to be highly effective in semantic parsing BIBREF7, BIBREF8, BIBREF9. To obtain better performance, it is worthwhile to study how context modeling methods collaborate with the grammar-based decoding. Last but not the least, there is limited understanding of how context modeling methods perform on various contextual phenomena. An in-depth analysis can shed light on potential research directions..In this paper, we try to fulfill the above insufficiency via an exploratory study on real-world semantic parsing in context. Concretely, we present a grammar-based decoding semantic parser and adapt typical context modeling methods on top of it. Through experiments on two large complex cross-domain datasets, SParC BIBREF2 and CoSQL BIBREF6, we carefully compare and analyze the performance of different context modeling methods. Our best model achieves state-of-the-art (SOTA) performances on both datasets with significant improvements. Furthermore, we summarize and generalize the most frequent contextual phenomena, with a fine-grained analysis on representative models. Through the analysis, we obtain some interesting findings, which may benefit the community on the potential research directions. We will open-source our code and materials to facilitate future work upon acceptance. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "36\n",
      "Question 1: What is the dataset used in the task of semantic parsing in context?\n",
      "\n",
      "Answer 1: The dataset used in the task of semantic parsing in context is composed of dialogues, where each sequence of natural language questions is denoted by $\\langle \\mathbf {x}_1,...,\\mathbf {x}_n\\rangle $ and their corresponding SQL queries are denoted by $\\langle \\mathbf {y}_1,...,\\mathbf {y}_n\\rangle $.\n",
      "Question : for the text In the task of semantic parsing in context, we are given a dataset composed of dialogues. Denoting $\\langle \\mathbf {x}_1,...,\\mathbf {x}_n\\rangle $ a sequence of natural language questions in a dialogue, $\\langle \\mathbf {y}_1,...,\\mathbf {y}_n\\rangle $ are their corresponding SQL queries. Each SQL query is conditioned on a multi-table database schema, and the databases used in test do not appear in training. In this section, we first present a base model without considering context. Then we introduce 6 typical context modeling methods and describe how we equip the base model with these methods. Finally, we present how to augment the model with BERT BIBREF10. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "37\n",
      "Question 1: What is the process used to enhance the embedding of questions and schemas in the model?\n",
      "\n",
      "Answer 1: The model employs BERT to augment its features through enhancing the embedding of questions and schemas. This is achieved by concatenating the input question and all the schemas in a deterministic order with [SEP] as a delimiter, and then feeding it into BERT to obtain schema-aware question representations and question-aware schema representations. These contextual representations help to substitute $\\phi ^x$ subsequently, while other parts of the model remain the same.\n",
      "Question : for the text We employ BERT BIBREF10 to augment our model via enhancing the embedding of questions and schemas. We first concatenate the input question and all the schemas in a deterministic order with [SEP] as delimiter BIBREF17. For instance, the input for $Q_1$ in Figure FIGREF1 is “What is id ... max horsepower? [SEP] CARS_NAMES [SEP] MakeId ... [SEP] Horsepower”. Feeding it into BERT, we obtain the schema-aware question representations and question-aware schema representations. These contextual representations are used to substitute $\\phi ^x$ subsequently, while other parts of the model remain the same. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "38\n",
      "Question 1: What is the capital city of France?\n",
      "Answer 1: The capital city of France is Paris.\n",
      "Question : for the text We employ the popularly used attention-based sequence-to-sequence architecture BIBREF11, BIBREF12 to build our base model. As shown in Figure FIGREF6, the base model consists of a question encoder and a grammar-based decoder. For each question, the encoder provides contextual representations, while the decoder generates its corresponding SQL query according to a predefined grammar. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "39\n",
      "Question 1: How does the decoder in this system produce a SQL query?\n",
      "Answer 1: The decoder outputs a sequence of grammar rules that have one-to-one correspondence with the abstract syntax tree of the SQL query. At each decoding step, a nonterminal is expanded using one of its corresponding grammar rules, which are either schema-specific or schema-agnostic. The output is obtained by computing the output probability of the action. The model may encounter schemas never appeared in training due to the cross-domain setting, so the unnormalized linking score is computed between the token in the input and the schema in the action using handcraft features and learned similarity.\n",
      "Question : for the text The decoder is grammar-based with attention on the input question BIBREF7. Different from producing a SQL query word by word, our decoder outputs a sequence of grammar rule (i.e. action). Such a sequence has one-to-one correspondence with the abstract syntax tree of the SQL query. Taking the SQL query in Figure FIGREF6 as an example, it is transformed to the action sequence $\\langle $ $\\rm \\scriptstyle {Start}\\rightarrow \\rm {Root}$, $\\rm \\scriptstyle {Root}\\rightarrow \\rm {Select\\ Order}$, $\\rm \\scriptstyle {Select}\\rightarrow \\rm {Agg}$, $\\rm \\scriptstyle {Agg}\\rightarrow \\rm {max\\ Col\\ Tab}$, $\\rm \\scriptstyle {Col}\\rightarrow \\rm {Id}$, $\\rm \\scriptstyle {Tab}\\rightarrow \\rm {CARS\\_DATA}$, $\\rm \\scriptstyle {Order}\\rightarrow \\rm {desc\\ limit\\ Agg}$, $\\rm \\scriptstyle {Agg}\\rightarrow \\rm {none\\ Col\\ Tab}$, $\\rm \\scriptstyle {Col}\\rightarrow \\rm {Horsepower}$, $\\rm \\scriptstyle {Tab}\\rightarrow \\rm {CARS\\_DATA}$ $\\rangle $ by left-to-right depth-first traversing on the tree. At each decoding step, a nonterminal is expanded using one of its corresponding grammar rules. The rules are either schema-specific (e.g. $\\rm \\scriptstyle {Col}\\rightarrow \\rm {Horsepower}$), or schema-agnostic (e.g. $\\rm \\scriptstyle {Start}\\rightarrow \\rm {Root}$). More specifically, as shown at the top of Figure FIGREF6, we make a little modification on $\\rm {Order}$-related rules upon the grammar proposed by BIBREF9, which has been proven to have better performance than vanilla SQL grammar. Denoting $\\mathbf {LSTM}^{\\overrightarrow{D}}$ the unidirectional LSTM used in the decoder, at each decoding step $j$ of turn $i$, it takes the embedding of the previous generated grammar rule $\\mathbf {\\phi }^y(y_{i,j-1})$ (indicated as the dash lines in Figure FIGREF6), and updates its hidden state as:.where $\\mathbf {c}_{i,j-1}$ is the context vector produced by attending on each encoder hidden state $\\mathbf {h}^E_{i,k}$ in the previous step:.where $\\mathbf {W}^e$ is a learned matrix. $\\mathbf {h}^{\\overrightarrow{D}}_{i,0}$ is initialized by the final encoder hidden state $\\mathbf {h}^E_{i,|\\mathbf {x}_{i}|}$, while $\\mathbf {c}_{i,0}$ is a zero-vector. For each schema-agnostic grammar rule, $\\mathbf {\\phi }^y$ returns a learned embedding. For schema-specific one, the embedding is obtained by passing its schema (i.e. table or column) through another unidirectional LSTM, namely schema encoder $\\mathbf {LSTM}^{\\overrightarrow{S}}$. For example, the embedding of $\\rm \\scriptstyle {Col}\\rightarrow \\rm {Id}$ is:.As for the output $y_{i,j}$, if the expanded nonterminal corresponds to schema-agnostic grammar rules, we can obtain the output probability of action ${\\gamma }$ as:.where $\\mathbf {W}^o$ is a learned matrix. When it comes to schema-specific grammar rules, the main challenge is that the model may encounter schemas never appeared in training due to the cross-domain setting. To deal with it, we do not directly compute the similarity between the decoder hidden state and the schema-specific grammar rule embedding. Instead, we first obtain the unnormalized linking score $l(x_{i,k},\\gamma )$ between the $k$-th token in $\\mathbf {x}_i$ and the schema in action $\\gamma $. It is computed by both handcraft features (e.g. word exact match) BIBREF15 and learned similarity (i.e. dot product between word embedding and grammar rule embedding). With the input question as bridge, we reuse the attention score $a_{i,k}$ in Equation DISPLAY_FORM8 to measure the probability of outputting a schema-specific action $\\gamma $ as: generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "40\n",
      "I'm sorry, there is no question provided for me to answer. Can you please provide a question for me to answer?\n",
      "Question : for the text To capture contextual information within a question, we apply Bidirectional Long Short-Term Memory Neural Network (BiLSTM) as our question encoder BIBREF13, BIBREF14. Specifically, at turn $i$, firstly every token $x_{i,k}$ in $\\mathbf {x}_{i}$ is fed into a word embedding layer $\\mathbf {\\phi }^x$ to get its embedding representation $\\mathbf {\\phi }^x{(x_{i,k})}$. On top of the embedding representation, the question encoder obtains a contextual representation $\\mathbf {h}^{E}_{i,k}=[\\mathop {{\\mathbf {h}}^{\\overrightarrow{E}}_{i,k}}\\,;{\\mathbf {h}}^{\\overleftarrow{E}}_{i,k}]$, where the forward hidden state is computed as following: generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "41\n",
      "Question 1: What is required to use the usage of $\\mathbf {y}_{i-1}$ in Figure FIGREF27?\n",
      "\n",
      "Answer 1: To use the usage of $\\mathbf {y}_{i-1}$ in Figure FIGREF27, a SQL encoder is required which employs another BiLSTM to achieve it. The $m$-th contextual action representation at turn $i\\!-\\!1$, $\\mathbf {h}^A_{i-1,m}$, can be obtained by passing the action sequence through the SQL encoder.\n",
      "Question : for the text Besides recent questions, as mentioned in Section SECREF1, the precedent SQL can also be context. As shown in Figure FIGREF27, the usage of $\\mathbf {y}_{i-1}$ requires a SQL encoder, where we employ another BiLSTM to achieve it. The $m$-th contextual action representation at turn $i\\!-\\!1$, $\\mathbf {h}^A_{i-1,m}$, can be obtained by passing the action sequence through the SQL encoder. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "42\n",
      "How does the proposed action-level copy mechanism work for grammar-based decoding?\n",
      "Answer 1: The proposed action-level copy mechanism enables the decoder to copy actions that appear in $\\mathbf{y}_{i-1}$ if they are compatible with the current expanded nonterminal. The copied actions lie in the same semantic space as the generated ones, and the output probability for action $\\gamma$ is a mix of generating and copying probabilities. The generating probability follows Equation DISPLAY_FORM10 and DISPLAY_FORM11, while the copying probability is obtained through a learned matrix $\\mathbf{W}^c$ and a sigmoid function. The final probability is computed by taking into account both the generating and copying probabilities.\n",
      "Question : for the text To reuse the precedent generated SQL, BIBREF5 presented a token-level copy mechanism on their non-grammar based parser. Inspired by them, we propose an action-level copy mechanism suited for grammar-based decoding. It enables the decoder to copy actions appearing in $\\mathbf {y}_{i-1}$, when the actions are compatible to the current expanded nonterminal. As the copied actions lie in the same semantic space with the generated ones, the output probability for action $\\gamma $ is a mix of generating ($\\mathbf {g}$) and copying ($\\mathbf {c}$). The generating probability $P(y_{i,j}\\!=\\!{\\gamma }\\,|\\,\\mathbf {g})$ follows Equation DISPLAY_FORM10 and DISPLAY_FORM11, while the copying probability is:.where $\\mathbf {W}^l$ is a learned matrix. Denoting $P^{copy}_{i,j}$ the probability of copying at decoding step $j$ of turn $i$, it can be obtained by $\\sigma (\\mathbf {W}^{c}\\mathbf {h}^{\\overrightarrow{D}}_{i,j}+\\mathbf {b}^{c})$, where $\\lbrace \\mathbf {W}^{c},\\mathbf {b}^{c}\\rbrace $ are learned parameters and $\\sigma $ is the sigmoid function. The final probability $P(y_{i,j}={\\gamma })$ is computed by: generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "43\n",
      "Question 1: How does the model incorporate the SQL context using attention over $\\mathbf {y}_{i-1}$?\n",
      "\n",
      "Answer 1: The model incorporates the SQL context using attention over $\\mathbf {y}_{i-1}$ by computing attention score based on $\\mathbf {h}^A_{i-1,m}$ and obtaining the SQL context vector. This vector is then used as an additional input for the decoder in Equation DISPLAY_FORM7.\n",
      "Question : for the text Attention over $\\mathbf {y}_{i-1}$ is a straightforward method to incorporate the SQL context. Given $\\mathbf {h}^A_{i-1,m}$, we employ a similar manner as Equation DISPLAY_FORM8 to compute attention score and thus obtain the SQL context vector. This vector is employed as an additional input for decoder in Equation DISPLAY_FORM7. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "44\n",
      "What is the tree-level copy mechanism in the decoder?\n",
      "\n",
      "The tree-level copy mechanism is a feature that allows the decoder to copy action subtrees extracted from the previous decoding step. This reduces the number of decoding steps required and has similarities to a non-grammar based decoder's approach. A subtree is an action sequence starting from specific nonterminals, such as ${\\rm Select}$, and its representation is obtained from the SQL encoder's final hidden state. The output probabilities of subtrees are normalized together with other probabilities.\n",
      "Question : for the text Besides the action-level copy, we also introduce a tree-level copy mechanism. As illustrated in Figure FIGREF27, tree-level copy mechanism enables the decoder to copy action subtrees extracted from $\\mathbf {y}_{i-1}$, which shrinks the number of decoding steps by a large margin. Similar idea has been proposed in a non-grammar based decoder BIBREF4. In fact, a subtree is an action sequence starting from specific nonterminals, such as ${\\rm Select}$. To give an example, $\\langle $ $\\rm \\scriptstyle {Select}\\rightarrow \\rm {Agg}$, $\\rm \\scriptstyle {Agg}\\rightarrow \\rm {max\\ Col\\ Tab}$, $\\rm \\scriptstyle {Col}\\rightarrow \\rm {Id}$, $\\rm \\scriptstyle {Tab}\\rightarrow \\rm {CARS\\_DATA}$ $\\rangle $ makes up a subtree for the tree in Figure FIGREF6. For a subtree $\\upsilon $, its representation $\\phi ^{t}(\\upsilon )$ is the final hidden state of SQL encoder, which encodes its corresponding action sequence. Then we can obtain the output probability of subtree $\\upsilon $ as:.where $\\mathbf {W}^t$ is a learned matrix. The output probabilities of subtrees are normalized together with Equation DISPLAY_FORM10 and DISPLAY_FORM11. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "45\n",
      "Question 1: What is the purpose of providing the base model with recent questions as additional input?\n",
      "\n",
      "Answer 1: The purpose of providing the base model with recent questions is to take advantage of the question context and improve its performance. It is done by incorporating the recent questions as context, which can be done in three different ways as shown in Figure FIGREF13.\n",
      "Question : for the text To take advantage of the question context, we provide the base model with recent $h$ questions as additional input. As shown in Figure FIGREF13, we summarize and generalize three ways to incorporate recent questions as context. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "46\n",
      "Question 1: How does the method concatenate recent questions with the current question?\n",
      "\n",
      "Answer 1: The method concatenates recent questions with the current question in order, making the input of the question encoder be $[\\mathbf {x}_{i-h},\\dots ,\\mathbf {x}_{i}]$ without inserting special delimiters between questions as there are punctuation marks.\n",
      "Question : for the text The method concatenates recent questions with the current question in order, making the input of the question encoder be $[\\mathbf {x}_{i-h},\\dots ,\\mathbf {x}_{i}]$, while the architecture of the base model remains the same. We do not insert special delimiters between questions, as there are punctuation marks. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "47\n",
      "Question 1: How does the proposed gate mechanism compute the importance of each question in the joint decoder attention?\n",
      "\n",
      "Answer 1: The proposed gate mechanism computes the importance of each question in the joint decoder attention by using a set of learned parameters, including $\\lbrace \\mathbf {V}^{g},\\mathbf {W}^g,\\mathbf {U}^g\\rbrace $, and by employing the question-level importance $\\bar{g}_{i-t}$ as the coefficient of the attention scores at turn $i\\!-\\!t$.\n",
      "Question : for the text To jointly model the decoder attention in token-level and question-level, inspired by the advances of open-domain dialogue area BIBREF16, we propose a gate mechanism to automatically compute the importance of each question. The importance is computed by:.where $\\lbrace \\mathbf {V}^{g},\\mathbf {W}^g,\\mathbf {U}^g\\rbrace $ are learned parameters and $0\\,{\\le }\\,t\\,{\\le }\\,h$. As done in Equation DISPLAY_FORM17 except for the relative distance embedding, the decoder of Gate also attends over all the encoder hidden states. And the question-level importance $\\bar{g}_{i-t}$ is employed as the coefficient of the attention scores at turn $i\\!-\\!t$. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "48\n",
      "Question 1: How did BIBREF4 implement hierarchical encoding in their dialogue system?\n",
      "\n",
      "Answer 1: BIBREF4 employed a turn-level encoder that used an unidirectional LSTM to encode recent questions hierarchically. At turn i, the turn-level encoder took the previous question vector as input and updated its hidden state, which was then fed into an implicit context in $\\mathbf {LSTM}^E$. They also used a relative distance embedding in attention computing to make the decoder distinguish hidden states from different turns.\n",
      "Question : for the text A dialogue can be seen as a sequence of questions which, in turn, are sequences of words. Considering such hierarchy, BIBREF4 employed a turn-level encoder (i.e. an unidirectional LSTM) to encode recent questions hierarchically. At turn $i$, the turn-level encoder takes the previous question vector $[\\mathbf {h}^{\\overleftarrow{E}}_{i-1,1},\\mathbf {h}^{\\overrightarrow{E}}_{i-1,|\\mathbf {x}_{i-1}|}]$ as input, and updates its hidden state to $\\mathbf {h}^{\\overrightarrow{T}}_{i}$. Then $\\mathbf {h}^{\\overrightarrow{T}}_{i}$ is fed into $\\mathbf {LSTM}^E$ as an implicit context. Accordingly Equation DISPLAY_FORM4 is rewritten as:.Similar to Concat, BIBREF4 allowed the decoder to attend over all encoder hidden states. To make the decoder distinguish hidden states from different turns, they further proposed a relative distance embedding ${\\phi }^{d}$ in attention computing. Taking the above into account, Equation DISPLAY_FORM8 is as:..where $t{\\in }[0,\\dots ,h]$ represents the relative distance. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "49\n",
      "Question 1: How is the approach of this work different from the related works in the area of semantic parsing?\n",
      "\n",
      "Answer 1: The approach of this work is different from the related works as it performs an exploratory study not fulfilled by previous works, and focuses on context modeling for semantic parsing by summarizing contextual phenomena by fine-grained types and performing an analysis on context modeling methods.\n",
      "Question : for the text The most related work is the line of semantic parsing in context. In the topic of SQL, BIBREF24 proposed a context-independent CCG parser and then applied it to do context-dependent substitution, BIBREF3 applied a search-based method for sequential questions, and BIBREF4 provided the first sequence-to-sequence solution in the area. More recently, BIBREF5 presented a edit-based method to reuse the precedent generated SQL. With respect to other logic forms, BIBREF25 focuses on understanding execution commands in context, BIBREF26 on question answering over knowledge base in a conversation, and BIBREF27 on code generation in environment context. Our work is different from theirs as we perform an exploratory study, not fulfilled by previous works..There are also several related works that provided studies on context. BIBREF17 explored the contextual representations in context-independent semantic parsing, and BIBREF28 studied how conversational agents use conversation history to generate response. Different from them, our task focuses on context modeling for semantic parsing. Under the same task, BIBREF1 summarized contextual phenomena in a coarse-grained level, while BIBREF0 performed a wizard-of-oz experiment to study the most frequent phenomena. What makes our work different from them is that we not only summarize contextual phenomena by fine-grained types, but also perform an analysis on context modeling methods. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "50\n",
      "Question 1: Who supported the work of Liang, Xu, and Chen?\n",
      "Answer 1: Liang, Xu, and Chen were supported by the National Natural Science Foundation of China, the Beijing Municipal Natural Science Foundation, and the International Science and Technology Cooperation Program of the Ministry of Science and Technology.\n",
      "Question : for the text We sincerely thank the anonymous reviewers for their thorough reviewing and insightful suggestions. Liang, Xu, and Chen are supported by the National Natural Science Foundation of China (Contract 61370130, 61976015, 61473294 and 61876198), and the Beijing Municipal Natural Science Foundation (Contract 4172047), and the International Science and Technology Cooperation Program of the Ministry of Science and Technology (K11F100010). generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "1\n",
      "Question 1: What does the color depth denote in the example sentence?\n",
      "\n",
      "Answer 1: The color depth in the example sentence denotes the semantic relatedness level between the given aspect and each word, with more depth indicating a stronger relation to the given aspect.\n",
      "Question : for the text To give an intuitive understanding of how the proposed A-GRU works from scratch with different aspects, we take a review sentence as an example. As the example “the appetizers are ok, but the service is slow.” shown in Table TABREF2, it has different sentiment labels towards different aspects. The color depth denotes the semantic relatedness level between the given aspect and each word. More depth means stronger relation to the given aspect..Figure FIGREF43 shows that the A-GRU can effectively guide encoding the aspect-related features with the given aspect and identify corresponding sentiment. In another case, “overpriced Japanese food with mediocre service.”, there are two extremely strong sentiment words. As the above of Figure FIGREF44 shows, our A-GRU generates almost the same weight to the word “overpriced” and “mediocre”. The bottom of Figure FIGREF44 shows that reconstructing the given aspect can effectively enhance aspect-specific sentiment features and produce correct sentiment predictions. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "2\n",
      "What are the three types of errors that can occur in AGDT sentiment analysis?\n",
      "\n",
      "The three types of errors that can occur in AGDT sentiment analysis are: 1) unclear decision boundaries among sentiment polarity, 2) misclassifying \"conflict/neutral\" instances as \"positive\" or \"negative\" due to imbalanced label distribution in training corpus, and 3) difficulty predicting the polarity of complex instances.\n",
      "Question : for the text We further investigate the errors from AGDT, which can be roughly divided into 3 types. 1) The decision boundary among the sentiment polarity is unclear, even the annotators can not sure what sentiment orientation over the given aspect in the sentence. 2) The “conflict/neutral” instances are extremely easily misclassified as “positive” or “negative”, due to the imbalanced label distribution in training corpus. 3) The polarity of complex instances is hard to predict, such as the sentence that express subtle emotions, which are hardly effectively captured, or containing negation words (e.g., never, less and not), which easily affect the sentiment polarity. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "3\n",
      "Question 1: What is the main contribution of the proposed AGDT model in ABSA?\n",
      "\n",
      "Answer 1: The main contribution of the proposed AGDT model is that it guides the sentence encoding from scratch for aspect-specific feature selection and extraction, and it also includes an aspect-reconstruction approach to reconstruct the given aspect with the generated sentence representation. Furthermore, empirical studies show that AGDT outperforms existing state-of-the-art models substantially on both aspect-category sentiment analysis task and aspect-term sentiment analysis task of ABSA without additional features.\n",
      "Question : for the text In this paper, we propose a novel aspect-guided encoder (AGDT) for ABSA tasks, based on a deep transition architecture. Our AGDT can guide the sentence encoding from scratch for the aspect-specific feature selection and extraction. Furthermore, we design an aspect-reconstruction approach to enforce AGDT to reconstruct the given aspect with the generated sentence representation. Empirical studies on four datasets suggest that the AGDT outperforms existing state-of-the-art models substantially on both aspect-category sentiment analysis task and aspect-term sentiment analysis task of ABSA without additional features. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "4\n",
      "Question 1: What is the TD-LSTM model's approach to generating target-dependent representations for sentiment prediction?\n",
      "\n",
      "Answer 1: The TD-LSTM model uses two LSTMs to capture the left and right context of the term and generate target-dependent representations for sentiment prediction.\n",
      "Question : for the text To comprehensively evaluate our AGDT, we compare the AGDT with several competitive models..ATAE-LSTM. It is an attention-based LSTM model. It appends the given aspect embedding with each word embedding, and then the concatenated embedding is taken as the input of LSTM. The output of LSTM is appended aspect embedding again. Furthermore, attention is applied to extract features for final predictions..CNN. This model focuses on extracting n-gram features to generate sentence representation for the sentiment classification..TD-LSTM. This model uses two LSTMs to capture the left and right context of the term to generate target-dependent representations for the sentiment prediction..IAN. This model employs two LSTMs and interactive attention mechanism to learn representations of the sentence and the aspect, and concatenates them for the sentiment prediction..RAM. This model applies multiple attentions and memory networks to produce the sentence representation..GCAE. It uses CNNs to extract features and then employs two Gated Tanh-Relu units to selectively output the sentiment information flow towards the aspect for predicting sentiment labels. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "5\n",
      "Question 1: What are the predefined aspects in the SemEval 2014 \"restaurant-14\" dataset?\n",
      "\n",
      "Answer 1: The SemEval 2014 \"restaurant-14\" dataset contains five predefined aspects, according to the text.\n",
      "Question : for the text For comparison, we follow BIBREF1 and use the restaurant reviews dataset of SemEval 2014 (“restaurant-14”) Task 4 BIBREF0 to evaluate our AGDT model. The dataset contains five predefined aspects and four sentiment labels. A large dataset (“restaurant-large”) involves restaurant reviews of three years, i.e., 2014 $\\sim $ 2016 BIBREF0. There are eight predefined aspects and three labels in that dataset. When creating the “restaurant-large” dataset, we follow the same procedure as in BIBREF1. Statistics of datasets are shown in Table TABREF19. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "6\n",
      "Question 1: What datasets were used to evaluate the model?\n",
      "\n",
      "Answer 1: The restaurant and laptop review datasets of SemEval 2014 Task 4 were used to evaluate the model.\n",
      "Question : for the text We use the restaurant and laptop review datasets of SemEval 2014 Task 4 BIBREF0 to evaluate our model. Both datasets contain four sentiment labels. Meanwhile, we also conduct a three-class experiment, in order to compare with some work BIBREF13, BIBREF3, BIBREF7 which removed “conflict” labels. Statistics of both datasets are shown in Table TABREF20. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "7\n",
      "Question 1: What is the purpose of extracting a hard dataset from each \"DS\"?\n",
      "\n",
      "Answer 1: The purpose of extracting a hard dataset from each \"DS\" is to measure whether a model can detect different sentiment polarities in one sentence towards different aspects by ensuring that each sentence only has different sentiment labels associated with different aspects.\n",
      "Question : for the text We conduct experiments on two datasets of the aspect-category based task and two datasets of the aspect-term based task. For these four datasets, we name the full dataset as “DS\". In each “DS\", there are some sentences like the example in Table TABREF2, containing different sentiment labels, each of which associates with an aspect (term). For instance, Table TABREF2 shows the customer's different attitude towards two aspects: “food” (“The appetizers\") and “service”. In order to measure whether a model can detect different sentiment polarities in one sentence towards different aspects, we extract a hard dataset from each “DS”, named “HDS”, in which each sentence only has different sentiment labels associated with different aspects. When processing the original sentence $s$ that has multiple aspects ${a}_{1},{a}_{2},...,{a}_{n}$ and corresponding sentiment labels ${l}_{1},{l}_{2},...,{l}_{n}$ ($n$ is the number of aspects or terms in a sentence), the sentence will be expanded into (s, ${a}_{1}$, ${l}_{1}$), (s, ${a}_{2}$, ${l}_{2}$), ..., (s, ${a}_{n}$, ${l}_{n}$) in each dataset BIBREF21, BIBREF22, BIBREF1, i.e, there will be $n$ duplicated sentences associated with different aspects and labels. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "8\n",
      "Question 1: What are the evaluation metrics in the experiment?\n",
      "\n",
      "Answer 1: The evaluation metrics in the experiment are accuracy.\n",
      "Question : for the text The evaluation metrics are accuracy. All instances are shown in Table TABREF19 and Table TABREF20. Each experiment is repeated five times. The mean and the standard deviation are reported. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "9\n",
      "Question 1: What is the method used to initialize word embeddings and handle out-of-vocabulary words in the models?\n",
      "\n",
      "Answer 1: The models use pre-trained 300d Glove embeddings to initialize word embeddings, which are fixed. For out-of-vocabulary words, their embeddings are randomly sampled by the uniform distribution U(-0.25, 0.25). The averaged word embedding is used as the aspect representation for multi-word aspect terms, following previous studies.\n",
      "Question : for the text We use the pre-trained 300d Glove embeddings BIBREF23 to initialize word embeddings, which is fixed in all models. For out-of-vocabulary words, we randomly sample their embeddings by the uniform distribution $U(-0.25, 0.25)$. Following BIBREF8, BIBREF24, BIBREF25, we take the averaged word embedding as the aspect representation for multi-word aspect terms. The transition depth of deep transition model is 4 (see Section SECREF30). The hidden size is set to 300. We set the dropout rate BIBREF26 to 0.5 for input token embeddings and 0.3 for hidden states. All models are optimized using Adam optimizer BIBREF27 with gradient clipping equals to 5 BIBREF28. The initial learning rate is set to 0.01 and the batch size is set to 4096 at the token level. The weight of the reconstruction loss $\\lambda $ in Eq. DISPLAY_FORM17 is fine-tuned (see Section SECREF30) and respectively set to 0.4, 0.4, 0.2 and 0.5 for four datasets. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "10\n",
      "What is the impact of utilizing \"AG\" in AGDT?\n",
      "\n",
      "Utilizing \"AG\" in AGDT has a significant impact for highly competitive results, particularly in the \"HDS\" part, demonstrating a stronger ability to identify different sentiment polarities towards different aspects.\n",
      "Question : for the text We conduct ablation experiments to investigate the impacts of each part in AGDT, where the GRU is stacked with 4 layers. Here “AC” represents aspect concatenated embedding , “AG” stands for A-GRU (Eq. (DISPLAY_FORM8) $\\sim $ ()) and “AR” denotes the aspect-reconstruction (Eq. (DISPLAY_FORM14) $\\sim $ (DISPLAY_FORM17))..From Table TABREF31 and Table TABREF32, we can conclude:.Deep Transition (DT) achieves superior performances than GRU, which is consistent with previous work BIBREF18, BIBREF19 (2 vs. 1)..Utilizing “AG” to guide encoding aspect-related features from scratch has a significant impact for highly competitive results and particularly in the “HDS” part, which demonstrates that it has the stronger ability to identify different sentiment polarities towards different aspects. (3 vs. 2)..Aspect concatenated embedding can promote the accuracy to a degree (4 vs. 3)..The aspect-reconstruction approach (“AR”) substantially improves the performance, especially in the “HDS\" part (5 vs. 4)..the results in 6 show that all modules have an overall positive impact on the sentiment classification. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "11\n",
      "Question 1: How does the AGDT model compare to the GCAE model in terms of accuracy in detecting different sentiment polarities in a sentence?\n",
      "\n",
      "Answer 1: The AGDT model outperforms the GCAE model by a large margin (+11.4% and +4.9%) on both datasets in detecting different sentiment polarities in a sentence, indicating that given aspect information is crucial for accurate sentiment prediction, especially when the sentence has different sentiment labels.\n",
      "Question : for the text We present the overall performance of our model and baseline models in Table TABREF27. Results show that our AGDT outperforms all baseline models on both “restaurant-14” and “restaurant-large” datasets. ATAE-LSTM employs an aspect-weakly associative encoder to generate the aspect-specific sentence representation by simply concatenating the aspect, which is insufficient to exploit the given aspect. Although GCAE incorporates the gating mechanism to control the sentiment information flow according to the given aspect, the information flow is generated by an aspect-independent encoder. Compared with GCAE, our AGDT improves the performance by 2.4% and 1.6% in the “DS” part of the two dataset, respectively. These results demonstrate that our AGDT can sufficiently exploit the given aspect to generate the aspect-guided sentence representation, and thus conduct accurate sentiment prediction. Our model benefits from the following aspects. First, our AGDT utilizes an aspect-guided encoder, which leverages the given aspect to guide the sentence encoding from scratch and generates the aspect-guided representation. Second, the AGDT guarantees that the aspect-specific information has been fully embedded in the sentence representation via reconstructing the given aspect. Third, the given aspect embedding is concatenated on the aspect-guided sentence representation for final predictions..The “HDS”, which is designed to measure whether a model can detect different sentiment polarities in a sentence, consists of replicated sentences with different sentiments towards multiple aspects. Our AGDT surpasses GCAE by a very large margin (+11.4% and +4.9% respectively) on both datasets. This indicates that the given aspect information is very pivotal to the accurate sentiment prediction, especially when the sentence has different sentiment labels, which is consistent with existing work BIBREF2, BIBREF3, BIBREF4. Those results demonstrate the effectiveness of our model and suggest that our AGDT has better ability to distinguish the different sentiments of multiple aspects compared to GCAE. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "12\n",
      "Question 1: Which model performed the best in both \"DS\" and \"HDS\" domains, according to Table TABREF28?\n",
      "\n",
      "Answer 1: According to Table TABREF28, the AGDT model consistently outperformed all baseline models in both \"DS\" and \"HDS\" domains.\n",
      "Question : for the text As shown in Table TABREF28, our AGDT consistently outperforms all compared methods on both domains. In this task, TD-LSTM and ATAE-LSTM use a aspect-weakly associative encoder. IAN, RAM and GCAE employ an aspect-independent encoder. In the “DS” part, our AGDT model surpasses all baseline models, which shows that the inclusion of A-GRU (aspect-guided encoder), aspect-reconstruction and aspect concatenated embedding has an overall positive impact on the classification process..In the “HDS” part, the AGDT model obtains +3.6% higher accuracy than GCAE on the restaurant domain and +4.2% higher accuracy on the laptop domain, which shows that our AGDT has stronger ability for the multi-sentiment problem against GCAE. These results further demonstrate that our model works well across tasks and datasets. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "13\n",
      "Question 1: Which model outperforms the AGDT on the laptop dataset and why?\n",
      "\n",
      "Answer 1: TNet outperforms the AGDT on the laptop dataset because it incorporates additional features such as position features, local ngrams, and word-level features, compared to AGDT's reliance on only word-level features.\n",
      "Question : for the text We also conduct a three-class experiment to compare our AGDT with previous models, i.e., IARM, TNet, VAE, PBAN, AOA and MGAN, in Table TABREF41. These previous models are based on an aspect-independent (weakly associative) encoder to generate sentence representations. Results on all domains suggest that our AGDT substantially outperforms most competitive models, except for the TNet on the laptop dataset. The reason may be TNet incorporates additional features (e.g., position features, local ngrams and word-level features) compared to ours (only word-level features). generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "14\n",
      "Question 1: What does Table TABREF40 show?\n",
      "Answer 1: Table TABREF40 shows the results on four test datasets for aspect-term reconstruction, demonstrating the effectiveness of the approach.\n",
      "Question : for the text Here, we investigate how well the AGDT can reconstruct the aspect information. For the aspect-term reconstruction, we count the construction is correct when all words of the term are reconstructed. Table TABREF40 shows all results on four test datasets, which shows the effectiveness of aspect-reconstruction approach again. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "15\n",
      "Question 1: How is the impact of lambda investigated for aspect-oriented objectives?\n",
      "\n",
      "Answer 1: The impact of lambda for aspect-oriented objectives is investigated by increasing lambda from 0.1 to 1.0 and analyzing the results on four \"HDS\" datasets. The results show that reconstructing the given aspect can enhance aspect-specific sentiment features and obtain better performances.\n",
      "Question : for the text We randomly sample a temporary development set from the “HDS\" part of the training set to choose the lambda for each dataset. And we investigate the impact of $\\lambda $ for aspect-oriented objectives. Specifically, $\\lambda $ is increased from 0.1 to 1.0. Figure FIGREF33 illustrates all results on four “HDS\" datasets, which show that reconstructing the given aspect can enhance aspect-specific sentiment features and thus obtain better performances. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "16\n",
      "Question 1: What is the optimal depth of the AGDT for achieving the best results?\n",
      "Answer 1: The optimal depth of the AGDT for achieving the best results is 4 in most cases, and further depth does not provide considerable performance improvement.\n",
      "Question : for the text We have demonstrated the effectiveness of the AGDT. Here, we investigate the impact of model depth of AGDT, varying the depth from 1 to 6. Table TABREF39 shows the change of accuracy on the test sets as depth increases. We find that the best results can be obtained when the depth is equal to 4 at most case, and further depth do not provide considerable performance improvement. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "17\n",
      "What is ABSA and what are the two subtasks involved in it?\n",
      "\n",
      "ABSA is Aspect based sentiment analysis, a fine-grained task in sentiment analysis which involves predicting sentiment polarity towards certain aspects. The two subtasks involved in ABSA are Aspect-category sentiment analysis which aims to predict the sentiment polarity towards a given aspect in a predefined category, and Aspect-term sentiment analysis which predicts the sentiment polarity over the aspect term which is a subsequence of the sentence.\n",
      "Question : for the text Aspect based sentiment analysis (ABSA) is a fine-grained task in sentiment analysis, which can provide important sentiment information for other natural language processing (NLP) tasks. There are two different subtasks in ABSA, namely, aspect-category sentiment analysis and aspect-term sentiment analysis BIBREF0, BIBREF1. Aspect-category sentiment analysis aims at predicting the sentiment polarity towards the given aspect, which is in predefined several categories and it may not appear in the sentence. For instance, in Table TABREF2, the aspect-category sentiment analysis is going to predict the sentiment polarity towards the aspect “food”, which is not appeared in the sentence. By contrast, the goal of aspect-term sentiment analysis is to predict the sentiment polarity over the aspect term which is a subsequence of the sentence. For instance, the aspect-term sentiment analysis will predict the sentiment polarity towards the aspect term “The appetizers”, which is a subsequence of the sentence. Additionally, the number of categories of the aspect term is more than one thousand in the training corpus..As shown in Table TABREF2, sentiment polarity may be different when different aspects are considered. Thus, the given aspect (term) is crucial to ABSA tasks BIBREF2, BIBREF3, BIBREF4, BIBREF5, BIBREF6. Besides, BIBREF7 show that not all words of a sentence are useful for the sentiment prediction towards a given aspect (term). For instance, when the given aspect is the “service”, the words “appetizers” and “ok” are irrelevant for the sentiment prediction. Therefore, an aspect-independent (weakly associative) encoder may encode such background words (e.g., “appetizers” and “ok”) into the final representation, which may lead to an incorrect prediction..Numerous existing models BIBREF8, BIBREF9, BIBREF10, BIBREF1 typically utilize an aspect-independent encoder to generate the sentence representation, and then apply the attention mechanism BIBREF11 or gating mechanism to conduct feature selection and extraction, while feature selection and extraction may base on noised representations. In addition, some models BIBREF12, BIBREF13, BIBREF14 simply concatenate the aspect embedding with each word embedding of the sentence, and then leverage conventional Long Short-Term Memories (LSTMs) BIBREF15 to generate the sentence representation. However, it is insufficient to exploit the given aspect and conduct potentially complex feature selection and extraction..To address this issue, we investigate a novel architecture to enhance the capability of feature selection and extraction with the guidance of the given aspect from scratch. Based on the deep transition Gated Recurrent Unit (GRU) BIBREF16, BIBREF17, BIBREF18, BIBREF19, an aspect-guided GRU encoder is thus proposed, which utilizes the given aspect to guide the sentence encoding procedure at the very beginning stage. In particular, we specially design an aspect-gate for the deep transition GRU to control the information flow of each token input, with the aim of guiding feature selection and extraction from scratch, i.e. sentence representation generation. Furthermore, we design an aspect-oriented objective to enforce our model to reconstruct the given aspect, with the sentence representation generated by the aspect-guided encoder. We name this Aspect-Guided Deep Transition model as AGDT. With all the above contributions, our AGDT can accurately generate an aspect-specific representation for a sentence, and thus conduct more accurate sentiment predictions towards the given aspect..We evaluate the AGDT on multiple datasets of two subtasks in ABSA. Experimental results demonstrate the effectiveness of our proposed approach. And the AGDT significantly surpasses existing models with the same setting and achieves state-of-the-art performance among the models without using additional features (e.g., BERT BIBREF20). Moreover, we also provide empirical and visualization analysis to reveal the advantages of our model. Our contributions can be summarized as follows:.We propose an aspect-guided encoder, which utilizes the given aspect to guide the encoding of a sentence from scratch, in order to conduct the aspect-specific feature selection and extraction at the very beginning stage..We propose an aspect-reconstruction approach to further guarantee that the aspect-specific information has been fully embedded into the sentence representation..Our AGDT substantially outperforms previous systems with the same setting, and achieves state-of-the-art results on benchmark datasets compared to those models without leveraging additional features (e.g., BERT). generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "18\n",
      "Question 1: What is the purpose of the aspect-guided encoder in the AGDT model?\n",
      "\n",
      "Answer 1: The aspect-guided encoder in the AGDT model is designed to guide the encoding of a sentence from scratch for conducting the aspect-specific feature selection and extraction at the very beginning stage. Its purpose is to extract aspect-specific information for more accurate predictions.\n",
      "Question : for the text As shown in Figure FIGREF6, the AGDT model mainly consists of three parts: aspect-guided encoder, aspect-reconstruction and aspect concatenated embedding. The aspect-guided encoder is specially designed to guide the encoding of a sentence from scratch for conducting the aspect-specific feature selection and extraction at the very beginning stage. The aspect-reconstruction aims to guarantee that the aspect-specific information has been fully embedded in the sentence representation for more accurate predictions. The aspect concatenated embedding part is used to concatenate the aspect embedding and the generated sentence representation so as to make the final prediction. generate 1 question and answer and present it in the format Question 1: \n",
      "Answer 1:\n",
      "19\n",
      "in except\n",
      "19\n",
      "in except\n",
      "0\n",
      "in except\n",
      "0\n",
      "in except\n",
      "0\n",
      "in except\n",
      "0\n",
      "in except\n",
      "0\n",
      "in except\n",
      "0\n",
      "in except\n",
      "0\n",
      "in except\n",
      "0\n",
      "in except\n",
      "0\n",
      "in except\n",
      "0\n",
      "in except\n",
      "0\n",
      "in except\n",
      "0\n",
      "in except\n",
      "0\n",
      "in except\n",
      "0\n",
      "in except\n",
      "0\n",
      "in except\n",
      "0\n",
      "in except\n",
      "0\n",
      "in except\n",
      "0\n",
      "in except\n",
      "0\n",
      "in except\n",
      "0\n",
      "in except\n",
      "0\n",
      "in except\n",
      "0\n",
      "in except\n",
      "0\n",
      "in except\n",
      "0\n",
      "in except\n",
      "0\n",
      "in except\n",
      "0\n",
      "in except\n",
      "0\n",
      "in except\n",
      "0\n",
      "in except\n",
      "0\n",
      "in except\n",
      "0\n",
      "in except\n",
      "0\n",
      "in except\n",
      "0\n",
      "in except\n",
      "0\n",
      "in except\n",
      "0\n",
      "in except\n",
      "0\n",
      "in except\n",
      "0\n",
      "in except\n",
      "0\n",
      "in except\n",
      "0\n",
      "in except\n",
      "0\n",
      "in except\n",
      "0\n",
      "in except\n",
      "0\n",
      "in except\n",
      "0\n",
      "in except\n",
      "0\n",
      "in except\n",
      "0\n",
      "in except\n",
      "0\n",
      "in except\n",
      "0\n",
      "in except\n",
      "0\n",
      "in except\n",
      "0\n",
      "in except\n",
      "0\n",
      "in except\n",
      "0\n",
      "in except\n",
      "0\n",
      "in except\n",
      "0\n",
      "in except\n",
      "0\n",
      "in except\n",
      "0\n",
      "in except\n",
      "0\n",
      "in except\n",
      "0\n",
      "in except\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# Looping through the above list, passsing section to the api along with the query and saving the dataframe to csv\n",
    "\n",
    "for i in range(len(a)-1):\n",
    "    count = 0\n",
    "    try:\n",
    "        for section_text in section_data[a[i]:a[i+1]]:\n",
    "            ss=GetMessageMemory('for the text {} generate 1 question and answer and present it in the format Question 1: \\nAnswer 1:'.format(section_text) ,[])\n",
    "\n",
    "            rr=ss.split('\\n')\n",
    "            ans = rr[-1].replace('Answer 1:','')\n",
    "            ques = rr[0].replace('Question 1:','')\n",
    "\n",
    "            qa_df.loc[-1] = [section_text, ques, ans]\n",
    "            qa_df.index = qa_df.index + 1  # shifting index\n",
    "            qa_df = qa_df.sort_index()\n",
    "            count += 1\n",
    "            print(count)\n",
    "            time.sleep(20)\n",
    "\n",
    "    except:\n",
    "        print(\"in except\")\n",
    "        print(count)\n",
    "        path = r'C:\\Users\\stamhane\\Desktop\\spring2023\\nlp\\project\\dataset_files\\''\n",
    "        file_name = path + 'qa_df_' + str(a[i]) + \"_\" + str(a[i+1]) + '.csv'\n",
    "        qa_df.to_csv(file_name, index=False)\n",
    "        \n",
    "    path = r'C:\\Users\\stamhane\\Desktop\\spring2023\\nlp\\project\\dataset_files\\''\n",
    "    file_name = path + 'qa_df_' + str(a[i]) + \"_\" + str(a[i+1]) + '.csv'\n",
    "    qa_df.to_csv(file_name, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73b4f91b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
